I discovered that the Lex Fridman Podcast covers some of the most interesting topics, and the people he interviews are incredible. Therefore, I've decided to use his channel as the initial subject for testing language data.
[Lex Clips](https://www.youtube.com/@LexClips/videos) -> Mostly Short
[Lex Fridman](https://www.youtube.com/@lexfridman/videos)

-----
--99--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--98--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--97--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--96--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--95--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--94--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--93--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--92--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--91--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--90--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--89--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--88--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--87--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--86--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--85--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--84--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--83--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--82--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--81--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--80--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--79--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--78--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--77--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--76--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--75--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--74--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--73--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--72--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--71--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--70--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--69--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--68--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--67--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--66--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--65--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--64--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--63--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--62--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--61--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--60--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--59--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--58--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--57--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--56--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--55--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--54--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--53--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--52--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--51--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--50--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--49--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--48--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--47--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--46--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--45--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--44--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--43--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--42--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--41--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--40--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--39--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--38--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--37--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--36--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--35--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--34--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--33--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--32--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--31--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--30--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--29--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--28--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--27--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--26--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--25--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--24--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--23--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--22--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--21--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--20--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--19--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--18--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--17--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--16--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--15--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--14--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--13--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--12--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--11--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--10--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--09--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--08--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--07--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--06--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--05-- https://www.youtube.com/@lexfridman/videos

-----
Date:
Link:
Transcription:

paste here

----------

-----

--04--

-----
Date: 2016.09.27
Link: [Foundations of Deep Learning (Hugo Larochelle, Twitter)](https://www.youtube.com/watch?v=zij_FTbJHsk)
Transcription:

The talks at the Deep Learning School on September 24/25, 2016 were amazing. I clipped out individual talks from the full live streams and provided links to each below in case that's useful for people who want to watch specific talks several times (like I do). Please check out the official website ([http://www.bayareadlschool.org](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbks4OTJuUFNUSkNVeFZEQWRhdkM0cmNNSkN0UXxBQ3Jtc0ttNEY4RVk3QlhKMmJfZk4tRzJ2M25CUlRiaDY2a3hxMHJkMXhtRV84T1U0eUlxN09jczdUZVdzd3k1ZHlkSjd2dGhscnJuazZNMXBYcGplNWJYNmRMOFNuNi1jWmt1VXpFbGhxZzM3eUk3VDZWUmhyRQ&q=http%3A%2F%2Fwww.bayareadlschool.org%2F&v=zij_FTbJHsk)) and full live streams below. Having read, watched, and presented deep learning material over the past few years, I have to say that this is one of the best collection of introductory deep learning talks I've yet encountered. Here are links to the individual talks and the full live streams for the two days:
Intro

that's good all right cool so yes I was asked to give this presentation on the

foundations of deep learning which is mostly going over basic feed-forward neural networks and motivating a little

bit deep learning and some of the more recent developments and and some of the topics that you'll see across the next

two days so I as Andrew mentioned I have

just an hour so I'm gonna go fairly quickly on a lot of these things which I think will mostly be fine if you're

familiar enough with some machine learning and a little bit about neural nets but if you'd like to go into some

of the more specific details you can go check out my online lectures on YouTube it's now taught by a much younger

version of myself and so just search for you go to a shell and I am NOT the guy

doing a bunch of skateboarding and the geek teaching about neural nets so go

check those out if you want more details but so well I'll cover today is I'll

start with just describing and laying out the notation on feverel neural

FOUNDATIONS OF DEEP LEARNING

networks that is models that take an input vector X that might be an image or some text and produces an output f of X

so I'll just describe for propagation and the different types of units and the type of functions we can represent with

those and then I'll talk about how we actually train neural nets describing things like loss functions back

propagation that allows us to get a gradient for training with stochastic gradient descent and mention a few

tricks of the trade so some of the things we do in practice to successfully Train neural nets and then I'll end by

talking about some developments that are specifically useful in the context of

deep learning that is neural networks with several hidden layers that came out you know at the very after the beginning

of deep learning say in 2006 that is things like drop out batch normalization and if I have some time unsupervised

pre-training so let's get started and just talk about assuming we have some

neural network how do they actually functions how do they make predictions so let me lay down the notation so a

multi-layer neural feed-forward neural network is a model that takes as input

some vector X which I'm representing here with a different note for each of the dimensions in my input vector so each

dimension is essentially a unit in that neural network and then it eventually produces at its output layer a an output

and we'll focus on classification mostly so you have multiple units here and each

unit would correspond to one of the potential classes in which we would want to classify our input so if we're

identifying digits in handwritten character images and so we're focusing

on digits you'd have ten digits or you would have sort of zero from zero to nine so you'd have ten output units and

to produce an output the neural net will go through a series of hidden layers and

those will be essentially the components that introduce non-linearity that allows us to capture and perform very

sophisticated types of classification functions so if we have L hidden layers

the way we compute all the layers in our neural net is as follows we first start

by computing what I'm going to call a pre activation I'm going to note that a and imma go I'm going to index the

layers by K so a K is just the pre activation at layer K and that is only

simply going to be a linear transformation of the previous layer so

I'm going to note HK as the activation and the layer and by default I'll assume

that layer zero is going to be the input and so using that notation the pre

activation at layer K is going to correspond to taking the activation at the previous layer K minus one

multiplying it by a matrix WK those are the parameters of the layer those

essentially corresponds to the connections between the units between adjacent layers and I'm going to add a

bias vector that's another parameter in my layer so that gives me the pre activation and then next I'm going to

get a hidden layer activation by applying an activation function this will introduce some non-linearity in the

model so I'm going to call that function G and we'll go over a few choices we

have four common choices for the activation function and so I do this from

layer 1 to layer L and when it comes to the output layer I'll also compute a pre

activation by performing a linear transformation but then I'll usually apply a different activation function

depending on the problem I'm trying to solve so having said that let's go to

some of the choices for the activation function so some of the activation functions you'll see one common one is

this sigmoid activation function it's this function here it's just 1 divided by 1 plus the exponential of minus the

pre activation the shape of this function you can focus on that is this here it takes the pre activation which

can vary from minus infinity to plus infinite and it squashes this between 0 & 1 so it's bounded by below and above

below by 0 and above by 1 okay so it's a it's a function that saturates if you

have very large or very large magnitude positive or negative pre activations

another common choice is the hyperbolic tangent or tange activation function on

this picture here so squash is everything but instead of being between 0 & 1 s between minus 1 and 1 and 1

that's become quite popular in neural nets is what's known as the rectified

linear activation function or in papers you will see the relative unit that

refers to the use of this activation function so this one is different from

the others in that it's not bounded above but it is bounded below and it's actually it will output zeros exactly if

the pre activation is negative so those are the choices of activation functions

for the hidden layers and for the output layer if we're performing classification as I said in the our output layer we

will have as many units as there are classes in which an input could belong and what we'd like is potentially and

what we often do is interpret each units activation as the probability according

to the neural network that the input belongs to the corresponding class that it's labeled Y is the corresponding

class C so C would be like the index of that you in the output layer so we need an

activation function that produces probabilities produces a multinomial distribution over all the different

classes and the activation function we use for that is known as the softmax activation function it is simply as

follows you take your pre activations and you exponentiate them so that's going to give us positive numbers and

then we divide each of the exponentiated pre activations by the sum of all the PD

exponentiated pre activations so because I'm normalizing this way it means that all my values in my output layer are

going to sum to 1 and they're positive because I took the exponential so I can interpret that as a multinomial

distribution over the choice of all the SI different classes ok so that's what I'll use as the activation function at

the output layer and and now beyond the math in terms of conceptually and also

in the way we're going to program neural networks often we will do is that all these different operations the linear

transformations the different types of activation functions will essentially implement all of them as an object and

object that take arguments and the arguments would essentially be what other things are being combined to

produce the next value so for instance we would have an object that might correspond to the computation of pre

activation which would take as argument what is the weight matrix and the bias vector for that layer and take some

layer to transform and that would this object we sort of compute its value by applying the linear activation the

linear transformation and then we might have objects that correspond the specific you know activation functions

or like a sigmoid object or a 10 shop jacked or raloo object and we just combine these objects together chain

them into what ends up being a graph which I refer to as a flow graph that represents the computation done when you

do a forward pass in your neural network up until you reach the output layer so I mentioned it now because that's you'll

see you know the different software's that we presented over a weekend will essentially sort of you know exploit

some of that representation of the computation and neural nets it also be handy for computing gradients which I'll

talk about in a few minutes and so that's how we

perform predictions in neural network so we get an input we eventually reach an

output layer that gives us a distribution over classes if we're performing classification if I want to actually classify I would just assign

the class corresponding to the unit that has the highest activation that would correspond to classifying into the class

that has the highest probability according to the neural net and but then

you might ask the question okay what kind of problems can we solve with neural networks or more technically what

kind of functions can we represent mapping from some input X into some arbitrary output and so if you look at

if you go look at my videos I try to give more intuition as to why we have this result here but essentially if we

have a single hidden layer a neural network it's been shown that with a linear output we can approximate any continuous function arbitrarily well as

CAPACITY OF NEURAL NETWORK

long as we have enough hidden units so that is there's a value for these biases and these weights such that any

continuous function I can actually represent it as well as I want I just need to add enough hidden units so this

result applies if you use activation functions nonlinear activation functions like sigmoid and tan H so as I said in

my videos if you want a bit more intuition as to why that would be you can go check that out but that's a

really nice result it means that by focusing on this family of machine learning models that our neural networks

I can pretty much potentially represent any kind of classification function however this result does not tell us how

do we actually find the weights and the bias values such that I can represent a given function it doesn't essentially

tell us how do we train a neural network and so that's what we'll discuss next

let's talk about that how do we actually from a data set train a neural network

to perform good classification on for that problem so what we'll typically do

is use a framework that's very generic in machine learning known as empirical risk minimization or structural risk

MACHINE LEARNING

minimization if you're using regularization so this framework essentially transformed

a problem of learning as a problem of optimizing so what we'll do is that will

first choose a loss function that I'm noting as L and the last function it

compares the output of my model so the output layer of my neural network with the actual target

so I'm indexing with it exponent here with T to essentially ask the index over

all my different examples in my training set and so my loss function will tell me

is this output good or bad given that the label is actually Y and well I'll do

I'll also define a regularizer so theta here is you can think of it as it's just

the concatenation of all my biases and all of my weights in my neural net so those are all the parameters of my

neural network and the regularizer will essentially penalize certain values of

these weights so as I'll talk more specifically later on for instance you might want to have your way to not be

too far from zero that's a frequent intuition that we implement with regularizer and so the optimization problem that

we'll try to solve when learning is to minimize the average loss of my neural

network over my training example so summing over all training examples I have capital T examples plus some weight

here that's known as the weight DK some hyper parameter lambda times my regular Iser so in other words I'm going to try

to have my loss on my training set as small as possible over all the training

example and also try to satisfy my regularizer as much as possible and so now we have this optimization

problem and we learning will just correspond to trying to solve this problem so performing this finding this

argument here for over my weights and my biases and if I want to do this I can

just invoke some optimization procedure from the optimization community and the

one algorithm that you'll see constantly in deep learning is stochastic gradient descent this is the optimization

algorithm that will often use for training neural networks so SGD

stochastic gradient descent functions as follows you first initialize all of your parameters that

is finding initial values for my weight matrices and all of my bio C's and then

for a certain number of epochs so an epoch will be a full pass over all my examples that's what I'll call an epoch

so for a certain number of full iterations over my training set

I'll draw each training example so I pair X input X target Y and then I'll

compute what is the gradient of my loss with respect to my parameters all of my

parameters all my weights and all my biases this is what this notation here so nabla for the gradient of the loss

function and here I'm indexing with respect to which parameter I want the gradient so I'm going to compute what is

the gradient of my last function with respect to my parameters and plus lambda

times the gradient of my regularizer as well and then I'm going to get a direction in which I should move my

parameters since the greyman tells me how to increase the loss I want to go in

the opposite direction and decrease it so my direction will be the opposite so that's why I have a minus here and so

this Delta is going to be the direction in which I'll move my parameters by taking a step and the step is just a

step size alpha which is often referred to as a learning rate times my direction

which I just add to my current values of my parameters my biases and my weights

and that's going to give me my new value for all of my parameters and I iterate like that over going over all pairs x

wise computing my gradient taking a steps out in the opposite direction and then doing that several times okay so

that's how stochastic gradient descent works and that's essentially the learning procedure it's represented by

this this procedure so in this algorithm there are few things we need to specify to be able to implement it and execute

it we need a loss function the choice for the loss function we need a procedure that's efficient for computing the

gradient of the loss with respect to my parameters we need to choose a regularizer if you want one and we need

a way of initializing my parameters so next what I'll do is go through each of these these four

different things we need to choose before actually being able to execute the classic gradient descent so first

LOSS FUNCTION

the last function so as I said we will interpret the output layer as assigning probabilities to each potential class in

which I can classify my input X well in this case something that would be

natural is to try to maximize the probability of the correct class the actual class in which my example XT

belongs to I'd like to increase the value of the probability assigned by computed by my neural network and so

because we set up the problem in which we have a loss that we minimize instead

of maximizing the probability what we'll actually do is minimize the negative and the actual log probability so the log

likelihood of assigning X to the correct class Y so this is represented here so

given my output layer and the true label Y my loss will be minus the log of the

probability of Y for minor according to my neural net and that would be well take my output layer and look at the

unit so index the unit corresponding to the correct class so that's why I'm indexing by Y here we take the log

because numerically it turns out to be more stable we get nicer looking gradients and sometimes in certain

software's you'll see instead of talking about the negative log likelihood or log probability you'll see it referred as

the cross entropy and that's because you can think of this as performing a sum

over all possible classes and then for each class checking well is this potential class the target class so I

have an indicator function that is one if Y is equal to C so if my iterator

Class C is actually equal to the real class I'm going to multiply that by the

log of the probability actually assigned to that class C and this this function

here so this expression here is like a cross entropy between the empirical distribution which assigns 0 probability

to all the other classes but a probability of 1 to the correct class and the actual distribution over

that my neural net is computing which is f of X okay that's just a technical

detail you can just think about this here I only mention it because in certain libraries it's actually mentioned as the cross-entropy a loss so

that's for the loss then we need also a procedure for computing what is the gradient of my loss with respect to all

of my parameters in my neural net so the biases and the weights you can go look

at my videos if on the actual derivation of all the details for all of these different expressions I don't have time

for that so all I'll do and presumably a lot of you I actually seen you know these derivations if you haven't just go

check out the videos in any case I'm going to go through what the algorithm is I'm going to highlight some of the

key points that will come up later in understanding out actually back propagation functions so the basic idea

is that we'll compute gradients by exploiting the chain rule and we'll go from the top layer all the way to the

bottom computing gradients for layers that are closer and closer to the input as we go

and exploiting the chain rule to exploit or reuse previous computations we've made at upper layers to compute the

gradients at the layers of below so we usually start by computing what is the

gradient at the output layer so what's the gradient of my loss with respect to

BACKPROPAGATION

my output layer it actually it's more convenient to compute the loss with respect to the pre activation it's

actually a very simple expression so that that's why I have the gradient of this vector a L plus 1 that's the pre

activation at the very last layer of the loss function which is minus the log f of XY and it turns out this gradient is

super simple it's minus II of Y so that's the one Hut vector for class Y so

what this means is a of Y is just a vector filled with a bunch of zeros and then the one at the correct class so if

Y was the fourth class then in this case it would be this vector we have a one at the fourth dimension so e of Y is just a

vector it's we call it the one Hut vector full of zeros and the single one at the position corresponding to the

correct class so this part of the grain is essentially saying is that I'm going to increase I

want to increase the probability of the correct class I want to increase the pre activation which will increase the

probability of the correct class and I'm going to subtract what is the current probabilities assigned by my neural net

to all of the classes so f of X that's my output layer and that's the current beliefs of the neural net as to in which

class what's the probably of signing the input to each class so what this is

doing is essentially trying to decrease the probability of everything and specifically decrease it as much as I

the neural net currently believes that the input belongs to it and so if you

think about the subtraction of these two things well for the class that's the correct class I'm going to have one

minus some number between zero and one because it's a probability so that's going to be positive so I'm going to

increase the probability of the correct class and for everything else it's going to be zero minus a positive number so

it's going to be negative I'm actually going to decrease the probability of everything else so in two Li and it

makes sense this gradient has the right behavior and I'm going to take that pre activation gradient I'm going to

propagate it from the top to the bottom and and essentially iterating from the

last layer which is the output layer l plus 1 all the way down to the first layer and as I'm going down I'm going to

compute the gradient with respect to my parameters and then compute what's the gradient for the pre activation that the

layer below and then iterate like that so at each iteration of that loop I take

what is the current gradient of the loss function with respect to the pre

activation at the current layer and I can compute the gradient of the loss function with respect to my weight

matrix so not doing the derivation here it it's actually simply this vector so

my in my notation I assume that all the vectors are column vectors so this pre activation gradient vector and I

multiply it by the transpose of the activations so the value of the layer right below the layer K minus one so

because I take the transpose that's a multiplication like this you can see if I do the outer product essentially between these two vectors

I'm going to get a matrix of the same size as my weight matrix so it all checks out that makes sense it turns out that the

gradient of the loss with respect to the bias is exactly the gradient of the loss with respect to the pre activation so

that's very simple so that gives me now my gradients for my parameters and now I need to compute okay what is going to be

the gradient of the pre activations at the layer below well first I'm going to get the gradient

of the last function with respect to the activation at the layer below well

that's just taking my pre activation gradient vector and multiplying it by for some reason does it show here but

and multiplied by the transpose of my weight matrix super simple operation just a linear transformation of my

gradients at layer cake linear and transform to get my gradients of the activation at the layer K minus one and

then to get the gradients of the pre activation so before the activation function

I mean to I'm gonna take this gradient here which is the gradient of the activation function at the layer K minus

one and then I applied the gradient corresponding to the partial derivative of my nonlinear activation function so

this here this refers to an element-wise product so I'm taking these two vectors this vector here in this vector here I'm

going to do an element-wise product between the two and this vector here is just a partial derivative of the

activation function for each unit individually that I've put together into a vector okay this is what this

corresponds to now the key things to notice is first that this path computing

all the gradients and doing all these iterations is actually fairly cheap its complexity is essentially the same as

the one is doing a forward pass so all I'm doing are linear transformations

multiplying by matrices in this case the transpose of my weight matrix and then I'm also doing this sort of nonlinear

operation where I'm multiplying by the gradient of the activation function that's the first thing to notice and the

second thing to notice is that here I'm doing this element-wise product so if any of these terms here for a unit is

very close to zero then the pre activation gradient is going to be zero for the next layer and I highlight this

point because essentially whenever that's something to think about a lot when you're training neural nets

whenever this gradient here these partial derivatives come close to zero that it means the grain will not

propagate well to the next layer which means that you're not going to get a good gradient to update your parameters

now when does that happen when will you see these terms here being close to zero

ACTIVATION FUNCTION

well that's going to be when the partial derivatives of these nonlinear activation functions are close to zero

or zero so we can look at the partial derivative say of the sigmoid function it turns out it's super easy to compute

it's just the Sigma itself times 1 minus the sigmoid itself so that means that

whenever the activation of the unit for sigmoid unit is close to 1 or close to 0 I essentially get a partial there that's

close to zero you can kind of see it here the slope here is essentially flat and the slope here is flat that's the

value of the partial derivative so in other words if my pre activations are

very negative or very positive so if my unit is very saturated then gradients will have a hard time propagating to the

next layer that's the key inside here same thing for the tension so the turns

out the partial derivative is also easy to compute you just take the tangible you square it and going to subtract it

to 1 and yeah indeed if it's close to minus 1 or close to 1 you can see that

the slope is flat so again if the unit is saturating gradients will propagate I

have a hard time propagating to the next layer and for the relu the rectified

linear activation function the gradient is even simpler it's you just check

whether the pre activation is greater than 0 if it is the partial derivative is 1 if it's not at 0 so actually either

we're going to multiply by 1 or 0 you essentially get a binary mask when you're performing the propagation

through their value and you can see it the the slope here is flat and otherwise you have a linear function so actually

here at the shrinking of the grade and toward 0 is even harder it's exactly multiplying by

zero if your have a unit that's saturating below and beyond all the math

in terms of actually using those in practice during the weekend you'll see three different libraries that

essentially allows you to compute these gradients for you you actually usually don't write down backdrop you just use

all of these modules that you've implemented and it turns out there's a way of automatic automatic ly differentiating your loss function and

getting gradients for free in terms of effort in terms of programming effort with respect to your parameters so

conceptually the way you do this and you see essentially three different libraries doing it in slightly different

ways what you do is you up meant your flow graph by adding at the very end the

FLOW GRAPH

computation of your loss function and then each of these boxes which are conceptually objects that are taking

arguments and computing a value you're going to augment them to also have a method that's a backdrop or B prop

method you'll often see actually this expression being used be prop and what this method should do is that it should

take as input what is the gradient of the loss with respect to myself and then it should propagate to its arguments so

the things that its parents in the flow graph the things that takes to compute its own value it's going to propagate them using the chain rule what is their

gradients with respect to the loss so what this means is that you would sort

of start the process by initializing well the gradient of the loss with respect to itself is 1 and then you pass

the B prop method here 1 and then it's going to propagate to its argument what

is by using the chain rule what is the gradient of the loss with respect to f of X and then you're going to call B

prop on this object here and it's going to compute well add the gradient of the loss with respect to myself f of X from

this I can compute what's the gradient of my argument which is the pre activation at layer 2 which is back to

the loss so I'm going to reuse the computation I just got and update it using my what is essentially the

Jacobian and then I'm going to take the pre activation here which now knows what is the gradient of the loss with respect

to itself activation it's going to propagate to the weights and the biases and the layer

below update them with informing them of what is the grain into the last with respect to themselves and you continue

like this essentially going through the flow graph but in the opposite direction so the library torch the basic library

torch essentially functions like this quite explicitly it you construct you chain these elements together and then

when you're performing back propagation you're going in the reverse order of these chained elements and then you have

libraries like torch other grand piano and tens of which you learn about which are doing things slightly more

sophisticated there and you'll learn about that later on okay so that's the

REGULARIZATION

discussion of how you actually compute gradients of the last with respect to the parameters so that's another

component we need in stochastic grain in this end we can choose a regular Weiser one that's often used is the l2

regularization so that's just the sum of the squared of the all the weights and the gradient of that is just twice times

the weight so it's a super simple gradient to compute we usually don't regularize the biases

there's no particularly important reason for that it's just it there much for

your by see so it seems less important and often this l2 regularization is

often referred to as weight DK so if you hear about weight decayed that often refers to l2 regularization and then

INITIALIZATION

finally and this is also a very important point you have to initialize

the parameters before you actually start doing back prop and there are a few tricky cases you need to make sure that

you don't fall into so the biases often we initialize them to 0 there are

certain exceptions but for the most part we initialize them to 0 but for the weights there are a few things we can't

do so we can't initialize the weights to 0 and especially if you have 10 H activations the reason and I won't

explain it here but it's not a bad exercise to try to figure out why is that essentially when you do your first

pass you're going to get gradients for all your parameters that are going to be 0 so I'm going to be stuck at this 0

initialization so we can do that we can't initialize all the weights to

exactly the same value if again you think about it a little bit what's going

to happen is essentially that all the weights coming into a unit within the layer are going to have exactly the same

gradients which means they're going to be updated exactly the same way which means they're going to stay constant the

same that comes them but they're going to stay the same the whole time so it's as if you have multiple copies of the

same unit so you essentially have to break that initial symmetry that you would create if you initialize

everything to the same value so we end up doing most of the time is initialized the weights to some randomly generated

value often we generate them there are few other recipes but one of them is to initialize them from some uniform

distribution between lower and upper bound this is a recipe here that is

often used that has some theoretical grounding that's was derived specifically for the 10h there's this

pepper paper here by exactly Goho and yoshua bengio you can check out for some intuition as to oh you know how you

should initialize the weights but essentially this should be initially random and this should be initially close to zero random to break symmetry

and close to zero so that initially the units are not already saturated because

if the units are saturated then there are no gradients that are going to pass through the units you essentially going to get gradients very close to zero at

the lower layers so that's the main intuitions they have weights that are small and close to zero small and random

okay so those are the pieces we need for running stochastic gradient descent so

that allows us to take a training set and run the certain number of epochs and app the neural nets learn from that

training set now there are other quantities in our neural network that we haven't specified out to choose them so

those are the hyper parameters so usually we can have a separate validation set most people here are

familiar with machine learning so that's a typical procedure and then we need to select things like ok how many layers do

I want how many units per layer do I want what's the step size the learning rate of my stochastic gradient descent

procedure that alpha number what is the weight decay that I'm going to use so a

standard thing in machine learning is to perform a grid search that is if I have to our

MODEL SELECTION

parameters I list out a bunch of values I want to try so for the number of hidden units maybe I want to try a hundred a thousand and two thousand say

and then for the learning rate maybe I want to try 0.01 and 0.001 so a grid

search would just try all combinations of these three values for their hidden units and these two values for the

learning rates so that means that the more I providers there are it's the

number of configurations you have to try out blows up and grows exponentially so

another procedure that is now more more common which is more practical is to

perform a form of random search in this case what you do is for each parameter you actually determine a distribution of

likely values you'd like to try so it could be so for the number of hidden units maybe I do a uniform distribution

over all integers from a hundred to a thousand say or maybe a log uniform distribution and for the learning rate

may be again the log uniform distribution but from 0.001 to 0.01 say

and then to get an experiment to get values for my hyper parameters to do an

experiment with and get a performance of my validation set I just independently sample from these distributions for each

hyper parameter to get a full configuration for my experiment and then because I have this way of getting one

experiment I do it independently for all of my jobs all of my experiment that I will do so in this case if I know I have

like enough compute power to do 50 experiments I just sample 15 dependent

samples from these distributions for parameters perform these 50 experiments

and I just take the best one what's nice about it is that there are no unlike grid search there are never any holes in

the grid that is you just specify how many experiments you do if one of your jobs died well you just have one less

but there's no hole in your experiment and also one reason why it's particularly useful this approach is

that if you have a specific value in grid search for one of the hyper parameters that just makes the

experiment not work at all so learning rates are a lot like this if you have a learning rate that's too high it's quite

possible that convergence of the optimization will not converge well if you're using a grid search it means that

for all the experiments that use that specific value of the learning rate they're all going to be garbage they're all not going to be useful and you don't

really get this sort of big waste of computation if you do random search because most likely all the values of

your hyperparameters are going to be unique because their sample say from a uniform distribution over some some

range so that actually works quite well and and and quite recommended and there

are more advanced methods like methods based on machine learning bayesian optimization and or sometimes known as

sequential model based optimization that I won't talk about but that works a bit

better than random search and and that's another alternative if you think you

have an issue finding good hyper parameters is to investigate some of these more advanced methods now you do

KNOWING WHEN TO STOP

this for most of your hyper parameters but for the number of epochs the number of times you go through all of your

examples in your training set what we usually do is not grid search or random

search but we use a thing known as early stopping the idea here is that if I've trained a neural net for 10 epochs while

training a neural net with all the other hyper parameters kept constant but one more epoch is easy I just do one more

epoch so I shouldn't try to I shouldn't start over and then do say eleven epochs from scratch and so what we would do is

we would just track what is the performance on the validation set as I do more and more epochs and what we will

typically see is the training error will go down but the validation set performance will go down and eventually

go up the intuition here is that the gap between the performance on the training

set and the performance on the validation set will tend to increase and since the training curve cannot go below

usually some bound then eventually the validation set performance has to go up

sometimes it won't sell go up oh is sort of stay stable so with early stopping what we do is that if we reach a point

where the validation set performance hasn't improved from some certain number of iterations which we refer to as the

look-ahead we just stop we go back to the neural net that had the best performance overall in the validation set and that's

my neural network so I have now a very cheap way of actually getting the number of iterations or the number of epochs

over my training set a few more tricks of the trade so it's always useful to

OTHER TRICKS OF THE TRADE

normalize your data it will often have the effect of speeding up training if you have real valued data for binary

data that usually keep it as it is so what I mean by that is just subtract for

each dimension what is the average in the training set of that dimension and then dividing by the standard deviation

of each dimension again in my input space so this can speed up training we

often use a decay on the learning rate there are a few methods for doing this

one that's very simple is to start with a large learning rate and then track the performance on the validation set and

once on the validation set it stops improving you decrease your learning rate by some ratio maybe you're divided

by two and then you continue training for some time hopefully the validation set performance starts improving and

then at some point it stops improving and then you stop or you divide again by two so that sort of gives you an

adaptive using the validation set an adaptive way of changing your learning rate and that can again work better than

having a very small learning rate than waiting for a long time so making very fast progress initially and then slower

progress towards TM also I've described

so far the approach for training neural nets that is based on a single example

at a time but in practice we actually use what's called mini batches that is we compute the last function on the

small subset of example say 64 128 and then we take the average of the loss of

all these examples in that mini batch and that's actually we compute the gradient of this average loss on that

mini batch the reason why we do this is that it turns out that you can very

efficiently implement the forward pass over all of these 64 128 examples in my

mini batch in one pass by instead of doing vector matrix multiplications when

we come the pre activations doing matrix matrix multiplications which are faster than

doing multiple matrix vector multiplications so in your code often there will be this other hyper parameter

which is mostly optimized for speed in terms of how quickly training will proceed of the number of examples in

your mini batch other things to improve optimization might be using a thing like

momentum that is instead of using as the descent direction the gradient of your

last function I'm actually going to track a descent direction which I'm going to compute as the current gradient

for my current example or mini-batch plus some fraction of the previous

update the previous direction of update and better now is a hyper parameter you

have to optimize so what this does is if all the update directions agree oh across multiple updates then it will

start picking up momentum and actually make bigger steps in those directions and there are multiple even more

advanced methods for adding adaptive types of learning rates I mentioned them

here very quickly because you might see them in papers there's a method known as a de grab where the learning rate is

actually scaled for each descent for each dimension so for each weight and each by seize it's going to be scaled by

what is the square root of the cumulative sum of the squared gradients

so what I track is I take my gradient vector at each step I do an element-wise square of all the dimensions on my

gradients my gradient vector and then I accumulate that in some variables that I'm noting as gamma here and then for my

descent direction I take the gradient and I do an element-wise division by the square root of this cumulative sum of

squared gradients there's also rmsprop which is essentially like a de grab but instead of doing a cumulative stuff a sum we're

going to do an exponential moving average so we take the previous value x sub factor plus one minus this factor

times the current squared gradient so that's rmsprop and then there's adam which is essentially a combination of

rmsprop with momentum which is more involved and i won't have time to describe it here but that's

method that's often you know actually implemented in these different softwares and that people seem to use with a lot

of success and finally in terms of

GRADIENT CHECKING

actually debugging your implementations so for instance if you're lucky you can

build your neural network without difficulty using the current tools that are available in torch or 10 to Flora

Theano but maybe sometimes you actually have to implement certain gradients for a new module and a new box in your flow graph

that isn't currently supported if you do this you should check that you've implemented your gradients correctly and

one way of doing that is to actually compare the gradients computed by your code with a finite difference of

estimate so what you do is for each parameter you add some very small epsilon value say 10 to the minus 6 and

you compute what is the output of your module and then you subtract the same thing but where you've subtracted the

small quantity and then the divide by 2 epsilon so if epsilon is converges to

zero then you actually get the partial derivative but if it's small it's going to be an approximate and usually this

finite difference estimate will be very close to a correct implementation of the real gradient so you should definitely

do that if you actually implemented some of the gradients in your code and in another useful thing to do is to

DEBUGGING ON SMALL DATASET

actually do a very small experiment on the small data set before you actually run your full experiment on your

complete data set so you say 50 examples so just taking a random subset of 50

examples from your your data set actually just make sure that your code can over fit to that data can

essentially classify it perfectly given you know enough capacity that you would

think it should get it so if it's not the case then there's a few things that

you might want to investigate maybe your initialization is such that the units are already saturated initially and so

there's no actual optimization happening because some of the gradients on some of the weights are exactly zero so you

might want to check your initialization maybe your gradients are just you know you're using a model you implemented

gradients for and maybe there are gradients are not properly implemented maybe you haven't normalized your input

which creates some instability making it harder for stochastic gradient descent to work successfully maybe your

learning rate is too large then you should consider trying smaller learning rates that's actually a pretty good way

of having a some idea of the magnitude of the learning rate you should be using and and then once you actually over fit

in your small trainings that you're ready to do a full experiment on on a larger data set that said this is not a

replacement for gradient checking so backdrop is and stochastic gradient descent it's a great algorithm that's

very bug resistant you will pretend potentially see some learning happening

even if some of your gradients are wrong or say exactly zero so you should that's great you know if you're an engineer and

you're implementing things spun would code is somewhat bug resistant but if you're actually doing science and try to

understand what's going on that's that can be a complication so do do both gradient checking and a small experiment

like that all right and so for the last few minutes I'll actually try to

motivate what you'll be learning quite a bit about in the next two days that is

the specific case for deep learning so I've already told you that if I have a

neural net win enough hidden units theoretically I can potentially represent pretty much any function any

classification function so why would I want multiple layers so there are a few motivations behind this the first one is

taken directly from our own brains so we know in the visual cortex that the light

that hits our retina eventually goes through several regions in the visual cortex eventually reaching narrow known

as v1 when you have units that are or neurons that are essentially tuned to small forms like edges and then it goes

on to v4 where it's likely more complex patterns that the units are tuned for and then you reach AIT where you

actually have neurons are specific to certain objects or certain units and so the idea here is that perhaps that's

also what we want another artificial say you know vision system we'd like it if

it's detecting faces to have a first layer that detects simple edges and then another layer that perhaps puts these

edges together detecting slightly more complex things nose or mouth or eyes and then eventually have a layer that combines

these slightly less abstract or more abstract units to get something even

more abstract like a complete phase there's also some theoretical justification for doing using multiple

layers so the early results were mostly based on studying boolean functions or a

function that takes as input can think of it as a vector of just zeros and ones and you could show that there are

certain functions that if you add the essentially a boolean neural network or

essentially a boolean circuit and you restricted the number of layers of that

circuit that there are certain functions that in this case to represent certain boolean functions exactly you would need

an exponential number of units in each of these layers whereas if you allowed yourself to have multiple layers then

you could represent these functions more compactly and so there's that's another motivation that perhaps with more layers

we can represent fairly complex functions in a more compact way and then

there's the reason that they just work so we've seen in the past few years great success in speech recognition

where it's essentially revolutionized the field where everyone's using deep learning for speech recognition and same

thing for visual object recognition where again deep learning is sort of the method of choice for identifying objects

and images so then why are we doing this only recently why didn't we do deep

learning way back when back prop was invented which is essentially in 1980s and even

before that so it turns out training deep neural networks is actually not that easy there are few hurdles that one

can be confronted with I've already mentioned one of the issue which is that some of the gradients might be fading as

you go from the top layer to the bottom layer because we keep multiplying by the derivative of the activation function so

that makes training hard it could be that the lower layers at very small gradients are barely moving and

exploring the space of correct you know features to learn for a given problem so

that sometimes that's the problem you find you have a hard time just fitting your data and you're essentially underfitting

or it could be that with you know deeper neural nets Oh bigger neural nets we

have more parameters so perhaps sometimes actually overfitting we're in a situation where all the functions that

we can represent with the same neural net represented by this gray area function actually includes yes the right

function but it's so large that for a finite training set the odds that I'm going to find the one that's close to

the true classifying function the real system that like to have is going to be very different so in this case I mean

I'm essentially overfitting and that might also be a situation we're in and

unfortunately there's never there are many situations where one problem is

observed over fitting or under fitting and so we essentially have you know in

the field develop tools for finding both situations and I'm going to rapidly touch a few of those which you will see

will come up later on in multiple talks so one of the first hypothesis which

might be that you're under fitting well you can essentially just fight this by waiting longer so training longer if you

have your grayness are too small and this is essentially why you're progressing very slowly when you're training well if you're using GPUs and

are able to do more iterations over the same training set within less time that

might just you know solve your problem of underfitting and I think we've seen some of that and this is partly why GPUs

have been so game-changing for deep learning or you can use just better optimization methods also and if you're

overfitting well we just need better regularization i've been involved early

on in my PhD on using unsupervised learning as a way to regularize neural

nets if I have time I'll talk a little bit about that then there's another method you might have learned heard

about known as dropout so I'll try to touch at least two methods that are

essentially trying to address some of these issues so the first one that I'll talk about this dropout it's actually

DROPOUT

very easy very simple so the idea of if our neural net is essentially

overfitting so it's too good at training on the training set well we're essentially going to training

when I make it harder to fit the training set and where we're going to do that and dropout is that we will

stochastically remove hidden units independently so for each hidden unit

before we do a forward pass we'll flip a coin and we'd probably have we will

multiply the activation by zero with probability 1/2 we'll multiply it by 1

so what this means is that if a unit is multiplied by 0 it's effectively not in the neural net anymore and we're doing

this independently for each hidden units so that means that in a layer a unit

cannot rely anymore on the presence on any other units to try to sort of

synchronize and adapt to perform a complex classification or learn a

complex feature and that was partly the motivation behind dropout is that this procedure might encourage types of

features that are not co-adapted and are less likely to overfit so we often use

0.5 as the probability of dropping out a unit it turns out it often surprisingly

is the best value but that's another hyper parameter you might want to tune and in terms of how it impacts an

implementation of back prop it's it's very simple so the forward pass before I do it I just sample my binary masks for

all my layers and and then when I'm performing back drop well my gradient on

the oh sorry so that's the Ford pass yeah I'm just multiplying by this binary mask here so super simple change and

then in terms of back prop well I'm also going to multiply by the mask when I get

my gradient on the pre activation and also you know don't forget that the activations are now different they

actually include the masks in in my notation it's a very simple change the forward and backward pass when you're

training and also another thing that I shouldn't emphasize is that the mask is being resampled for every example so

before you do a forward pass you resample the mask you don't keep it you know sample at once and then use it the

whole time and then that test time because we don't really like a model

that sort of randomly changes its output because it will if we stochastically change the masks what we do is we

replace the mask by the probability of dropping out a unit so actually of

keeping a unit so if we 0.5 that's just 0.5 we can actually show

that if you have a neural net with a single hidden layer doing this transformation at test time multiplying

by 0.5 is equivalent to doing a geometric average of all the possible neural networks with all the different

binary mask patterns so it's essentially one way of thinking about drop out in the single layer case is that it's kind

of an in sembly method we have a lot of models an exponential number of models which are all sharing the same weights

but have different masks that intuition though doesn't transfer for deep neural

nets in the sense that you cannot show this result it really only applies to a single neural networks in gold hidden

layer so in practice it's very effective but do expect some slowdown in training

so often we tend to see that training or network to completion will take twice as many epochs if you're using dropout with

0.5 and here you have the reference if you want to learn more about different variations of dropouts and so on and

I'll and I'll probably won't talk about the unsupervised retraining for lack of time but I'll talk about another thing

that you'll definitely probably hear about and that's implementing these different packages which is Bachelor

organization Bachelor ization is kind of interesting in the sense that it's been shown to better optimize that is certain

networks that would otherwise under fit would not under fit as much anymore fuse Bachelor ization

but also it's been shown that when you use batch normalization dropout is not as useful and drop out being a

regularization method that suggests that perhaps patch normalization is also regularizing in some way so these things

are not you know one or the other they're not mutually exclusive you can have a regularizer that also turns out

helps you better optimize so the intuition behind batch normalization is

BATCH NORMALIZATION

you know much like I've suggested that normalizing your inputs actually can help speeding up training well how about

we also normalize all the hidden layers when I'm doing my forward pass so now

the problem in doing this is that I can compute the mean and the standard deviations of my inputs once and for all

because they're constant but my hidden layers are constantly changing because I'm training these parameters

so the mean and the standard deviation of my units will change and so I it

would be very expensive if every time I did an update of my parameters I recomputed the means and the standard

deviations of all of my units so bachelors ation addresses some of these issues as follows so the way works is

first batch the normalization is going to be applied on actually the pre activation so not the activation of the

unit but before the non-linearity during training to address the issue that we

don't want to compute means over the full training set because that would be too slow I'm actually going to compute it on each mini batch so I have to do

mini batch training here I'm going to take my small mini batch of 64 128 examples and that's the set of examples

on which I'm going to compute my means and standard deviations and then when I do back prop I'm actually going to take

into account the normalization so now there's going to be a gradient going through the computation of the mean and

the standard deviation because they depend on the parameters of the neural network and then that test time we'll

just use the global mean and global standard deviation once I finished training I can actually do a full pass

over the whole training set and got all of my means and standard deviations so

that's the essentially the pseudocode for that taken out of the paper directly so if X is a pre activation for a unit

and have multiple pre activations for a single unit across my mini batch I would

compute what is the average for that unit pre activation across my examples in my mini batch compute my variance and

then subtract the mean and divide by the square root of the variance plus some epsilon for numerical stability in case

the variance is too close to zero and then another thing is that actually batch normalization doesn't just perform

this normalization and outputs the normalize pre activation it then actually performs a linear

transformation on it so it multiplies it by this parameter gamma which is going to be trained by gradient descent and

it's often called a gain parameter of batchelomez ation and it adds a bias

better and the reason is that if I'm subtracting by the mean then each of these you

have the biased parameter so if I subtracted then this essentially here

there's no bias anymore it was present here was present here and now it's been subtracted to have to add the bias but

after the bachelor ization essentially so these betters here are essentially the new bias parameters and those will

actually be trained so we do gradient descent also on those so bachelor ization adds a few parameters all right

and I as I said I'm just gonna skip over this and you know I'm not showing what the gradients are when your backdrop through the mean and so on it's

describing the paper for Necedah gradients but otherwise in the different packages you actually have access to

you'll get the gradients automatically it's usually been implemented skipping

UNSUPERVISED PRE-TRAINING

over that I'll just finish if you actually want to learn about unsupervised retraining and why it works

NEURAL NETWORK ONLINE COURSE

videos on that so you can check that out and I guess that's it thank you

thanks you go so we have a few minutes for questions which are intermingled with a break so feel free to I your go

for our break or ask questions to Google I believe there are microphones and I'll also stick around so if you want to ask

your questions offline that's also fine if you want ask questions you can go to the mic

go to the microphone hi I mentioned the

rail ooh adds varsity can you explain why yeah so um so the first thing is

that it's observed in practice and they add some sparse some sparsity in part because you have the non-linearity at

zero below so it means that units are going to be exactly potentially exactly sparse exactly essentially absent of the

hidden layer the real there are a few reasons to sort of explain why you get

sparsity it turns out that this process of doing a linear transformation followed by the value activation

function is very close to some of the steps you would do when you're optimizing for sparse codes in the

sparse coding model if you know about sparse coding so they're like essentially in optimization methods that

given some sparse coding model we'll find what is the sparse representation hidden representation for some input and

it's mostly a sequence of linear transformations followed by this sort of like relu like activation function and I

think this is partly the explanation otherwise I don't I don't know a like solid you know explanation for why that

is beyond you know it's observed in practice more questions if not let's

thank you again and we are we reconvene

in ten minutes

summary:
**Key Points**:

1. **Introduction to Deep Learning**:
    
    - Hugo Larochelle presented on the foundations of deep learning, focusing on feed-forward neural networks, deep learning motivation, and recent developments.
    - Covered topics include basic neural network structure, loss functions, backpropagation, stochastic gradient descent, and various practical tips for training neural networks effectively.
2. **Neural Network Functioning**:
    
    - Explained the computation of neural networks starting with input vectors, going through hidden layers introducing non-linearity, and producing outputs for classification tasks.
    - Discussed the importance of non-linear activation functions like sigmoid, hyperbolic tangent (tanh), and rectified linear activation (ReLU) for hidden layers.
    - For the output layer, the softmax activation function is used for classification tasks to produce probabilities for each class.
3. **Training Neural Networks**:
    
    - Discussed the use of empirical risk minimization framework which includes choosing a loss function, an optimizer like stochastic gradient descent, a regularizer, and parameter initialization methods.
    - Highlighted the importance of the loss function in training, specifically the negative log likelihood or cross-entropy for classification tasks.
    - Emphasized the backpropagation algorithm, which is used to efficiently compute gradients of the loss function with respect to network parameters, utilizing the chain rule.
    - Talked about the importance of proper weight initialization to avoid issues like vanishing or exploding gradients.
4. **Challenges and Solutions in Deep Learning**:
    
    - Addressed common challenges such as underfitting and overfitting, emphasizing the use of GPUs for faster training and methods like dropout and batch normalization for regularization.
    - Dropout randomly deactivates neurons during training to prevent co-adaptation and overfitting.
    - Batch normalization normalizes the output of each layer to stabilize learning and also acts as a regularizer.
5. **Hyperparameter Tuning and Model Selection**:
    
    - Discussed methods for selecting hyperparameters like the number of layers, units per layer, learning rate, and weight decay, including grid search, random search, and more advanced methods like Bayesian optimization.
    - Introduced the concept of early stopping to prevent overfitting and select the number of training epochs based on validation set performance.
6. **Debugging and Improving Models**:
    
    - Highlighted the importance of normalizing input data, using learning rate decay, mini-batch training, momentum, and other optimization methods like AdaGrad, RMSprop, and Adam.
    - Advised on using gradient checking to ensure correct implementation of gradients and starting with a small dataset to ensure the model can overfit to it, indicating a correctly functioning model.
7. **Closing Remarks**:
    
    - Hugo concluded by reiterating the potential and challenges of deep learning, referencing his online course for a deeper understanding of the topics discussed.
    - The session ended with a Q&A, addressing specific questions about the ReLU activation function and its ability to add sparsity to the model.

The presentation provided a comprehensive overview of deep learning foundations, practical training tips, and strategies for effectively training and tuning neural network models.


----------

-----

--03--

-----
Date: 2014.12.23
Link: [Jimmy Pedro: Judo | Take It Uneasy Podcast](https://www.youtube.com/watch?v=7bO8rKtvDoE)
Transcription:

Jimmy Pedro is an American judo competitor and coach, World champion, 3x World medalist, 2x Olympic medalist; we talk about his father (Big Jim Pedro Sr), his early career, the times he wanted to quit, overcoming a neck injury, coming back from retirement, the life of an athlete vs the life of a coach, a system for developing elite-level judoka, Japanese vs Russian judo, periodization, a weekly program for an elite-level judoka, toughest moment as a coach, watching Travis Stevens lose the semifinals at the Olympics, mental game, visualization, IJF, judo as a spectator sport, the future of judo in the United States and the rest of the world, and more.
Based on the comments, the interview with Jimmy Pedro, an acclaimed American judo competitor and coach, was highly appreciated and resonated deeply with the judo community and listeners. The interview appears to have covered a comprehensive range of topics, reflecting Jimmy Pedro's extensive experience in the judo world. Here are the main takeaways based on the audience's reactions:

1. **Inspiring and Sincere**: Viewers found Jimmy Pedro's narrative inspiring and appreciated his sincerity and straightforwardness. His experiences, especially those involving his father, his early career struggles, and his coaching journey, seemed to strike a chord with many.
    
2. **Insightful on Professional Judo**: The interview provided valuable insights into the professional life of a judoka, discussing the intricacies of training, competition, and coaching at an elite level. Comments indicated that listeners found the discussion on the development of elite-level judoka, periodization, and weekly training programs particularly enlightening.
    
3. **Emotional Connection**: Some comments reflected a personal connection with Jimmy Pedro, indicating that his guidance and career had a significant impact on their judo journey. The mention of watching Travis Stevens lose in the Olympics suggested that the interview didn't shy away from discussing the emotional aspects of the sport.
    
4. **Educational Content**: The interview seems to have offered an in-depth look into the mind of an Olympic-level athlete, with Jimmy Pedro sharing detailed insights into mental preparation, visualization techniques, and the overall mindset required for high-level competition.
    
5. **Addressed Contemporary Judo Topics**: The interview tackled current issues and developments in judo, such as the rule changes by the International Judo Federation (IJF) and the future of judo in the United States and globally. This aspect of the discussion appeared to resonate well with judo enthusiasts concerned about the sport's direction and representation in the Olympics.
    
6. **Judo as a Martial Art and Sport**: Comments indicated that the interview addressed the balance between judo as a martial art and as a competitive sport. Discussions about judo's self-defense applications and its evolution for television ratings and Olympic standards seemed to engage listeners who are passionate about the sport's integrity and future.
    
7. **Affirmation of Jimmy Pedro's Legacy**: Numerous comments recognized Jimmy Pedro as a legendary figure in American judo, applauding his contributions to the sport both as a competitor and a coach. His influence on American judo and his role in shaping the careers of other notable judokas like Kayla Harrison, Ronda Rousey, and Travis Stevens were highlighted and celebrated.
    

In summary, the interview with Jimmy Pedro was well-received, with listeners praising the depth, honesty, and comprehensiveness of the conversation. It seemed to offer a blend of personal storytelling, professional insights, and thoughtful discussions on the future of judo, making it a valuable and enjoyable experience for the audience.

The interview with Jimmy Pedro, an American judo competitor and coach, covers a broad range of topics related to his illustrious career in judo, his coaching experiences, and his insights into the sport. Key discussion points include:


1. **Early Life and Career**: Jimmy talks about his upbringing and the influence of his father, Big Jim Pedro Sr, on his judo career. He reflects on the initial phase of his career and moments when he contemplated quitting.
    
2. **Injury and Comeback**: The conversation delves into a significant neck injury Jimmy sustained and how he overcame this obstacle, including his remarkable comeback from retirement.
    
3. **Athlete vs. Coach**: Jimmy contrasts the life of an athlete with that of a coach, shedding light on the different challenges and rewards each role presents.
    
4. **Development System for Elite Judoka**: The discussion covers Jimmy's approach and system for nurturing and developing elite-level judoka, offering insights into his coaching philosophy.
    
5. **Judo Styles and Techniques**:
   
   

----------

-----

--02--

-----
Date: 2014.06.05
Link:  [Ryan Hall: Value of Competition | Take It Uneasy Podcast](https://www.youtube.com/watch?v=94MBVD_tZeU)
Transcription:

Ryan Hall is an American black belt and instructor in Brazilian jiu-jitsu, and a professional mixed martial artist currently competing in the featherweight division of the Ultimate Fighting Championship (UFC). He is known for a number of competitive achievements, ranging from Mundial and ADCC victories to dozens of Grapplers Quest championships. He is the winner of The Ultimate Fighter Season 22. He holds notable victories over former UFC Lightweight and Welterweight Champion BJ Penn, former UFC Lightweight Title challenger Gray Maynard and Artem Lobov.

  
you have been in both the supporter and

a critic of competition what do you

think is the value of competition for

martial artists I believe that the value

of competition is in that it teaches you

the true purpose of martial arts and in

ending and the true purpose of martial

arts is being able to defend yourself

and whatnot in all of these other things

in real life yes because you let's say

you win a DCCC gold medal but you know

you get slapped around by someone bigger

and stronger you at a bar that people

will talk about how sweet that gold

medal is but for the rest of your life

it'll feel pretty Hollow that wouldn't

you know that's not what we're looking

for but what I'm talking about I guess

is what I believe competition develops

if approached properly is proper focused

proper dedication because anytime you

have a very defined goal and strong

opposition it will force you to be

better period the better your opposition

is if you focus and you take your what

you're doing seriously the better you

become period people are better

wrestlers today than they once were

people are better basketball players

today than they once were military is

better now that it was in the past

because of all of the competition

that you know that is existed over the

course of time and you know if you go

out competition if you go out and just

kind of it if you're like oh

I'm going to go out and see how it

happens like that's a cowardly way to

approach competition and that gets you

nothing that doesn't teach you to really

do the right things and the same thing

not for pot not properly preparing even

if you win that was a cowardly way to

approach it because you intentionally

left yourself an out which was if I win

man I'm talented in blah-dee-blah and if

I lose it's well you know I mean I could

I didn't really train that hard well if

that's the case then you shouldn't have

been out there win lose or draw I don't

care if I've got a student that I think

is going to win gold at the World

Championships he or she does not train

properly ahead of time I will not allow

them to go and if they go I will send

them off the team you know and hey they

can do what they want they're a grown

adult I'm not the boss of them but I am

the boss of my team it wasn't my gym and

that's not how we conduct ourselves and

it has way less to do about the physical

you know the result and it does about

the proper preparation is proper

preparation and proper focus and

dedication over the long haul yields

positive results but most importantly

it's about are we conducting ourselves

in a in an honorable and respectable

manner so I believe that the competition

really teaches us that because in the

room you know there's always like oh it

was practice so I was kind of this -

that happened today the other thing when

you go to competition everyone is that's

on everyone is on that day because

everyone is trained

for that specific moment and we'll see

what happens so you get the most honesty

out of it out of a time like that and

the higher the level the better it gets

in you know provided that there's not a

lot of cheating but regardless you know

from an athletic performance perspective

it is the most honest thing because and

it's the it's the toughest as well

because it takes it takes courage and it

takes some heart to really properly

prepare and put it on the line because

you're risking horrible disappointment

I've prepared so hard before and tried

so hard and I've won and I've prepared

other times and I've tried so hard and

I've failed and it hurts it really hurts

it doesn't hurt nearly as much if you

kind of half-ass it because you didn't

put that much into it but again that's

how a coward approaches things if you

have if you're going to conduct yourself

the right way you prepare properly you

train hard and then win lose or draw you

deal with the results and that's what I

believe is the real benefit of

competition if approached properly do

you admire somebody who sacrifices you

know like 10 20 years of their life in

that singular pursuit of competition

towards a gold medal at the Olympics a

almost of the Olympians do goodness

absolutely I mean I admire anyone that's

willing to sacrifice and willing to work

hard in any area of life actually a book

that I'm reading again that I really

really like is dune it's a I'm kind of

like sci-fi nerd hangout on everybody

but basically a you know one of the

things that you know one of the one of

the you know things that the the author

was going to Frank Herbert and it's why

the regarded is one of the you know

greatest science fiction novels ever if

not the preeminent but anyway well the

things you said you know is if you if

you search for freedom you actually end

up becoming a slave to your own desires

ironically and if you search for

discipline you find liberty because

you're able to make yourself do what you

want in the long run whereas if I'm like

oh I'm going to do whatever I want all

the time and screw you dead I'm going to

do what I want that's kind of like a

teenager type attitude you end up

getting into a bunch of nonsense but

anyone that's able and again this

doesn't matter it doesn't mean that it's

athletic it could be in any area of

human endeavor any area of life it could

be parenting it could be military it

could be athletics to be business it

could be school to be anything but as

long as you're making you know an

incredibly large commitment I have an

immense amount of respect for the for

the level of dedication that and the

level of commitment and a level of

risk that it that you're taking

emotionally psychologically because hey

like you said you work twenty years you

get that gold medal but there's other

people that work twenty years and got

the silver most people know the medal

now most people most people that think

they work hard don't I'll be frank

you know like seriously I said that in

class the other day like again it's like

I don't want to be too negative but most

the people that most people to think

they work hard do not how do you know

that if you're working hard or not I

think you know but most people are not

very honest with themselves they were

most people would press a button in my

experience you know they would prefer to

be look like the thing and then be the

thing and you know that's that's fine

but it really I can't I think Sun Tzu

said it's a victory is reserved for

those willing to pay its price and there

is a price and now that doesn't

guarantee that if you pay the price that

you will have victory but you guarantee

but I mean from a physical perspective

but you will have the moral victory

regardless because you will have you

will have learned discipline you'll have

shown not only to others forget others

you do it not for others but for

yourself you you show that you are the

master of your own mind and of your own

body and of your own circumstance and

you can discipline yourself and focus

and you deny yourself certain things in

the pursuit of something something that

is valuable to you and that is

incredibly useful in any area of life

and that's not mean not shocking to me

why the same reason that you'll see guys

that were you know like high-level

military like kind of big dogs on SF

world get hired by let's say for

instance a fortune 500 company because

what would they know about business

nothing but also everything because that

level of focus and dedicate like you

don't get to that level of ability in

something by accident and that's what I

think you know like again the value of

competition and what they do competition

only it's as serious as it gets you know

because if you don't get the gold medal

it you may not you may not walk out of

it but basically I have an immense

amount of respect for anyone that is

willing and able to over the long haul

put that time in but I have to trying

hard doesn't mean just getting on an air

Don bike and walking off the mat or

having to be carried off the mat it

means thinking approaching reassessing

reevaluating saying how could I be

better and it takes honest on a

self-analysis and

and also it takes a lot of times because

let's say you know I think I'm doing

well but I got to say hey Lex you know I

mean no matter how how well I believe I

look at myself I'm still biased I'm

still looking at myself what should I be

doing better I'm gonna find other people

that I respect and people that I think

can tell me and I'm gonna ask them and

then I'm gonna have the courage to

listen to them and not just dismiss what

they're saying out of hand and if you're

doing those things then I believe that a

lot of times you're working hard but I

know plenty of people that come in it's

just like in jiu-jitsu this point if you

limit training for 15 years it frankly

suck and there's plenty of people that

have been training for four that are

pretty dang good you know for real and

again are they the best person that's

been training for four years is still

compared to like Kareena not that good

but they could be really really really

good because they understand how to be

directed and how to focus and I believe

this is something I've discussed you

know before with some other you know

friends of mine you know that some of

whom were at a very high level may

others that are the high level of

jiu-jitsu wrestling or whatever um look

at guys like Randy Couture guys a Rick

Hawn they're on their second career they

started MMA when they were like 32 and

yet they got to the top maybe they

weren't always champion but they were

fricking good why because they're 26

year old self would be scary but you

know like hey um they know what it is to

be dedicated and work hard and because

again they're Olympians like you said on

a level that a regular person has no

concept of so I think that that is

ultimately the skill it's not the oh man

this person's dangerous because he's got

good judo or good wrestling no this

person is dangerous because he or she

knows how to work their ass off and be

focused on a level that most people

can't comprehend and that's what

produces success in any area of life in

might hit you yeah be brutally honest

with yourself at all times and it stings

sometimes you know I think yeah it's

like the price of of looking inward you

know objectively is that you're not

going to like what you see a lot you

know and because even if you're like oh

man I'm 90% the way I want to be it's

like that if you are going to take that

next step in my opinion it's you're

going to focus on the 10% because it's

like oh man we're doing a lot of things

good yeah who gives a let's talk

about what we need to improve on you

know and that's a little bit less fun

but in the long run I think it's what's

it's what's going to drive you to a

higher level but at the same time I

think it's what makes a lot of people

that are like that a little bit neurotic

and nutty by compared by a normal

standard

right but again you show me someone

that's super well adjusted and I'll show

you someone that's probably not a high

achiever

you talked about moral victory can you

explain how your morality contributes to

the way see the value of winning sure

I think there's absolutely such a thing

as a moral victory and sometimes people

that are trying to manipulate you or

trying to get you to buy something will

tell you differently and you know there

is there it goes in two directions for

instance let's say you're a blue belt

and you compete against you know a real

black goal and there's plenty of people

running around plat belts that are not

particularly at this point but you know

let's say for instance your blue belt

and you compete against a real black

belt the likelihood of you winning is

almost zero however if you go out there

and you try hard and you do your best

and again whether you come off the mat

you know a winner which would be very

fortunate and unlikely but or you come

off the mat you know on the other side

of things if you went after and you

tried and you you know let's say you had

some nerves but you kept that in check

and you fought hard you didn't let it

get the better of you you know that

would be in my opinion a moral victory

and and there would be nothing wrong for

recognizing it as such now that's not

the same thing as an actual physical

victory but there's nothing wrong with

saying let's say you're your opponent's

260 pounds in your 120 and you tie they

get the decision hey you know I remember

that happened to me at the quarter-final

or not the quarter-final rather in the

the third round at the absolute in the

Worlds in 2008 you know and you know

it's against a heavyweight or a super

heavyweight and in the 0-0 and I wasn't

happy about losing by any stretch of the

imagination but looking back I'm like

okay well you know generally speaking if

you end up level considering that I have

all of the resources you don't that is

you know you definitely performed a

little bit better than I did now at the

end of the day you know wins and losses

do matter and you do want to try to make

sure that I'm not shooting for the moral

victory I'm shooting for the actual

victory but every now and then it's very

very important to keep in mind that you

know am I just actually myself am i

conducting myself in a way that I

respect that hopefully other people of

value or respect and also a way that I

believe is going to produce actual

victory and actual positive results in

the long run as well I think it's

important to recognize that because

sometimes you'll see people get very

frustrated let's say for instance if I

box against people that are much more

experienced than me all the time

you know I'm not going to win anyone

that tells you differently has either

not training with people that are very

good or B they have no idea what they're

talking about

but what I can say is hey did I do a

little bit better today and better

doesn't mean that I land more punches in

this it was I more under control was I

more able to kind of keep my keep my

focus and execute what I wanted to

execute and if the answer to that is yes

you know I'm moving in the right

direction so as far as I'm concerned

there's all sorts of different types of

moral victory but it would be the the

same thing as you know let's say for

instance you know fade or slaps your

mother you got to hit him

you have to he's going to kick the

out of you almost certainly but you have

to hit him it would not be it would be a

technical like well I didn't get hurt so

that's a win if you ran away but that

would be the opposite of the moral

victory in that case trying your best

and losing would still be I would say

the honorable thing to do so what you're

saying is sometimes you have to pay the

price for a moral victory

absolutely but the reality is is that

martial arts doesn't just teach us about

how to beat someone up or technique or

this is that that's really not the core

of the martial arts the core of the

martial arts is heart discipline

dedication focus and if you have those

things they'll always be people better

than you and they'll always be people

lesser than you but that's not the only

metric by which you can judge

performance or judge a person

yeah I competed in boxing judo wrestling

jujitsu MMA have a man yeah now what do

you think is the best martial art for

defending yourself against an untrained

opponent there's so many different

factors but let's say for instance okay

most fights I've seen are one-on-one

they're the fights we hear about are an

ass-whipping like seven people versus

three people and okay the more variables

you add in it gets very very difficult I

would say them the best fighting style

is using your brain and because less

voiding the fight well avoid in the

fight but I would say intelligent it's

just like investment now I know nothing

about investing which is why I don't do

it but um let's say for instance a

Warren Buffett is looking at a stock

page now I'm speculating because I don't

know mr. Buffett but I do have someone

who's standing up how the world works so

I would say that some days he looks at

the page it says haha there's a good

investment here and other days

regardless of his skill and investment

he says there's nothing to do the best

thing to do is wait but your Warren

Buffett tell me the best investment yeah

the best investment today still sucks

and I'm not going to make it talked to

me next week and I'll see what's out

there

so till next week we're gonna make we're

gonna we're going to invest some money

maybe and then you know he'll make the

read as he sees it so let's say for

instance you walk into a room and your

goal was to kill everyone in the room

and you are armed but you walk in to

50/50 Jitsu and you're going to shoot

everybody but for whatever reason it's

bringing your gun to the gym Tuesday and

everyone else is sitting there polishing

their fully-loaded again rounding the

chamber safety off weapons hot all that

good stuff and you see everyone else is

armed what do you do wait until

Wednesday

you come back in on Wednesday when no

one's armed then you shoot us all to be

great so it's that would being the best

tactical shooter you could be as ninja

as you want we're going to kill you

there's too many of us in too few of you

could you make it out of there and like

some sort of Boondock Saints awesome

luckiness and managed to get everybody

sure I wouldn't bet on it

that's like Floyd Mayweather against

three people I would bet on him sometime

seriously anybody that tells you

differently is never fought an untrained

person because regular people can't

fight for [ __ ] and the other thing is

they get scared so the one Floyd

literally cripples the first guy with a

right hand the other two guys unless

they're really seriously probably go

oh and then hesitate but they may not

but let's so if you think about it

though four people five people three

people ah let's say it's one on one but

Floyd's minding his own business against

snuck over the shoulder as he's sitting

there ordering a drink at a bar all

these different things factored in so I

would say that when it comes to the

physical expression of how to best

defend yourself the most important thing

is situational awareness and

understanding what's going on around you

because you could be the world's

greatest ninja warrior and still run

yourself into a lose-lose situation

could I knock out Floyd Mayweather yeah

sure I could if you let me hit him but I

don't think I would come within spitting

distance of him if he didn't want me to

I would have no teeth before I even

tried but if he sits there and lets you

hit him he is a man he is mortal so

basically under the right circumstances

anyone can win onto the wrong

circumstances anyone can lose so I would

say that understanding that is two step

one to being able to be an effective

effective strategist who can read a

situation you say should I fight this

one out should I not should I get out of

here should I fight for four seconds and

run for it and you know when you take

into account the physical expression of

everything and you want to let's say for

instance you know the most important

things for defending yourself in real

life I would say probably wrestling or

jiu-jitsu probably jiu-jitsu really in

my opinion but that's just perfect for

fighting I would say other things if I

want to be able to beat up more than one

person I know it implies that I can

wrestle or I can because regular people

can't wrestle for [ __ ] you will you

could pick him up and slam your head on

the ground or something horrible you

know but boxing would be nice as well

but let's say you're a great boxer and

someone tackles you could a regular

person tackle a good boxer yes alright

could a regular person sucker a good

jujitsu guide for sure but it also

really comes down to the to the mental

and everything like that but basically

if I had to pick one art and the

everyone knows nothing I would picture

issue but in my opinion jiu-jitsu at a

high level involves wrestling so just

like you said grappling is this bigger

thing that involves wrestling's you know

everything no doubt that's like you take

an Olympic level wrestler and you let

him lean on top of your inside control

it's not pleasant

in class you you emphasize that we're

working with basic laws of physics so I

just read Einstein's biography he was

obsessed with finding a single theory

that would unify all the fundamental

forces of nature Wow

do you think there exists the unified

theory of grappling we can boil

everything down to just a few principles

I will

well first off if Einstein wasn't able

to come up with a unified theory I would

sincerely question my ability to go that

way but do I believe that something like

that could potentially exist absolutely

and I think that even if it doesn't a

belief in the possibility of it and the

search for it would would leave you

better off than where you started

whereas if I was a no no that's [ __ ]

that would never and then I don't look

even if I'm right because I didn't look

there's certain things I won't learn so

I think you know a lot of times just

let's say alchemy the idea that you're

going to turn lead to gold all right

let's a little bit nuts but who knows

maybe that's that kind of nutty and you

know on highly unlikely is highly

unlikely probability of success pursuit

yielded scientific progress elsewhere in

the search for that even though again

someone would look back and say oh

that's stupid who would blush and who

would do that

well if you spent years trying to figure

it out I guarantee you're going to learn

some other things as well so I think

that there at the very least the

principle based approach to grappling is

incredibly important with your process

like for learning new details and

understanding the principles behind the

techniques I certainly don't believe

that I have like a singular or perfect

approach by any stretch of the

imagination but you know I guess what I

try to do is block out extraneous

nonsense like for instance both Pete a

lot of people want to talk about 55

details and reasons for something that's

going on and the reality is is that

you're clouding your thought process for

instance there was a recent not to get

too political but there was a recent

issue where I remember an inmate was

executed and they used a new drug and it

was painful and oh my god and he died it

was horrible again that's when whether

you believe that capital punishment is

valid or not I think there's plenty of

arguments against it in fact I think

most of the arguments that make sense

are against it but pain has fuck-all to

do with it you know I don't care that's

like done hey Lex I'm going to kill you

but don't worry it's not going to hurt

man I mean don't know okay then yeah

it's like that doesn't make it okay it's

like believe me I'm for free I'm is

again if you're gonna kill me I prefer

that you don't burn me at the stake but

if someone was like don't worry it's not

even a sting I'm still going to try to

fight you to the death I'm absolutely

not allowing this to happen if someone

wants to say okay hold on let's get

let's cut the [ __ ] like feely

feelings out of here and say look forget

the pain does this person deserve it

uh-uh

let's let's step that back again is

there a potential for human error is

there potential for someone having this

guy behind bars for political gain like

okay these are the real reasons that you

say hey no way on the death penalty it

has nothing to do with does it hurt or

which drug is it are blah-dee-blah

or is it inhumane it's like none of that

if hey we're focusing on the wrong thing

so it reminds me of jiu-jitsu in the

same sense and again we're fighting were

generally speaking in my opinion debates

that happen in the public you know arena

they always focus on the wrong dang

thing and always focus on the wrong

aspect again there's 25 good reasons or

bad reasons to do almost anything but

generally speaking people will focus on

the hundred other ones that are

extraneous and [ __ ] so what I want I

guess what I would say is it reminds me

of jiu-jitsu is striking like man Floyd

likes to hold his hand this way or that

way or the other way and and this guy

likes to jab like this and it's really

important this person says you land with

this knuckle in that person says you

land with net knuckle and in jiu-jitsu

it's very very important that you grip

three inches up on the lapel and two to

the right

but Roger Gracie doesn't like this but

cabrini says it like this clearly they

all work under the right circumstances

and don't work on to the wrong ones and

it has nothing to do or very little to

do with these other things like hey does

it matter again I'm going to kill you

would you prefer for it to be painless

or horrific ly painful okay if it's

already a foregone conclusion death

alright yeah now we'll start to talk

about the the extreme like whether or

not it's going to sting but until we get

to that point

hey let's focus on the do we is it even

right or do I have the ability or the

capacity to do this justifiably okay so

that's where you come down to the

principles in my opinion say all right

yeah it doesn't matter which knuckle you

land with yeah I'm sure it does but it's

a hell of a lot less important than 25

other little things that make all of the

difference and in my experience a lot of

coaches and a lot of people particularly

guys that are trying to [ __ ] you

will focus on 45 little details and oh

it's there's 15 details to this ten

okay that's true but what are the two

most important ones because hey don't

get me wrong I'm not saying that these

details don't matter but just like

anything else in life there's a

hierarchy because would you say that

would you say that happiness and

self-actualization is a valuable thing

in life yes I would as well and you know

what we have the luxury of saying that

type of thing because we're sitting at

fifty fifty Jitsu in Falls Church

Virginia and there's no one trying to

kill us rape us and we are also I know

where food is tonight yeah if you were

to walk down if you were to talk to

someone like 2,000 years ago I'll be

like how are you feeling they would

stare a chili what are you [ __ ]

[ __ ] I'm starving yeah I'm hungry

that's my issue

are you are you satisfied in your life

it's like I'll be satisfied when you get

out of my way so I can find some food so

that's even the deeper question is are

you eating something tonight right and

so it's Maslow's hierarchy of needs when

we take care of the base needs first

then we start to work our way up toward

self-actualization and this and that and

but until you got your food water

shelter don't tell me about where you're

placing your grip it's like you're all

leaning out and you're you know your

posture is poor and you're out of

balance and you want to tell me your

grip that's like I'm starving to death

the you know the barbarians are at the

gates but I'm sitting here giving you a

philosophy lesson it's like this is

[ __ ] it doesn't make any sense

build a better wall a sharper knife and

get some food and then we'll cover all

the other stuff so I think that in my

when it comes to how I approach martial

arts in terms of learning as well as

teaching I really try to boil it down to

what I feel to be the most important

component parts and then if I one day

reached the level of where these tiny

details matter that's fantastic because

again the difference between you know

the ability to pass that try to pass

successfully against a cabrini or how

file Mendez and against a regular

run-of-the-mill black belt or you know

does come down to little details but

it's also presupposing that you're in

proper position that even allow these

details become relevant and I think that

a lot of times we put the cart before

the horse and that's not that's

problematic there's still in your

opinion undiscovered position

submissions of techniques and you just I

would say that there have to be there

absolutely are um you know I think that

what we see is jiu-jitsu now don't get

me wrong the core never changes because

physics doesn't change physics is the

same thing that's why I get a kick out

of

love like self-defense arguments it's

like [ __ ] it has another no physical

difference in self-defense beyond the

fact yes you can I guess mean though

I've never seen anything like that in

real life that you're crossing a pretty

serious psychological line if you're

putting your knuckles to you know your

thumb to knuckles deep into somebody's I

forget the fact that did you know that

we're legal more all other things like

that it's like I've never done that

before I wouldn't do it lightly there

probably be some hesitation there what

is so anyway what makes it different is

the the psychological component and all

these other things going on but

physically there's no difference again

people like Aldridge it's is really

different in MMA no it's not not in my

opinion not in my experience it is

absolutely not what are you talking

about physics are different inside of a

cage than they are on a mat and they are

out in a field

it's exactly the same now if I try to

sport grapple you under a non sport

grappling rule set then I may run myself

into trouble but that had nothing to do

with jiu-jitsu jiu-jitsu is physics is

proper expression of physics the same

way boxing is and the same way of

wrestling is the same way all these

things are so I would say that as long

as something adheres to the principles

that that allow something to be

effective and you know and fundamentally

sound that you can do almost anything

and I think that people will continue

particularly in the ghee it's going to

get nuts you know just the level of the

amount of things that you can get away

with and do and different grips that you

can make but I'd say what we're looking

at right now is going to look only

somewhat like what jujitsu is going to

look like in 30 years the same way the

jujitsu we see now is so much

fundamentally better honestly and and

more evolved and adaptive than it was

twenty years ago and people will swear

up and down like all back in the day

it's back in the day people did not

fight very well even twenty years ago

the level of people understanding how to

deal with jujitsu was very reduced so

you could get away with all sorts of

pretty questionable stuff like sitting

in front of someone in close guard and

have them not completely kick the [ __ ]

out of you but um yeah I think you know

with particularly the advent of Baron

bull the 50/50 position all these

different things which have always

existed there's they've always existed

and one that's I always hesitate to say

invent I don't like the word invent like

that certain people use a lot

I'd say Discoverer because you could

show me I can come up with something

let's say Lex you know you've got a

really good straight foot lock I was

watching a train last night

and I could be doing that in a way that

no one ever taught me that doesn't mean

I invented it because you've been doing

it forever but basically it's let's say

no one showed me the details you were

using and I managed to stumble across

them I didn't invent those details eyes

go yeah I discovered them that was neat

but again none of us have invented a

dang thing people have had two arms and

two legs for certainly as long as I can

remember and probably longer than that

and today here yeah that's that's the

word so in the history books

you

summary:

Ryan Hall, a notable martial artist and UFC fighter, delves into the intricacies of competition, dedication, and the essence of martial arts in this insightful interview. Here are the key points from the discussion:

1. **Value of Competition in Martial Arts**:
    
    - Hall recognizes competition as a means to understand the real purpose of martial arts: self-defense and real-life applicability. He emphasizes that the worth of a competition medal pales in comparison to the ability to defend oneself in practical situations.
2. **Competition as a Catalyst for Improvement**:
    
    - He argues that defined goals and formidable opposition in competitive environments compel individuals to improve. This principle, he notes, is evident in the evolution of various fields, including sports and the military, over time.
3. **Approach to Competition**:
    
    - Hall criticizes a lackadaisical approach to competition, labeling it cowardly. He stresses the importance of proper preparation, focus, and dedication, regardless of the outcome, and condemns using lack of preparation as an excuse for failure.
4. **Integrity and Team Standards**:
    
    - He maintains strict standards for his team, insisting on proper training and preparation for competitions. Hall values the process and integrity over results, advocating for honorable and respectable conduct in and out of competition.
5. **Honesty in Competition**:
    
    - According to Hall, competitions are the most honest platform where everyone brings their best due to the focused preparation for the specific moment, making it the toughest and most revealing environment.
6. **The Pain of Failure and Moral Victory**:
    
    - He openly discusses the pain associated with trying hard and failing in competitions, highlighting the importance of full commitment and dealing with the results, whether positive or negative. Hall believes in the concept of moral victory, where the effort and character shown in competition are as important as the actual outcome.
7. **Respect for Dedication in Any Field**:
    
    - Hall expresses immense respect for individuals who show long-term dedication and sacrifice in any field, emphasizing the importance of discipline and focus, which are applicable and beneficial across various aspects of life.
8. **Unified Theory of Grappling**:
    
    - While skeptical about formulating a unified theory of grappling akin to Einstein's pursuit of a unified theory of physics, Hall believes in the value of searching for fundamental principles that govern effective grappling techniques.
9. **Principle-based Approach to Learning**:
    
    - He advocates for focusing on the most important components in martial arts, cautioning against getting lost in minor details. Hall emphasizes the importance of understanding the fundamental principles and building upon them.
10. **Evolution and Discovery in Martial Arts**:
    
    - Acknowledging the constant evolution of martial arts, Hall anticipates future discoveries and innovations, especially in disciplines like jiu-jitsu. He prefers the term "discovery" over "invention," recognizing that martial arts techniques are often rediscoveries of pre-existing principles given the unchanging nature of human physiology.

The interview with Ryan Hall offers a profound look into the mindset of a dedicated martial artist, highlighting the importance of commitment, integrity, and continuous learning in the pursuit of mastery in martial arts and beyond.

----------

-----
--01--

-----
Date: 2014.05.08
Link: [Ido Portal: Movement](https://www.youtube.com/watch?v=o8nZaDw_mOs)
Transcription:

Ido Portal has spent the past few decades honing a physical credo and method that's now practiced by thousands of people all over the world - from office workers, to former CrossFitters, to NBA players, to the ever-controversial UFC titan Conor McGregor. Known as The Ido Portal Method, or simply "movement," his approach purports to take the "most potent" parts from a range of physical disciplines by shedding the dogmas that often accompany them. As he puts it: "I want the contents, not the container."
Intro

we choose to go to the moon and district eight and do the other things not

because they are easy but because they are hard

[Music]

in any particular sport with well-defined rules mastery is achieved

through specialization taking a few skills and perfecting them so naturally

most experts and teachers of movement are specialists of skillsets like

gymnastics hand balancing Olympic lifting capoeira Jitsu wrestling judo

and other martial arts so it's rare to come across a generalist someone who

takes a holistic approach to movement my guest today is IDO portal he is a guru

of movement a teacher with a large and quickly growing following as he says

movement is big bigger than any specific discipline were all human first mover

second and only then specialists I actually just finished reading a

Journey to becoming a generalist

biography of Albert Einstein by Walter Isaacson and the two of you have

something in common I desire to arrive at a unified theory in his case there's

a unified theory of physics you know like forces of nature in your case it's a unified theory of movement can you

tell me the story of your journey to becoming a generalist first I'd just say

that I'm no guru you know it's something that people use but I have a really hard

time with the word master or guru and I didn't arrive with the gospel truth I'm

not sitting on any mountains and I'm I'm on my way so people who join me as students are

basically following you know in the same journey maybe in certain circumstances

I'm a bit further ahead or sometimes I'm a bit behind and but it's definitely

walk along and not walk behind kind of thing yeah my journey I started as a

mover first and then I became a specialist and then I went back to

movement basically so yeah it all started in a young age and some some

Chinese martial arts and developed into some physical sports and

games in school primary school and high school and then I met capoeira and I I

was completely amazed basically by this dis art form and then pursued that for a

good 15 years in the middle somewhere military service and other physical a

physical pursuit and throughout changing and developing and moving between

disciplines and exploring just kind of got the same realization again and again

that there is some thread through all these disciplines something that is

attracting me back and basically it was movement I realized and the next thing

was okay I want to learn movement I want to get better as a mover in a general

way I seeked out a movement teachers and went around the world and looked around

and read a lot of books and there were some people who mentioned the word movement and I went to them and you know

heard myself as a student and they kept on teaching me disciplines another

isolated approach another speciality and I was very disappointed so eventually I

decided okay I'm going to become that person I'm going to become the movement teacher and the next realization after

years and years of trying was it's impossible and with that I stayed

basically because I realized if that's impossible it's a good goal to have in life something that will keep on moving

myself and my students and anybody involved forward that's where I'm at

right now I'm in teaching and learning moving around and trying to try to gain

some more knowledge about this impossible task yes you're still yourself or forever a student I would

prefer I prefer to be a student any day of the week than a teacher and but being

a teacher is part of human culture we are all teachers we we teach all the

time but whether you want it or not somebody asks you for direction in to it you become the teacher you have a

child he looks at you you're a teacher practicing teaching and practicing to

the student the discipleship both of them are extremely important for your development as a movie what is the price

of specialization what do we lose when we specialize we lose humanity first and foremost as humans we we evolved to

become humans as generalist we are the most generalist of all animals we are

able to imitate the ape and imitate the tiger and and we can hold our breath

underwater and we can do everything just a tiny bit we can't really run very fast

we can't really fight very well we can't you know climb as good as other animals

but we can do the most complex and generalize tasks out of all animals and

no animal even come close so speciality the price is humanity the price is your happiness the price is

your fulfillment as a human being it's deep beats it's philosophical but that's

that's we are do you think there is some beauty and fulfillment and some value

Specialization

and specialization in becoming the best at a very specific movement at a particular sport giving your body to you

know dedicating it to that sport that's an interesting thing because as a human race we benefited tremendously from the

work of specialists but those specialists suffered that's a very very

important points like without specialists we would never be here we would never be skyping right now on

these computers and and you know wearing these t-shirts and all kinds of stuff

but those specialists those human beings suffered the result of their highly

specialized nature and we become more and more specialized the reason move

towards being generalist again in the last few years maybe the last decade

there is a bit more talk about that but definitely we are also still

pursuing highly specialized fields if I make a joke in my workshop and I say

nowadays you go to it if you break your you know you break your hand you go to a hand specialist orthopedic surgeon in

five or ten years you'll go to a left hand specialist and the most evident problem is also our

leaders who are experts but now they're required to be generalist leaders of a

lot of stuff and their shitty leaders we keep on having the same problems again

and again because they are ex specialists whether it's a lawyer or a

military person or an economist these are not specialities that allow you the

full grasp of running a country yeah I think I think you put it beautifully that in my specialization might lead to

innovation but you lose the humanity mmm what do you find is the most

underdeveloped range of motion in athletes like what movement is most

restricted in pi level athletes in your experience is there any one that stands

out shoulders any particular other joints well it's it all depends on their

speciality of course in habits the shoulder the glenohumeral joint is the

most hyper mobile joint in the body even when it's restricted it still offers tremendous range of motion compared to

other areas but when it's restricted even if it's just a little bit it can

cause huge problems because we are dependent on that range of motion and

mobility around the glenohumeral joint and the simple reason is because with

the hands humans manipulate that's what we're meant to do with our hands and we

need that complexity around the scapula it's been a few years since I've said it

first that the scapula craves complexity but this complexity around the scapula

and range of motion is so important across the board I've read of your concept of isolate in

are graded and improvised okay describe the role of improvisation in movement

Improvisation

any profession and speciality should arrive at improvisation in the top and

tier the top level and whether you play the violin or you know you box you're

going to reach improvisation improvisation above all is the human condition it's the human the human

ability the highest form of living is improvisation you improvise basically life is improvisation you're born you

die and in between you improvise a shitload of improvisation movement is no

different the thing is people started to isolate concept and some people went the

next level and integrated them they present themselves as improvisation but

actually they're cheating people it's just a bit more integration yet it's

just another integration improvisation open improvisation real improvisation as

we call it that's very rare and that's the most enjoyable state it's also

called the zone it's also called the tunnel you just experience this

beautiful thing to be empty just to let things happen through you as Bruce Lee said I don't hit it hit it just happens

you know and that is improv that's what you need to do with movement if you

aspire for the highest things I like how a guy on reddit described you as IDO

portal may not be the nicest guy in the world but he's a great coach so that nicest

guy part I come from the wrestling world where the goal of a good coach and a

good program is to basically make you quit to break you there's zero patience for people who don't want to put in the

work to work hard do you find that tough love is the best approach to coaching

Tough Love

people whatever their level of ability no not necessarily I don't like the term

tough love because it kind of assumes the it's importance is itself it's not

enough for me it's like that's how that's the best way why that's the best way but on the other hand I don't think

people are made of sugar and I really believe that we've lost a bit side of

you know how resilient we are and another is people don't like the truth

you know it's dishonesty is above all so when people describe me is tough love

it's not because I believe in Tuffle 100% of the people who has been who have

had issues with me on a personal level or through coaching are people who

couldn't accept criticism what I offered you know took it personally weren't able

to deal with it etc I can't even you know in my head find one example of a person I've been

working with who received the criticism worked with it and still complained but

it's always these complainers and who cares complain first and do nothing yes yeah yeah you know it's like

when you go mainstream as we've went to a certain level you have to deal with it

because I'm not operating my elite unit my Special Op unit anymore now it's an

army and I need to accept the fact that I'm gonna meet a lot of slackers a lot of Poindexter's and all kinds of

you know they don't want to work they want to talk about it they want to do this they want to do that they don't

want to hear the truth they don't want to accept criticism or hear how much they suck and I just don't do that so

you know I'll have to accept the fact that from now from now and again you know what I'll have this issue and I'm

sure it will continue you've travelled all over the world you think there's a difference in this aspect in attitudes

in the United States and Israel and other countries big time horse big time

yeah there are many countries where I don't have this issue or very rarely we

have a word in in Hebrew actually it comes from Jay and I think or Yiddish it says touchless

it's like down to it you know the heart of it tough less people are people who

are like no you know directly tell me as it is and this stuff less it

it exists in certain cultures in other cultures it's a lot of chitchat and walk

around and you know I didn't know how to chitchat a few days ago one of my

students told me you know I can't cheat you it's exactly how I felt you know when I first came out of Israel started

to teach around it was Russia thank God and that was so similar to him where I

come from in many ways so no problem but then when I went to the US or Canada I

had a lot of issues with the chitchat with politically correct and walking

around the bush and don't give it to me too harshly you know cover it with a lot of sponges around it soft in the heat

and yeah it's definitely different between various countries and I need

nowadays I need filters which are my my stood my top students were helping me

teach and some of them are great filters in and in certain countries they'll do

much better than me what is perfect practice look like for you so do you believe in the value maybe this applies

more to specialized sports but like I I come from Russia actually and from the

wrestling world where repetition you know putting in ten fifty thousand hundred thousand repetitions on a

specific movement is is how you achieve success do you believe in the value of

that repetition even for generalist framework the repetition is the mother of skill yes there have been those that

corrected and said perfect repetition is the mother of skill well those who

usually say it are those who don't achieve Heights usually usually so I'll

be very frank again I'll be very extremely honest a lot of people talk about perfect perfect perfect but but

life is not perfect itself our surroundings are not perfect and when I

practice and when I move it's never on the perfect conditions it's never with the right optimal blood

sugar level and under the specific you know height of I don't know what and and

riding the wave of super compensation in the perfect way and usually when people try to adhere to that concept in a

perfect way they end up falling off the wagon on the other side don't be don't

be stupid don't just drill yourself into the wall and lose sight of everything it's not black and white the truth is

somewhere in between and it varies between people for me after seventeen

years teaching 18 years now teaching moving seeing people the hardest workers

are usually the elite performers of course some of them are carrying a

certain talent or this or that but it's always with very dedicated practice they

have built up that work capacity through that dedicated practice and they can then move that ability to other

disciplines true in a grappling world I'm not sure how much you're aware of it but Marcel Garcia is one of the Great's

and he believes boldly against the status quo I think that you should only

train jiu-jitsu his sport jiu-jitsu and not do anything else so to achieve

success trained only that but the majority of other athletes in the sport believe that you should do strength and

conditioning programs and all around that so they at least move slightly towards the more generalist framework

what do you think do you think that's value for the generalist mindset for like an elite athlete or should they

Generalist Mindset

just focus on their sport to some level to some level speciality can can reach a

plateau because of lack of general base of the pyramid in some cases but it's

not a very high level of you know Janet generalism nowadays you're you're

practicing against specialists and they devote more and more time to this specialty

when you're doing other stuff so it's a complex rhythm you know and and to each case his own but I'll tell you something

else when you reach the top of your field like marcelo garcia did in BJJ you

stopped being inspired by your own scene you can't gain inspiration knowledge and

motivation from your own scene because you are the leader you're on top of the mountain you have nowhere else to climb

so what you do you look to other scenes and that's where it's really really

valuable to become a bit more generalized yes you mentioned an

interview related to that a very interesting point the many people in the US in particular focus on learning more

than doing so focus too much on acquiring knowledge versus using that knowledge do you struggle this yourself

like how do you approach learning new things versus putting more time into old

things that you've already met Sinead it's not only a u.s. thing or North

American thing it's generally all across the globe all those are more practical people and less practical people you

know in each country has its own orientation habits you know characteristics but it's a good question

you need to be it's kind of being super intelligent and oriented towards the

information but then have this dumbed down practical mind it's like okay now I need

to work you know and having a balance across that and that's probably it that

probably means that you know a certain IQ for example will start to work

against you in certain fields and vice versa so you when you become too much

you know as the Chinese say the man who lives inside his head you start to have

this issue you know you you have a thirst for information great thirst but

information is toxic it's exactly like water water is toxic as well almost all compounds

toxic and then we drink we drink with it we kill ourselves we kill the process

and the knowledge it turns against us and that's a serious problem and that's

the problem of the age of misinformation that we live in it's not only that the

knowledge is toxic even when it's good knowledge now we also have bad knowledge

mostly bad knowledge mostly shitty advice the combination is listen just

people become paralyzed or just you know move from link to link to link with a

you know glazy eyes and just never actually do anything yes I know you

advocate building a huge word capacity so how many hours a day do you think this is also a debate for specialists

how many hours a day do you think is the most a person can train movement

intelligently before becomes not sustainable before their mind becomes

uninspired maybe as you said 24 hours 24 hours 24 hours a day there is a

choreographer in Israel very known choreographer called ohad Naveen he says when you wake up in the morning in bed

between the sheets you can you can practice movement and and is not there

talking about with your partner yeah so even there you can practice you know breathing moving it's it's all the time

around you but serious practice you know practice oriented that you know

repetition and success and building skill and moving from isolation to integration to improvisation in most

disciplines it's around six to eight hours a day some people go more and reach even the

ten hour mark and I've done that for periods of time in the military you go

even further than that other disciplines require less and it's also a highly

individual thing so let's say even within the sports of gymnastics you have a woman like in Nast who trains eight

hours a day and neck to her the same team also winning gold medals at the same level more or less

you have Shawn Johnson training three hours a day and she reached the top of

her field or liquid gold medal in the Olympics so how highly individual this

is very rare to see this three hour gold medal thing but definitely it

exists the difference there might be mental so the question I have is out of

the various elements like mind breathing developing muscular strength or joints

which is the biggest challenge to master as a student a movement highly

individual it depends on the person depends on is orientation some people

never require any form of mental training for example or psychological training especially in fields and like

sports and then team sports yeah so so that aspect is covered they're winners

they're oriented they're focused you know and then other people require help

in that regard some people have great difficulty developing mobility and just the nervous

system is panicked it holds on it protects them too much other people are hyper mobile and have a

difficulty creating tonus and in strength and that is a great challenge

for them and other people are you know great complex learners they can name

they can coordinate complex actions and learn movement very quickly while others are highly limited so it's very

individual for that process of learning that journey is individual to everyone

Listen to your body

so how does one take that journey just listen to your own body now now you can

listen to your body until tomorrow Hypatia you're not hearing anything you

know you're not hearing anything you need to learn you need to create a relationship with your body and you need

the help of of teachers right that's there is only you know a lot of

people say no I'll do it myself then you deny collective knowledge the most

powerful knowledge that mankind holds you know because we're the only animal that have collective knowledge we've

been able to move knowledge across generations and that's how we have reached space build the Internet

you know do all these crazy surgeries and and you know solve you know genetic

issues and etc it's you're not gonna do it by yourself you're just one small

person and we have collected knowledge generations upon generations so listen

to your body that's nice to say most people don't hear it's completely

silent and then you need to start to decipher the signals that the body gives

you and that goes through a practice and learning discipleship and exploring a

lot of different different stuff and it's a highly individual thing nowadays we don't have so much any anymore this

mentor student or teacher disciple relationship but I I really believe in

that I wouldn't be here without my mentors and my teachers the shoulders of

giants that lifted me up I still believe in it in in a way there is no other way

yeah on that do you think that training and learning movement for the majority

of the time is a fundamentally solitary activity or do we gain from like the

presence of others so when you when you think of movement when you're training is most of your training like the

repetitions done alone or with others both and I've trained years you know

alone and with my students and I spend large periods of time alone just

training alone but I also spend a lot of time being in a community in movement is

the best reason for gathering around in a community and you know people for

example nowadays they go do CrossFit or they do yoga what ever and then they have their yoga

friends and they have the real friends that that's you know your yoga friends can be your real friends because

we've been gathering around movement since the age of time creating communities around movement around

hunting gathering dance around the fire we've been moving together nowadays I can recommend move with your loved ones

moved with the people around you you know you join a BJJ Club it's a community you know you go there you meet

you move around you go to a capoeira Club it's a tribe you go to a CrossFit

gym it's a community you go to yoga it's a community you can move with your

children you can move with your dog in the park and it's important to move

together but it can also be done alone and some things are better done alone and some things are better than together

how do you think movement changes from solo movement you know that that whole

Movement

pattern of movement where you're moving alone versus the pattern of movement where there's two people either working

together against each other so together is like dancing partner dancing and against each other is like wrestling or

jiu-jitsu do you think the principles of movement are different for when it's two

people versus one person this is a whole nother world first they spend more time

moving with others against others in martial art because I spend most of my

life in martial arts and less time exploring stuff alone but definitely

there are some concepts that still exist like the quality of movement how you

organize your body in space not in relation to the partner only but first a

BJJ practitioner or or a stand up fighter he needs to organize his body in

relation to space first and then in relation to the partners well so some of

the concepts exist in both while others are very different and you can train

alone or all your life when somebody else is in the equation it's going to change the game completely a major

reason why we are under the fight laboratory we've departed in reality from a lot of

traditional martial arts and the delusions of training alone and doing forms and and repetitive you know

movements alone and then it's a shitstorm and you can't apply anything

and you you don't have any live practice and now we see that and definitely in

the fight game and the practices that stayed very real stayed very dirty in a

way but very real they are the ones who are providing tools for the chaotic

environment of a fight terms of injury how do you treat recover and work around

Injury

injury hmm injuries are a certainty

they're not a learnt the probability injuries and diseases they are also

required as nothing talib one of my biggest inspirations this day is a great

philosopher and in order to to anti fragile eyes to become anti fragile to

become robust to become more than resilient you must be able to enjoy

volatility you must be able to grow from this stuff and so first I oh I said I

said it before and I'll say it again I injure my students this happens and I

can do anything beneficial without it and basically we all get injured

constantly on a micro level a macro level it's part of our lives of course

we don't want to push into meaningless injury and we want to be able to grow

from it an in and basically develop from it how do you train around it how do you

train around it it's a hard question it involves a lot of stuff first and a big

believer in movement as a therapeutic tool movement itself if it offers you

adaptation and it does it's the way out not lack of movement rest

I don't believe in rest I believe in moving which means when I'm resting

I might help on the short term with certain aspects of the injury but at the

same time I'm creating a new problem because there that the adaptive process is taking me somewhere else

I'm not recovering towards movement I'm recovering towards no movement so we

have a problem here now in some cases you must rest and then deal with the

consequences later but in most cases there is a better approach than just

resting in that that requires a lot more taking responsibility which doctors

don't believe in your ability to take responsibility for yourself to be

intelligent and to know the amounts and the levels and that requires some some

form of a knowledge and experience and most people can't be trusted with it so we offer them you know this advice of

like just rest resting stop yeah but but definitely after years and years of working with

people and and taking them through crazy injuries near my right hand or Delia she

she went through a car accident she lost the kidney she broke her back she went through three knee surgeries which my

sister performed by the way nowadays she can move like few people I know on this

planet and and just the answer was always movement movement going back into

movement beautiful continue move continue to move yeah don't move

stupidly don't don't hurt yourself but that's an obvious no I guess not because when I say these things people write a

comment the but the bad people you know yeah but you're gonna injure yourself yeah genius

haha don't go into the injury and and

again deteriorate the escalate the situation of course not you must move

around it and you must be smart in the way that you allow adaptation to take you out like a wave you need to ride the

wave of adaptation out of the out the problem and that's tricky we know

and that's something that we need to educate people on and we need to believe in people's intelligence and ability to

take this responsibility in China where they still have in some areas and they

used to have bone setters they didn't put you in a cast you broke your hand or they they did it through bone setting

and yes they didn't have x-ray so that's a lot more complex to do and not as

successful as nowadays but having said that they did achieve amazing rates of

recovery because these reps that they use and the process allows some form of

movement and that creates an adaptation now take an arm a healthy arm your right

arm put it in a cast for six months take it down the cast what do you see the

observer the arm is basically moving towards death yes it's gotten good at not moving the arm

is great you have weird hairs growing out of it it stinks it really smells and

looks like death because movement is life no movement death we know that we like to just kill your

arm a tiny bit so that so the bones can reform together and then we'll bring it

back to life that's one approach but in other cases you can maintain the life

and the demands on the tissue safely enough at the same time allow the

recovery to happen yes and diet what are

Diet

some diet principles you follow well diet is very individual thing I've

been personally following a Paleolithic a caveman diet for a long time long

before it was called the Paleo diet and since 1997 or even 96 I've been doing

this for a long time I feel great on it still you know growing older and older

and functioning only better and being able to sustain maintain improve but that is

very very individual there is a lot of exploration to be done there what you can withstand how resilient is your

system yes nowadays there is a new movement towards not improving the fuel

sources not improving the quality and the quantities of the food but actually

making the system more resilient so it's able to basically withstand any almost

any quality and source and that's where we have been lacking and we will be

neglecting this area and that's something that and I believe is the latest innovation although it's still

highly misunderstood and a lot of a lot of folks are abusing this concept and

giving really poor advice just to be different just to say I'm not the paleo

guy you know so that's that's a small addition that will get bigger and bigger

I think so it's almost like how you recommend a movement to go outside of

quote-unquote proper alignment this is kind of the diet version of that is going outside of some kind of proper

framework of diet to some level but that's so that was an obvious thing you

know always right the problem is it's not enough because in fact it's more

what I'm suggesting with movement to go outside of the proper alignment it creates an adaptation but what if

that adaptation cannot happen for example a celiac disease person you'll

expose him to gluten and you'll have terrible consequences now maybe if you

can minimize enough the amounts and the dosages you can actually train him out

of tell yak to some level but death adaptational last very very far down the

road he's going to get some gains and then is going to plateau but what if you could take that celiac disease person

put them in the garage fix his mechanisms change his tires change his

engine you know oil him up everything and then put him back on the track as a

new animal and that that is where you know a lot of stuff is happening

nowadays so the genetic part of it and the gut biome and our digestive tract

that is is so so complex and and we discover that you know we live in

symbiosis with all these microorganisms that just are all over skin inside of us

and we live in combination with them and that's how you see some dudes in Brazil

and in Russia walking around with you know 5% body fat eating one cracker for

breakfast one cracker for dinner and you know training BJJ all day long

high-performance fueling with coca-cola and then at the same time you see people

doing everything almost perfectly and still having poor poor performance and

inability and they gain weight with any you know extra calorie or macronutrient

that they brought in because their system is different and it's not only

about genetics it's more about epigenetics and it's more about what

kind of system besides your own DNA what about these organisms that are supposed

to help us and live in symbiosis with you what kind of a system do you have there and that's just two areas and I

think it's going to get it's going to expand more and more in work and we're going to realize that there is a lot to

learn there do you think technology and science is ultimately a positive force for you know you look at movement as as

an element of our humanity do you think technology is taking humanity away or

it's adding to it I'm not smart enough to answer you that man yeah it's a big

question you know it's a I have no idea it's just it's a huge question and I think we're going to

struggle with that question for many generations to come still it definitely

created a lot of positive stuff but also brought tremendous suffering and

problems and perhaps will be the you know the end of us so technology might

have been you know the most terrible thing that ever happened to us who knows yes that beautiful no how can

people join the IDO portal movement if you have a website a hero portal calm

yeah IDO portal calm in phase B we are

on facebook you can find us on Facebook they do aim it or portal method a little

portal I do P ort al you can join the

movement culture on our website and that will lead to some updates coming up soon

you have a beautiful website by the way amazing thanks very much hey thank you very much I've been a very

fortunate to have a great team around me that helped me with that so I saw that you posted a couple of Tom

Waits songs and I even a Bukowski reference on your Facebook page and I

immediately understood something that I think only another fan or maybe I should

say student of weights and Bukowski can understand you don't shy away from the

strange and the profound wherever you can find it maybe that's how one way to put it is there a Tom Waits song that you find

yourself returning to often in your life mmm so many man so many is just that Tom

way you know I I can barely listen to anything else frankly it's been a real issue and Tom Tom Waits is his

discography it's like it it's a lifetime of discoveries I I it's been

accompanying me for years now it's not only it's not you know it's not something you go through very quickly

and I've spent a lot of time on Alice for example and

yeah just so much stuff so much that's always discover also you know I I find

this this weirdness this is eccentric part of things it's so important it's

the only thing really that can be you in many ways because as a culture

we're so blend we're becoming this one thing you know like you walk around in London it looks exactly like Hong Kong

it looks exactly like Tokyo it looks exactly like you know Sydney and we have

this like huge human thing going on which is great and we communicate very

easily but then we lost a lot on our our own stuff there is only one Tom Waits

you know because of his eccentric part because of his you know this this weird

genius and that's why it's so beautiful to me and then I try not to not to shy

away from my own eccentric side and when I was younger I I definitely hit that part more and you know you know guarded

and try to try to be to fit in but that's definitely an important thing I think so you recommend we cultivate the

weird yeah yeah cultivating the words cultivating yourself because that that's

truly you you know everybody's weird everybody there is no Homer Simpson you

know and everybody has this bar everything he'll everybody has this interesting stuff that's what also

interests me when I teach I want to see that weird you know I want to see that weird in your movement I want to see

that weird in you and that then I really met you but most people they hide it and

they don't a lie and they put this perfect picture but it well I'm not

stupid I know it's not perfect you know so just allow the weirdness to come out I think

it's a great lesson yeah from-from weights and and Bukovsky as well or so if you don't mind I'm gonna torture you

or something I would like to close by reading a Bukowski poem roll the dice

I want to force you to listen to it go ahead if you're going to try go all the way

otherwise don't even start this could mean losing girlfriends wives relative's

jobs maybe your mind it could be not eating for three or four days it could mean freezing on a park

bench it could mean jail it could mean division it could mean mockery isolation

isolation is the gift all the others are a test of your endurance of how much you

really want to do it and you'll do it despite rejection and the worst odds and it will be better than anything else you

can imagine if you're going to try go all the way there's no other feeling

like that you will be alone with the gods and the nights will flame with fire you will ride life straight to perfect

laughter it's the only good fight there is amen thanks al thanks for talking to

Daymond thank you so much man for

Summary:
The interview with Ido Portal, a prominent figure in the movement culture, offers a wealth of insights into his philosophy and approach to physical movement, teaching, and the human experience. Here are the key points from the interview:

1. **Holistic Approach to Movement**:
    
    - Ido Portal emphasizes a holistic, generalist approach to movement, transcending the confines of specialized disciplines like gymnastics, martial arts, or Olympic lifting. He believes movement is fundamental to human experience, preceding specialization.
2. **Journey to Generalism**:
    
    - Portal's journey began with early involvement in martial arts, evolving through various physical disciplines. His realization that movement is the common thread across all these led him to seek a generalist path, ultimately deciding to become a movement teacher himself.
3. **Challenges of Generalism**:
    
    - The quest to become a generalist in movement is an ongoing, perhaps impossible goal, which Portal embraces. It's a journey shared with his students, not a fixed destination or a set of absolute truths.
4. **Humanity and Specialization**:
    
    - Portal warns against the loss of humanity and fulfillment when one over-specializes. While specialization has driven technological and societal advancements, it often comes at the individual's expense, leading to a narrow, less fulfilled human experience.
5. **Movement and Community**:
    
    - Movement is not just a solitary activity but also a communal one. Portal values the communal aspect of movement seen in practices like yoga or martial arts, where people come together, forming communities and bonds.
6. **The Role of Improvisation**:
    
    - Improvisation is seen as the pinnacle of any discipline, including movement. It represents the ability to adapt, create, and live in the moment  an expression of the highest form of human ability.
7. **Approach to Coaching and Criticism**:
    
    - Portal believes in honesty and directness in coaching, distancing himself from the notion of 'tough love'. He emphasizes the importance of criticism and feedback for growth, acknowledging that not everyone is receptive to this approach.
8. **Movement and Adaptation**:
    
    - The human body is adaptable and should be exposed to a variety of movements and conditions. Over-specialization can lead to a plateau in performance. Exposure to a range of movements contributes to a broader base of skills and physical capabilities.
9. **Dealing with Injuries**:
    
    - Portal views injuries as opportunities for growth and adaptation. Rest is not always the best response to injury; instead, intelligent, adaptive movement can lead to better recovery and resilience.
10. **Diet and Individuality**:
    
    - Diet is highly individual. Portal follows a Paleolithic diet but acknowledges the need for resilience and adaptability in dietary habits. The relationship with food should be flexible and responsive to individual needs and environmental conditions.
11. **Technology and Humanity**:
    
    - Portal is cautious about the role of technology in human life, recognizing its immense benefits but also acknowledging the potential for it to lead to disconnect and even existential threats.
12. **Cultivating Individuality and Eccentricity**:
    
    - Embracing one's unique, 'weird' qualities is essential for true self-expression and fulfillment. Portal encourages embracing individual eccentricities, believing that they make us authentically human and contribute to genuine connections and learning.

The interview provides a rich tapestry of ideas on movement, teaching, human nature, and the importance of embracing a broad, adaptable, and deeply human approach to life and learning.

----------
