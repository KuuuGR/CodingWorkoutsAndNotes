
[The TWIML AI Podcast with Sam Charrington](https://www.youtube.com/@twimlai/videos)

-----
--99--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--98--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--97--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--96--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--95--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--94--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--93--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--92--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--91--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--90--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--89--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--88--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--87--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--86--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--85--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--84--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--83--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--82--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--81--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--80--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--79--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--78--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--77--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--76--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--75--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--74--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--73--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--72--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--71--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--70--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--69--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--68--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--67--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--66--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--65--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--64--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--63--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--62--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--61--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--60--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--59--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--58--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--57--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--56--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--55--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--54--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--53--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--52--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--51--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--50--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--49--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--48--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--47--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--46--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--45--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--44--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--43--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--42--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--41--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--40--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--39--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--38--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--37--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--36--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--35--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--34--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--33--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--32--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--31--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--30--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--29--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--28--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--27--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--26--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--25--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--24--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--23--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--22--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--21--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--20--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--19--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--18--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--17--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--16--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--15--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--14--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--13--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--12--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--11--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--10-- [The TWIML AI Podcast with Sam Charrington](https://www.youtube.com/@twimlai/videos)

-----
Date:
Link:
Transcription:

paste here

----------

-----
--09--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--08--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--07-- 

-----
Date:
Link:
Transcription:

paste here

----------

-----

--06--

-----
Date: 2017.03.01
Link: [Generating Labeled Training Data for Your ML/AI Models with Angie Hugeback - #6](https://www.youtube.com/watch?v=WhYrLA-fOK0)
Transcription:

My guest this time is Angie Hugeback, who is principal data scientist at Spare5. Spare5 helps customers generate the high-quality labeled training datasets that are so crucial to developing accurate machine learning models. In this show, Angie and I cover a ton of the real-world practicalities of generating training datasets. We talk through the challenges faced by folks that need to label training data, and how to develop a cohesive system for achieving performing the various labeling tasks youâ€™re likely to encounter. We discuss some of the ways that bias can creep into your training data and how to avoid that. And we explore the some of the popular 3rd party options that companies look at for scaling training data production, and how they differ. Spare5 has graciously sponsored this episode; you can learn more about them at spare5.com.


[Applause] [Music] hello everyone and welcome to another episode of twill talk the podcast where
I interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Carrington so I'm recording this intro in New York City
where I've been attending the O'Reilly AI and strata conferences I did a ton of
great interviews here at the events and I'm really looking forward to getting these posted over the next few weeks
today though I've got a show that I know you're going to really enjoy my guest
this time is Angie huge back who is principal data scientists that spare five a company focused on helping its
customers generate the high-quality training data sets that are so so crucial to developing accurate machine
learning models in this show Angie and I cover a bunch of the real-world practicalities of generating training
datasets we talk through the challenges faced by folks that need to label training data and how to develop a
cohesive system for performing the various labeling tasks that you're likely to encounter we discuss some of
the ways that bias can creep into your training data and how to avoid it and we explore some of the popular third-party
options that companies look at for scaling training data production and how they differ before we dive into the
interview though I really want to take a moment to acknowledge spear 5 who stepped up to sponsor this episode of
the show now I'm not going to spend time talking about their service here because Angie and I do cover that in the course
of the interview but I will say these three things first what spare 5 is doing
is really cool and if you have a training data problem and you know who you are if you do you should definitely
take a look at what they've got to offer as you explore your options second they've put together a great offer for
25 lucky twimble talk listeners will hear towards the end of the interview and third I'm just very grateful to
spear 5 for helping to make this podcast possible for all of you and I want to really encourage you all to show them
some love so please hit them up on Twitter there at spare 5sp a re
the number five and just thank them visit their website sign up for a demo all of these things let them know how
much you appreciate this podcast and their support for it as always I'll be
linking to Angie and the various things we mentioned on the show in the show notes which you'll be able to find at
twill a Icom slash talk slash 6 and now
on to the interview so hey everybody
welcome to another episode of twit Moll talk i've got Angie huge back the principal data scientists at spare five
on the line Angie why don't you say hi hi everyone hi Sam hey so happy to have you have you on
here today digging into some good stuff awesome awesome so why don't we get
Angies background
started by having you give us a little bit about your background and how you got started in machine learning yeah
sure so so I started out as a math major in college and I took a stats class I
was at the University of minnesota-duluth and I really fell in love with the idea that you know it was math but it was
applied and you could learn about the world around you you know through math so I I got really interested in
statistics I ended up getting my masters in PhD in statistics I got my PhD at the
University of Chicago and and when I came out of school so I had worked with
my PhD advisor was really big on teaching me how to creatively solve a
problem do you know creative algorithm development you know just start from the basics you know what are you trying to do and construct from there and I was
really interested in in doing that I had a strong interest in machine learning types of topics so when I came out of
school I had this idea you know that I want to work in machine learning but I want to do you know the creative algorithm development and trying to find
that and at the time you know the term data scientist didn't exist yet but
that's essentially what I was interested in doing um so it just took me some time from there to kind of blend between the
traditional definition of Titian and sort of the engineering end of machine learning and find a good
balance and this is where I landed awesome now what were some other kinds of problems that you were interested in in
Other grad school projects
grad school yeah statistician by looking into machine learning yeah sure
there was when I was doing my master's degree had a master's thesis problem that was really fun I was you know there's the game master mind where you
have the little colored pegs and someone has a code which is the an ordering of the of colored pegs and you're trying to
guess through making proposals of you know what you think the code might be and getting some feedback right and so I
had a lot of fun playing around with I ended up building like a metropolis Hastings important sampling style
algorithm to solve the game mastermind in a very limited number of steps and
you know the game is you know fairly straightforward when you're dealing with six pegs in the traditional sense but
then I was taking it up you know well what if it's 14 pegs or 50 pegs and the space of the problem becomes incredibly
complex and I was really interested in the metropolis Hastings algorithm is kind of like I stimulated a kneeling
algorithm where you're you're able to speak to explore a really high dimensional space very quickly and kind
of rapidly move around in this space and figure out where you're making progress and work toward an optimal point so so
that was fun I I also worked on a lot of problems in astronomy so astronomy was something I'd always
been interested in but never had a chance to learn in school so my advisor
was awesome mark coram he invited me to you know come on over into the astronomy
department at the University of Chicago and talk to the professors there and figure out what kinds of problems they were working on where I might be able to
help out so I did some work with quasars I got to do some work on some solar
science research got to do so I had a internship at NASA working on some solar
research continued to do some consulting with them for a while and and then the last project that always sticks out out
in my mind wasn't really part of my thesis work but when I was in my ph.d program Netflix announced they're not
slick prize competition which was it was a competition on predictive modeling to do
movie ratings right so so they released this publicly available data set and it
was all these pairs of a movie ID and a user ID and then a rating and then you
were supposed to be able to predict how certain users would would rate certain movies and I played around with that
problem for about six months that came up with a great solution and that I
would that was competitive in the in the contest but then through that I actually
came to a different understanding of what I what worked better in terms of
actually making recommendations so I built a movie recommender out of that and had it up on the web actually up
until about six months ago I was still using it to recommend movies for myself
but so those those those kinds of problems were the things that I was generally interested in Wow I went
through school mm-hmm so does that mean it takes you less time to pick a movie to watch then it takes
Movie recommendations
the rest of us yeah the best the best
feature was being able to combine two movies so you say you know I want to see something that's like you know I want to
watch a movie that's like Footloose meets fatal attraction or you know something like that and then it would
come up with some you know some great recommendations crossing between those two that was really fun oh nice you
mentioned a whole bunch of really interesting stuff in there yes I want to
I want to mention since you mentioned astronomy I don't know if you've had a chance to hear the last one we'll talk
about that I just posted is with Joshua Josh bloom who's a astronomy professor
at Berkeley and also the CTO of a company that uses machine learning I think you'll find it super interesting
yeah oh absolutely Thanks I mean you've given us an entre
Metropolis Hastings
to go deep kind of quickly here metropolis Hastings importance good
question I think it's you know it was sort of sitting in the in between the two fields and probably a little bit more in ml yeah so metropolis
Hastings is some is one thing important sampling is another yeah and so yeah so metropolis Hastings is it's really just
a optimization technique you know for you know maximizing a function in a high
dimensional space where rather than say you know following the derivative or
doing something more mathematical in that sense you use a random component so you say you start with a space in the
high dimensional field do you say okay this is my initial starting point and then you propose a point around there
that you may go to next and so you you come up with a proposal function so you
say okay maybe my proposal is I pick one of my dimensions at random and then I
perturb that that value a little bit with some random you know in some random
distance in some direction something like that and you say okay that's my proposed point and then you compare your
function on that proposed point to the function on the initial value and if
you're in a better place and you say oh yeah this is a better place to go to you'll you know you'll always move there
but if you're if it looks a little bit worse than your initial position you'll still move there with some small probability and so what that allows you
to do is it allows you to move away from you know local minima things like that that you might get stuck in yeah and and
it just it yeah in in many many problems it it provides a rapid way to search
through a very high dimensional space would it be fair to say that if your proposal function was was your slope in
What is a proposal
the N dimensional space that it metropolis Hastings with kind of approximate gradient descent um not
exactly know the proposal is always another it's it's basically a sampling you're gonna sample from the collection
of all possible points that you might evaluate okay so I mean I suppose I suppose you could create a proposal
function that says I always select the point that follows you know thus the slope but you know that sort of
thing it's good you come up with that but typically you yeah your proposal is supposed to have a random component and then there's the additional random
component that you may choose it even if it moves in the wrong direction okay nice so that that raises a question for
Data Science Philosophy
me coming from strong stats background
you know what how does how do you feel like this guide your perspective as a
data scientist data science has come to mean a whole ton of things for many it's
you know heavy programming for others it's heavy data engineering you obviously it's heavy stats have you do
you have kind of a philosophy on data science and kind of what that all means to you yeah I mean yeah definitely
definitely data Sciences is a big umbrella covering a lot of different things and you know I think those though the whole field is is evolving right and
you know and there's there's more and more applications in this area many more people going into this field yeah
so I'd say now there are a lot more people coming out of a computer science
background going into this type of work you know whereas back you know when I
was coming out of school I you know it's almost 50/50 it would like machine learning was really sitting in between statistics and computer science at least
that's the way that it was it at University of Chicago um yeah and I do feel like I think I have a I tend to
approach problems more from the predictive modeling viewpoint I I do a
lot with just constructing probabilities likelihood estimation things like that
that maybe wouldn't be as commonly used coming straight from a more computer science engineering machine learning
perspective where it may be more about deep learning and you know specific types of algorithms so so I guess I see
a little bit of a difference there but I you know and I and I have background in more traditional statistics you know
with just doing you know I don't also you know experimental design and doing
you know more more just classic kinds of testing issues and distribution comparisons and things like that but I
would say that's a small part of my daily work it's one of those you know yeah we do a/b testing we do things like that and
I'll participate in assisting with those kinds of experimental analysis but typically I'm doing more
yeah just constructing from probabilities from likelihoods you know working with predictive modeling things
like that to you know to solve our product goals other things that you see
commonly in the industry that you think would be different or approaches that
folks take that they might take differently if more people had a stats
background yeah I don't know I I see
them I see them blending and I see you know in people today that are coming out of you know strong computer science
programs with machine learning background really there's quite a bit of overlap in terms of you know the
materials that's been taught the skills that are there so yeah so I'm not sure not sure okay okay
What are you up to now
and so what are you up to now yeah so right so now I'm here it's fair five um
yeah and yeah so what we do here is four five we do we collect training data for
computer vision and natural language models so other companies that are building out AI building out their own
machine learning models and computer vision and natural language types of problems need really good labeled
training data in order to power you know the algorithms that they're trying to build and so that's what we do
so um I got really interested in this space because at my at my prior company
we started we were building out to natural language models there and we had some really good stuff it was working
really well but we wanted to push it to the next level and the thing that was preventing us was just getting that really good labeled
data um and so I was thinking a lot about that problem and then I heard that
Darren who's our CTO here I heard that he was at spare five and I heard about what they were doing and I just saw a
huge opportunity in terms of I was like you know you know AI is getting really
big machine learning it's getting really big you know it's no longer kind of a fringe thing that a few companies are trying out it's that you
know it's like to be in the in the space you know to be competitive companies need to be building building these
things out and as far as I can see the real bottleneck is in the training data so you know that was where I was excited
to to jump in and and be a part of that be a part of that business nice I think
Availability of training data
there's growing recognition to that the availability of training data is one of
the biggest issues that new entrants to you know the folks that are trying to
apply machine learning to various problems take on and you know for a lot of people they look at it and say and
I've heard this you know I've heard this coming from several different angles but you know something along the lines of
you know soon if not now it'll be very difficult for a start-up for example to
you know compete with Facebook or Google or you know large company and Industry
you know X because they'll have all the data and they are not well you know will
not be able to gather it and and label it and all that I do agree with that in
general oh yeah oh yeah absolutely absolutely I mean the way that I see you
know I think for quite a while though the focus was on the algorithms themselves and you know how do we get
better algorithms better algorithms but at this point you know we have so many sophisticated algorithms a very flexible
algorithms right for solving so many different types of problems that that really the defining factor becomes the
training data that you have underneath it to power it in terms of what you can actually do so yeah I think that's a really valid concern um although you
know I would say you know I mean it definitely depends on what you know what industry you're trying to jump into if
you have a start-up and they're trying to do something new I think yeah I definitely think you know if they have a
specific problem that they're trying to tackle that you know requires a very
specific type of data I think there are still a lot of opportunities to get into that space if you have access to right
there would be ability to get that get that label data that you need right which is which is where we come
okay so walk us through specifically what you guys are doing to help help
Helping companies
companies sure absolutely so I guess yeah so I would start by saying you know
when so when customers come to us you know typically the number the number one
thing that they're looking for they're looking for high quality data right and they need that data at scale and so and
I would say you know traditionally companies may you know initially they
may start trying to label that data in-house right and they may say you know everybody take a few hours and you know
look through this data add some labels that sort of thing and then quickly find out like okay this is gonna take forever and we don't have the resources and then
a next step that companies sometimes would go to is trying to use these you
know publicly available crowd sourcing things like Mechanical Turk where you know yeah there's a crowd out there
maybe you can you know put your data out to them and they can label it but that can be just incredibly painful in terms
of you know you never know the quality of the work that you're getting back it's like you you end up having to design an entire workflow around just
trying to QA the data that's coming back to you you end up having to send the data out many many times over again to
multiple different people and try to assemble and make some sense out of the results that you're getting back I'm so I can be a real headache so by the time
customers get to us so what we're doing instead is we handle all of that
headache for you so basically the customer comes to us what all they need to communicate to us is exactly what
correctly labeled labeled data constitutes to them right so so they
usually the customer will present us with some examples and you know these are some images here's some example
annotations that would be the correct annotations for these images that sort of thing and from there we do all the
heavy lifting in terms of we we can we will take on the task we will create and
generate that labeled data using our own community which is like a thoroughly vetted community we do all of the QA
and we guarantee the level of quality coming back to you and your data right so if you say these are the specs this
is exactly what correctly labeled data means to us we want ninety-five percent
of the data coming back to us to be correctly labeled to spec right then that's what that's what we can guarantee
and that's what we can provide and so yeah so definitely it's it's quality is
a major challenge that we're providing just speed and scale and then one other important piece is that we we provide we
can provide diversity among the annotators and so in particular if
there's a specific audience that the customer is interested in so say they're building an AI model and this AI model
is going to be used by you know women age you know 20 to 30 typically right in
the US we can target annotators from that audience so that you know the
keywords or whatever the the labels that they're providing are relevant and the types of things that that audience would
typically use which is really going to improve the performance for the models themselves you know in those types of
settings so I would say you know those are kinds of three three pillars of problems the customers have that that we
were able to solve for them okay let's come back to the diversity yeah because
Augmenting data
that's super interesting but even before you get there if I'm a company and I
want to solve a given problem in my industry do you are you able to help me
find the right data or augment the data that I do have with other data that might help me drive better
predictability yeah so we don't we don't um go out and find like publicly
available datasets for you you know we are in the business of generating the data but we definitely do augment data
so we've done data verification sometimes you know validation sometimes companies have already gone through you
know some steps of a process to assemble data on their own but they're hitting the point where they're realizing like the quality's just not there and what
they need you know what they need our community to help with is just going through and validating which ones are
correct which ones are incorrect so that they can increase the quality there we also we can definitely augment data you know
maybe they have images that have certain annotations maybe the images have tags
for objects that appear in the image but the customer now wants to identify where
in the image does that tag appear so the team might be dog and they need to know exactly where in the image the dog is
and you know we can have our community either do pixel level polygon annotation
around the dog in the image do a bounding box imitation around the dog of
the image those types of things so yeah there's there's a there's a really wide variety of things that we could do in
that sense okay how much of this is best thought of as a services or consulting
Services vs Platform
engagement versus some platform that you guys have built up that that automates a
ton of the you know the second work I mean it's really both and I guess it
depends on I mean the platform I think is really core and central to what we're doing and what we are able to do here
but I would say the consulting aspect on the front end of you know making sure we design the tasks precisely to you know
making sure the customer is gonna be very happy with the data that they receive and then it's going to do what they needed to do for their models is is
also absolutely essential so I would say you know we do have some customers that
come to us that that have you know teams that have been working on these these problems for a long time and they know
exactly what they want and they've already got an idea you know you know exactly how to kind of organize the
logic you know what what a correct annotation looks like and in those cases it can be relatively straightforward for
us to move that right into our platform in other cases you know we've got customers that you know they know the
types of models they want to build they've been struggling and they may even want you know some initial
consulting on you know what what types of data are available to us you know what can we do to to improve you know
improve the the quality of their own models and we can we can do some initial consulting there as well so it really
just defined depends on the customer but but basically you know that yeah the consulting aspect is is all setting up
all the work making sure we design the task correctly going through an iterative you know process in the
beginning with the customer where you know we say okay we think we understand your specs you know we've designed two
tasks the way we think will work we run you know maybe we do a thousand annotations for the customer return that
back to them and they verify yes this is what we're looking for you know yeah you're you know yeah the annotators are understanding
the tasking and this looks good before we go to scale okay so you mentioned a
Crowdsourcing
bunch of things that customers have typically tried before they come to yeah
are you doing all those things as well plus some other things like for example you know farming the labeling out to
multiple people and doing some kind of quorum or voting or something like that right so so no so yeah so we we are not
doing crowdsourcing so first of all we don't use Mechanical Turk we don't use any external community we use our own
community through our spare five app through the web that are you know
everyone is fully vetted we have great detailed information on our users we get Facebook and LinkedIn data from our
users we do lots of survey and skill assessments we're continuously monitoring their they're tasking
behaviors in real time monitoring the quality of you know of the tasks that are being submitted and so but we are
not crowdsourcing so on the flip side what we do is we get really really good
at understanding the quality of the work that the that the community members are able to provide so that we are targeting
we're targeting the right users from the beginning right and then so we've got
predictive models in place to identify when a new task comes in we can identify who are the users that we believe are
most likely to do well on that task and so we can initially target the right
subset of our community then we turn the task on and we have a real time
monitoring system so we we turn on that process and as soon as the task is live all of that
real-time monitoring is feeding back into our predictive models for quality assessment on the user and so we're
making real-time decisions about when to potentially remove access to a
particular to a particularly for a particular user because we're not seeing the level of quality that we need and so
we have a kind of that user quality model running in real time and then we
also have an additional layer which is an answer quality model so depending on the task for some for some task types
there are other types of data and information available just based on the
answer that itself that's provided and so we have that additional layer just to make sure that the answer itself is is
meeting our quality bar okay that's interesting my initial reaction was uh sounds like crowdsourcing but yeah you
know semantics here but it sounds like the key distinction your that you guys would make is that you know with
crowdsourcing you kind of put the task out there and anyone can kind of take it and what you guys are doing is you know
targeting it to specific people yeah developed a relationship with over time
is that the right way to think about it that's true and I think you know it is a little bit semantics and just culturally
how how people use the terms but I think that crowdsourcing often connotates that
you're going to send a question to multiple users and then assemble a correct answer from those multiple users
right and so that's that's kind of the main distinction that I made so a lot of you know oftentimes if we're talking to
customers especially if they've already got a lot of experience themselves working with Mechanical Turk they're really interested in you know well what
are the metrics you're using to decide whether you have enough consensus on a specific answer to move forward and how
many users do you need to ask a question and that's for thing and in our in our setting it's actually irrelevant that's
not that's not the approach we use that's not the perspective that we follow okay so is it fair then to think
Mechanical Turk vs CrowdFlower
about this space as like Mechanical Turk is an API on top of the people but you
have to build everything you're yes figuring enough figuring out how to get them your test you're
figuring out how to do all this voting stuff so that you can get decent quality data whatever you're writing yeah you're
writing the instructions you're doing yeah you're filtering you know which users are you gonna use try and trying
to track their quality all those things yourself right and then CrowdFlower who I think a few months ago announced some
specific as some specific offerings around labeling that I covered on the podcast like they're kind of taking they
actually originally were on Mechanical Turk but I think that's like their own community now and they they're kind of a
slightly higher level of abstraction that's doing a little bit more of the
stuff but still fundamentally this we're gonna take the task and push it to a bunch of people and that's great that's
the results and you guys are you know we're gonna look at your problem and
design a solution to get you quality data and you know that's exactly right
okay and can you talk a little bit about you know as a statistician like what are
Challenges as a statistician
some of the interesting problems that you've you've come up against helping to
build this for four spare five yeah I think I mean the the most interesting
and challenging problem for me was just how how do we design a system in a
platform that can work in a general sense at scale so you know when I would
not when I started out you know we were still doing there was still a component
of manual review internally just to make sure the various processes that we had in place were working as expected and we
had a lot of tailored you know for this task type we managed it in this way for this task type of management in this way
and so when I came in that was one of my initial goals was how do i how do i come
to understand this entire system with all the complexity for so many different types of tasks and we had objective
tasks we have subjective tasks we're looking at images we're looking at text we're doing all these different types of
problems how do we how do we develop a cohesive system
to attack and address all of these different task types and ensure the right level of quality so that was that
was really exciting for me and the you know that's been the focus you know over
the last several months and and now we're there and and so that to me is has just been a really exciting
accomplishment that sounds pretty huge can you talk or can you talk to whatever
Tech stack
level of detail you can oh sorry can you
could you clarify the last part oh you're you know data science pipeline slash technology stack slash you know
kind of anything that can help folks get a sense for you know as they're building
labeling platforms what are some of the things that they need to consider sure
well so I can tell you just for the tech stack that we have here so on the back end we're using Ruby
we use our Postgres and then we have a lot of different AWS services and then
on the front end we have a web client as well as a native iOS application and
yeah and so let's see yeah so I think
that there's a little bit sorry maybe you can give me a little more guidance on exactly what direction you want oh
yeah so from so that that is very helpful on understanding the tech
platform it sounds like you guys are taking advantage of the cloud in AWS in particular as you you know when you
thought about this challenge of oK we've got all these different types of data
how do we unify this into a single platform that eliminates like a lot of
the manual steps that you described are there any lessons that you've learned
about building data science pipelines that helped you achieve that goal that
you think would be transferable to other people yeah and I don't know that this is specific to data labeling I would say
I would say one thing that I've learned that that's worked really well both here and
in previous companies that I've been in in terms of integrating data science with the existing tag an existing
product is I think what's what's really essential is you want you want your data
scientists to be focused on prototyping like rapid prototyping you want you want
them to be really nimble in terms of you know hey if something about the product changes next week we want to be able to
like dig into the guts of our models make our changes really quickly and be able to push that back out into
production in a seamless way and and you don't want your data scientists to have
to be spending the majority of their time you know maintaining and these larger systems and really having to do
having to be so focused on the engineering side in terms of you know let's make sure everything's staying up
and stable and doing what it needs to be doing so so one solution in the team that I led previously at my previous
company one solution that we came to which worked really well it was we developed so we still wanted our data
scientist to be able to to prototype as quickly as possible so I would say you
know our is fantastic in terms of prototyping you have your you know the graphics and visualization component is
you know on it was unsurpassed by any other software you know you have Python
has lots of packages that are fantastic you can definitely do some good data visualization there but our team was
focused on our and so we wanted our our teams to be able to to do their modeling
and we had a lot of predictive modeling kinds of work going on there we wanted our data scientist to be able to do the
predictive modeling in our and hand off the actual model component to the larger
system in such a way that you know it was sort of a you know plug it in so
that you have you know these are the inputs coming into the model these are the outputs coming out of our and we
want that we want to be able to just take that that chunk of code that is the
model pass that over into production and and get that plugged in and going at scale and and that you know what what
worked really well what we've done in both situations is we used actually used our serve it doesn't
have a fabulous amount of documentation out there but it works really well and there's a yeah so anyway so so building
out a system with our sir building out some software around that to allow kind
of just a kind of input-output portion to be handed over and so that so that you can have you know other standard
systems you know picking it up and hitting your models and you know moving that all into the cloud so that you can you know if you start getting a lot more
volume hitting your model than you originally anticipated right you just spin up some additional clusters and manage that traffic is our serve
open-source yes mm-hmm okay so what I think what I'm hearing is
that you've got the the exploration and
the model development that's all happening in our and then you're able to take those models and essentially deploy
them into our serve and use that for prediction as opposed to having to throw
that over to an engineering group - exactly exactly exactly and and what I
would say to is I mean it really depends on the algorithm that you come up with if you're prototyping in at the end you end up using something that's
mathematically very simple then you know then maybe you just pass along you know
your pseudocode you know what I mean I'm like okay these are the steps you have that reimplemented in a faster language
but but if you're using you know there there are many many fantastic predictive
modeling packages available directly and are and if you if you get your system set up correctly you know you know we
were using and we had predictive models with you know thousand features in in
real time returning results in about 100 milliseconds right coming straight out of our if all your hitting our for is
the actual modeling component it once it's already constructed so you know and I think and I think keeping the bottle
code in our and that sense just makes it all that much easier for you know any modifications down the line that needs
to be made and you know and and I think another layer that you can out on top of
that you know which can really well is is if you can start to integrate some automated model
rebuilding mechanisms right so you've got got one system going on that's pulling in new data continuously
updating your models and then you've got another system that's just plugging into the existing most current model to
actually get the results that can work really long it's interesting I'm glad you raised that I was thinking about
Model Drift
that as well and if you what you've done to address like model drift over time
and if you've been a build in like 360-degree feedback loops that kind of thing yeah so just some what I haven't I
haven't gone too deep on those sorts of things so what well I have some tricks that I like to use in terms of kind of
monitor model model monitoring you are doing automatic updates and things and so there's various kinds of sanity
checks there's a there's a paper out of Microsoft that was just fantastic in terms of like these are all the things
that can go wrong when you put your model on autopilot and right and these are the things that you need to be
monitoring and so with just as they're like seven step-by-step suggestions in terms of things that you might want to
get implemented and that's fantastic I can I can try and pull up that paper
for you oh that would be amazing that sounds like a great resource yes I
Computer Vision Natural Language
believe it was in the last couple of years yeah I've got to look it up just
making it up yep and then you guys are
focused on computer vision and natural language is that right yeah that's right
that's our focus right now and how do those domains influence the approach
you've taken um yeah that's an interesting question I mean so so clearly with computer vision we're
dealing with with image data right and so so I think it you know it's had a
huge influence in terms of the the types of tooling that we're building out to allow our users to you know correctly
annotate the data you know our our end resulting data is only as good as the
tooling allows for user accuracy right so so it's
definitely had a major influence in yeah the the types of tasks the types of tooling things like that that we've been
designing um but but I would say at this point in the you know in the in the
general system for QA that we've constructed that that is general it's
not it's not specific in those areas but what we would like to do continue I
guess delving into is internally you know what can we do in terms of natural
language and computer vision modeling ourselves internally to improve the quality of the results itself and and
and that's something that we're just now starting to dive into okay so in
Deep Learning
providing the services that you guys were provide are you needing to get into
things like deep learning and other things or are these more the things that
the customers would use to train to train with the data that you're
providing yeah that's right so so we we purely handle delivery of the training
data so we can do we can do some consulting in terms of yeah you know if you use data like this you know this
this is how you know that may affect the models but it's religious more of a consulting aspect we're not doing any of the model training ourselves we're not
hosting any models nothing like that okay okay interesting do you have a set of you
Training Data
know they're like a top three list of things that you would want everyone to
know about training data oh let's see
yeah I think yeah I mean I think anyone who's got you know any experience in the
field knows that you know your model is only as good as your training data if you're if your training data really only
represents you know a subset a specific subset of the space in which you expect
your model to to function then you know you're gonna have you're definitely
gonna have low accuracy in in areas where you haven't provided as much training data
um so definitely you need good good coverage in terms of you know whatever whatever the inputs that you're going to
be anticipating that will be coming into this final model let you build you want to make sure that your training data
represents those inputs as closely as possible in order to get the best results okay one more one more well and
I think I guess one interesting thing to think about is it you know it depends I
think in the in the computer vision and natural language applications that were focused on the quality of the data data
is really essential but in in other in
other areas there are situations where you can get away with you know less with
lower quality training data and the model is you know as long as the as long as the patterns are there and the
patterns are present you know your model model may still be able to pick up those patterns right and so I guess just
keeping in mind what level of quality is important or essential for the type of
model that you're building the type of methods methods that you're using you know there can be some variation there
okay how would you characterize the scenarios in which you can get away with the lower quality training data um I
Lower Quality Training Data
would say definitely when you're when your model is more about summarizing and
generalizing the data then you know having a few odd observations and there
isn't going to dramatically affect you know effect that that summary um are
there any sequels that come to mind of customer basics let's see um well I'm I
Customer Examples
know there's some well that was so here's an interesting examples a little bit on the edge of what you're talking
about here so so sentient is is a customer of ours the that is there a
provider of AI we have a task that we've been running for them for quite some
time now what they're interested in understanding is user perception of
similarities between shoes so so they're building out you know that you know they
have models that they're building out that are trying to decide what shoes belong together not from some specific
taxonomy or you know hierarchy that some human person wrote down but you know like Oh first you go by color and then
by size about nothing like that what they want to understand is what a human person looking at a collection of shoes
you know if you say here's one pair of shoes now look at these other ten pair of shoes which one is most similar and
they're really trying to understand the human's perspective of you know which of these ten shoes do you think is similar
so it's a very ambiguous task it doesn't actually have a right or a wrong answer but but when we throw this task out to
our users and we return the data back to sentient they are able they've had very good success in terms of improving their
models at which you know in terms of what types of results they're able to surface you know as a result of getting
a very deep understanding of what similarity means at a human level so you
know there are many many varying degrees of you know of going you know the ambiguity versus you know essentially a
very precise you know correct answer and and it just absolutely depends on the model that you're trying to build
exactly what you okay and that example are they ultimately trying to make
Other Use Cases
recommendations or do something I believe so I believe so okay and I feel
like I should have asked you about customer examples are there other
interesting kind of use cases that you guys have taken on there sure yeah well
so here's one we did recently this was kind of different for us and it was a lot of fun so there's a company called in it in it AI and they're building AI
chat bot technology and so they they're interested in building these chat BOTS
in specific contexts and so they really they need conversation data like they
need you know text of conversations in these contexts and they're having a difficult time going out and you know
finding that data publicly all things like that and so um so we were talking with them um you know and
you know we've already typically done lots of categorization of text we will do what's called like aspect opinion
linking of taxi of various kinds of and I'll tasks but what what we ended up
doing for in it was we actually helped them produce the conversations and then
took those conversations and labeled the data the text of those conversations so we actually ended up designing a task
that we put out to our community which was you know pretend you're selling flowers and you know and now you'd be
the be the person selling flowers and then to another user we're saying you know pretend you're buying flowers and maybe we you know give some guidance
things that the customer is interested in learning more about and we actually send this in the tasks back and forth to
collect a complete conversation um yes I think we assembled something I forget
our there was something like ten thousand conversations that we assembled and then for each of those conversations we went back and we did the labeling
that they needed in order to understand the content of what's going on you know in those conversations to help train the
AI models that they're building so that was that was fun that was fun for us in terms of just designing it out but it
was also fun for the community we got so much feedback from people saying like I love this I could do this all day you
know because you're just and the conversations were fantastic so I mean it was it was really it was a lot of fun
that reminds me of someone had a month ago or so set up basically these three
Diversity
kind of conversational chat BOTS you know in a chat room and they were talking and when they got stuck he just
throw out some random thing I don't know
that I'll be able to find that but if I can I'll stick it in the show notes so
one of you talk one of your top three things was subsetting the space which
reminded me that we needed to talk about this diverse any point oh right I want you to kind of
start us off in this discussion with you know the examples that come to mind for you of like where it's been done really
poorly well well actually I have an example this is something we actually
had an article in TechCrunch about this recently so we've been thinking about this a lot just since you know it's
obviously the benefit that we can provide to our customers to actually get the diversity and the you know the community that they need but so we did a
little experiment in-house so we started thinking about the question of you know how different are the results like how
different does the training data look if you're sampling you know various types of populations and so you know and and
it turns out we really didn't have to dig very deep so one of the first datasets that I went to analyzed we had
we have a task that we had put out to our users just as sort of a fun mental
break once in a while which was called rape the puppies and so we just show you pictures of puppies and then you rate
them from one to five stars and you know and you know cute or okay maybe not so
cute and and so we'd collected we've been collecting that data actually over over quite a long period of time just
you know a few puppies to rate you know here and there for our users and so the first thing I thought was okay let me
just take a look at the data and see how the ratings differ by gender and so I split the data by gender and it was just
dramatic and obvious difference that the women were rating the puppies as tutor
consistently across the board across all puppies and with and there was a wider
gap on the cuter end of the spectrum and the gap was more narrow on the not so cute end of the spectrum but it was but
it was still there and yeah it was just striking right and you know it's a simple example and you know okay well
how does it matter you know how cute the women are the better rating the puppy is okay probably not but it's such a it's
such a clear example of the difference differences that you can get in the
training data itself right just by sampling a different population and of course the training data is what's
guiding your model in terms of that output that's coming up together end mmm-hmm yeah but yeah so it's art that's article
in TechCrunch we can put a link to that article as well okay nice nice and so do
Challenges
you run into any challenges and in identifying diverse communities to to
target like what are the challenges generally they come come up for you guys in trying to solve this problem for
people sure so each of our customers is gonna have you know their own their own
demographic that they're that they are targeting right and so at this point we
have a broad international community we have many many users in the u.s. we also have many you have a good presence
across just internationally and we have good data in terms of who our users are
because we put out surveys to collect demographic information we put up you
know many different surveys through our users you know that they're compensated for for completing these surveys to be able to collect that information so we
have a good understanding of our our current user base which is you know which is very diverse but we we
occasionally still have a customer come in and ask for something very specific that we haven't targeted before so they
potentially could say you know we have this task and we need who knows we need
experts in bird identification to label these images of birds you know and tell
us precisely what species of bird you know this is you know something something like that right and so when that comes up you know we can first of
all we can go out and survey our community and find out you know in our in our broad community do we have people who are able able to do this type of
classification already we can identify them but and if we find that we don't have enough members in the community yet
to meet the velocity needs or you know the volume needs for the customer then
we can go out and do specific targeting to bring like you know online marketing to bring those individuals into our
community and we've had good success with that okay so we've talked about demographic targeting primarily thus far
have you ever done anything with psychographic targeting like I don't
know for whatever reason I'm thinking hey we want this to be answered by Terry Briggs ENTJ
absolutely we've definitely talked about it there's yeah there's you know there
are all sorts of interesting just personality profiles out there and that's what's kind of lovely is because we do have this stable community but
that's working with us is if we put a survey out to the community right you know and we compensate them for their
time in completing that survey we can get any any data that we need so it's actually it's actually very easy for us
to target the user that the customer is interested in okay okay this is a little bit of an aside but I often think that
Uber Ratings
in the example of like uber ratings you know I think that there's probably well
not probably there's you know there are you know for average Raiders and there are three average Raiders like you know
hard graders and easy graders I don't have the impression that you know an uber for example would normalize you
know a person's rating against their average rating has a strict
interpretation right three stars four stars that's what the user said yeah dude is it does anyone do something like
that or you know how towhat it agreed it to what degree to your knowledge to
folks think about that in you know thinking about like rating schemes and what's the current kind of thinking in
the industry around that kind of stuff yeah so yeah this is a little outside my my area it's totally really yeah I was
working on the Netflix prize competition because in that case have the 1 to 5 stars rating system and so you know so I
went really deep and it's just an understanding like you know what are these different user profiles or you know what types of users are out there
and you know one of these distributions generally look like and yeah it's like on a 1 to 5 star rating system you basically get ones fours and fives and
occasionally a three you almost never get a two and you know so so I have some
interesting too but you know your thoughts about that in general but you know it currently at spare five from
from our perspective you know we occasionally do ratings tasks it's not
one of the it's not one of the common customer requirements and we'll talk with the customer
whether you know is a is a binary answer you're gonna be more informative for them or a three-level and services a
five-star answer and and so we do have some experience in background and thinking about what type of data is
going to come out of those different kinds of rating systems and you know and besides just the rating system itself
you know the wording of the question is so important in terms of you know you know the words that you use if you if
you have a three-layer system do you say you know perfect okay terrible you know
or do you make it more you know more nuanced right and you're gonna get you're gonna get a different data based
on different wording that you use which thing yeah which actually that too just to segue
into you know another like a whole whole another area of things that we are thinking about constant we here at
square five is just just in the the wording and the framing of of the
question it is just such an essential piece of what we do here whether you know even if it's not a rating question
even if it's a totally open-ended you know writing writing captions for an image or free text keywords versus you
know very objective taxonomy categorization and things like that the wording of the question makes all the
difference right and so in our case you know we actually take our users through or there's a there's a complete process
so when when a new task comes in right where we're iterating with the customer trying to design the task when we start
preparing our users to complete that task we will typically put up a tutorial
we'll start with a tutorial so the users just working through the tutorial they can work through it as many times as they want it's giving them direct
feedback on whether they're you know doing what's appropriate for the task the next stage is a qualifier which is
more like a quiz you don't get the feedback as you're doing it at the end you find out if you passed or not and
typically we won't allow users to continue into the task without completing the qualifier right and so so
we have instructions for the task that we're writing we have the tutorial that we're writing we have a qualifier that we're writing and then we have to task
itself from the questions that we're designing inside the task right and all
of the wording logic the orientation and design of that information on the page it is
all part of the you know the whole formula of how do you get the right data
coming out at the end it's oh so that's that's a really important piece that I think people that are new to this field
are just finding out about it for the first time don't realize what an intense amount of work and thought and effort
goes into getting that right that's really interesting to what degree are you relying on or do
Research
you have the benefit of relying on other folks research to figure some of this
out or is it all empirical analysis on your part great so there is definitely a
lot of great learning already out there you know that's been published so we we
have Dan weld is a computer science professor at UW and he consults with us
regularly he's been fantastic he's a crowdsourcing expert and he's been
fantastic about pointing us to you know all the good research out there about different things that have been tried in
terms of yeah in terms of instructions and tutorials a designing tasks and all those kinds of things so so there's
definitely a lot of learning there but I would say you know given given that you
know we are working in a somewhat this is somewhat different space and that we
are not doing traditional crowdsourcing we are not you know farming answers questions out to multiple users at a
time and so there has been a lot of just individualized learning on our part in terms of how do we work with our users
and what level are our our users at and you know how do how do we how do we bring them through the training process
to get them to the level that we need so there's definitely a lot of internal learning and I would say you know each
time we do another task you know each you know we have certain task types that we've done again and again and again at
this point and we're learning each time we do them along the way how to make refinements how to optimize that process you know how to make it even more clear
anything that we can do to me to make the instructions more clear it just saves in terms of efficiency because we
have that many more answers coming through they're actually accepted and allowed to the deliverable for the
customer nice nice I think we're coming up on an hour anything else you'd like to share with
the audience yes absolutely well I would say yeah if anyone is interested in
getting in touch with us you know there's a couple of ways to get in touch you can always head over to spare five com you can also email me directly I'm
Angie at spare five comm and I guess one other thing that we wanted to let the
listeners know about so we have a blog series that we started a couple of months ago it's called conversations in
machine learning and it's just all about any interesting new applications in AI
and ml things you know that are popping up all over at various companies that we're watching in this space and for
your listeners were offering a fantastically fabulous spare five t-shirt to the first 25 people who
subscribe to the blog series and if anyone's interested they can sign up it's a spare five comm slash podcast
well that's awesome that's awesome and I'll include a link to that of course in the show notes ok wonderful thing
awesome well thanks so much Angie this has been a great conversation and I really enjoyed it
catch you next time thank you yeah absolutely thanks Sam all right thanks bye
[Music] alright everyone that's it for today's
show thanks so much for listening and thanks once again to spare 5 for
sponsoring the show please don't forget to sign up for their t-shirt offer at
spare 5 comm slash podcast and of course we both want to hear your feedback on
Twitter I'm at twiddle a I Twi MLA I and
spare 5 is simply at spare 5 reach out to us and let us know what you thought about the conversation thanks so much
for your continued support and catch you next time

----------

-----
--05--

-----
Date:
Link: [# Machine Learning for the Stars & Productizing AI with Joshua Bloom - #5](https://www.youtube.com/watch?v=B6gbGB_fWkM)
Transcription:

My guest this time is Joshua Bloom. Josh is professor of astronomy at the University of California, Berkeley and co-founder and Chief Technology Officer of machine learning startup Wise.io. In this wide-ranging interview youâ€™ll learn how Josh and his research group at Berkeley pioneered the use of machine learning for the analysis of images from robotic infrared telescopes. We discuss the founding of his company, Wise.io, which uses machine learning to help customers deliver better customer support. That wasnâ€™t where the company started though, and youâ€™ll hear why and how they evolved to serve this market. We talk about his companyâ€™s technology stack and data science pipeline in fair detail, and discuss some of the key technology decisions theyâ€™ve made in building their product. We also discuss some interesting open research challenges in machine learning and AI.

hello everyone and welcome to another episode of twiddle talk the podcast
where I interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Carrington I think you all are really going to get a kick out
of this show my guest this time is Joshua Blum professor of astronomy at
the University of California Berkeley and co-founder and chief technology officer of machine learning startup wise
i/o I was in California last week and Josh graciously agreed to host me in his
company's office for this interview we had a wonderful discussion and as you
might have guessed if you happen to have noticed the length of this episode we covered quite a lot of ground but I
promise you that you'll find this 84 minute interview to be jam-packed with great information ideas and war stories
in this show you'll learn how Josh and his research group at Berkeley pioneered the use of machine learning for the
analysis of images from robotic infrared telescopes we talk extensively about the
challenges they faced in doing this and some of the results they achieved we also discussed the founding of his
company why's that IO which uses machine learning to help customers deliver better customer support but that wasn't
where the companies started and you hear why and how they evolved to serve that market we talk about his company's
technology stack and data science pipeline in fair detail and discuss some of the key technology decisions they've
made in building their product we also discuss some interesting open research challenges in machine learning and AI of
course I'll be linking to Josh and the various things we mentioned on the show in the show notes which you'll be able
to find at twimble a i.com slash talk slash 5 that's Twi ml
AI comm slash ta LK slashed the number 5
and now on to the interview [Music]
hey everyone I am here at the wise that I owe offices with CTO Joshua Blum and
Joshua Bloom Introduction
we got a great conversation lined up for you and we'll start with Josh why don't you introduce yourself to the audience
here great so this is Josh and I am CTO and one of the cofounders of Waze IO I'm
also a professor at UC Berkeley in the Astronomy Department one of the I think
important things that we'll touch on today is how does somebody go from astronomy and teaching to building a AI
application company it's I think a big part of the origin story of of course so
the company is is is my history but I think it also has some interesting
lessons for how we think about AI in production systems and and why having
diverse backgrounds is is pretty important these days yeah that's a lot
Joshua Bloom Background
of good stuff to talk about there why don't we start by learning a little bit more about you and your background and
how you got to where you are so I was trained as a physicist and an astronomer
went to Harvard as an undergrad and caught a bit of the research bug over
over the summer was working in Los Alamos then went to Cambridge England to
do a master's and back to Caltech where I did my PhD all in the context of
astronomy and then back to Harvard where I was a postdoc and all the while
working on what we could broadly term time-domain astrophysics understanding
the variable sky and why things do what they do explosively cataclysmically or otherwise
and while there's a deep interest in understanding the origins of those
events and how they're connected to other things that we study in the universe
I got more and more interested over time in the informatics of actually just
doing the the science statistics on variable sources in the presence of noise and then as I became a
faculty member at Berkeley started looking ahead to really a series of new
surveys particularly imaging surveys of large swaths of the night sky and one of
the great interests for myself and many others was in finding new events essentially new explosions or a new
variable erupt of stars that hadn't been known about before and doing that as quickly as possible now the traditional
way in which that was done and in some cases is still done today is that as you
acquire more data you linearly hire more grad students that scales with the total
number of the images that you're getting and you need to sift through and as I
was becoming involved in some of those projects we'll end up realizing that as time went on that really wouldn't scale
anymore and we we needed to find effectively a replacement for domain
experts who otherwise would have been looking at an ax pining on data using
alternate techniques and about 79 years ago I stumbled upon machine learning as
a real interesting potential Avenue and at the time machine learning really
hadn't been applied to anything in astronomy 'men the variable sky sort of
in a time domain context there had been a number of studies in using machine learning to do special types of
inference on the static sky and understanding you know sort of
demographics of stars and galaxies and their distribution in space so we really
felt like there wasn't a lot of precedent in us applying some of the
capabilities to this data but we wound up realizing it was sort of an imperative and one of the things that
people who know astronomers would probably say about them is they tend to like to use tools that help them and
seek those tools out be those new types of detectors so seats see CDs for
instance we're something that astronomers adopted almost as soon as they were invented and
obviously statistical techniques and computational techniques astronomers are willing to try things out to solve to
solve their problem that the classic example I go back to is Galileo who said you know hey there's this new thing
that's been invented to look at the horizon for ships coming towards us what
if I just took it and pointed it to the Stars what could I do with that and so our use of the telescope was essentially
a co-option of the use of a technology that had been built for other purposes and so that kind of precedes a pace
throughout the history of astronomy and so the idea of bringing a essentially a fairly new technique into the fold is
not at all unusual do you remember how you stumbled across machine learning yeah so part of it was just asking the
Discovering Machine Learning
question if I've got a if I've got a bunch of data and I need to decide you
know is this part of the sky interesting or not as is this event new or not what what type of event could this be very
quickly you wind up realizing this is a classification problem of some sort and
you know talking to people at UC Berkeley and the stats Department as I was starting to introduce some of these
interesting challenges became very clear that machine learning and particular
particularly supervised learning would be a you know a fertile ground for us to
start exploring but one of the challenges that I saw is that even though we're in a very rich and fertile
environment in at UC Berkeley and there's a lot of crosstalk between departments and individuals within
departments it was very hard to get even the kind of the language on both sides
up and running where both sides understood the methodological folks who deeply understood what machine learning
was and what it could be used for and then people like myself from the physical side of even learning to ask
the right questions so thankfully wound up getting a group from the stats
department and and those folks from computer science together with me and my my postdocs and
we were able to get a proposal together the National Science foundation-funded that allowed us to basically start
building out new ways of doing inference on on astronomy data that turned out to
be a very fruitful place for us for me in particular to learn about the landscape of what other techniques were
out there that we hadn't been taught in school mm-hmm can you tell us a little bit about from that initial discovery
Research Arc
what the research arc looked like what were some of the first things you started exploring and how that evolved over time yeah so I really just started
looking at you know toy amounts of data that we already had in the can and we
could start applying these different techniques to and looking for tools that would be useful for us and really the
best thing out there the time that we started with something called Weka which was a and still is a collection of
machine learning algorithms that one can apply in a sort of GUI graphical way all
kind of written in Java and really that was our original playground and
benchmark and used that as a launching off point to start understanding what are these different modeling techniques
that are being exposed here what is a support vector machine what does it mean when people say random forests and use
that as a way to sort of educate myself and those in our group I mean started seeing some interesting results right we
started seeing accuracies that were better than what you could get from random and then as we poked farther and farther wound up seeing how far can we
take these algorithms how well does one of them work relative to the others to
get the kinds of answers that we want how do we build in a loss function which turns out to be very important to get
good answers because in the case of what we did when we're discovering something
in the sky it's not easy to articulate that loss function and and and by that I
mean what is the cost of missing an interesting place in the sky it means
that you don't get to do new science versus what's the cost of saying everything in the sky
is interesting which means you burn all of your follow-up resources and starting
then to think about context-aware classification now just not in the in
the context of really just resources but now time constraints making making an
inference about something that could be of interest may be more important than
waiting to get another couple of data points and saying something with even more confidence so understanding how to
calibrate confidences and probabilities doing this in the presence of sort of
missing data and irregularly sampled data and time all of these also started
wind up showing to us that there were parts of the machine learning sphere in
the at least in the academic world that were not often exposed to the kinds of
data that we were exposed to and so noisy data for instance now whenever
when they talk about the iris dataset they don't say you know the petal of this is red plus or minus purple so even
just having uncertainties in your features let alone your labels became an interesting challenge and we wanted
realizing perhaps there were some new techniques that we needed to start innovating on to even do the kinds of
inference we wanted to do with our data so you mentioned the go loss function
Loss Function
and needing to wrap your arms around what that means can you succinctly
describe how you grappled that what did you end up how did you approach it and what did you end up coming up with for
the types of data that you were looking at one of the things we wind up realizing is that one person's loss
function is not the same as another person's loss function and so to get traction on on your answers one needs to
at least be clear about what it is that you're optimizing for and at least give
people the ability to imbue their own loss functions if for instance you're producing a catalog of different types
of variable stars on the sky we have a specific notion of what it means to get
something wrong about saying very minority class versus a majority class and I wouldn't say that we solved
that problem by any stretch but at least we were trying to be clear about what our assumptions were of the loss
function and articulate you know what it is that we're optimizing for you know
when people are doing AI or machine learning in a production environment there is always going to be an
optimization of some sort and the typical one people will go to without
knowing exactly what the kind of business value is or scientific value is
of the answer is you go for some notion of an accuracy and then when you get a level deeper and that you say well what
I really want to do is I want to minimize false positives at a false negative rate of 0.1 and then that is an
implicit statement of what your of what your loss function is and you hope that
by defining it that way and by optimizing on it that way that you're
actually getting very close to an optimization of what is you know the result of what you're what you're
emitting out of your out of your modeling mm-hmm and so you're you're
Other Fields
primarily looking at image oriented data over time are there other fields where you've seen
them adopt the same types of approaches to what you were working with well one
of the nice things is you can work at the sensor level data which is effectively photoelectrons in a CCD and
counting those up as a function of position and X Y and then trying to map that back in the sky so that's what you
might call noisy image sensor data and we worked at that level but then we also worked at a metadata level which was now
let's use traditional astronomy techniques to extract the brightness of a star as a function of time and so we
got ourselves out of the out of the image plane and into the time domain and
then we were basically working with effectively tabular data and again you
know there are lots of different models and feature engineering approaches that one can take to all of that I wouldn't
say that there was a common thread in our work other this across a bunch of these different sort of sub questions
other than say that over time we wind up realizing that there were only really a
couple of different machine learning models that did as well or better than
everything else and so even though for instance support vector machines are very popular because they have some
great sort of theoretical provable properties they tend to be kind of unwieldy and for dealing with the kinds
of data we were working with which is heterogeneous noisy dirty sparse missing
and multi class where you needed to also get probabilities out that you could
then calibrate models like support vector machines really fall short for
practical purposes and so we wound up recognizing in our group and I think that was validated in a conference that
we ran at at UC Berkeley on essentially streaming inference with machine learning it was a sort of week-long
conference that involved folks from Netflix folks from Google and then
domain experts you know everything from biomechanical to to physics a number of
people would stand up give their talk and say yen we wound up realizing that decision for us pretty much always won
now this was in 2012 before the resurgence of deep learning I bet if we ran this conference again half of the
talks would be about how that's a better algorithm as it were but it was pretty
eye-opening and it was one of the things that we took to heart as we wound up starting the company is a recognition
that to exceed to produce value sort of
very generally the algorithm itself is not necessarily the key in some sense
the way I view this now is that algorithms and their accuracy that they can produce and their ability to to
optimize them around a loss function is really only table stakes for the utility
of these in in a real and real environment and so yes you need to use a model that's very very accurate and
potentially can be retrained and get slightly better but as most data scientists or most people at work
machine learning workflows we'll say almost all that work is in feature engineering and if you're a deep
learning person you'll say almost all that work is in figuring out what the shape of the network should be in order
rating over that all right on that note before we jump into what you're doing at
Results from Astronomy
the company today what were some of the results you saw out of your research on the Astronomy side so we we looked at a
couple of different realms one was looking at large catalogs of variable
stars and coming up with probabilistic classifications of what type of variable
stars they were what was the physics that that drove them and we did that in a bootstrap way starting with
effectively a few hundred known classes and few hundred or a few thousand known
labels and then extrapolated that to tens of thousands hundreds of thousands of variable stars and produced
probabilistic catalogs one of the things I became adamant about as we were doing
that was producing a catalogue where you say hey this object in the sky is of
this type with this probability is effectively useless unless it's then
used for some new kind of science and one of the things that I became I won't
say frustrated with but I noticed often is that people started using not just in astronomy but in many other fields
machine learning as an end to itself saying I'm going to apply machine learning to this data and I'm going to
get a result until that result itself is novel or until that becomes a stepping
stone to another result which becomes novel you know it was it's sort of an
empty exercise and so what we wound up saying is what can we do with with this probabilistic catalog that couldn't have
been done with any other means and so one of the things we did is we looked for very strange types of stars that had
certain properties and then followed those up with big telescopes and actually wrote science papers with with
those so we use that as a launching off plane in a real time environment where we actually were looking at images as
they were streaming off of telescopes in Southern California off of pal
Mountain every 60 seconds or so it would basically get transferred up to Lawrence
Berkeley National Lab and we'd apply our machine learning to that to find new interesting objects in the sky and then
populate databases of you know for tonight here are the interesting objects and then also add another machine
learning code which would go into those databases and periodically make statements about what types of objects
those window might be what what they could be and we went up having I think
of order a hundred maybe two hundred papers that came out of that refereed papers which again the machine learning
part of that was really the stepping stone to discovery the other parts of the machine learning were the stepping
stones to initial inference and obviously in the end you needed people to to actually write the paper but don't
try it out to the grad students what's that it all goes back it all goes back to grad students exactly but I really
thought about kind of removing people from the real-time inference loop and getting as far up the inference stack as
we could we even got to the point where we were finding interesting objects in the sky without any humans in the loop
identifying that not only is it a new object but it's something we probably should be following up and we were
issuing alerts to robotic telescopes to go follow those up so by the time people woke up in the morning we not only had
the discovery we not only had the initial inference we then also had real
follow-up scientific follow-up one of the I think great achievements of the
work that we and others did and one of our collaborations was to build this
production system that you know had real consumers on the other side of that and when it was broken or was wrong or
didn't take feedback properly you know we'd get nasty emails from our collaborators and say your thing didn't
work for an hour you kind of screwed my science while I was at the telescope so having having an end user really keeps
you heavily focused on making sure the things that it needs to do does it right
and robustly but because we were discovering things even faster than a whole
I mean grad students wouldn't been able to pour over all of these images we're able to find for instance the nearest
type 1a supernova that had been found in 25 years and get a whole bunch of people
looking at that part of the sky and taking lots of data that led to a bunch of papers in nature and science Wow not
because that object wouldn't have been found by even amateurs because it got so bright you could have seen it with
binoculars eventually but because the interesting science happened hours after
the event blew up right the event happened and so it wind up also driving
home for me the need for not only something that's working and is robust etc but where it's able to make
statements quickly and do it in a way that's reliable interesting interesting
so I'm sure that that has led you to some interesting perspectives on the
AI and Jobs
relationship between this technology and in society and jobs and stuff like that
I I'm hearing parallels to you know a lot of people kind of projecting that as
AI is deployed shifting shifts in the job market will take place that put a
lot of people out of work but I'm also hearing in your example kind of the
counter-argument you often hear that really what it does is and empowers people and allows people to do different
things that aren't it don't necessarily want to go deep into the society stuff at this point but yeah it's it's it's a
AI and Customer Support
certainly a valid concern what we do in our company at waz IO
is help customer support agents become more efficient at their work by
suggesting answers of how they can respond to an incoming inquiry by automatically triaging incoming increase
in or tickets emails etc that is getting them to the right person or the right
group who's going to be the best at answering that question and then in some cases we will automatically respond to
incoming increase so when you write into e-commerce site and say my package didn't arrive there's a
growing chance that us or somebody else may be answering what looks like a bespoke question of yours with it looks
like a bespoke answer but in the end it's just a templatized response that
where we ourselves are using for us to be able to do that obviously we can talk
more deeply about how that works from an AI perspective we have to get very confident in what our answers are but
what does this mean on one side to your question about labor displacement
companies don't need to hire as many support agents so where would they have gone the other side of that is that the
agents that they do have become better and more tuned at working in some of the
harder parts of what their own products are about and where their customer complaints are about in a way they
wouldn't have been able to because they would have been distracted by the mundane so if you say how do I reset my
password and there's a person or sets of people to have to look at that email and decide how to respond that's time that
those people are not spending on really complex problems where empathy is required as well so we think of it as
our product and and what we do as a way of freeing people to work on the things
that people are uniquely suited at that machine's really aren't going to be that good until somebody solves you know the
Turing test chatbots are not going to be able to understand people in the subtle
ways that they need to but we can take a lot of easy stuff off the table we have and so there was certainly a concern as
we were starting to roll this out that this was we were part of this labor displacement movement but we heard time
and time again from our customers that their support agents became more and
more happy the more involved we were there was an entire team in in Asia who
had been tasked with basically reading an incoming inquiry or a ticket and then deciding who else should be reading this
to the problem and because our our triage capability came into play they
effectively deprecated a 20-person team one of our clients off of triage because
we're effectively automatically triaging now and we were worried what's gonna happen to these people they had they
have families to feed and we got a whole bunch of really great quotes from them saying because they had been reassigned
to actually work these support tickets rather than push them along they were much more happy in their job
that's fantastic so we jump right into what you're doing it was that oh that I
From astrophysics to software company
oh but the the transition is a fascinating one as well how do you get
from astrophysics to a software company doing CRM stuff and I know there was an
intermediate step there as well yeah so so going back to the original part of
the conversation we had recognized in the team that that I'd build in the
people I'd work with that a we had some great sort of technical orthogonality
some were good software engineering somewhat good at ml some UI etc and be
that what we had learned to do of recognizing the importance of putting AI
into production and having real end-users give real feedback in potentially a real time loop was
something we at the time didn't see anyone else doing we knew obviously that the googles and
the LinkedIn's and the Netflix is the world had this kind of baked into their overall data flow but we certainly
weren't seeing companies helping other companies do it and one of my now
co-founders had more or less while he was between jobs figured out how to make
one of the algorithms that we liked a lot for our scale very very well at least on a single machine in a
multi-core environment and so we realized that we might have some interesting firepower and given that
there seemed at the time to be so much emphasis on massive scale machine
learning it was it was certainly a pre spark era but was very much in the new pay day it looked
like most of the interesting ml that was starting to come out and some of the other companies that were coming out we're really focused around helping the
you know I won't say exits gala but certainly petascale level Google scale amount of data companies bring machine
learning into their into their workflows so we thought about sort of skating to a place you know using the analogy that's
heavily overused to the part of the where the puck was going to be which was helping smaller companies and midsize
scale companies bring machine learning into production environments and that
was the impetus for starting the company what the domain was going to be we
didn't know who the customer was going to be and who the buyer is going to be we didn't know we were I'd say
blissfully ignorant about all the business challenges that we would wind up encountering over the next couple of
years in bringing this to market and when we emerged out of our first
accelerator I gave a talk in that our demo day was the alchemists accelerator
where I said we're going to be github for data scientists and produce some
interesting you eyes of interactions to help data scientists like ourselves more
easily build models that they could then push into a production environment we wound up seeing over that over the next
couple of months the challenges of selling products like this into data science teams first data science teams
were few and far between and the ones that existed were either too sophisticated to believe they could
build it all themselves or not sophisticated enough to get a large enough budget to pay for the things that
we wanted to provide them from a tooling perspective all the while we were
building out our underlying platform to be able to do exactly that to be able to
build templated machine learning models against certain types of data for
certain types of use cases and then even though you built it at a laptop level push it into the cloud and have you know
in in Amazon or other compute frameworks the scalability to be able to serve
large numbers of customers around those same use cases we're what's emerged for us is the
difference between a customer is not new code it's just a config file if they're using that same use case so all of that
to say that we evolved you could call it a pivot if you'd like but I think of it as a series of of pivots to a place
where we wind up seeing in customer support a lot of data a lot of manual
work and some really nice CRM systems
with open api's and a fairly thick schema so the sales forces and Zendesk
and service now's of the world really are the data Lake and the transactional
layer for doing customer support and related activities and we thought if we
could build now an intelligent system on top of that and do all the things that I mentioned empowering these agents to
become more efficient of their job and make the whole support desk more efficient at serving customers we would
solve a bunch of being pain points and that as we wound up going into the market and started leading with products
that could be more or less installed by a non-technical user and could be used by a non-technical user wind up getting
a lot of feedback that indeed we were solving some pain points there's obviously the efficiency question of
needing less headcount but there's also some really interesting customers of ours who were growing very quickly and
one of them said to us that if the CEO had given him an infinite budget he wouldn't be able to hire good customer
support agents quick enough so helping them capture all the internal knowledge was something that it turns out machine
learning actually does quite well at what were some of the biggest challenges
Biggest challenges
in going from product direction focused on generalized tools and platforms to
one focused on a very specific application area I mean interestingly it
was all the things that we hadn't thought about which was product management and how do you get structured
feedback from customers what what does it mean to build an MVP roll that out
iterate on it etc a lot of kind of lis startup 101 stuff was something that we
hadn't really been thinking of when we started the company and certainly didn't have a you know frankly a lot of
expertise in and and then as we started scaling it was a recognition that there
are large parts of a machine learning pipeline that don't naturally scale so
figuring out ways to containerize the parts that need special attention from
you know Phe levels and data science and abstract that away from other parts of
our engineering group that don't need to know about what's happening deeply but need to be able to ask predictions of
some other part of the stack restfully in a services oriented way we just wind
up realizing that what worked for us from a scaling perspective compute
scaling perspective also wound up being what we needed to do from organizational and HR perspective when we hire
front-end engineers and and middleware engineers who are great at writing
scripts against data bases and managing Redis cues etc those folks don't need to
know about machine learning they need to know that there is a contract between their part of the stack and and
somewhere deeper in the stack that if I ask you for a prediction for this client for this model I'm expecting to get it
back in this format on this timescale and if I don't then our contracts broken
but likewise I'm going to hand to that deeper part of the stack that's going to be providing those predictions you know
effectively some data and JSON or otherwise that will have a fixed schema so that the group that built that
machine learning pipeline knows that you know this column is going to be of type date/time this column is going to be you
know an int and it's going to join using these four indices on some other data so
once we wind up realizing that we could lock down the schema for a given use
case it meant that we could write data science pipelines against data we hadn't
seen before well you need to see it once to make sure it's all working and make
Crosse validates in an offline sense and has the kinds of accuracy properties that you want but then it means that we
could basically start spinning up new customers where they get the sort of base template that operates on their
data and when we need to make changes those can happen really from a more or
less technical person than you know somebody with a PhD in statistics so
there were a bunch of challenges around that and as we kind of started solving those it just sort of fell out that our
stack really mimics what our organization looks like okay okay can
Customer data
you talk a little bit about the the data that you typically see in a customer environment I'm you know imagining just
loads of trouble tickets but I'm but I imagine as well that there's ancillary data supporting data as well and you
mentioned that there's lots of it can you talk about the size you typically see those kinds of things yeah so our
typical our typical customer is doing of order five to twenty thousand tickets a
month okay and we need to be working with companies that are that are sort of
achieving that level of volume a because the price points are reasonably high and
so it's typically the companies that have those large volumes that are willing to we're going to pay for what we do and and be because the machine
learning models are built specifically for and on their their data and we don't
use a common model for instance across our customers so we need a lot of training data for a given customer
now this isn't again this is not petascale amounts of data we're talking sort of tens to hundreds of gigabytes
you know at the per month level per customer the data is indeed a lot of
human a human interaction from emails web forums even chat and there's also a
lot of metadata so what is the value of this of this customer what products are
they using how often have they been emailing so there's a time series component to this as well and you know
we've had to build these pipelines that are generic enough that we can then apply them to other use cases but specific enough
that they give you no good enough accuracies that wind up rivaling what you know what humans can do and so
oftentimes our our goal is to get to naught we don't call it accuracy we call it matching capability because
oftentimes when humans labeling something and saying it belongs to this bucket or this person should answer it
or it should have been answered with this template they oftentimes can be wrong so we want to get our we've all
had that experience we want to get ourselves to that kind of level of quality let's say so it's mostly from an
from a feature ization perspective we're doing natural lots of natural language processing getting it to the point of
you know sort of rectangular eyes data where each row is a different instance
in each column is a set of features and then we have a bunch of labels of what
the answers are so we're working almost entirely in a supervised sense where we know from past data to particularly
close tickets what the actual right answer is quote unquote we've got a couple of unsupervised
models that we also wind up running where we wind up discovering for instance that there is a grouping and
semantic space of outgoing tickets that is how agents are responding that don't look like templates that are sanctioned
by the by the company which means that they're coming up with their own templated responses and potentially even
sharing those with other agents so we have a dashboard for instance we show our customers than one of those running
the support desk of potentially new templates that they can use obviously if
there's a new issue for instance with a product then agents who are on the ground have to figure out a way to
answer it and if it's a recurring problem within a couple of emails that went up essentially having the right
answer that they've already pre formulated so that's an unsupervised problem and do you see in that last
Future of generative AI
example a future place for generative types of AI approaches or is that more
do you think of when you hear that is that like the technology you know looking for chasing the problem kind of
thing yeah it's a good question we've shied away from the generative component and
in fact we make that a big part of our sales pitch of to say you are a
potential client really know the voice that you want to speak in and speak with
your customers and it's who is it for us to come in and say we're gonna Auto
generate at the character level you know CN NS or something a bespoke answer the
way that you know Google inbox does well if it gives you you know five five different words sure all sounds good
I'll see you then or how about Friday those are fine but because we're we're
really focused on not just getting results into the hands of agents where
they can actually see in a UI sense within the dashboards they normally see what our predictions are I'm consuming
in a way that they like to we also want to take a lot of these tickets off the table in an automatic sense the only way
our customers get comfortable with that is if we're basically showing to them in an offline way here's our accuracies for
these types of templates so we're gonna send every now and then somebody says I'm very unhappy with what you've done
we're gonna send thanks for your feedback when it should have gone a different path but we're only going to
do that you know one percent of the time at this at this level of false false positive and once we can do that then
our customers essentially can turn on a specific macro for us to auto respond with the idea that we're going to auto
respond without any agents in the loop to something like that to potentially irate customer is is pretty challenging
so we don't certainly we'll fill it out but we certainly don't think about ourselves as producing generative
answers in a bespoke way we're just more or less turning all of our problems into
multi-class classification problems of what of the hundreds or potentially thousands of canned responses is the
right one to answer with okay and just so I understand the comment that you made a second ago in terms of sending
Exploration vs Error
out a given response a small percentage of the times are you describing an error
type of situation are you describing a feature where your explore like an
exploration type of feature good good point so we try not to do we there's an explorer
exploit component to what we do in a multi-armed bandit sense that's
typically not you know exposed or a knob that's tunable by our customers so that
will happen and some of that will happen naturally in the case of auto response we hold back 10% of the ones that work
we know what the answer is or we believe we know with a certain threshold of confidence and then compare after the
fact whether an agent who wound up now having to see it because we didn't know how to respond gave the same response we
did so there's there's an exploration where the agents job is now to do the exploration implicitly know that the the
one I was pointing out is ones where we are essentially wrong yeah and that gets
back to the question of the loss function of what does it mean to be wrong right if if one of the can dress
Ponce's is I'm so sorry for your loss I will refund your entire vacation you
know in the amount of $10,000 the cost of being wrong of that is very very high
but if somebody is mad and says my vacation got ruined because of something you did I want my 10k back and we say
thanks for your feedback it's being wrong on that side is not nearly as bad
as being wrong on the other side of that and so we give and empower our customers to basically make the decision about you
know let's do the easy stuff where the cost of being wrong is not a big deal but because and that's for the automatic
response but for the recommended types of responses if our first canned responses here's your money back and an
agent looks at that and says no that's crazy the right answer is farther down the list they'll select that and that
becomes the feedback that our model you know our models wind up getting better as they wind up learning over time what
Challenges
are some of the most interesting challenges that you've run into in
putting together this kind of hybrid you know ml + human solution like in one of
the things that pops to my mind is you know just user experience user interface like are there challenges they
that are interesting or you know what surprised you the most in trying to feel
these types of solutions certainly because this we're getting in we're
getting into the space and the face of agents who do this all the time when we
first started releasing our products we didn't have a good training program for them and so when they would see even
though what we thought was an intuitive set of responses in the form of widgets that would show up on their on their
desktop you know they didn't know how to consume it and they didn't know how to use it as effectively as we thought they
should you know there's all the mundane stuff around you eyes like responsiveness and somebody's saying
well doesn't look like your products working because now there are no responses and we'd say well that's because you've already responded and
you're bringing up a new ticket you bring up an old ticket that already has a whole conversation and we're only
getting involved in at least for now in the first part of the conversation what's the first response you should do
okay so then we weren't showing the results and so how can we you know modify our widgets so that the agents
understand we're not showing it for a purpose it's not that our system is is broken and then realizing also that many
agents wanted parts of our UI that and
and UX were more generally that doesn't have any anything to do with ml so they
wanted keyboard shortcuts because we thought everyone would just click on stuff but high velocity support desks
want to just use the keyboard so having to build that end for a set of customers
because the essentially is like the keyboard had some disease or the mouse had a disease on it they didn't want to
touch it getting feedback from the UI itself back
into our system making sure that we're getting the right metrics back making
sure that the KPIs that we're measuring or that were aligned with the KPIs that our customers wanted I think one of the
hardest things for us and it frankly continues to be a challenge is really
just thinking about how fault-tolerant ml needs to happen and again Google going back to
in box for those of you that have used it it makes a couple of suggestions
about how you could respond to an email if you don't want to use those you don't use it so I would call that a great
fault tolerant all right ml experience and the same thing in this you know spam
filter within your within your mail system it'll say we think this is spam if it's not move it over and then later
on we'll figure out how not to call these things spam anymore that are like that that sort of fault tolerance where
you're also getting feedback either implicitly or explicitly it's just something we've had to build up over
time but I think more broadly that that kind of approach needs to be built into
any AI system in a production environment unless the AI outputs that you're building are going to be consumed
entirely by machines you need to have some level of understanding of who it is
it's going to be consuming it what are their concerns and how can they give you feedback so that your model is one of
getting better over time can you talk a little bit about the algorithms that
Algorithms Pipelines
you're employing and the toolchain the pipelines what does all that look like
yeah so we we stay out of what we call the our algorithms arms race internally
a we're not really selling the the platform to other data scientist so it gives us the freedom to focus on parts
of the the pipeline that we find most important all of our algorithmics are
learning parts and then prediction parts are built in C++ and then surface back
out into Python which is where the data science team winds up working we have
our own notion of what a pipeline needs to be and the data science team works
entirely within that the confines of what that pipeline ought to be which is some sort of pre filtering so for
instance if a ticket is from a voicemail don't predict on it or don't use it for
a build so you get rid of those that have this in this column this value then
there's the data transformation parts of that in the joining multiple across multiple data sets if
that's needed and then the feature ization which will often use open source
tools for that in the Python ecosystem pandas is a we use very regularly and
then once we wind up realizing that we've created a bottleneck which typically will happen not so much in time but in RAM usage
will wind up rewriting other people's algorithms or code so that we create you
know a RAM efficient pipeline and then once the features a ssin happens basically the learning winds up
happening in the c++ layer and we've built a whole bunch of hyper parameter optimizations and feature selection
capabilities and then post process capabilities to get calibrated
probabilities out of a multi-class problem so we have a bunch of pieces
that we've been building up that are not in the open world and it's something that we've decided not to open source
for now that allow us to work efficiently so we think of as you know
high velocity data science and building out a template for the first time but then because the models have to rebuild
every single day for every single customer on you know cloud infrastructure which is not super cheap
we needed to make the cost of doing that as small as possible and what we wind up
realizing is that open source tools you know that that many people use like the
psychic learns and and the tories slash datos of the world and even spark and
now i'm we're vastly more costly to run even if you could do it in the same
amount of time which we think we're much much faster than most of those tools because of the RAM requirements needed
on multiple machines or even a large single machine in Amazon the costs of
building a model just was X percent higher and X being you know in the thousands so having a ram efficient
speed efficient and obviously again getting back to the original conversation about table six highly
accurate said algorithms which produce the kinds of answers we want that we could then get
into and modify if we needed to was kind of where we went up settling is where we
needed to spend our kind of our de / engineering time now one of the areas
Data Science vs Production
that many of the machine learning platform companies are focused on is
trying to close this gap between data science and production yep and in
essence eliminate the hey I've got this model that kind of works throw it over the wall to developers and have it
implemented and it sounds like you guys have maybe embraced that and you're
using that as a way to build out the models in C++ presumably for a
performance are there ways that you've then compensated for that in terms of
automation tooling or do you just accept that or you know even you know we just
have you know the best people on both sides of that fence that can deal with the you know the existence of the gap
like how do you maintain a level of efficiency and innovation in terms of
the development pipeline and not the machine learning pipeline so that it all works for you
yeah so there's definitely this separation of concerns which again is
both an organizational one and then is also a computational one to the level
where we think we often talk about what we call the organizational API of who
within this stack is the customer of who and so for instance the people who are
the sort of core ml and algorithms folks in the company you're working in C++ and
surfacing the great results back in into a Python layer their customer is the
data science team the customer of the data science team is the the people
working on our architecture who have to make maintain you know this scalable
robust infrastructure and you know their customers are the people working in the
middleware and their customers are the ones who are the in the UI and so each of them have a set of
contracts of what it is that each part of that stack is looking for and and how
in fact they're supposed to engage with each other and that's become very very helpful for us because you know what you
find is that when you put somebody in a box they figure out a way to innovate very highly within that box so if there
is a very strong contract of what data is expected to come in and what data is expected to come out and everything in
between there is really up to you to decide how to do this well and efficiently that's where our for
instance our data science team and implementation team will wind up working and building at a new template they can work at their laptop or glorified laptop
level on a toy dataset get some confidence that the pipeline is working and offline accuracies look good and the
whole thing is going to work and they once they're comfortable with that they literally are just pushing a new version
of a docker image into our registry which then farther upstream from
something they ever have to think about from a production sense once a new-build winds up getting kicked off for that
customer for that type of template then you the new image will just get pulled and it will just get built with the
config file for that customer and so the data science team can wind up working
within their confines and of course we have a whole testing suite to make sure that if they build something new they're
not going to break something downstream from them they gain confidence in that and then they're literally just pushing
the results of what they're doing on a semi weekly basis into the docker registry that becomes the latest
template for let's say triage and then all the customers in production are automatically migrated to that so having
the data science team be able to pushed up into production without having to be on the ops side of things nor have to
think about the architecture has really freed us up in great ways I think to
innovate and likewise when they need a new Bell or and or whistle from the core
algorithm folks because they say this part of our entire build chain is really
inefficient they can ask of the people working on that to improve it and they go through their own
testing suite and I think read 300,000 regression tests in our core ml we're
also testing against every open source algorithm on customer data to make sure
that we're staying as efficient or more on all these different axes before we
cut a release then the data science team can just pull essentially python egg from our registry and and use that in
their system so having those separations has been great obviously if you're
abstracting everybody from what the ends use cases are there can be a huge danger
but it's the job of people like myself to make sure that everyone is focused
and innovating towards the right set of goals oh great great I'm glad that docker came up you guys
published you published and maintained a set of docker images for data science
tools I've come across that my impression is that in general docker
adoption within the data science machine learning community is not particularly
high is that yours as well I certainly haven't heard of many other companies
Containerization
using it in the ways that we are but it seems like such a natural way to
literally containerize and abstract the work of one part of an organization from
the other so long as they you know that container will respond with a slash build predict endpoint you know feedback
endpoint etc in the way that everyone expects it to I think that's a I think
that's a wonderful way to do abstraction so and then it obviously also helps you
wind up achieving scale because for us scale is not you know can we serve you
know a billion of our customers with the same app it's instead well we've got a new customer that we just spin up more
containers to do the builds and the predicts for that customer and we need more compute capability that's elastically scaling for us for free on
top of Amazon so I think of as very natural way to separate concerns you
know from a stack perspective and an also very natural way to do what is for a company that's serving lots and lots
of customers a very embarrassing li parallel type of of compute interesting
I got into a conversation on Twitter or reddit or someplace where someone was kind of griping about just the
dependency hell with Python and pandas and trying to come to terms with
managing different versions of you know different tooling versions and things like that and I suggested I might have
even pointed to your docker repository and the response was now I want to make
this simpler not more complex and obviously you find it to be simpler can you give folks that aren't familiar with
docker in containers like your 30 second you know docker for data science pitch
and where they can learn more about it yeah so docker is a way of basically
Docker
explicitly specifying what not only your let's say Python requirements are which
you can do with a simple file but also what the entire OS shall be for running
whatever scripts you're gonna you're gonna need and once you build that and
you you've you're you gain confidence that that image is doing what it what it
ought to you can essentially very rapidly turn a container on that is the
almost instant instantiation of that entire OS plus that script and all of
the dependencies built inside of that and you can hand somebody a link to the
docker hub registry or if you maintain your own private registry explicit URI
to that explicit version of that explicit image and more or less
guarantee that when they run that with whatever data is contained inside of
that or whatever will be pulled over so long as it's the same you'll get the same answer out so I tend to think from
a data science workflow and then getting back to you know just doing science more
generally docker is a very nice framework for reproducibility and so the
idea that now I'm I'm not I don't have to share a machine with you or an Amazon machine image or with you I'm just
handing you effectively a docker file and says if you run this you're gonna wind up getting the same answer that I
got but again because I don't think doing the types of work that we do and
wise and in some cases what we do on the science side of things as the you know
the the final result is not what comes out of the docker image or container
it's not okay here's a report of what my ROC curves going to be my false positive
versus false negative curve and then let me write a paper about that it needs to be for us at least in a production
environment just now I've produced a prediction that now needs to get consumed by something that's farther
downstream so docker is quite nice in that sense as well because you can also
now connect docker containers explicitly using something like a docker compose
there's many other tools out there as well so that containers talk to other containers and you allow each container
to have again its own separated concern from the other ones but still pull the results and push results to the other
the other ones around in addition some containers can just contain data and you
can build databases around that data so now it allows you to build up a very
lightweight version of what might be your entire stack and do this in a way
that's programmable so we've found that to be incredibly useful for testing purposes so your github the place that
someone can go to learn more about what you're doing there yeah so we've got a public docker registry that you can go
to the docker registry and search for wise i/o or you can go to github slash wise i/o and see our other public
projects that we've pushed out so there's one around docker and data science
which in that case because we're not releasing any of our internal tools we've built we're basically building up
a container with open-source tools that we find are really useful for doing lots of different types of of data science on
the other major project which we have up there in github that's open is something
we call paratext which started as just sort of a weekend
hack from one of our engineers Damian EADS who wanted to see what it would be
like to read data from disk in parallel mm-hm just to see what kind of speed ups
you could wind up getting and it turns out pretty much every open-source tool out there doesn't read in parallel and
the ones that do are explicitly parallel lies like over multiple machines but if
you just made well mult use of the multi-core environment how well would you get and we went up getting 100,000 X
speed ups over some of the other tools that are out there and importantly also use vastly less memory that parrot X is
not yet in our production environment but we thought it would be a good example of kind of showing off the
philosophies that we try to adhere to within the company of creating
efficiencies that isn't just the one thing like around accuracy but you know around how fast can you read data in how
big is your model on disk all these other aspects of what it means to do machine learning that it has nothing to
do with the algorithm once you're happy and you've reached some level of plateau with the algorithm accuracy all that
you're left to do is optimize all these other pieces of that pipeline and so a
lot of our engineering over the last year in particular has moved away from just optimizing accuracy to you know
things like creating interpretability around the models that we we build making the model smaller on disk making
the other parts of the features Asian pipeline be more RAM efficient and once
you start sort of playing whack-a-mole with let's just say RAM usage you wind
up finding really interesting parts of your entire pipeline that very few people went to
talking about just you know again reading data which should be the easiest part of your entire tool chain is vastly
inefficient and you know whacking that mole turns out you save a whole bunch of
Amazon costs because now you need a smaller Ram machine that's great that's great
you mentioned interpretability have you spent a lot of time working on that and
what were the drivers for that we have spent a lot of time on that you know it's sort of one of what we think of as
our trade secret one of our trade secrets around getting back to the
question of UI and UX for end users we we were asked often at least in the
early days well why are you getting the answer that you're getting and you can't say well you know it's a thousand
dimensional feature space and there's covariance between all of these and you know the model importance is over the
entire thing you know says that this is the most important feature I don't know why we said for this one what the answer
is but right that answer is what what's called in in the financial services world reason
codes turns out to be really important so some some places it's actually regulatory ly required that you tell
somebody why you got the answer that you got even if it's a machine learning black box and so some of our early R&D
effort was around how to make at the instance level so an individual prediction level how do we make these
models interpreter by saying these are the important features and these are what's driving this specific prediction
so as an example if you're working on a customer churn and you want to predict
somebody going to churn in 90 days from now it's a use case that we've we've also used on our platform but not
something we go to market with necessarily to customers can have an
identical probability of churning but one of them may be churning because they haven't really used your product and
they haven't done the training videos and the other one may be churning because there's a high they're gonna go bankrupt in the first
case that's something you can do something about and the second case you know you're kind of Sol and so even
though they're identical and what their predictions are and they're and the probabilities of those predictions
coming to pass one is actionable and one isn't and so it's not just people
gaining kind of a warm fuzzy about why did you get these predictions and does it jive with my you know feeling about
why that that could be okay which is critically important it also then starts tying into next best action and because
I think again a important part of machine learning and production is to
drive value if the value isn't the prediction in and of itself then the
prediction and of itself is really just there to drive the next thing that happens and so next best action is
heavily coupled with you know the the importance is around which features are
driving the the prediction okay you mentioned value and that's a great
transition the one of the things that I really wanted to dig in to with you and that is the this blog post that you
wrote about cost optimized AI that I've incidentally mentioned on the podcast a
couple of times do you have time to go into that of course so I guess the first
quote come up several times in our conversation already this notion of cost
and value but was there a specific thing that prompted you to uh I really got to
write this down now what drove that so that was a bit of an intellectual journey I I was wondering to be really
frank why the hell are all these companies building these neuromorphic chips and all these specialized hardware
to do deep learning where we're you know because I think much of the world's data
and much of the world's value in data is tied up and I'll use the word or quote
unquote small data or medium data non massive scale Google scale data Facebook scale data
I was wondering why all these people are starting to build these very specialized pieces of a hardware when you know deep
learning I think magnanimously one could say or charitably is incredibly good at
a large number of of inference problems but not very good at a large probably
even larger space of inference problems that may be changing over time as people start applying it to these new realms
but the place where deep learning winds up shining is in really large amounts of
data right because effectively what you're doing is turning millions or even billions of knobs to optimize a model
and to do that credibly without overfitting one needs lots and lots of data so so I wound up asking this
question of myself why are people doing this and why isn't what we already have
out there and even just the GPU land good enough and if you look at the a
plot which I have in my blog post of the efficiency sort of gigaflops per watt
right which is something of if I put this amount of energy in which has this amount of cost
how many computations can I get out that efficiency has been growing over time
but it's nowhere near what some of these other chips are the specialized pieces of hardware can do for these specific
types of calculations and those themselves are nowhere near what the human brain can do right which is of
order if I remember right about ten to the five gigaflops per watt so your your brain is a you know 30 watt
supercomputer unrivaled at least for now by anything else it's out there and anything that else is out there is
likely going to take megawatts or hundreds of megawatts to get anywhere close the the computational capability
incidentally I don't know if you've come across it but there's a parallel to using DNA for storage and the storage
density per per unit energy is incredible in DNA yeah something like
you know the the drop of a you know in a teaspoon or something it
can take all the world's data as it's in it's incredible so so getting back to
this you know that's an obvious that's an obvious one and I started thinking about it when alphago had its big set of
results the national or international championships and you wind up looking at
the computational capability that it took to win those those competitions is
just huge thousands and thousands of computers thousands and thousands of GPUs the amount of power required there was
several orders of magnitude larger than what was going on in you know the the
the champions head that they was playing against so I was thinking about that sort of vast gulf and wound up realizing
that the companies that are pushing towards these specialized pieces of hardware is because they realized that
for a given amount of time and a given amount of data because these algorithms
are all basically saturating on your perfect answers the only thing left to
do is to get more energy efficient machine learning for building and that
the step after energy efficiency when it comes down to it is really cost efficiency and and so my my takeaway on
on that part was that people are building these chips because that's sort
of the last frontier of squeaking out and eking out the last amount of dollars
coming out of the system for the number of dollars going into the system and
then it's taking a step back from that it wound up realizing that or at least
realizing for myself it's probably obvious to most out there that because machine learning is optimization that
you're a good optimizer will find the optimal answer by definition that if
you're not writing down your your function that you're trying to optimize to get a minimum of our maximum of in
terms that actually matter then you're creating by definition a sub-sub optimal
answer or system and now that system doesn't just involve you know as my
algorithm more optimal at getting an accuracy better than yours but now translating the accuracy into well let's
go back to our loss function what's the cost of being wrong you know and saying this thing is a and this thing is B
translating that to a business term is something that's critical and almost everybody knows that that's that's
important but then you wind up realizing well if I'm going to build a model what
if it takes me 12 days to build one of these models to get an accuracy which is only epsilon better than one that takes
me to 10 seconds and what if you know I can build a model that may take 12 days
and the accuracy is much higher than one took me less time but the labor costs
are very different so I had to spend more data science time building one versus the other and what about the opportunity costs of
those data scientists not working on another problem in your business that may be more important and when you wind
up couching the problem that way you get out of just again focused on accuracy
and the algorithm - what is my cost of doing the entire pipeline and now the
entire pipeline isn't just running a machine learning model in production for
the specific use case but how does that couple to all the other things you're doing in your business are you hiring a
data science team to do this and then paying pensions or are you gonna do a third party to do this and just write a
check one time and then you know what are the societal benefits of all this and you know becomes unwieldy at some
point if you're actually being very honest about what's the cost of doing this but at least if people I just
wanted people start thinking about as we started thinking about within our company that accuracy is the table
stakes and let's assume that you all have your good algorithm that's going to do well is it going to have strong
scaling properties so that if you needed to get the model built you know an X amount of time that you could just have
n number of machine that get you X / n amount of time on the clock because maybe you need that model
bill very quickly very often right um and then you know questions around the pipeline and and RAM usage and AWS costs
in the end as a small start-up when you
start getting down to the brass tacks of what's our revenue and what's our cost
of goods sold with our cogs the cogs component is really what does it cost to
build a model and predict and until we were able to boil down the fact that the
cost per prediction for one of our customers is X and we're going to be making x times some number you know
everything else is sort of moot if you're losing you know every time you make a prediction effectively
hand-over-fist then you've got something wrong that's unsustainable so I started
thinking about it as we were going through the exercise of what's our cost of doing business and the cost of doing
business is running an AI system in a cloud with real customers and the labor
part we can get but all the other pieces there in the end there's an Amazon bill and because we put it all inside of
Amazon you know and we know how much money we're taking in we can see how those two things relate to each other
mm-hmm so you started with the question and and I've ran through the thought
exercise what's next there whether it's you or someone else that does it Dean do you see this evolving or you know Co
evolving with someone else thinking about you know analytical frameworks for thinking about this or you know tools
you know whether that's a spreadsheet or you know it almost lends itself to
machine learning algorithm yeah and to figure out how to deploy resources to you know do the machine learning yeah
it's a great question and you know what a nice things about blog post is you and made it out to the world and you hope somebody writes you know if somebody
runs with it it's been helpful and focusing for me and my own thoughts and as we drive
those sorts of efficiencies in in our company and then again more broadly you
know in doing science doing astrophysics choosing the right tools I'm choosing
the right skill sets and people choosing the right problems to work on or not work on those those are very obvious
sort of outcomes from me having thought about and framed it that way one of the
challenges is and I think people may wind up being able to pick pieces up of
this and work with it is coming up with articulations of essentially what is the
conversion term between that item in the
in the entire optimization equation and dollars so one I'll throw out there that
I don't know the answer to is what's the dollar value of interpretability and
once somebody starts getting some handle on that then optimization takes it's
wonderful you know toll or approach or at least shell lead to a great outcome
which is you know once you once you can really put a dollar cost to all these different pieces then I think you can do
a real honest-to-goodness optimization so I know what the dollar cost for instance of needing a ram machine of
this size versus that size on Amazon okay great but what's the real dollar cost of and
can I know what how much time it's going to take for a data science team to build up this template from scratch and then
push that into production and how many people do I really need on that is it good to have one data scientist or
multiple ones right and so all those things I think wind up becoming really
interesting over time once people want to potentially even wind up agreeing upon what this equation what's in band
for this equation what's not obviously out of scope is you know what's the
probability that you know my machine learning algorithm is going to start World War 3 probably not worth talking
about right right something smaller than that smaller and scope at the company level
is probably worth starting to get some clear understanding around so now we've
maybe come back full circle to graduate students a lot of interesting research project
questions in here for a PhD student or something yeah I think in the you know for for those in computer science
thinking about systems optimization who are also interested in machine learning this is hopefully some fertile ground to
start to start thinking the other statement which hopefully is clear from
what we've been talking about is that doing machine learning for machine learning sake really doesn't make sense
it's it's probably the last thing you want to do if somebody hands you data
Madhu it because you have to do it it's painful and to run it in a production environment given all the crazy Bugaboos
that many many people have talked about there's a great paper from the folks at Google by Dee Scully is the first author
called machine learning is the high interest of a credit card of technical
debt yes I'm not surprised it's an important paper it's got I think no
equations in it but it's a whole bunch of important lessons about how machine
learning systems tend to be very different than typical engineering systems so so so there's a lot in there
to get right a lot of Bugaboos there that people who haven't done this before tend to get wrong but when you wind up
realizing is that once you realize machine learning or more broadly AI is the right set of tools to apply to the
problem that you have what you'll often wind up finding I think is at the
graduate student level in terms of graduate student projects they could be working on is that it's still very much
early days for that for the types of algorithms pipelines etc in dealing with
real world data I've often said to my my colleagues on campus that real data is
not doing sentiment analysis on Twitter right and yet many many many papers
saying my scaling algorithms better than your scaling algorithm will wind up using that as a toy dataset the real
world is not tweet yes we need to have benchmark data to
have a lingua franca of who's doing better and these different axes but when you wind up getting exposed to real
questions you wind up realizing that all the stuff that people know out there in the academic world that people write
about and do Cagle blog posts about are not what you really need if you're being
truly honest about what needs to get optimized mmm that's great so how does one manage being CTO for you know the
high-growth startup and you know being a astrophysics professor it's becoming
increasingly common to see folks particularly in the machine learning community have professorial posts and do
academic or do you know work in these research labs and things like that but
yeah so I I been on a what's called an industry leave for a number of years and
so it's allowed me to have also that separation of concern so so not getting
not getting paid by the University and not having healthcare has made it easier for me to spent all my time as need be
on the on the company while still maintaining the kinds of links that I
think are important as I you know start thinking about coming back into the university setting obviously a number of
things I've picked up and management they you know ideas and and capabilities and then also thinking about how to
evaluate new technologies when is it appropriate to bring this into your toolkit or when is it appropriate to
wait those become really practical uses that you know I can take I can take with
me but then also again recognizing that as I was saying before there's a whole
interesting set of problems out there that are not being addressed by pure
academic Rd research means that I can also start you know looking for those
white spaces to actually do some pure academic research and around those I'm particularly interested in questions
around interpretability and how you put metrics on interpretability and that's something that I think I benefit from
having come from you know felt the pain of custom is asking about that right that you know
at least have a fresh lens on that mm-hm doesn't mean I'll solve any of those problems but at least I'll have a
direction of potential interest so it's a it's certainly a challenge but I think
despite the challenges the the benefits to both myself the company and the
university and my students at the university are far outweigh all the gray
hairs that I wound up and getting I'm teaching a data science class
essentially a Python ecosystem data science class right now it's aimed at
graduate students and the things that I've seen in the in the business world
have really helped me hone that that class and I'm very directly giving back
to the students from that from those learnings and is that a MOOC or is that available only to it is not a mooc other
incarnations of that class that i've done in the past are probably online somewhere in the itunes here or
elsewhere that can also be found on github all the material and then we'll
hopefully post some of the of the lectures online as well okay right so if
folks want to learn more about the the company or get in touch with you what are the best ways for them to find you
guys easiest is drop me an email and you can find that by googling around so I'll
add that as a little bit of a bar that if you really fun if I maybe you have a little bit of work you can tweet at me
so that's prof jsb as my twitter handle and and we can do a direct message maybe
that's probably the best way to get at me right great well I really appreciate you taking the
time it's great to finally meet you in person and I really enjoyed the conversation I think folks though will enjoy it as well
and get a lot out of it great well thanks so much thanks for your interest
[Music] alright everyone that's it for today's interview thanks so much for listening I
haven't asked you all to do this in a while but if you enjoyed this episode of the show please please please do these
two things first share it with your friends on Twitter Facebook guild email
or however you share cool things with your friends second reach out let me
know how you like the show who you'd like to hear on it and how I can make it better for you you can reach me on
twitter at at twitter i and at sam Charrington and you can email me
directly from the contact page on the Twilio comm site thank you so much for
your support and catch you next time

----------

-----
--04--

-----
Date: 2017.03.01
Link: [# Interactive AI, Plus Improving ML Education with Charles Isbell - #4](https://www.youtube.com/watch?v=CyiLyMdplP8)
Transcription:

My guest this time is Charles Isbell, Jr., Professor and Senior Associate Dean in the College of Computing at Georgia Institute of Technology. Charles and I go back a bitâ€¦ in fact heâ€™s the first AI researcher I ever met. His research focus is what he calls â€œinteractive artificial intelligence,â€ a discipline of AI focused specifically on the interactions between AIs and humans. We explore what this means and some of the interesting research results in this field. One part of this discussion I found particularly interesting was the intersection between his AI research and marketing and behavioral economics. Beyond his research, Charles is well known in the ML and AI worlds for his popular Machine Learning course sequence on Udacity, which he teaches with Brown University professor Michael Littman, and for the Online Masterâ€™s of Science in Computer Science program that he helped launch at Georgia Tech. We also spend quite a bit of time talking about whatâ€™s really missing in machine learning education and how to make it more accessible.


[Applause] hello everyone and welcome to twill talk
the podcast where I interview interesting people doing interesting things in machine learning and
artificial intelligence I'm your host Sam Charrington we've got another great interview for you this
time around but first a quick update on the drawing we've been running in conjunction with O'Reilly Media as you
know if you've listened previously O'Reilly media is holding their first ever AI conference on Monday and Tuesday
September 26th and 27th in New York City the conference will span both low-level
talks on implementing AI and high-level talks on the impact of AI in society and
I'm personally looking forward to speeches by AI luminaries such as Google's Peter Norvig Facebook's Yann
laocoÃ¶n and Intel slash Nirvana's Naveen rau and we're giving away a ticket to
one lucky winner here today in addition right after the AI
conference on Wednesday and Thursday the 28th and 29th is the O'Reilly strata
plus Ahdoot world big data conference which is one that I've been attending for years now you may have heard me
mention this one before strata is a much bigger event and while it's not strictly focused on AI there are tons of really
interesting AI machine learning talks at strata as well along with talks focusing on what I consider to be the core topics
of the of that event data infrastructure and data engineering and O'Reilly has
been kind enough to offer us a ticket to strata as well which will be giving away today so about that giveaway if you went
ahead and entered into the contest via either Twitter or the Twilio comm website before the cutoff date your name
or Twitter ID went into a spreadsheet and you actually had a pretty good chance of winning as far as giveaways go
I chose winners using a random number generator to pick four numbers in the
range of my spreadsheet rows the first winner who was lucky number 17 is Lance
Poole who entered via Twitter Lance gets to choose Eve conference ticket for his prize the
second prize winner is Jenka also from Twitter and he gets the ticket remaining
after Lance's choice I've also chosen to runner ups who may be called upon to
fulfill the duties of one of our winners if either of them can attend the event our first runner up is Samuel W and our
second runner up is Dennis a who both happen to have entered via the twilly I comm site if you hear this any of you
please reach out to me to claim your prize now if you didn't win it's not too
late to save 20% on your registration for either conference you can do that by using the registration code PCT WI ml
when registering and I'll include a link to the registration page in the show notes on behalf of the podcast and our
partner O'Reilly thank you to everyone who entered and now onto the show
alright folks I am super excited to bring you this interview my guest this time is Charles Isbell jr. professor and
senior associate dean in the College of Computing at Georgia Institute of Technology Charles and I'd go back a bit
and in fact he's the first AI researcher I ever met his research focus is what he
calls interactive artificial intelligence a discipline of AI specifically focused on the interactions
between AIS and humans Charles and I spent a good chunk of time in our
interview exploring what this means and some of the interesting research results in this field one part of the discussion
I found particularly interesting was the intersection between his AI research and the related fields of marketing and
behavioral economics beyond his research Charles is well known in the ML and AI
worlds for his popular machine learning course on Udacity which he teaches with Brown University professor Michael
Lippmann in addition Charles helped launch the online masters of computer
science program at Georgia Tech we spend quite a bit of time talking about what's really missing in machine learning
education how to make it more accessible of course I'll be linking to Charles and the
resources we mentioned in the show notes which you'll be able to find at - male Icom slash talk slash four and now on to
the interview alright everyone so I'm here with Charles Isbell Charles's
senior associate dean and professor at Georgia Tech and actually Charles and I
go way back so this has been a weird conversation because we're already like 20 minutes in and just getting started
with the interview Charles say what's up to everyone and we'll get started what's
up everyone how are you doing I'm happy to be here I'm happy to be having this conversation awesome well thank you so
much for joining us for this interview now I think we figured out that that
it's been like 20-something years since we met and that was pretty interesting
in that we were roommates during a I guess summer internships at Bell Labs
and that was when you were at MIT and studying AI tell us a little about your
experience at MIT and and what you said you're in the famous AI lab there right
I was although the AI lab no longer exists it merged with the laboratory for computer science and is now known as
csail so I loved my time at MIT and I loved my time at Bell Labs and
eventually you know 18t labs sort of my journey through through AI is a I don't know it's it's a bit of a wandering one
so here I'll just give you my entire history up to now in like 15 seconds and we'll see how that goes
so as you can tell by my accent I was born in Chattanooga Tennessee but my earliest memory is arriving in a moving
truck at the age of three and a half in Atlanta so I think of myself as being from Atlanta but very very early on I
cared a lot about computers and computer science and I knew when I was eight years old that I was gonna do computer
science although I didn't know what it was I knew I was gonna be a professor although I didn't know what it was and I knew I was gonna do AI even though I had
no idea what that was something about building robots yeah at eight years old you know it took me a very long time to
realize that not everybody thought they knew what they wanted to do when they were eight years old I think I was probably a senior in
college before I realized this but I had always sort of wanted to to build intelligent things although I couldn't
have articulated it that way when I was eight years old but I always wanted to build smart things I always thought I thought that computers were great at
least what I thought computers were and I basically just wanted to build you know an intelligent friend that's
basically what I was into at the time and so everything I kind of did from at that point on was about that my actual
first encounter with Bell Labs long before we met I was the summer before
ninth grade so I was 13 years old or so and I built a computer at Bell Labs as a
part of this summer science program what I say I built a computer I mean there was a kid and another engineer did all
of the work while I stood there and watched it yeah it felt like it was a Timex Sinclair t1000 and I remember yeah
it was a little Chiclets thing and it didn't have an on/off switch so when you turned it off you had to unplug it great
and the first program I ever wrote was a piece of code that would fill up the
screen with inverse spaces and it ran out of memory before it could finish doing it and that was my introduction to
real computer so um you know that that's what I figured I needed to fix that and so that whole summer we spent well the
two or so weeks that I was there for that program I spent a lot of time trying to figure out how how to make computer smart and how to make them do
what you wanted to do and it just verified for me that that's what I wanted to do for all of my life so I
kind of dove in from there and I kept getting bigger and better computers and convincing my mom that you know an Apple
2 GS was the right thing and it was the best thing she could do for my education she kind of nodded politely eventually
gave me the things that I wanted and I sort of moved through and one of the advantages of knowing what you want to
do with your life is that you sort of move towards it there's some disadvantages we can talk about those
but really admit that you know I knew I wanted to go to Georgia Tech because I wanted to stay in Atlanta and I thought
that it was the best place for me to be so I went to Georgia Tech as an undergrad I completely dove into AI
didn't do a lot of research at the time because that you know in the 1980s it was a little there
in as many places where you could do the kind of research that you can do now as an undergrad but no matter
sort of what you're into and then decided well there was basically one place you're gonna go to grad school and
I applied to MIT and I went to MIT and I wrote this long essay about building
robots and and trying to make them smart and and and trying to make certain that they wouldn't run out of memory and it
was a it was a lot of fun so I ended up going to MIT immediately started diving
into machine learning which at the time was sort of new for me I knew about AI and I knew I wanted to build robots but
it didn't occur to me that you needed to do something separate to make machines learn and I decided almost immediately
once I was exposed to it that this was the central question you couldn't be smart unless you could learn right and
our machines were never going to be able to do the interesting things that I wanted them to do when I was 8 9 years
old unless they were smart enough to learn how to do them on their own and so I dove into that became a part of the AI
lab went through a couple of advisors I'm still good friends with with all of them and eventually ended up where I was
the side story where we met is I at the same time that I was going through grad
school I got to go to Bell Labs every summer so part of this this fellowship program you know all about this of
course and there I did a lot of really interesting things in AI that had
absolutely nothing to do with what I was doing in grad school but it was so
interesting what they were doing they're trying to build these knowledge representations and kind of really understand how it is you could think and
you could represent thought that I just you know at the time it felt okay that I was at making progress in grad school
because I was still getting to do these cool things and so by the time we met I was doing six months out of the year at
Bell Labs and six months out of the year at MIT more or less oh wow I don't think
I realized that at the time yeah because I take four and a half months over the summer I'd start before everyone else
and I would end after everyone else and I would go back during the winter breaks okay okay so that I think the time that
you kind of came up in AI was during the quote-unquote AI winter is
that right more or less yeah sort of at the tail end of the ad winner nobody told me that I didn't figure that out
until much later so how is that impacted your and your contemporaries perspective
on AI and and the work you've done and how do you like what do you think about
the current popularity of AI and where it's all going
so I think basically what it's mainly done is it the people who are about my
age and a little bit older who lived through the AI winter I think basically spend a lot of their time wondering when
the next AI winter is going to come so a lot of us are very very sort of naturally and reflexively worried that
we're overhyping what's going on right it was it wasn't it was difficult to get funding it wasn't it it was it was
difficult to do work it wasn't that there were weren't people interested in the problem that we were interested in it's that any minute now the federal
government would take away all of the funding and we would you know we would go from having 10 graduate students to having two graduate students and I kind
of think that little fear is always there in the in the back of our heads and we find ourselves thinking please
stop overhyping deep neural networks or you know getting people convinced that we're gonna we're gonna build the next
data or that you know the next Android and self-driving cars and that any minute it can all kind of go wrong so I
think it's probably made us somewhat more cautious at least it's made me somewhat more cautious and trying to think a little bit of about the hype
that sort where it's kind of driven me but you know the other advantage of
being a part of your part is sort of AI when it was during the winter is that you knew that you and the people you
were talking to her in it because we were truly passionate and motivated about solving the problem as opposed to starting a company that would make you
really rich or you know this is the hot thing you were doing it because you you actually cared about it and and I think
that you know that's important right certainly when you're when you're doing research you have to be passionate about the the things that you're doing and
really believe that somehow it's gonna get you someplace interesting and so do you think fear notwithstanding do you
feel like the is the industry structured in the same way such that the risk is
the same or is it different and in particular I'm thinking about is there you know that the funding
source is more distributed now is the level of industrial activity you know more greater now or is it all you know
from a research perspective all still fundamentally the government funding everything and you know when they decide
to change their when the winds change there it all collapses well I think
structurally two things have happened one is computers computing and that sort
of way of of crunching things and data are now ubiquitous they're everywhere so industry is deeply into this it's not
going away Google exists right and everything is driven by data and it turns out that the
parts of AI the parts of computer vision the all the sort of pieces of building intelligent things they're driven by
data now and since we everyone has access to data and everyone has access to computing everyone has access to
really fast machines I'm not worried about sort of it structurally going away in fact the problem is sort of the
opposite it's that everyone has a piece of it now it's driven as much by commercial interest as it is by sort of
pure research and so really the difficulty in some ways is that there's so many opportunities to do what I would
have thought of is AI what we would talk about is machine learning and those kinds of related things that it's easy for things to become diffuse in a way
that wasn't true 25 years ago I don't think this is a bad thing I mean the the
fact that Facebook exists the fact that Google exists the fact that everything is about you're about data and about you
know sort of modeling what people are doing and what things are happening is definitely a good thing and it does mean that there's always gonna be funding for
some piece of it even if it's not being called AI there's not being called machine learning the kind of ideas
metastasized so I'm not really worried about it going away the only thing that worries me is that people are concerned
that bad things will happen because of what we're doing and for good reasons right they're concerned about their
privacy we now have all these abilities its ability to track everything that you do I guarantee you Google is well aware
that you and I are having this conversation right now they probably know what we're gonna say before we say it you know they've got more data on us
than you can possibly a man truthfully we I'm not entirely sure that we mind facelet knows everything about
us there companies out there neither was ever heard of who know kind of everything about it so people worried
about privacy they're also worried about cars running off the road and killing other people they're worried about
robots you know rising up and Terminator style killing us all so they're the kind
of fears is the hype has actually gotten to the point if not what you haven't given us what we promised it's that
you've given us more than what we asked for I think that's where the danger is coming from now but in terms of funding
in terms of people being interested in these problems know that that's driving everything even things you don't think of as being a are being machinery mm-hmm
it's interesting that in in some ways it's in some ways the the industry is
given more in some ways like we're still waiting like you know if you if you serve a sci-fi and you know even the
Jetsons you know we're we're sci-fi thought we would be in you know 2016 and
a lot of ways we're we're not there yet right like a lot of movies would have had the self-driving cars all over the
street but some of this stuff it takes longer it takes longer to develop then
you think and some of the stuff is happening quicker than you think I think well some of the things are
happening that nobody ever thought about I mean you go back and you start thinking about sci-fi it wasn't self-driving cars or self-driving
jetpacks right I still haven't gotten a ship I'm still waiting for that and it's true we we haven't gotten the flying
machines we haven't gotten the the really smart Butler's that are that are taking is everywhere although they and
we've gotten a lot of other things right we've got access to information that we've never had access to before we can
ask questions and we'll get the answers back we can look up anything we want to we can teach ourselves we've gotten a
lot more of things we never thought about than we thought we would and we've gotten less of the kind of obvious
things that that I think people sort of hoped that we would one day get so you know it's a mix I I'm okay with that I
mean I people ask me all the time you know when are the computers gonna achieve sentience and and take over the
world I think the answer is probably never or at least probably not for a very very long time not in the way that people
think about it but we're gonna have very smart machines and we already do doing a whole lot of things for us that we never
sort of expected them to do and the interesting thing is we won't even notice and it won't seem like that big
of a deal I mean for example with the Tesla autonomous cars uber and all the
things that they're doing that's amazing have you ever been in one of these cars if you ever did do this that's amazing
that you can sit in that car and it can drive you through traffic on a highway at 65 miles an hour that's it's amazing
i if you had asked me how you would do something like that 25 years ago I don't I don't even I can barely figure out how
human beings do it and in fact being on the road it's pretty clear to me lots of human beings don't do a very good job of
it but that's a more that's a miracle and we barely notice right every time you get an airplane right the pilots not
flying the airplanes flying itself right and we just take this every day miracle is just the another little thing in fact
you know one of the big complaints if you're in the a I write is that you never actually get credit for the cool
things that you do right ai is kind of the the science and the engineering
making computers act the way they do in the movies right but one of the things
that sort of tied into that is if it's got to be intelligent then it's got to be like humans and if it's got to be
like humans it has to be mysterious and something we can't understand so the problem is every time we do something
even if it's amazing once we know how to do it and we understand it well that can't be real intelligence and so we
don't give the credit to AI so AI sort of has this problem where you you can't
ever win because anything interesting you do well we understand that and that's not real intelligence so it's no longer AI that's just learning or just
it's just computed right it's never this thing right where you succeeded it's just oh that's not the real part the
real intelligent part is this thing and then when you can suddenly do beat you know people at jeopardy well that's
that's not really intelligence the real intelligence is other things so you you basically just keep you know innovating
your way out of out of business and so AI get sort of smaller and and smaller and what it's allowed to call itself because the mystery gets
smaller and smaller is it smaller smaller or further further well it's always sort of infinitely far away all
right it's it's it's something that we can always look for but we can never quite get to sort of Zeno's paradox of
AI but there's not like a you know there's not some finite set of things
that we need to do to figure out AI and we're chipping away at it and it's getting smaller and smaller it's like
the goalpost is moving yeah well so I think both are both oh I think I think
both of those things are true I think there are a finite number of things we need to do we're definitely chipping away at it and so the stuff we need to
do sort of get smaller and smaller though it's still really big but the goal posts keep moving right we've got
cars that can drive themselves more or less and now that's no longer remains it so it's got to be something else but
that was that's amazing and by the way it's not just amazing it has an amazing impact on the world have you seen this I
know you're on Facebook you remember this map that was going around for a while that showed the the most common jobs in every state
you remember this hold about a year ago yeah and do you remember what the most common job is in almost every state in
the u.s. truck driver right yeah truck driver a delivery person taxi driver right that's something like 42 or 44
this diagram the right number but it's over 40 for some reason in other states it's Elementary School teacher I don't
know why but but mostly it's truck driver well you know we're five years away from all the cars delivering
driving themselves right right uber is it's not going to have people involved anymore
my old advisor one of my PhD advisors you know is heading the work at prime
air right so things are gonna be delivered to us by drones and people aren't going to be involved anymore well
that's the most common job in the country and it's going away mm-hmm
so the the goalposts are moving the the things we have to do or getting smaller
or not and people have this sort of feeling what AI is but whether or not you want to call it a air or not it's gonna have a massive impact on our
day-to-day lives it's gonna have a massive impact on the economy it's gonna have a massive impact on sort of how see ourselves and how we interact with
one another and whether you decide that it's a Arnaud or it's intelligent or not whether you move the goalposts or not
it's changing everything around us in deep and profound ways yeah absolutely absolutely so I want to talk about a
couple of really specific things with you and we'll take these in in turn the
first is in the realm of education and the second is in the realm of your
research focus area and reinforcement learning but let's start with the first
of those we got we got through year grad school experience at MIT then you went back to Georgia Tech and most recently
you've been doing a lot of work in online education and around machine
learning maybe walk us through what you're doing there and in particular I'm
curious and maybe as a bit of background here I I didn't go through your entire
course but I took a look at the course that you did with Michael Lippmann and it was I really enjoyed the the
presentation having gone through a number of ML MOOCs and it made me wonder
like what you know what unique views do you bring to you know teaching and/or
learning machine learning and AI that you surfaced in the course work as well
as you know which of the you know are there any views you have that you think kind of go against the grain of the way
people other people are approaching it yeah so so I'm glad you enjoyed the the
classes Michael and I had a ball just a total blast doing it and if you haven't you should watch the the Michael Jackson
parody video we did about machine learning you get to see getting to see Michael dressed up as Michael Jackson
and dancing which is well worth the price of admission which is freak so the
you talk about this kind of interaction we have one of the things that Michael and I tried to do is we decided that
we've been wanting to do things for long together for a long time but you know these are one part of the country I'm in another part of the country we wanted to
do this this machine learning mukhin and this gave us the opportunity to do it and the way we decided to come at it was it's much like
we're doing this now we said you know what education like this should be more
like a podcast you should have a conversation so every time we did one of these these lectures one of us would be
the professor who would try to present the material and the other person would try to be the student so the professor
would do all the preparation and and come up with a sort of lesson and get everything together and the student would do no preparation at all in the
coming cold so you know in that way it's just like regular school and we would
just talk and of course he's an expert I'm an expert and this is what we do all day so it's not like we didn't really
kind of understand what was going on but it turned out and I think this really does come out in the conversations that
we had that we actually have very different views of what's important right so Michael is much more of a
theoretician if you asked him what a I and machine learning is he might say something like computational statistics
I'm much more interested in thinking about it it's kind of practical applications and you know sort of what
you can do as a practitioner to to use these tools to make them work and get synthesis I want people to see that this
thing over here is just like that thing over there which is just like this thing over there and they're all tied together and I'm much less interested in proving
in the abstract what it is that that you actually can learn and what you actually can't look it's not that these things
aren't important I just you know I'm just less interested in them then then Michael is and so we would spend all of
these times kind of arguing some of them sometimes obviously sometimes not about what's going on and what I vote came out
of that you can tell me if you think it's true or not is that the student was drawn into this conversation and at
least got the feeling that not only were they learning some equation or getting ready for some test or doing some assignment but that there really is a
deep conversation going on about AI and machine learning and there's lots of different ways to think about it and and
really that kind of gets to my larger philosophy about the way education works and why I'm so excited about the online
education that that we've been able to do to me what's really missing in education is access right the ability
for people to really to participate in the in the Commons that is education
that is research that is learning and one thing that I think is important for people to understand is that when you
say access some people turn that into affordability you know is it cheap enough you know
tuitions too high you know and that is a part of access but access is actually very different access is just the
ability to be able to participate in the conversation and that if you're capable of getting through it being able to have
the real opportunity to get through it for debility is only a small part of that so one of the things that we've
been doing and and I'm actually quite proud of this over the the last three years is we decided that we wanted to
push on this idea of access and affordability and that online education in MOOCs were one way of doing it and
while we were working on this this machine learning class we wanted to make it a part of something bigger and so Georgia Tech when and when I was there
in my senior associate dean Rowe I guess I still am and am a professor role we wanted to build an entire degree a
graduate level degree that anyone who could get access to the internet and
then who had the time and had the desire would be able to get through an actual full fledged course a whole fledge and
not just a course a full fledge degree a real program and so we created this online MS program it's exactly the same
as our on-campus program same requirements same degree you get through this you get a you get a Master of
Science computer science from a top ten department and it's indistinguishable from the the one that you get on campus
and here's the thing that we did two things just sort of push on this notion of access one is we decided to make it
as inexpensive as possible so the entire the cost of the entire degree is something around sixty six
hundred dollars Wow depending upon how fast you you get through the program so somewhere between six thousand eight
thousand dollars sort of depending upon what you do you can get an entire degree that's pretty inexpensive if you came on
campus and you were out-of-state student it would cost you more like forty six thousand dollars so that was the first thing that we did make it affordable but
the other thing that we decided to do is we decided to admit every single student we believed who could succeed this is a
pretty big deal right if you if we think about our on-campus degree we accept about 10% of our applicants mm-hmm why
do we accept 10% of our applicants because it's all the space we have right I'd estimate somewhere between 60 and 70
percent of the students who apply to our graduate program are above bar but we only got room for 10% of them so only 10% of them get in
and by the way it's it's it's basically a lottery I mean you know when when
you've got your place like Stanford and you're accepting 4 or 5% of the people coming at your undergraduate program
there's no way that that 4 or 5% really better than the next 4 or 5% the 4 or 5% after that you're almost closing your
eyes and just picking people and this is about what we were doing the graduate level we don't like that for our online
degree which again is the same degree as our on-campus degree at this point we're accepting about 60% of applicants ok we
have gone from zero students 3 years ago to 4,000 students this term 4,000
currently enrolled students or is that accumulative 4,000 currently enrolled students okay wow that's pretty good
many on campus uh about two or three hundred okay in fact I by the way it's
not just that we've got 4,000 students they're performing as well as the on-campus student mmm oh by the way it's
not just that we have 4,000 students who are behaving who are performing as well as the on-campus students they look very
different so if you look at our on-campus degree about eighty-five percent of the applicants or our
Nationals vast majority of whom were from India following behind China so about 15 percent of US citizens for our
online degree it's the compliment about 85 80 to 85 percent of the applicants
are US citizens or permanent residents okay right they're in their early 30s early mid-30s not in the earlier mid-20s
most of them are working full-time they've got you know jobs mostly NIT
though not all of them they've got mortgages they've got kids and they're trying to sort of get
through their day but they can't take the time to get further education or to
do that thing they want to do because again they've got mortgages and kids I've got responsibilities right so what's interesting is we've done studies
of this we partnered with Harvard and looked at it we think that of the people who are coming through our program almost none of them would have pursued
an advanced degree otherwise they weren't because they just simply didn't have the option they couldn't take two
years off from their lives to go and pursue a degree because they had too many responsibilities and things that they
had to do but this gives them the option of doing that and so in fact the overlap between them and the people who normally
would get education is almost zero current estimate is that we'll add between eight and ten percent every year
to the number of graduate IT workers in the United States then we otherwise
would have seen and have you looked at what what they're doing afterwards how long is the program been in place and
how long have you been tracking that and to what degree so it's been about three years in fact I think we're beginning
our will be we're just ending our third year now we'll be starting our fourth year so people have just begun to
graduate we had twenty people graduate two terms ago and this semester we're
expecting to see closer to about 250 and we're expecting to see a steady state of closer to a thousand people graduating a
year most of them already had jobs so you know usually the way you measure
success you say okay do people get jobs when they graduate well most of these people already had jobs so they didn't
lose their jobs I guess it's a good thing but it's hard it's hard to know what that it what that impact is because
the usual measures don't really make sense but they're all they all seem to
be happy 97% of them said that they would you know recommend this to two other people many of them do get jobs
while they're in the middle of the program a lot of them get promotions and they move through so you'll have to ask me in five years with what the real
impact is but right now it appears that people are happy they're getting a lot out of it some of them are able to
change careers get promotions and to do things they wouldn't otherwise be able to do because they just couldn't take
the time off to do it so I'm very happy with that and happy with the sort of impact it appears to be having on
students let me ask you this a lot of people who listen to the podcast or you know somewhere along the progression of
learning and and entering machine learning as a field as a profession and
I'm wondering what what do you think the right set of set of educational tools
that take advantage of right MOOCs are a piece of that but there's obviously other pieces that go into making a full
kind of a well-rounded student of machine learning and AI how
do you recommend that students approach that or do you have a philosophy around that well so I sort of do and I do think
it comes out of my in my class if you actually take the class as opposed to watch the lectures you get my
assignments and I'll just describe my first assignment to because I think it actually captures a lot of at least what I believe matters in becoming a either a
machine learning researcher or a machine learning practitioner or even AI or more broadly speaking so here's my first
assignment the first assignment is go find two data sets I don't care what they are so long as they're interesting
they have to be interesting by themselves and they have to be interesting together and you have to convince me that they're interesting
then I want you to implement these five or six algorithms and when I say implement I mean steal the code I don't
really care you'll get any credit whatsoever for implementing and running the code you'd still libraries you know
go get your your favorite implementation of KN or boosting from somewhere else I don't really care and I want you to run
all of those algorithms on those two datasets and I want you to do analysis and explain to me why you got the behavior that you did
why did some of those algorithms we should all work why did some of them behave better on some date on one of the
data sets than the other what sort of things did you learn by applying those algorithms and doing the data analysis
convince me that you've thought about it convince me of what experiments you would need to run in order to really get
the answers to the questions and then run those experiments do all of that and then write it up in 12 pages not 13
pages about 14 pages 12 pages why do I have an assignment like that I have an
assignment like that because I think much about machine learning much about the field that we're in is really about
the practice of doing it you know theoretically all of these algorithms is
particularly in supervised learning they're all very similar they all can learn the same kinds of things you know but there's no free lunch right so there
has to be built into what you're doing deep assumptions about your data what is you're trying to accomplish and you have
to be able to surface those things so if somebody want asked me if I wanted to really do machine learning what do I
need to learn I give them two answers one you need to learn the foundations and the fundamentals yes you need to know the math you understand information
theory you need to understand you know what linear algebra is you need to not flinch or somebody mentions an eigenvector and eigenproblem to you you
need to get the math you and you need to get the computing because it's a fundamentally computing pace discipline and computing is not
math computing is not engineering computing is not science you need to internalize the computing part of
machine learning but just as important and in many ways more important I believe is you have to really dive
deeply into the empirical side of it you have to get dirty with data you have to understand what the difficulties are in
answering the questions you want to answer and you have to really realize that the questions you're asking aren't necessarily the right ones
most of what traps us in machine learning and in lots of other things we do are the unspoken assumptions you have
to surface what those things are and I think that the best way of doing that is by getting your hands and your feet
dirty so my classes are designed to do that to force you to get into a messy
ill-defined situation and to work your way out of it so if you want to do data analysis if you want to do machine
learning that's great it's wonderful I can think of nothing more interesting to do but you have to get out of the
textbooks you have to play through the data and understand why it works the way that it does why the algorithms have the
effect that they do why you can learn some things you can't seem to learn another thing and that I think is actually really missing I think people
either dive down the empirical side and just try to get stuff working but with no understanding of the fundamentals
doesn't even know how to ask the questions or they get so caught up in the fundamentals they don't worry about
whether it actually works in practice or how you would actually apply your ideas and you have to do both especially in a
field like machine learn
they use all the tools of social media tools that are out there to build community to talk to each other to talk
to the faculty to talk to advisors they really build an entire community around what they're doing and really the people
who are in that community do well and the people who are not a part of that community do poorly so one of the things
that's important about the trips that I've been taking in the traveling around the world I've been doing is making certain that we provide the tools so
that people can build local community that makes sense to them because that's how the learning happens
you guys might be single-handedly propping up Google+ well there's no lag
because no one else is using it so you got that nice nice nice so let's switch
gears a little bit and talk about your research your research is your research
focus as I understand it anyways primarily around reinforcement learning or maybe you tell me tell us what your
research focus is nowadays and how you think of that area yes so I I you know
like I said earlier I really been into AI and machine learning for a very long time and it took me a while to figure
out what it was about it that I really cared about and it was easier to see when I was reflecting back on it and
what it is that you know I found interesting what I didn't the kind of machine learning that I care about the
name that we we kind of given in the field is interactive machine learning and or interactive AI I I sometimes
referred to as interactive AI because I care about the AI problem as much as I do the the machine learning problem and what it really is is about what happens
when you instead of just saying oh look here's some data and I'm gonna look at that data and then I'm gonna build a
function and now I can do some prediction you know that you're gonna have a fundamentally incremental and interactive process so I want to model
human beings because I actually care about messy data and there's nothing messier than people so I want human
beings to be a part of the story of how I learned when I say that I think that people learn only through social
communities or they learn best through social communities I think that's actually true for our machines as well so that ends up looking a
like and I spend most of my time worrying about reinforcement learning so you're right about that and the reason I
care about reinforcement learning is that reinforcement learning is really I think trying to do something big and hairy which is actually model what it
means to be an autonomous agent so when people ask me for the one sentence description of what it is that I what it
is that I do I said I care about interactive machine learning I care about building intelligent agents that
have to interact with other intelligent agents perhaps hundreds of thousands of them at a time and some of those intelligent agents
might be human they don't all have to be human but some of them will be and since some of them are human you can't just go
around sending XML packets back and forth you have to actually engage in conversation you have to worry about the
fact that human beings change over time they're inconsistent their error prone they're highly non Markovian there's all
kinds of interesting things about people and you need to be partners with people and you need to be long-lived
in order for you to make progress in the area so that's what I really care about I care about building a system that
doesn't just predict whether you know a car's gonna run into the side of the
road or not but actually deals with the fact that there are several million other people on the road at the same time and you have to interact with those
other people and you have to learn by talking to them and interacting with it and so reinforcement learning is a
subset of that yes and that's right I spend a lot of my time worrying about
game theory I spend a lot of my time worrying about marketing believe it or
not about social behavior and how people tend to interact and and I work with one
another and how you can convince them to to work with you or how you can deal with them if they're trying to work
against you so it's the whole gamut of what it means to interact with other intelligent beings that have their own
set of goals and an interest that might not be the same as yours so something
you mentioned marketing tell me more about how that plays into your research or maybe even give us an example of some
of the research topics you've been looking into recently so I like the marketing question so so I spent a lot
of time with a friend of mine with one of my students is now a professor in North Carolina on something called drama
management so the short version of drama management is well you know you've played video games right uh yep and you know the
thing about video games is the interesting ones are ones where you're you know involved an entire world and an
entire story so what's actually going on is that you're the person building the system for you is trying to build a
story but most stories you just read in you're a passive participant of it and things like games you're actually an
active participant which means there's this trade-off between your sense of autonomy and agency on the one hand and me making certain that you have a good
experience or a good story so you can actually think of lots and lots of things like this you can think about conversations that you have in the
interviews and the podcast is like a story where you're negotiating back and forth and trying to trying to figure out
how to tell the story that you want to tell while still allowing people to say the things that they that they need to say or that they want to say you can
think about all kinds of examples like those can kind of go on for a while but the the thing that the thing there is
that it turns out that because your player or the person who's participating
in building this story with you has their own ideas they might take your ideas off track they might turn your
murder-mystery into a horror story they might turn your interview where you're supposed to be going back and forth and
having conversation into a series of you asked me a questions and I say yes or no and it's not much of an interview for
you right so you have to influence what the player is doing with the human participant is doing or otherwise you
don't end up with a good story that you want to have so there are two ways of doing that one and I think you know you
and most your listeners if you ever heard the expression a game that's on Rails sure so you know that's where well
I'm sorry I'm just not gonna let you go through this door because if you do it breaks the video game or breaks the story and so you're on Rails and the
thing about being on Rails is it takes you out of the story takes you out of the experience and that's what a lot of
people do and a lot of the drama management stuff is about that as well but there's another way of doing and in
fact the right way of doing it if you can make it work is you get the other person the person you're interacting
with you're trying to learn with the story you're trying to get to participate in the story to actually accept your goals as his or her own and
it turns out marketing is very good at this so we build this kind of system where you get people to do the things
that you want them to do by putting them in situations where it's just natural for them to do those things so rather than lock every
door except one - we're in a room so you go through it I make something happen maybe some noise or something
interesting that makes you want to go through that door right so the more like themes of behavioral economics and
incentives and things like that coming into play here right oh that's exactly right so in fact the example of this
that everyone's familiar with is one called scarcity so that's where it turns out that people if they believe that
something is going away they suddenly find it more valuable right right so anybody with kids knows certainly anyone
with kids ten years ago know that Disney has this habit of saying oh we're gonna release on DVD Beauty and the Beast and
then we're never going to release it again uh-huh and so everybody buys it right because it's about to go away or I
mean Black Friday's like this right you're gonna every year at the day after Thanksgiving you go to the store to buy
a bunch of stuff it doesn't make any sense whatsoever there are not any things you want to add but they're going away you're gonna get a price right now
it's on sale and so people react to that they can't help themselves it's a scarcity is just one of is one of
the particular that's very easy to understand there's tons of others of these or something called liking which is it turns out people will do things
for you if they if they like you people react to Authority actually my favorite
example is a something called consistency where if you can get someone to say something out loud that they
believe something they have an almost pathological need to be consistent with it over time so you know you have
anybody in your neighborhood won't mow their lawn here's the way you get them to mow the lawn you wait till it's winter right and so
all the grass is you know kind of dead and it's all the same height and you start up a conversation and you say man
you know it really looks great around here when it's like this you know everything's the same color everything's the same height and you get the person
to agree with that yeah it looks really nice of this like this the next summer they'll know the lawn because they
basically believe that's the way it's supposed to be and you know what's really nice about it it's not that you got them to mow the lawn it's that they
believe that they are in complete control of the idea that they're the one
who made the decision are in charge so that's a long story but the short we built systems like this that
basically convinced people to do the things that we wanted them to do we influenced them so I'm not using machine
learning just to predict your behavior I'm using machine learning to figure out how to intervene to get you to do something and what I really want to
happen is for you to believe it's your own idea so we built this little story just a quick example we built this little story where the whole goal was to
get you to buy a fish at a market not the most exciting story in the world and there are lots of ways we can influence
you to do this with scarcity and various other things and it and so we had people play this game and the people we tried
to influence were much more likely than the people we didn't in buying the fish
and doing the things that we tried to get done sure sure now that's interesting but what's more interesting
is that when you ask the people whether they felt manipulated or not the people
who are not men that manipulated were much more likely to say they felt they were being manipulated than the people who actually were manipulated that's
interesting why is that because the whole the whole way this works the whole way the kind of psychology works is you
feel as if you have agency that you're making the decision when something goes on sale and you decide you're gonna buy
it you don't feel that you've been tricked into buying it you made the decision to do it right and so what's
really interesting the data and doing machine learning it's actually understanding about human behavior its
understanding behavioral economics it's understanding the way marketing tricks work it's it's all about getting the
person to make the decision you know themselves that they want to do this
thing and then they have agency they have control and they're much more likely to see it through the fact that
you kind of tricked them into doing it is neither here nor there so a quick note for listeners anyone that's
interested in digging deeper into some of these ideas there's a great book called influence by Robert Cialdini that
is super accessible and as covers all the things that you you talked about
consistency and scarcity things like that but this brings up a question for
me and that is a lot of the a lot of the work we read about
reinforcement learning nowadays is your training these agents to navigate a
world right and the work you're describing is you've got this world that's essentially training the human to
navigate it and there's an interesting complementary nough stew it and I'm wondering if if that complementary
nichÃ©'s Plourde at all like the things that i'm thinking around like adversarial networks like can you have
the ones rating the other thinking it's training the other does that make any sense is anything happening there oh yeah that's actually very common way of
doing it so the way so the thing that really got me into reinforcement learning when I was a young graduate
student a couple hundred years ago was actually playing games so there was this a guy named sorrow who had built
something called TD gammon which was a particular way of doing reinforcement learning to learn how to play backgammon
and the way it learned to play backgammon was through self play so it it played both sides of the game and it
learned by playing itself how to get better and this is a well when I think of pretty well understood so the
technique for learning right you it's difficult to it if it's too hard you can't learn if it's too easy you can't
learn you need to be just about a little beyond your current level of understanding and so yeah this kind of
thing happens all the time now it is true that a lot of people who worry about machine learning do not think about the kind of complementary nature
that rather than there being an agent that's training in an environment the environment could be in fact training
the agent and people don't always see that in fact my biggest complaint or complaints not the right word but my
biggest I don't know let's say complaint my biggest complain about the way machine learning is portrayed is that
it's portrayed as a supervised learning problem you know I'm gonna give you a bunch of input-output examples and
you're gonna learn the function that map's input/output and that's interesting but I think reinforcement
learning is more interesting because it's this bigger problem you don't have inputs and outputs all you've got is actions you can take and feedback you
get from the world and then from that you have to figure out how to behave that feels richer to me even though in
some sense they're equivalent the unsupervised learning is a very different way of thinking about the world even though again sort of
mathematically they're all kind of equivalent and that kind of Brett what machine learning and AI is is something that I don't think we spend
enough time really thinking about I think people tend to focus on the supervised learning part instead of the
reinforcement learning and the unsupervised lying probably stin the kind of popular press ok hmm so maybe
taking a step back how do you think about the current state of reinforcement
learning like can you characterize the the major research efforts or even is it possible to characterize the major
research efforts into a handful of directions and kind of who's doing what
so I think there's so the answer is no it's way too broad but there's a couple of things that I think are really
interesting one is all this work on deep networks and deep neural networks which you know is the the current thing that
everybody is really into and by the way it's really good work you know I know the people who've been pushing on that for years and years and years and and
they've really been able to to do a lot of interesting things they got the kind of fundamentals right with the math and
they're taking advantage of the fact that we have insane amounts of data so that we can actually sort of take
advantage of those algorithms to do cool things a lot of what's going on at least in my world that people are paying a lot of attention to is figuring out how to
use the stuff that we know from deep networks and even deep learning and applying it to reinforcement work okay
and and rather than doing the supervised learning take where you said well okay here's the state of the world tell me what to do you're actually treating it
the way you treat a reinforcement learning problem you're talking about building value functions over what the
states are in the world and what things are better and then using that to figure out how to make a decision and use what
you learn from making the decision to affect your view of what's valuable in the world and kind of having each one
feed into one another and so recognizing that there's at least two parts of that problem instead of one part of that
problem is a really big deal and being able to marry the kind of math that's come out of supervised learning has been
I think really important that I think has been really interesting as has pushed forward a lot of a lot of what
we've been able to learn in the last couple of years for sure the second thing which I think is interesting in
part because it's my own work and and place where I lived is very related to what we just got through talking about and it's this interactive machine
learning it turns out you know I mentioned earlier that there's no free lunch right so for those of you don't
know the no free lunch theorem basically just says that all algorithms are equally good and in fact not only are
all algorithms equally good but none of them are any better than behaving randomly and the reason that's true is
because overall the infinite number of problems that one could encounter you know any algorithm has just as good a
chance of doing well as that as any other algorithm but it turns out in practice we don't care about every
possible problem in the universe when you care about a small set of problems in the universe and what allows us to
get leverage over that small set our built-in assumptions about that about that world so the problem of learning is
difficult and in some ways impossible but it turns out people are really good
at solving the problems that people are really good at what they're really bad about is explaining to you how they do
what they do but they're really good at solving these problems so a lot of what's been going on in the reinforcement learning world in
particular is taking advantage of people learning from getting people to tell you something or to demonstrate something to
you about how to do something so that you can learn much much faster than you ever would and really what you're
getting out of it is you're getting human beings the human beings assumptions about the way the world
works and you're taking advantage of those assumptions to narrow down the genero down the search space sorry I'll
give you really I'll give you a really quick example so it turns out that people do not think about things in
atomic actions they tend to think about them in these big temporally extended views of the world so they take
something like pac-man right if you asked I've asked you to explain pac-man to me you wouldn't be describing in terms of up-down left-right or what you
would say things like oh well look you need to get the power pellet you need to avoid the ghosts you need to you know
you need to do these four or five things and we run experiments on this where we ask people to create buttons that they
would use if to make pac-man go faster and they come up with these interesting buttons these sort of long term things
but dividing the world up in like that not from up-down left-right but in to get the power pellet avoid the ghosts is
something that is very difficult to learn from scratch but people have already figured this out so you build
systems where people are able to express to you those those shortcuts those assumptions about
the world and then you can learn so much faster than you would ever be able to do
on your own and that's kind of where we're getting a lot basically taking assumptions from the world and getting
them automatically from humans I think that's incredibly important and one of the reasons I think it's important by the way is because so many of the
problems that we actually care about involve people right they involve other people they involve interacting with
people and so you have to understand the fundamental assumptions that people are living and and you have to take advantage of them if you're ever gonna
learn so those are two areas that I happen to think are are really cool and the reinforcement learning space right now are we also learning how to enable
the machines to make the assumption the assumptions themselves like what's happening there yeah but the way they do
it is they kind of do it they do it by dint of observing the world right there's a Michael Lippmann always says a
couple of things that I really like and one is that you know if the person who's doing the programming is doing all the
learning and writing down the data structures then you're stuck right you need the machine itself to be able to
learn its own data structures through observation it needs to be able to to build its own assumptions and its own
models if you're always giving it the model then it's always depending upon you to give it the model it has to be able to to build it's its own model so
fundamental to that is this idea that that you're going to learn these you're gonna build in your own assumptions
you're gonna learn new assumptions and you're gonna build models that you're willing to adapt and so yes yes that's
definitely built into it it's definitely a part of what's going on but the problem is absent nothing
perhaps in anything you you can't know where to start and so this gets us back
full circle to this idea that learning is a social exercise right as human beings we interact with other human
beings that have a bunch of assumptions they built the world together and they kind of know how it works and a lot of
your job is to figure out what it is they've built into the world as assumptions so that you can begin to
learn and our machines have to be able to do the same thing or otherwise they're not actually living in the same world that we're living in hmm
interesting interesting one of the one of the papers that I pulled up of yours
on archive is a paper perceptual reward functions which is pretty recently
published and that goes into I think the the former of these two areas that you
mentioned where you're trying to map kind of the deep learning to you know a
broader set of problems can you describe that the paper there's a bunch of new
things that are coming coming out about that student miner Ashley Edwards is really buying into the fundamental idea
there is that you know people have people's reward functions so if you're a
machine learning guy right there particularly reinforcement learning guy you start talking about rewards and you start talking about States and you
divide the world up into the abstract space and you know that's how you solve problems but we spend most of our time never actually worrying about where
these things come from they're just given to us and this paper is a part of actually a larger body of work that that
I've been I've been paying a little bit of attention to the last couple of years of trying to figure out where those things come from are there principles
about where reward functions come from their principles about where state come from at least with respect to the way
human beings deal with it so that you can actually solve these problems in general and be more robust to small
changes in the environment one of the things that that's true about reinforcement learning is you know
there's a nice little math equation that you need in order to figure out how to learn and determine value and it's very
nice and it's very elegant but it's actually quite fragile so if I were to build a system let's say a robot and I
wanted this robot to get from one end of a hallway to another and along the way
it might do some other interesting things I can construct all my little alphas and my my learning rates and I
can put everything together so that eventually it will learn and that it will do exactly what you want it to do and it won't get so scared that
something bad will happen that it won't move and will get so distracted by some interesting thing over here to the left
that it'll never get to the end of the hallway I can actually do that pretty well but then if I take that robot at
all that it's learned and then I make the hallway five inches longer it will stop working right because the math is
very everything is set up just right so that everything kind of touches one another and what you want to do is you want to
build systems that are robust to that you want to build systems that adapt to that and it turns out that human beings
are very good I mean in fact optimized in some ways for dealing with you know it's still a niche environment right we
do pretty well on earth we don't we won't do pretty well on Mars right we don't do pretty well in space but but
you know it's still a rich environment that we're in and you want to build systems that can do that and so the
perceptual reinforcement learning stuff is about using what we get from our
perceptions directly as the notion of state and as our notion of reward that
we try to get things to look like what we see we try to imitate the things that we see through through our perceptions
rather than you know build simple or actually complex optimization functions
that tell us you know whether this thing actually is like that thing no you just think about what it is that you see what
it is that you're what it is you're perceiving and there's a sort of larger philosophy around that I'm actually quite excited about the work I think
what it allows us to do is to stop thinking about reinforcement learning as
five you know a five tuple where you have to set the values and start thinking about it as a larger
programming problem where the whole thing is it's reinforcement learning is not the thing that you start with it's
the mechanism by which you happen to solve the problem it is itself a programming language is itself a way of
viewing the world and you've got to step back to the level of task and problem instead of thinking about solving this
particular equation interesting yeah I thought the example that was provided in
the introduction to the paper was a good one that was cheap training a robot to
to fold origami like you know what's the state of an origami and how do you how
would you represent that traditionally you know whereas the what's natural for us as humans is to see a picture of the
final result and you know how do you define a you know a score metric or a
distance metric from you know a given current origami to this target yeah it's
and it's a rich problem too because it as soon as if I ask you to explain to me how to do origami we probably have
absolutely no idea how to do something magic that's your hands paper you do a flurry of things and then suddenly there's a dragon I really know that
happens but you know you start saying oh well you start thinking about folding and you start talking to this very high level just like with pac-man right and
the way of dividing up that world is actually important because if you don't divide up the world in the right way you
will never in a million years a billion year in the lifetime of the universe actually solved the problem because
they're just too many possibilities right this goes all the way back to language learning and you know it turns
out that people do not actually correct their children right so you don't get
any negative examples hardly at all in your kid and yet somehow children learn to speak their particular language even
though nobody's telling them when they're what you think you are but you don't actually correct your children and we can prove to you mathematically that
you can't learn under those circumstances so the only way it can be happening is if the world has been divided up in the nice little ways and
there's only a few possibilities and you're searching over those few possibilities because the worlds already been divided up for you if you have to
go through the trouble of dividing up the world yourself and you're just there isn't enough time there aren't enough examples there isn't enough time yeah
yeah yeah so okay I'm just gonna say so
to me if you pop up to the AI level instead of the machine learning level all right that's really the interesting
thing right but what's really exciting about AI right now what's really exciting about machine learning right now is that we finally have enough
computing power you finally have enough mathematical sophistication and we finally have enough data that we can
actually start solving really hard problems where we're gonna be forced to move beyond you know the equation that
we wrote down in 1965 that hasn't changed to thinking about bringing in all of these other things whether it's
marketing and behavioral economics whether whether it's game theory whether it's well engineering whether it's
control you know we're actually gonna have to bring in tons of other things in order to solve the problems we're now at
the point where we can actually do that so we're actually meeting in the middle so that so this is why this is an exciting time for me nice nice so at the
risk of asking a question that we've kind of touched on in a couple different ways already for someone who wants to dig
deeper into the kind of stuff we were just talking about interactive machine learning and AI and reinforcement
learning are there any places that you would point them to get started well I
would start with just a basic machine learning class particularly one that covers reinforcement learning if you really are interested in reinforcement
learning as a topic I mean you know rich Sutton's book is freely available online
it's a great place to start to kind of understand what's going on the class that I teach with Michael Lippmann is
freely online there's lots and lots of lots and lots and lots of examples out there I would actually start with that
and get the basics there's survey papers I mean Google is your friend in this case but if you're the if you're the
kind of person who wants to have someone give you a nice brief overview what's going on then you know hey start with my
class just pick Michael Lippmann oh you can go to Udacity you can get it for free just sort of skim through it and watch through it and you'll you'll
figure out from there where to go and I would really I would really encourage
people to pick a problem that they find interesting if you games are the things for you and
start looking up the deep learned the deep reinforcement learning work on on games there's a there's a bunch of work
done recently on solving most of the Atari games mm-hm using deep learning that's really interesting stuff the problem was
starting there though is that oh now you have to know about convolutional nets are and you know you're gonna find yourself distracted for nine months
while you learn enough math to figure out what's going on I would actually start top-down I would start thinking about the problems what
the issues are before I get so deep into the to the math that I get lost you don't want to lose the forest for the
trees here and it is very easy to lose the forest for the trees because there's so much kind of interesting and very difficult math that's underneath all of
this but really you want to keep sight of the goal right which is to build something that can learn over time can
adapt over a year can live for 20 years and continually learn and adapt and think about what that would mean think
about what it would mean to you as a person and then start asking what kind of background you would need to have in order to build a system that does that
that's great and I'll include links to a bunch of the things that you mentioned in the show notes
I thought I would let me add one thing to the show notes you mentioned the the
book influence I would also recommend the media equation immediate is evasion that is a fantastic book it's one of
these it's a short book about how human beings actually behave and how it turns
out that people will treat machines as if they're humans even though they know better because they'll treat anything
that acts like it has intention as if it has intention and I think that fact alone should influence everyone who's
thinking about building systems that have to interact with humans interesting we're not even all that good at ascribing intention other people the
thought of applying it to machines is and we're gonna have to work on that I
actually think it's the other way around I think the problem is we're incredibly good at ascribing intentions to other people it's just not always the right
intentions ah yeah yeah yeah great so I think we're this has been a great
discussion I appreciate you getting together with me for it especially on a Saturday morning and don't want to
monopolize your Saturday so we'll wrap things up here anything else you'd like to toss out no just I really enjoy this
and we should have this conversation again absolutely absolutely and then for folks that want to get in
touch with you have a fine do you want Google+ the one guy who's still there no
just send me an email it may take me a while to respond that I'm more than happy to respond just cool Val let's see
detect that e to you okay and are you on Twitter or any of the lesser used social networks I have I have a Twitter account
and occasionally I even use it but email is the only way to really get to me
unless you have my cell number and I'm not giving you my cell number nice nice all right great well thanks so much Charles really appreciate it and next
time look absolutely awesome
all right everyone that's our show for today thanks so much for listening if you're
one of our lucky winners or runners-up please reach out to me at sam at twilly
i comm a bunch of you have asked hey what's up with the newsletter no you
haven't missed anything I've just been crazy busy and haven't had a chance to get one out I'm so sorry about that I'm
still working on it and I'll keep you posted thank you so much for your support and catch you next time

----------

-----

--03-- 

-----
Date: 2017.03.1
Link: [# Open Source Data Science Masters, Hybrid AI, Algorithmic Ethics & More with Clare Corthell - #1](https://www.youtube.com/watch?v=YgQxlKPeC-g)
Transcription:

This week we interview Clare Corthell, Founding Partner of Luminant Data, recorded live at the Wrangle Conference. We cover her background and what sheâ€™s been up to lately, the Open Source Data Science Masters project that she created, getting beyond the beginnerâ€™s plateau in machine learning and data science, hybrid AI, the top 3 lessons from her time as a consulting data scientist, and, a recurring topic both here on This Week in Machine Learning and AI and also at the conference: Algorithmic Ethics.

hello everyone and welcome to the podcast if you're looking for this week in machine learning and AI you are in
the right place but if you're a regular listener you may be wondering what happened to the teaser and the awesome
intro music well this is going to be a different kind of show so I've skipped the usual intro to avoid confusion this
week we're going to take a break from the news format and I've got a really interesting interview to share with you in its place as you may recall I spent
Thursday at the Wrangell conference in San Francisco which was organized by cloud era who was kind enough to sponsor the past
couple of episodes of the podcast I'm really glad I went to that event the program was super solid and I met a
bunch of great people one of those people was Claire Cornell whose work was
discussed in one of the very first episodes of the podcast and she was kind enough to agree to be interviewed for
the show we had a really fun discussion and touched on a bunch of interesting topics including her background and what
she's been up to the open source data science master's project that she created getting beyond
the beginner's Plateau in machine learning and data science hybrid AI which is of course the topic of the
article of hers that we talked about on the podcast and a recurring topic both
here on this weekend machine learning and AI but also at the Wrangell conference and that is algorithmic
ethics Before we jump into the interview a few quick logistic notes first what
about the news well if you've already signed up for the email newsletter that I've been talking about on the past few
podcast episodes you'll be receiving a summary of the week's news right in your inbox on Monday morning if not it's not
too late to sign up at two malaya comm slash newsletter second if you're
excited about machine learning and AI and you've got research or writing skills I'm looking for correspondence to
contribute to the podcast and or the Twilio comm website shoot me a note at
Sam at to Amelia comm if you're interested finally I had a blast doing
this interview and I want to know what you think about it and the interview format in general as always you can reach out to me at at
Sam Carrington on Twitter SI m CH AR RI ng t ln with your comments questions or
suggestions alright let's get to it on to the interview alright so I'm here
with Claire Cornwall at the Wrangell conference in San Francisco hey Claire
right to finally meet you in person hi great to meet you in person too Sam yeah so what's particularly exciting
about getting to talk to you is I talked about your post a few I guess it was one
like the second the second podcast I did you wrote that post around the same time
on the hybrid AI mm-hmm and I thought that that was a really interesting post and it was one of the
things that I talked about on the podcast so I'd be looking forward to catching up with you on that as well as
kind of getting an update on what you're up to and what you've been you know digging into so maybe we can talk a
little bit about your background and kind of how you got into data science and machine learning yeah not a problem
Open Source Data Science Masters
so I I'm a little bit weird because I'm not a theoretical physicist or you know
some of some of us in data science are applied physicists too but I'm not in that camp I actually started with
product design and came into it from that perspective so I was actually
working on a very small product kind of a start-up within a start-up when I
decided that I wanted to understand more about our users and what they were doing so I went to the parent company and I
said hey I I don't know much about this but I think I should look into our user logs and try and understand more about
how people are accessing the product and what might be happening and do some basic analytics and the head of
engineering kind of looked at me and goes I what logs I thought well this is
this problematic how am I going to learn about my users if I don't have that and it kind of started me on this long
rabbit hole which has now turned into my career and I actually very much overshot that I I did not end
up in analytics I work on machine learning applications and that problem space now but I came at it from that
perspective and at that point I was really looking for a new direction and
decided to invest the next 7 months in learning everything I could to prepare
myself for a career in data sided data science so I built a curriculum because
at the time I think insight had just started so this was early 2013 ok and
there weren't any academies that focused on this so I built a curriculum around that and published it on github and
that's become a popular resource for people who want to get into data science and understand what it's all about
because there wasn't really a road map at that point but and obviously the open
data science masters is that what the open source data science masters yes be the very descriptive unimaginative name
that I gave it so it's exactly what it is though so that was a really
challenging project to work on and I'm really happy that I was able to put that
out in the open source world and to give you a preview ma'am I'm working on a second version I there were breaking
changes that floated up from Coursera who took a bunch of their courses offline and that's so yeah I'm working
on bubbling up those dependency problems and fixing them ok so after that I went
to a company called meta mark they're actually around the corner from where we are now and they tried to measure
private company growth so very similar to what Bloomberg does for public market companies and sure um at the time when I
joined that company the CTO was ready to divest in machine learning he was convinced it was not going to solve the
problems that they were facing it wasn't going to pose a reasonable set of
solutions for their near term and midterm goals as a company and I on my
first day designed the key components of how we would move that strategy forward
and that company has a I think it's a 10-person team
working on data analysis and machine learning and they're going strong and I
I moved on from that to consulting about a year and a half ago and have been
working with companies of various stages and sizes on getting started with data science or getting started with new
functions within data science that they want to spin up on so helping them understand how to get from A to B and
what it's going to cost them for a solution space that they're investigating there's a lot there to to
dig into on the open source data science master's that was that a little bit of
kind of building the building the parachute as you're jumping out of the plane or building the airplane as you're taking off that kind of thing right you
were learning you're collecting laundry as I was falling out of the sky something like that absolutely it was
perhaps most challenging because I had to rewrite it as I was going so I would
continually check in with people I knew in industry and try and navigate to figure out what what skills were
actually applicable what kind of depth I needed to go in on particular particular topics what was actually key to
understand and to this day this is something that I hear a lot about from people who are hiring managers that when
they try to hire people who are very fresh to the field sometimes they don't have the wealth of an intuition
distributed into the right places so they may know how to build a model but
they don't know how to validate it and they don't know perhaps how to test
data or work with data sets that are very messy there there are various kind
of drawbacks to having a self-guided
education and and having to retarget that as you go is certainly challenging
so and were you did you learn yourself through a self-guided kind of approach
did you collect all this by you in the process of learning it or what was I guess what was the background that you
brought to your getting into data science where did you start yeah I will be so I have a degree from Stanford in
product design but it's through a department called science technology in society and it's actually a hybrid
engineering program so you take two engineering tracks at once and then it ties together with this X component
which is the reason that I talk a lot about ethics publicly which we had an
STS at RPI alsa oh that's great that's great they think it's not get into various places but it's a sister program
to sims's which became a more known program recently because one of the
Instagram founders came from that borough in any case very small program at that point in time now it's one of
the biggest and I focused in computer science and product design through
mechanical engineering so it was very product focused but through those two lenses of engineering so I came into
this with a background in web stack engineering and UX and full digital
product design but I I wasn't coming at it from having no programming experience
so moving into a Python workflow and using tools and technologies like sequel
that I'd seen before was not not the primary challenge for it so I differently wasn't starting from zero
like some people do yeah yeah and how about the stats component where did that come from
I had taken some stats classes in college but there was actually one that I loved and the professor thought I was
the weirdest person in his course I'm sure because it was a bunch of people who wanted to go into management
consulting it was an operational statistics class like how done and
understand how cars pass through for toll this when you have you know two of
them open at any given time long you know he's kind of convex optimization problems and I thought it was just the
most interesting stuff and I couldn't come up with an application that was anywhere close to a career that I
thought I might have I just had no idea how this stuff would be applicable same thing with linguistics I for a long
time would read linguistics textbooks and read a lot of no Chomsky when I was
in high school and more behind it love that stuff my parents
thought I was gonna major in it and I said it's not applicable I can't use it yeah of course flash-forward to several
years later and I'm actually working quite a lot with unstructured text and that's actually the biggest request that
I hear in the market as a consultants how do we work with text and understand
it through a lens that works for us and isn't just a word cloud or a count of
various themes coming up how do we understand it from the perspective of
the customer service industry or we saw it talk here earlier about data science
and HR and understanding feedback those types of applications are becoming very popular and widely requested right so
things always come back right one of my favorite designers has has this well
this thing that he paints on Billboard's this guy Steven Sagmeister he says
everything I do always comes back to me and I think about this all the time because there are always these these
little things these old vignettes that you take and you never quite know when
you're gonna come come back to that and it's going to be relevant to what you're doing yes oh yeah so you've built the
open-source data science masters for folks that are starting at zero and trying to work their way to or starting
someplace and trying to work their way forward what it what are the things that you find folks struggle with the most
Biggest Challenge for the Curriculum
the biggest challenge for the curriculum and for people going through it right now I will say this is the two-sided
problem is that they can't people can't find problems that are appropriately
scoped to showcase their talent and the curriculum can't necessarily provide that right now I have investigated how I
might go about providing you know sample data sets and questions mmm alongside
them that would kind of give you a take-home package of something that would showcase your skills but it it's
actually a lot more work than you'd expect and it's very difficult because it's it's a scoping
problem at its heart you have to have something that has enough depth but isn't overwhelming and can showcase a
bunch of different skills so it's it's a big challenge for people to sell themself selves through providing that
type of portfolio piece yeah and at this
point I think Kaggle does a really good job of curating datasets and providing conversations around analysis and
modeling and predictive algorithms and ways to approach problems and I usually
direct people I was going actually if cadigal was one of the places that you point people yeah yeah I think they do a
really good job at that so they I think they're their entire model is built around that type of that type of work of
probably scoping scoping questions around a set of data and allowing people
to work on it and sometimes rewarding them for that yeah yeah it's interesting
it the podcast has a lot of folks that are you know somewhere on that curve yeah I hear from folks every once in a
while you know asking about how they might apply you know how much how might I apply machine learning and yeah uh you
know healthcare or some problem they have an interest in and it's difficult to to manage that scope but as a
beginner in part because you don't know what you don't know right but at the same time you a lot of times when you go
to some of the public forums where people are asking you know how do i how do I learn this stuff you know people
will say whoa go you know take this course or a course that Coursera course and then go work on a project and the
gap between take this Coursera course and then go work on a you know a pet project is actually pretty huge yes yes
Building Analytical Intuition
and it's a gap that you have to fill with building analytical intuition which
is something that it's very hard to teach but is very learnable so there's that counter intuition there that it's
something that you can learn and it's best learned from other people but it's very hard to learn it from a book so I
do encourage people to to use that practice and you know for example looking at a kegel
competition around healthcare data and taking a stab at it without seeing what other people are working on given a
question and then coming back to see how other people address that question very useful workflow and does provide you
some of the asynchronous communication that you would otherwise have yeah in
person in a company the other thing that I the other key component there that I think is really helpful is to have
questions that are actually appropriate for the data and to be very strict about your own workflow when you're when
you're answering that question because you can get lost in the weeds everywhere and in fact I'd say most data science
teams there their biggest struggle is not necessarily with structure but with
the rigour of having questions that they can actually test in hypotheses that
they can actually test against you know and I certainly do know teams that have
a more Rd approach and that can lead you to interesting places but it doesn't necessarily help you answer a question
because you're not necessarily restricting yourself to that path yeah yeah so you've got a teaching bent you
put together the set of resources and the natural step consulting right we're here teaching fines that are actually
trying to build these teams yeah exactly exactly it is very natural and I I think
Working with Product Managers
the biggest reward that I get in consulting is when I work with someone
who's a little less technical or more distant from the data science and they
start to understand and Intuit as people who would be new to data science they
start to intuit about what's going on with the data and how you can answer the question with the data and why it's
appropriate or not appropriate and what manipulations they need to make and what you know what type of data they need to
make in in intuitions about what they can do with it so that's really
rewarding I've had the pleasure of working with a couple product teams and product teams are great because they
they have a vision for what they want as an outcome and that outcome is really helpful much like a
driving question or hypothesis to guide you through a set of possible solutions
with a lot of rigor and direction so that's been really rewarding to see
product managers saying hey I think this will work because I know that it worked in this other case and we learned about
that a couple weeks ago and it's it is very much like teaching not knowing a little bit about your background now I
can almost imagine the context out of which the hybrid AI blogposts you know
came you know product teams telling you oh can we just throw a tie at this and not have any humans in the loop
yeah you hear a lot of that I definitely heard that like well we know that we
Hybrid AI
have to have some mostly human approach and we can use some predictive
technology alongside them for a while but we're really shooting for 100% at the end and that ultimate vision is very
problematic because as I explained in the post and I can summarize that
briefly but hybrid AI in cases where you need people to look at data where you're
not certain how to predict an outcome or or classify or whatever your your
objective is have them look at that poorly or less confidently predicted
data and make their own judgment about what should happen allowing that to
happen incorporates the possibility of future outcomes and and future inputs so
in cases in cases where you haven't seen everything that you could possibly see
because in the future there will be new and different options it's only necessary that you would always involve
people because you have to incorporate those new those new opportunities and
those new those new possibilities so I I
think tempering our expectations about how much work computers will do and what type of work they will do is really key
to building the right solutions because otherwise we we don't have a good Pareto
8020 approach to our problems where we can say hey let's set
aside this part of the problem because we know it's always going to be too hard for the computer it will cost us 80% of
our time to solve the many percent of the problem and it doesn't actually make sense let's just route that to people we
might learn more about that problems faced in the future but we also know that there's there's prove slop that we
always need to account for and that's important and it sounds like you don't think we're anywhere near you know
getting closing that gap getting to the humans out of the loop it depends on the
application you're looking at mm-hmm absolutely we used a human in the loop system at
matter mark for various tests on machine learning team and we actually had people
in-house in addition to some systems where we had outside labeling done and
we had used vendors for that type of thing there are a couple good options
there but I think the the pragmatism on
the shape of the solution the solution space that's possible to achieve in a
reasonable amount of time and any sort of reasonable cost for a solution all
drive us toward this hybrid case and you
also discover pretty interesting things when you use people or services that
that have people labeling data or providing you feedback because they will
they will give you more information than you asked for in some cases and we actually have had people in the past
come come back to us find our email addresses I don't think they were given them so they they actually went out and
did research now know how to email us and said hey you asked me this question I actually think there's kind of an
issue with how you phrased it it doesn't fully address this other issue have you thought about that I'm worried that I
answered the question wrongly meaning these are people that were on your labeling team yes felt so compelled
these are actually people that weren't weren't even on the team they were an outsourced group of people that were
paid to work on that data so mm-hmm you you learn a lot because you get more
perspectives and more eyes on the data which is which is always a good thing especially when you're thinking about
blind spots that you might have yeah even one of the simple things that I
thought was pretty interesting about that post was you presented some kind of broad brush stats and I don't remember
the specific stats about something along the lines of you know AI by itself right
now you know a machine learning solution can get to 90% accuracy for you know
generalized speech interpretation but in order to really be usable it needs to be 98 or something like that I forget the
numbers but I think you know I don't think people think about that enough
Data Perception
they don't they don't so it's funny when
humans look at data they have a very different perception of it than when they look at the metrics about the data
so for example if you have a classifier for five classes and you look at a
seventy-five percent accurate classifier over all of those classes it will look
like garbage to you as a human even though that's that's pretty high mm-hmm relatively speaking and you probably did
some work to get it to that point right you would probably still call it a
unacceptable option or a non preferable right classifier because that's it looks
like garbage to you and I think the cases where that garbage matters is
where we have to worry about the way that we build hybrid into solving that
last that last component and getting to 99 percent or 95 percent or whatever we
need to feel good about the application for example if we're predicting the
health outcomes for a person that's a very high stakes prediction and we would
probably want to skew much further in in the hybrid direction or in the human
Augmented direction otherwise because the stakes are actually very high so I think when we
start to discriminate between types of application that's where we see this coming in but even for consumer
applications like Google knowledge cards things like that people still curate a
lot of that information it's not necessarily summaries that are generated
by a computer sometimes there are people that are taught to create that data in a
particular way and I think we saw a great example of this a couple weeks ago
when news about how Facebook curates news articles came out and that's a very
good example of how your definitions of taxonomy is your acceptance of how
things are classified and your incorporation of new information all
impact your end user and sometimes in in
very critical ways they might sway how someone votes it might give someone a
perception of of the world that they otherwise might not have in that case so I think we're starting to see the
impacts as well from consumer applications that we thought were not so high in terms of risk and I look forward
to seeing what they invest in at Facebook because I think I wager that
they have people working on this that have an eye on how to make this better but at the end of the day you do end up
in in a semi political discussion about what what fair and balanced means and journalism and it becomes very
domain-specific so I I think it's it's healthy for society to grapple with that
and for us to think very critically about how these things are actually working instead of just engineering them
away and having a hundred percent machine solution are you aware of anyone
any groups working on the problem of hybridity they're from an academic other academic research
topic areas in there somewhere or tools platforms or is it you know everyone
kind of figuring this out on their own building their own custom thing and that's just the state of the art right
Custom Platforms
now um so the short story is that a lot of companies do build their own custom
platforms for for doing this right they usually leverage some sort of
marketplace for data entry data annotation a question-answering
and broader products like Amazon Turk is
a very broad products you can arbitrarily give people tasks and you place a bid on how much you would pay
people for those tasks and they can choose to accept it so a lot of companies will use that platform and
build on top of it and do a lot of integration of that type of system on
the backend so in some ways you know they call this artificial artificial intelligence in some ways the that
component is actually a technology interface itself which is very interesting to think about because there
there are people on the other side of on the other side of the technology but
there are a couple other vendors that do things to support hybrid CrowdFlower is
one in san francisco it does some some of that to my knowledge they do in our
observer validation basically to give
you multiple multiple sets of eyes on a given answer to a question to ensure
that it's correct so you don't have bigger sets of errors or unmeasurable
error and you know where things are going to be more ambiguous that in
itself can be very valuable too because you can you can basically say here's this big set of data or this big set of
questions let's say how would you answer these questions and give it to multiple people and you'll find out where people
disagree and that tells you more about the ambiguity of of the problem space
and where you're going to have to make stronger decisions about what you think is right so that's been a really helpful
thing for clients to understand in the past and I think they have a pretty good
understanding of how that works and we'll see if they build more products around that mm-hmm
do you have a other you know top three takeaways that you know client you found
that clients you know as you look across a set of clients you know these are the
top three things that you know they all you know either learned or need to learn in order to be successful at this stuff
hmm I can tell you the first one is always know what your question is be be
Questions
very precise and know exactly what the answer would look like if you saw it so mm-hmm if you see the answer you'll
you'll know that the right thing is happening I certainly worked with companies that say hey we have all this
data we want to learn from it and I say great what do you want to learn and they say anything and so that's that's a
perfectly healthy and normal place to start but at that point you don't have a question where you can build anything so
you have to formulate questions and decide what's actually valuable for your business which is more of a business and
product space question formulation task so that strategic involvement has
necessarily become part of the business coming from a product background I can
appreciate that I think there are a lot of other independent consultants and
people I know who work solely on questions after they've been fully formed and they say you know once you
have the specs ready happy to work on it but otherwise it's it's not it's not
what we do right and that initial step of defining your question knowing that
it's an appropriate question for the data it really is the space where we we
thrive and help our clients succeed so if they can come in with a strong
understanding of what they have in what they want that's all better I think that's true broadly in
business survive um what else so I was just having a really good conversation
over lunch with a couple people about how one of the things that we don't see
as often in data science machine learning land is a strong leadership
that knows how to market really well so a lot of what I've seen data science
team struggle with is marketing themselves internally or marketing themselves up and managing up to be
sweet or the VP of engineering whoever it is and it's it's really important to
develop those soft skills and understand what your value is relative to the
company sure and I can say that but at the end of the day it's actually extremely difficult to define that value
because your systems maybe giving some
feedback to a business team that allows them to make better decisions but really
they're making their own decisions and they're supporting them with data in some cases but you don't know what the
investments would have looked like otherwise and so comparing the alternate universe that you might have been in had
you not had the technology that your team is building can be extremely difficult to quantify but that is part
of the work of leadership right clearly so I look forward to seeing more
breakout leaders that are really good at that and I think it'll necessarily be
something that we see in the next few years I wouldn't call myself a pessimist
but I would say we're kind of high in the hype cycle right now and I'm not optimistic that we're going the market
will continue going up so to speak goes in a cycle of companies saying hey
we're gonna make this big investment data science we think that data science is a very valuable investment for us for
these reasons and then a couple years later they come back to the team and
they say so have we done and at that point the team really needs to sell what they've
done ideally they'd be selling them along the way as well and I think we're
coming to the end of one of those periods where companies expect to see those big wins and teams really need to
justify their existence and and be able to move the needle and describe how
they're moving the needle yeah yeah soft skills yes yeah take that Venn diagram
of all the things you're supposed to be as an ear I just add like four more things to it no problem yeah nice nice
so that's two but they're big so yeah well on the third eye third is probably
just managed expectations right relative to any other you know any other number
of sets of things in terms of expectations always you said you said it exactly right
Managing Expectations
yeah managing the expectations is probably the biggest thing I do with
clients the first thing I say is I can't do anything to pull a big one out of the Hat I won't be pulling a big one out of a
hat for you if you still want to talk about this and you want to find out what
this technology can do for you and how it can incrementally improve your business and create new opportunities
for products let's talk about that but it's not going to surface anything that you don't know about your own business
because frankly you you know about your business new businesses in existence so you must have some deeper understanding
of what you're doing and when I look at your your deal flow your best customers
or your best customers I'm not gonna tell you that there's there's a there's a sleeper whale somewhere deep inside
Salesforce and that's okay I can give you better confidence to make decisions
and and understand the differential value between things but no promises you
really can't make promises yeah absolutely so you know going back to
this conversation around hybrid AI we started to talk about
you know the role that human humans in a loop play relative to you know their
biases and and you know quote-unquote algorithmic bias and things like that
which actually that was the kickoff panel here it and wrangle conference is
that that's something that you're spending some time looking at now as well right yes so I things are all inter
Preexisting Bias
woven in some way the active learning and human loop patterns of hybrid are
certainly ways to combat actively
reinforcing pre-existing bias if you construct a system to to expose that or
amplify I or both you do not you don't do either and both it depends on what
you know about what you're doing right so if the example I give is a model that
was built at one of my previous employers where we wanted to predict who
would start a startup leave their job and started startup within the next six months and the company had an intent to
build this model create a list of people that were going to start companies soon and sell that list to investors as a
type of pre-crime basically algorithmic pre-crime for seed stage funds and it
could get in early before people even knew that they were going to start companies branch is a which is a like fascinating concept so they used a
number of factors to make this prediction like where you had gone to college what kind of degree you had what your
job title was what your previous employers were there was a bucketing for
the prestige of your college so you know the IV's were at the top and kind of cascaded down through bigger
institutions and that included age so
what we ultimately saw when we predicted who would be a founder in the next six
Bias from the World
months was pretty interesting because all of those factors seem to be directly relevant to how a person's career
which developed them to be a founder in the future and interestingly a lot of
the people in the list were thirty year olds management context management consulting or X I bankers who were white
males and it didn't deviate too far from that and at the time it I thought you
know this is this is pretty uncomfortable but I don't really know why and it took me it took me about a
year to examine that emotion a little more deeply and underneath is actually a
very good reason to be concerned because though you are making a prediction on
characteristics that you you think are fundamentally predictive of an outcome they have bias from the world rolled up
into those factors so all of the all of the decisions that were made to allow
people to get to where they were and become founders in the previous state of
the world and the training data is is your prior for your prediction of who
will be a manager founder and if you don't explicitly observe that you know I
think I clicked through on LinkedIn - maybe that top 120 people on this list
and that's the only way that I knew that there was a certain split of like where
people came from in terms of home country or home state where people came
from in terms of in in terms of age all
of these other characteristics but even name can be ambiguous for what gender
you are so I didn't get a sense of who was actually in this list until I went looked at it and we didn't have columns
that said male female we didn't test against that we didn't predict on that but there were only 13 women and uh-huh
at the top of the list or near the top of the list and I thought well that's that's somehow
unfair right and I think looking back on
that at the time it was we were not talking about that specific type of
diversity in the market for founder's now that that conversation is happening
more it's become a more unambiguous case where you can say all of the prior is
all of the pattern matching so to speak literally coming from VCS as is being
encoded into the algorithm that's making this ultimate prediction and that's not okay so the question then becomes what
do we do and there are a couple of people doing really great work on this I think there's one department at Carnegie
Mellon where someone's coming up with validation metrics that will help you test against the the characteristics you
Algorithms
know you might have a bias outcomes on so in this case you would say how many
men and women are there and in our outcome does it fit our expectation for
what we would want to happen and I think the the real key insight here is that we
want to build algorithms that will construct the world that we want to live
in rather than a world that existed in the past we know the flaws of our current society
to a large extent and some people more than others but as long as we can be
vulnerable to one another and try and
validate that we are not reinforcing unjust actions from the past and just
perpetuating them with algorithms in the future that that is actually key to our work and that's really important for us
to carry as it towards going forward so I'm very excited that we had actually
one talk so far and we will have another talk today about algorithmic bias and and harm and
how these systems affect users and I think it's a conversation that needs to
gain more traction in the practitioner space and we need to examine our own
practices much more closely and know what we're doing perhaps the most
egregious example of this in the press lately was a an algorithm that police
police stations were using across the country to predict recidivism which right this is the one that was exposed
in the Pro Publica article yes exactly and they they did a bunch of work that
Data
they put up on github along with the data set to to explain what what was happening and how they had analyzed the
the outcomes and as far as they they could see what was happening in that
technology and I believe the company still they're still holding it as private IP so even even the police
departments don't understand how this model works the journalist did a really good job of saying this is a big problem
here's here are the metrics and here's the full explanation with the data what's so wrong with this beyond the
anecdotal evidence of this is predicting that one person who is has like
committed one petty crime is more dangerous than someone who's a repeat
criminal and it has been violence it's it's a I think it's a really egregious
case but I don't want to say that it's good these things happen but I think a
few high-profile cases will push the regulatory system to to become more
serious about this so insofar as like it has to get worse before it gets better I'm I'm hoping that that we can get out
ahead of that as practitioners but regulation will certainly be getting there as more and more these cases are
uncovered do you have a vision for how regulation can play here without you
know overly suppressing innovation which is a big concern that you hear the other side it is and a good example of where
Overly suppressing innovation
you see that is in loan assessment and the finance base
where there are very strong regulations about how how you make decisions about
what credit lines people will get so bright get in again like things that got
worse before they got better redlining in the past and other actions that have been taken that were that were deemed
not legal after that that environment has responded extremely strongly to that
there's a there's a pretty good
understanding of what it what is important in making that work
transparent so that you can actually give someone feedback on why they were rejected for a loan or why they were
given a certain loan amount or a certain credit line and wow that's important I
think there are improvements further to be made so I would expect that if the
Regulation and innovation
regulation comes down really hard in the way that it has on that industry if it comes down similarly on others as I
think we're seeing that you it's becoming interested in or then it can stifle innovation and probably grind it
to a very slow pace but we're resilient we'll figure out ways to justify our
existence and how we do our work and I think that's very healthy in the
ecosystem and the ebb and flow of of these factors and regulation and
innovation are always battling it out and run ends the faster we can get out
ahead of it and say no no we actually know what we're doing and we actually know how this works and we are justifying these things and we are
taking the appropriate precautions and trying to be a self-critical as possible
and doing so honestly if we can do that then the regulation will be the regulate
the regulatory environment will be very different when it finally comes to bear
in these other areas that aren't just creditworthiness yeah great
right we've got additional talks here too yeah go check out anything you want to leave folks with point folks too I
would say keep watching the algorithmic harm and ethics arena there's a lot of
Outro
work being done there and there are people that are finding great solutions
and people that are also of course always coming out with more critique and
interesting philosophical perspectives to consider so yeah stay involved in that conversation because it's a it's an
active one and everyone can can be part of it yeah that's that's great and I think the you know your comment about
you know AI encoding the future that we want as opposed to the past that we you
know that we know I think it's a great one very very optimistic yes yes if you
care about that stay involved in the conversation and and yeah be a part of it
great all right thanks so much quick thanks am
all right everyone that's our show for today I really hope you enjoyed the
interview and thanks so much for listening of course you can find the notes for this and every show at the twiddle AI
dot-com website Twi m.l.a i.com the notes for this particular show can
be found that Twilio comm / 11 the number 11 as always I really appreciate
getting your tweets and emails and newsletter subscriptions and iTunes reviews so by all means keep them coming
of course we'd love to have you join the conversation you can tweet me at Sam
Carrington Clare is Clare Cordell and I'm also increasingly using the twimble
AI Twitter handle Twi MLA I looking forward to hearing from you and catch
you next time

----------

-----
--02-- 

-----
Date: 2017.03.01
Link: [How to Build Confidence as an ML Developer with Siraj Raval - #2](https://www.youtube.com/watch?v=aI0vwl2c3qs&t=1s)

Summary:
In the podcast, the host interviews Siraj Raval, a well-known machine learning hacker and educator known for his engaging and informative YouTube series on machine learning and AI. The discussion covers a wide range of topics, including Siraj's journey into machine learning, his advice for aspiring machine learning developers, and his thoughts on the future of the field.

Key Takeaways:

1. **Siraj's Journey into Machine Learning**:
    
    - Siraj was captivated by the potential of robotics and machine learning during his time at Columbia University.
    - His startup, Lucid Robotics, aimed to create home robots but faced challenges in object recognition, a problem now being solved by deep learning.
    - Disillusioned with traditional education, Siraj moved to San Francisco to immerse himself in the tech culture and focus on AI and robotics.
2. **Machine Learning Education and Content Creation**:
    
    - Siraj's YouTube channel started as a side project while he was working at Twilio. It aims to make machine learning accessible to developers.
    - He follows a structured process for his videos: research, coding, technical writing, production, editing, and marketing.
    - The channel serves a diverse audience, including research scientists, developers, and non-technical enthusiasts.
3. **Advice for Machine Learning Aspirants**:
    
    - Siraj emphasizes starting with Python and building simple projects to gain confidence.
    - He advises against being intimidated by the math in research papers, focusing instead on the abstract, background, process, and conclusion.
    - Diversifying learning sources, including videos, articles, and conversations, is crucial for a comprehensive understanding of machine learning concepts.
4. **Future of Machine Learning and AI**:
    
    - Siraj is optimistic about the potential of chatbots and envisions them replacing apps, citing the popularity of platforms like WeChat in Asia.
    - He is concerned about the ethical implications of AI and supports efforts by organizations like OpenAI to ensure the safety and benevolence of AI systems.
    - His future projects include collaborating with big ML to create a video series on pragmatic, real-world applications of machine learning.
5. **Closing Thoughts**:
    
    - Siraj encourages perseverance in learning machine learning, highlighting its potential for lucrative opportunities and intellectual growth.
    - He underscores the importance of machine learning in solving complex problems and believes mastering it can enhance cognitive skills applicable to various domains.

Listeners are encouraged to subscribe to Siraj's YouTube channel for insightful content on machine learning and to consider participating in the O'Reilly AI conference ticket giveaway through Twitter or the podcast's website.

Transcription:

Siraj Raval is a machine learning hacker and teacher whose machine learning for hackers and fresh machine learning youtube series are fun, informative, high energy and practical ways to learn about a ton of machine learning and AI topics. I had a chance to catch up with Siraj in San Francisco recently, and we had a great discussion. Siraj has great advice on how to learn machine learning and build confidence as a machine learning developer, how to research and formulate projects, who to follow on Machine Learning twitter, and much more.

hello everyone and welcome to the podcast if you're a regular listener of
the show I want to start out by saying thank you so much for your support it's been really great to get your notes and
feedback about the show I won't go into the backstory here but going forward I'm going to pivot a bit in my approach to
the show and focus on interviews with interesting folks in machine learning and AI and to accompany the podcast I'm
still going to bring you the news but now via the email newsletter if you'd like to know more about these changes
hop over to the show notes after listening which can be found at twill Malaya com slash talk ta LK / - the
number two okay so about the interview you're about to hear if you've listened
to a few of my previous shows you've probably heard ly mention the name Suraj Ravel Suraj is a machine learning hacker
and educator who's machine learning for hackers and fresh machine learning YouTube series are fun informative
high-energy and practical ways to learn about a ton of machine learning and AI
topics I had a chance to catch up with Suraj in San Francisco recently and we
had a great discussion Suraj has great advice on how to learn machine learning and build confidence as a machine
learning developer how to research and formulate projects who to follow on machine learning Twitter and much more
I'll include links to Suraj's shows and some of the things we discuss in the show notes
a quick note before the interview if you're new to the show you should know
that I've partnered with O'Reilly to give away a ticket to their upcoming AI conference I'll talk about how to enter
after the interview and in the show notes and now onto the interview
all right so I'm here with Suraj Rawal Suraj it's great to meet you in person I've been talking about your YouTube
videos on the podcast for I've talked about a couple of them and like I wanted
to talk about you like every week oh because there's so many great videos but I've held back a lot you know I got to
spread the love so it's great to get a chance to meet you in person and you know I just wanted
to spend a few minutes kind of talking about what you're up to and how you got how you got here sounds good yeah I'm totally down
appreciate you coming over nice so you know let's start there like how did you get into machine learning I so I mean
ever since I was in college like I was I was looking for something to really put
all my energy into and what it was for me was the robotics lab at my school at Columbia and the robotics lab was my
first foray into machine learning and I found that there were all these problems that I wanted to solve that at the time
deep learning wasn't really a thing that deep learning would then solve later like in two years and so I was looking
into like the initial types of machine learning like support vector machines and things like that and just gradually
over time I realized like hey neural Nets deep learning this stuff is like going to solve so many problems so yeah
I've just always been into intelligence and solving intelligence that's that's pretty much my main driver in life like
I want to help humanity solve intelligence because I think it's the most important thing we can do so
Columbia is you know not San Francisco and we're sitting here in San Francisco I what was the path how did you get
how'd you end up here and what are you up to yeah so yeah I was at Columbia and honestly I didn't feel like I really fit
into Columbia I I was you know I fit in really well here in San Francisco and
like Silicon Valley culture I think because I'm you know I'm I'm not so much into like going to classes in person and
just like studying subjects that I don't care a lot about like I just wanted to just study robotics and AI so once I was
at the robotics lab I felt like okay this is this is like my thing I'm going to keep doing this but that only lasted
like a year and then I had a startup called lucid robotics where I was trying to create a robot for home like a platform where each app
would be a physical task so you'd have an app for like cleaning the dishes and stuff clearly this was way out of scope at the
time but at a time you couldn't tell me that I had to see the computer science problems myself what actually ended the
startup I mean we raised funding from severe bhatia the founder pop mill we we had a team what ended the startup was we
couldn't get the robot to pick up a simple novel object they had never seen before mmm deep warning now solves this mm-hmm
so then so then after the starter failed I dropped out I dropped out of Columbia I just was so disenchanted with so many
things and I felt like San Francisco was a place where I could go to rediscover myself and it's been it's been a you
know quite a journey and there's been a lot of uncertainty in my life about what I should be doing the path I should be
moving towards but I'm lucky enough to have come to the conclusions that I have that intelligence is the most important
thing for us to solve in our lifetime because if we don't solve it then some
other catastrophe could wipe out our species whether it's biochemical terrorism or some natural disaster or
you know any something like that we have to solve intelligence yeah so how did
you how did that bring you to doing a YouTube channel yeah so I so I you know
I had a few jobs here as an engineer at CBS Interactive and at Twilio and they were they were incredible I love these
positions but engineering itself just I don't know I felt like I could I could
be having more impact at Tullio I mean Toyota was a great place they I was doing a company it was a great company i
was doing developer education and like that was my full-time role so I was doing technical writing it was the first
time I hadn't just been doing code and I found like okay this is this my thing like technical writing this is awesome I
guess I get to combine my writing ability and my coding abilities yeah but I think for me like the reason that I
left was that I wanted to do video documentation I believe in the future of video documentation and I feel like
tolya was going on a different path so I decided okay you know what I'm just going to do this full-time and so I
started the YouTube channel on the side while I was at Tullio okay so I was making one video a week but the quality
wasn't at the level that I wanted it to get in have enough like the production equipment wasn't good enough yeah I
would I wasn't giving enough time to the technical writing to the only option I had was to quit and do this full-time okay
and so then I was like all right here we go and now how many have you done video so far yeah I think it's like it's at
least at least like 28 videos now Wow almost 30 it's one video week every week
since like January 1st Wow nice nice and you've the original show was called
machine money for hackers is that right yeah machine money for hackers and you you just launched a new one yeah fresh
machine running and does that one replace machine money for hackers or they like to parallel tracks that
continue ongoing you know it's interesting because the idea with machine learning for hackers is that
it's meant for developers and fresh machine learning was also meant for developers but it was like a different topic subset it was like newer things
but what I've noticed is that I have so many subscribers I have three different
types of people who are watching me I have the research scientists the cool kids who are like developing the novel
algorithms then there's the developers who are honestly they're also the cool kids and those are the people I really
want to you know that they were my main motivation from the start like I want to make things for developers yeah and then
there's actually the third subset which I'm learning about which are people who are not really technical but they really
want to be mmm so it's like I have to make videos that are catering to each of them so I'm still kind of trying to
figure out like you know because sometimes my videos cater field research times you sometimes the developers
sometimes to the you know people who are not very technical so I think for now I'm making videos that kind of cater to
all three eventually I want to get up get to the point where I have channels dedicated channel for each of these subjects yeah and for that I have to
grow a little bit more okay right it sounds like in a lot of ways the parallel path to mind with this podcast
I am you know my initial vision was you know I just couldn't get enough machine
learning information like I you know spend the week like opening up a web browser tabs of articles that I wanted
to read or papers that I wanted to take a look at and I end up in a given week with like 80 to 100 of these
ABB's open and I'm like this is ridiculous not and then you spend some time going through it and half of it is
crap and like if everyone's doing the same thing then you know people would appreciate you know something that tries
to figure out what's good and what's not and just spend some time talking about what's good so hey I don't have to spend my we collecting this bag of okay
and you know it's been super rewarding but it's like a ton of work it's a ton
of work and then I wait at your stuff I'm like I can't imagine what goes into you know your videos because you're like
going deep into a topic and then you know you're writing code you're like you know publishing code up on github what's
the process is it is it the same every every week or you like still experimenting yeah so I've developed a
methodology for this over time like I'm building the process so what it is is like the first part is research like
what is the topic I want to talk about and let me just learn about it the second part is the code like programming
it like I'm going to program some very very simple what I like to call the quick start of X to the quick start of
autoencoders a quick start of support vector machines yep then it's the technical writing so research code
technical writing then it's the production so the actual video like shooting it and then it's editing and
then there's marketing and release so
yeah it's like five or six things in sequential order and and I can I manage
to fit all these things into a single week and it takes around 40 to 60 hours
for a single video generally it's closer to 60 hours now I have clients so I'm I'm increasing the
output from one video week to two so that's like 120 hours a week that's a lot this is actually the first week
where I have to make two videos in one week so I'm hiring yeah I'm hiring a
video editor a technical video editor which is like a new role because they have to be a video editor who also knows
kind of like how to code right because I have code and acted like you know they have to point those red arrows at what
I'm talking about this would be I'll leave a middles know what's important then you know what's important right and they have to know that cards you know
the car I'm talking like what I'm saying like oh this is irrelevant what he's talking about like support our Commission's or
whatever yeah so I'm looking for unicorns basically yeah yeah our wheel
well so so you're doing it all in one week you're not like you know researching one week and producing the
you know researching in next week's video one week and then producing it's all self-contained in that week it's all self-contained that weekend and how do
you determine what's you know what you're going to talk about next that's a good question I i yes
so I browsed the machine running subreddit look at what's hot what whatever interests me I look at hacker
news I look at Twitter like Twitter is actually a great learning tool for me I just follow people who I think are
really smart and you know young laocoÃ¶n and stuff like that and I think my the other data source is Facebook groups I
mean a lot of machine learning Facebook losses so yeah whatever is like new and hot and the intersection of what's new
and hot and like what I'm into generally I can figure that out in like one day mm-hmm but it takes all day yeah yeah
yeah the curation part is it's hard I mean as well just there's a lot of stuff
out there you know like I said before and there's a lot of stuff that you know looks really there's a lot of clickbait right it looks really interesting
anything you get you dig deep and it's just nothing there totally or it's like
just way way too technical and you didn't even think it would be like a lot of mess like ah here we go is there an
example of you know something that you thought you wanted to take on and then you just you know found out that it was
just that the math was just too ridiculous um I think uh well I can't
well like if I've decided I'm going to do it I'm just like I literally don't have time to like not do it because I
mean because I have to keep going but I can tell you that the closest I was to like not being able to finish a video
was generative adversarial networks so I was a video thank you that was the hardest video I've ever had to make
because make because that stuff yeah that stuff was pretty hard and I was
that's a video where you're like well this really should be two or three videos but I'm just going to you know cram it down to one and see how it goes
there was one of those where you said something like that I thought yeah no no I could definitely have more than one on
again yeah so what you know that's the topic that's come up on my podcast quite
a bit why don't you talk a little bit about you talk a little bit about Gans what you learn there in doing that
project yeah so yeah Ian Goodfellow
who's now a research scientist at open AI he's the guy who authored the paper but it's a generative model that can
create so if you give it some input data it's going to it's going to have some
output data that's similar to the input data but different so if you feed it
like a collection of faces it's going to generate faces that look similar but are different and at first I was like well
how is this going to be useful but it's a tool for any kind of engineer to design so if you feed it like you know a
collection of living rooms it's going to be able to generate novel living rooms that look photorealistic
which is super cool so it's a tool to help engineers like envision their ideas better and yeah yeah I like the idea of
two dueling entities mm-hmm you know how the discriminator is always trying to
full full full pool its counterpart or the counterpart is always trying to fold
a discriminator which is always trying to detect like oh just if it's false or real right it just keeps doing that
until eventually you know it just gets better and better it's a brilliant idea and like and you know deep mind has done this stuff with
like alphago when they trained to do old neural neural nets against each other to play go so it just got better and better so I think this idea of you know of
having this adversarial nature can be applied to a lot of other things in machine learning have you seen examples of that I've been
looking for that as well I've come across least ideas of
you know where hey if we can fit one machine moaning out where one a eye against another you know and let them
train each other have you seen besides from the the generous stuff that was covered in the
papers other examples of that yeah I think there's a lot of potential for
like game AI so if you have you know a bot versus a human or just two bots personally I think so deep mine is like
really into games which is cool and I think there's a lot of potential for
combining adversarial work with what
they're doing in 3d games mmm just as like an I mean it's kind of like a
suggestion on my part I'm sure they've already thought about this but if you
apply Dan's if you were to apply Gans to games I think that would be really cool
I haven't seen a paper don't we talk about like first-person shooters that getting's or the types of games that
they're playing you know in deep mind the Atari game no no yeah okay so okay
so like what I think is really cool so opening I just yesterday I think release this call for research scientists on
four problems there was number four was what was it it was like create a simulation that where all the entities
get better and better over time like you create an entity in the simulated world and then it learns to like what kind of
food it needs what kind of nutrition it needs to survive better and better like I think there's a lot of potential for
adversarial algorithms there are two entities forcing each other in this in
this simulated world so maybe not necessarily just a game but any kind of simulated environment where you have a
set of constraints and you want you want it you want some kind of AI to get better over time I think we're going to
see a lot of a lot of adversarial algorithms in the future and a lot of
one-shot learning I'd like to see more of that because right now you know all this machine learning stuff is
is kind of siphoned off to these big companies like Facebook and Google and
Apple but with you know if we advanced in one shot learning anybody who is going to be able to create these models
and learning algorithms from sparse data startups for example that only have like you know 100 users but they want to
apply machine learning to that right all right so you dig into a topic like this you
know ganz there's you know research papers how do you how do you how do you
make the leap from that to code to getting code up you know a lot of the
folks that listen to my podcasts I've you know heard from you know are in the process of learning and they're trying
to figure out projects to work on and you know getting from some of the things that they're reading about to you know
some working example like and you've got that down to a science right so at this point yeah term repetition how do you
approach it yeah so I think for me it's
a lot of it is what I learned from Twilio like the idea of having a quick start like a bare bones skeleton that a
developer can then build off of what is the minimum viable product for for demoing this this idea that you have
however simple you can make it do it so if I read something like you know a
paper on like for example there's so
much autoencoders what's the simplest
thing I can do with an autoencoder an autoencoder take some input compresses it and then
reconstruct it it's only it's a three layer it's a very simple neural networks what's it's the most simple demo I can
make with this and I just think about it and I'm like okay compression Oh compression just compression alone so
just use it as a compression algorithm so like a zip you know zipping yeah zipping and unzipping so then I was like
okay so then I'm like okay so how do i code it so what I first do is I search github so typing like very you know
auto-encoder and I look under Python because python is awesome and I see what's been done before
usually usually something has been done before and so I'll take that and like kind of like strip away the unnecessary
things and add documentation and that's going to be the demo okay and the rare case it's not then I have to go it myself okay yeah how often does that
happen the dad let me get yourself myself like entirely I'd say like off the top of my
head probably like 15 percent of the time okay yeah so one of the you know the two lessons I got from that are you
know simplify simplify simplify like you know you know whether it's the actual
coding or the you know trying to parse the research is like figure out what
this thing is that it's bare essence and focus on that and then like reuse like
figure out what's been done and trying to use that is there anything else and like any other pieces of advice that
you'd give to folks that are trying to work this process
yeah just like don't be intimidated by papers like there is a lot of math and
papers but like really like when I'm reading a paper it's the abstract and the background the
process and the conclusion which matter the most to me and there's really not a
lot of math it's when they start describing you know certain aspects of
the process that it can get really really confusing if you don't know math notation but math notation itself is in
serious need of an upgrade so it's more human readable right now it's kind of siphoned off to
just these research scientists who look at this stuff every day so I think you know we're going to start
to see innovations and how we publish scientific research so that anybody can
read it what that's going to look like I'm not sure but they're just they're just too much coming out right now
and it's too important for few people for only a few people to be able to read
it so so I would say if you just read
the abstract of a paper and you feel as you get the gist that's fine I you can go start searching github with
just that don't feel like you know guilty or or something and definitely look at
videos and and what I try to do whenever I'm trying to learn something is I try to get as many different types of data sources that can into my brain that
always helps videos articles conversations with people you know
there's a lot of content out there it's just going to increase exponentially mm-hmm yeah and I find the same thing
and find also that sometimes it doesn't work out like you expect like the I did
a review of the Google research wide and deep learning paper and you know they've
got this cool YouTube video that you know simplifies everything but I want something that in I didn't get it but then I went through the paper and it
made sense and then I went back to the video and like oh yeah I don't know why I didn't get that before yeah so I agree
that you know having lots of different types of input can make a big difference
so what's like what's your roadmap for for upcoming topics and research yeah so
in terms of like the topics themselves I kind of decide them week to week but the for the the larger vision is to just
focus on machine learning kind of be like Khan Academy for machine learning hmm and I'm going to start needing help
and from other people so I'm hiring and yeah I just try to get I'm just
optimizing for subscribers I want to get you know I want to get every developer on the planet to at least to a little
bit of machine learning I think it's super important there are about 10 million developers in the planet right
now and not nearly there's not nearly enough that are even aware of how
important machine learning is architecture engineering is a new feature engineering and if you want to
win if you have a start-up if you if you have an idea if you want to win you at
this point you have to implement some sort of AI because if you don't someone else will right so I want to make
machine learning you know democratize and make it accessible and understandable as possible to as many people as possible so I'm just going to
keep going down that path and do whatever it takes to make that and that's going to be lots and lots of videos in the future you've got a new
project that you're working on is that something that you can talk about that's going to be public as well that's going to be public yeah it's not going to be
on my channel it's going to be on theirs but a big ml I've done partner drive now you know I've signed a deal with big ml
so I'm going to be making a video series for them about their product and it's going to be it's called cloud machine
learning and it's using big ml to do a bunch of pragmatic real world
applications so the first one is going to be about climate change and how we
can use machine learning to prevent climate change okay so I'm super excited about that one and then so like because
video content takes up so much of my time I don't really have time to do things like client acquisition and yeah
you know all this all this stuff so that the clients that I do have other people who have come to me and right now have like seven or eight and they're kind of
in a queue and yeah I'm just taking on as much as I can handle at a time and as
I grow I'm going to start start looking at more you know ideally you know my
goal is to one day partner with deepmind I want to make videos for deepmind but
they're like I consider them like the Navy SEALs machines I've got to get I've got to get to that level you know the
Apollo program for intelligence uh-huh you know if we solve intelligence we can
apply to anything like just think of it as an objective function or X any problem you can ever think of if you
have the right learning algorithm and you say solve for X it could solve it
scientific research problems or even existential problems the questions that have plagued us since day one who are we
why are we here what's the point of the universe we might not be capable of figuring this stuff out ourselves but a
highly intelligent AI could we might not like the answers we might we might not
like the answers we might not like answers but we're so where do you fall on the whole singularity thing that's
what Rick wakes that's why I wake up in the morning I want to make it a benevolent singularity happen
as soon as possible uh-huh as soon as possible what do you think about the the open AI research stuff that they put out
a few weeks ago on safe machine learning have you been following that stuff also
so specifically like ways to they publish this framework for like four or
five different areas of research that need to be kind of dug into so that we can ensure the safety and you know
benevolence as you put it of they I like you know if we've got a AI powered robot
you know how do we how do we ensure that you know it doesn't learn how to gain
the system and and you know for example if it's being programmed to clean right
how does how do we know that how do we program it so that it doesn't sweep stuff onto the carpet right yeah I think
yeah and then Google had like the Killswitch paper which I thought was super cool I like that opening is
thinking about this I love opening I in general deep though the concept behind it I think yeah it's like preventing AI
from doing bad things is going to be
really important I mean technology has always been a double-edged sword you know with the engineering with the National iron and I think that you know
with security I think that's going to be one of the first where we're going to see we're going to see the power of AI
and when it comes to protecting humans if you have an AI and you train it to
get really good at breaking into systems the only thing that's going to be able
to stop that is an AI that's good at detecting an AI that can break into systems so I think it's a great thing
what they're doing I think it's really important I think it's really important and and what Miri is doing as well and
you know the ethics committee that deepmind has at Google to prevent you
know malevolent types of AI all this stuff is super super important marries the machine intelligence Research Institute yes yeah and Berkeley
yeah and you know there's always a question why can we stop it you know who knows but it's good to try
and honestly if malevolent AI doesn't
kill us then something else likely will so this is something that just something
that's really important so we'll see or maybe taking a step back like for folks
that are trying to do you have a quick like if someone you know Frank comes
with fuses okay I really you know I really want to learn this stuff now like what's your curriculum what's your you
know one two three list of stuff to do is it do you think are you trying to
build your videos so that someone could just follow those and get everything that they need or are there some set of
resources that you think are kind of canonical yeah so I think that my videos
are good if you know some basic Python if you know Python then my group my
videos are a great starting point but I think that my videos alone are not enough you know it's one of the things
of like combining different data sources so I think my videos in addition to some long-form content I think so for me big
ml has some great long-form content there's so you know I actually don't
think I think that a there's a deep
learning course on Udacity by a Google engineer who works at Google brain I forgot what it's called but if you if
you google just like Udacity deep learning that that course is really good that to me is even the tensorflow course
or not the tensor for course that's that's a great one but there's one specifically on deep
learning in general there's just so much
I think it's one of those things where it's like okay so if you're saying like I want to learn machine learning I would
say like okay first one Python by reading the book learn Python the
hard way and then once you once you feel like you're comfortable with Python just
start building things just start building things and and my videos are good because it's application-specific and it make it really easy for you to
you know just when you get compiled and you see your model train train and then you can apply to other things that is
like super useful for for your confidence as a machine learner and also
just as a developer so and also just go to github and search for machine
learning projects like search for like machine learning demo or machine learning simple and just look at those
read Me's download them compile them open it in a text editor and just like go through them one by one and like
really try to understand what's happening you know and and I would say start off at a high level because you
know some people would say it the other way I start off at a low level like learn exactly how to angle move these models from scratch no no I would say
startup at a high level and once you get it at a high level then you can start like trying to rebuild you know you know
you know neural net from scratch yeah like custom and implement from ground up
or implement some research or something like yeah yeah taraj torch lots of great libraries these days nice what a Cora is
well sorry Quora is awesome I've learned so much from Cora just like you know cuz
you because I'll find one question on Quora on deep learning and on the sidebar it's like oh my god all these
questions are amazing and then you have people like yawn Laocoon answering them and like Monica Anderson and like all
these like really famous for search scientists yeah so I've learned a lot other people that you is it primarily
like search based that where you find stuff or are you there following particular people and just kind of
keeping up with them there it's search based search base yeah it's search base how about on Twitter are there you
mention Jana there other folks that you yeah final good signal de noise machine
learning folks on for sure I think for me I think Chris
Dixon is a partner in injuries in Horowitz he he's good for like knowing you know what's up and coming in machine
learning I think is a good eye for that one person in general that I really respect about technology is Balaji
Sreenivasan who's also a partner in Greece in Horowitz that guy knows he
lives in the future and yeah bored yon Laocoon is also like a great twitter handle I don't think
I've come across that one yet but it sounds funny I really like it yeah yeah nice and yeah and then oh and following
these big companies like Amazon and Google is really important because you can see like oh they just released you
know DSST any their new machine learning library which needed to be renamed but yeah
destiny it's great I don't think you're not a fan I'm not a fan I mean in
general I think machine running needs better marketing like a lot ah you know
not I'm not going to do anybody so nice
so for folks that aren't familiar with your your videos are there you know two
or three that like man these were my favorite or these were my best or yeah I
think so the the one that ended up being was popular was AI composure that was the
second video I made for machine learning for hackers so AI composure so the top three would be like AI composure the one
I'm most proud of is generative adversarial networks because it was the hardest and the one that I thought was
the dopest was a build an AI artist because I just thought that application
was really cool like applying some style to some novel you know picture this like
the thing that prism is doing now that prism is doing yeah nice exact nice and so what is the what's composure
composure is generating machine like machine
generated music so you feed it some music like a data set of like you know 500 songs it will learn the style of
that song and then it can generate new music in that same style okay and I trained him in the video over British
folk music but you could apply anything to it one idea I thought would be really cool that should do is take hans zimmer
music and generate music in the style of hans zimmer so prism for music printer
that's kind of what the magenta magenta is current well I'm admit they're not trying to do specifically that but did
you use magenta any of their code in your in this project I didn't no no no this was a four that
it was um not that yeah I was uh I found
it I found it on github oh can I modified it okay yeah nice interesting
we're going to ask you oh you've done a you've done a couple videos on chat bots
and chat bots platforms I was a good one what do you think about that space and
like what would you learn and you know over a few attempts at at playing around with that stuff yeah I think you know I
with the marketing effort I expected I expected wit AI like Facebook's
acquisition that that chat bar chat bot building technology to be way better
than it was but what ended up happening is I found that API AI had a much better it was much easier for me to build a
chat bot with API oh yeah yeah I think that chat bots in general are going to get really popular and we're going to replace all of our apps with chat BOTS
this is already happening in Asia so like with WeChat like most lot of people don't even use apps anymore yeah you
know in China and stuff because it's so easy to say like you know you can even
combine different apps together like book me an uber in 30 minutes at this location and take me to my favorite
restaurant and that's querying like Yelp your Google or whatever you know your
preferences locally and the uber app or if it was in China Quoddy so there's a
lot of potential for chat BOTS and if you are like right now thinking about you know building a
startup I like if it was me if it was me I would be doing some kind of chatbot
cap off Forex where there is no chat bot because this is just going to get more more popular like I already use chat
bots and messenger for like you know like detecting like scores and stuff
like that it scores uh like like Ward sports in fluff yeah okay no I mean uh not that I watch sports but
like I just play around with them yeah yeah what does what are the other useful chat BOTS I haven't found anything
that's particularly useful like the thing that I haven't played around with a bunch of them but you know they're all
kind of it was it doesn't feel like we're there yet yeah we're not there yet we're not there
yet but we will be in like a year that's how fast this faces movies yeah yeah
it's just going to it's yeah right now there's a lot of people who are like really needy pin this stuff in their
building but we're going to see a lot of releases and like you know because the bigger players haven't caught on yet you
know that's one of the reasons but depth I promise you Eber has a team dedicated to this Airbnb has it's not getting late
right absolutely and then Facebook's releasing em which they're training right now full-time looks like humans and machines and just
getting better and better people internally at Facebook are using this and I talked to it you know some of these people and they really like it and
I was like please give me an invite to em like once you like I haven't invited oh my oh my god if anyone at face Facebook is listening
we both want invites to us yes please please nice nice
well that it's been great it's been great chatting with you anything that you'd want to leave folks with or point
them to or you know have them to check out yeah yeah I would say definitely
subscribe to my channel because I'm just getting started and that's where I'm putting all of my effort into right now
and what else I would say if you're a
unicorn video producer or univ machine mercy yeah or if you're a video editor who happens to know how to program as
well definitely you know I see me on Twitter because I'm looking for you because I need you and yeah just
don't don't give up if you know machine learning is you know it's kind of hard
but it's a worthwhile endeavor and you can make a lot of money for it from it
and you can learn a lot and it's going to and if you if you get good at
learning about machine learning which is one of the it can be one of the hardest things on the planet to learn like
solving intelligence like the human brain like how do we work is equivalent to asking like what is the universe if
you can get good at that it's just going to train your brain to be good at so many different things so yeah don't give
up awesome awesome well thanks yeah thanks alright
everyone that's it for today's interview before we go a reminder that this week in machine learning and AI and O'Reilly
have partnered to offer one lucky listener a free pass to the inaugural O'Reilly AI conference which will be
held at the end of September in New York City you can enter via Twitter or the twill Malaya comm website by doing one
of the following three things the preferred way of entering is via Twitter just follow at swim la I Twi ml AI and
retweet the contest tweet that I'll pin to the account and post in the show notes do those two things and you'll be
entered if you're not on Twitter you can sign up for my newsletter at 2 a Malay Icom slash newsletter and add a note
please enter me in the additional comments field finally if you're not on
Twitter and you aren't interested in the newsletter no problem just go to the contact form
on to Malaya comm and send me a message with that form using AI contest as the
subject the drawing will be open to entries through September 1st and I'll announce the winner on the September 2nd
show good luck and hope to see you in New York thanks again for listening


----------

-----
--01--

-----
Date: 2017.03.01
Link: [Engineering Practical Machine Learning Systems with Xavier Amatriain - #3](https://www.youtube.com/watch?v=PjI24wKYfcw)
Transcription:

My guest this time is Xavier Amatriain. Xavier is a former researcher who went on to lead the machine learning recommendations team at Netflix, and is now the vice president of engineering at Quora, the Q&A site. We spend quite a bit of time digging into each of these experiences in the interview. Here are just a few of the things we cover in our discussion: Why Netflix invested $1 million in the Netflix Prize, but didnâ€™t use the winning solution; What goes into engineering practical machine learning systems; The problem Xavier has with the deep learning hype; And, what the heck is a multi-arm bandit and how can it help us.


Intro
[Music]
hello everyone and welcome to twill talk the podcast where I interview interesting people doing interesting
things and machine learning and artificial intelligence I am very excited to share this interview with you
for the show my guest is cha VA imani a chav EA is a former researcher who went
on to lead the machine learning recommendations team at netflix and is now the vice president of engineering at
Quora the Q&A site cha VA and I spend quite a bit of time digging into each of
these experiences in the interview here are just a few of the things you'll learn from our discussion why Netflix
invested 1 million dollars in a Netflix prize it didn't use the winning solution
what goes into engineering practical machine learning systems anyway the
problem that cha VA has with the deep learning hype and what the heck is a multi-armed bandit and how can it help
us of course I'll be linking to the resources we mentioned in the show notes which you'll be able to find at twillie
Icom slash talk slash three its twi mla i comm slash ta lk slash the number
three a quick note before the interview you've got just a few days left to enter
into my drawing to win a free ticket to the O'Reilly AI conference I'll talk
about how to enter after the interview and in the show notes and now onto the show [Music]
hey everyone I'm here with javi I'm at 3 on and javi hey why don't we get started
Xaviers background
by your at your core now why don't we have you talk a little bit about what you do there sure so I'm at kora and VP
of engineering so I leave the whole engineering organization right now my
background though is more in machine learning previously - Korra I was at Netflix and I was leaving the machine
learning recommendation steam at Netflix and even before that I was doing
research and I was in academia and my background again is on recommendations
machine learning and so on and I've published papers on that space for some years so it's kind of interesting that
somebody with this kind of background is now the VP of engineering of a growing
company like Korra where I I need to deal with a lot of different controllers not only machine learning right but it
also tells you a little bit of story of what is important for Korra as a company
as a product and that also aligned with with some of the trends that we're seeing in industry right that more and
more the machine learning AI people that used to be like closed in a room by a
corner and they were like the weirdos in the lab now they're having a lot more influence on decisions that are being
made on how to design products and how to run companies and in my case I'm
that's probably like one of the reasons that I'm in this position now leading
the whole engineering organization because for us machine learning it's like like a big part of our success and
how we're growing all right so there's there's a ton in there and and we'd really like to get to know you a little
How did you learn machine learning
bit better so let's let's rewind a bit you mentioned that you spend some time in academia yeah how did you learn
machine learning where did you go to school and where did you where were you working in academia yeah that's a good
question so I'm I'm actually kind of old for what you see right now and I have a
long history behind me and I'm saying that because so I when I did my PhD which by
the way I did it back in Spain I'm originally from Barcelona Spain so when I did my PhD I was mostly interested in
signal processing and particularly in signal processing and systems design
related to audio and music actually that's what my PhD was based on and at
that point in time it was that age when
multimedia and signal processing was kind of like the hot thing and machine learning was not so much so I did use
some machine learning here and there for different aspects of my research and particularly for some of the initial
recommendation system that I work on that were related to music but it wasn't my core area so I was more into signal
processing and systems doing my PhD so I would say that I I got into machine learning more on the chops and after I
left my you know my I did my PhD I went
did some more multimedia related research in the University of California
Santa Barbara UCSB so I was there I was working on virtual reality and mersive
environments and that was also very cool it's kind of coming back again now but I
was really interested in that space combining signal processing and multimedia and this kind of immersive
and virtual reality environment but after that I became more and more
interested on the data side I was like how do we use the data and how do we infer information from the data and
particularly very interested in how do we understand users from the data right so that's what kind of led me to cook
forget a little bit more about the signals so that we're a little bit like you know more there's they're also data
but they're like cold data that come from systems and focus more on the human
generated data and try to build intelligent systems that understand so I
I did then I switched my research and went into working for a few years in
recommendations and using machine learning and different kind of approaches know in machine learning but
also human-computer interaction approaches to build this intelligent
sort of like assistance that tell you what you like and what you don't like so that's what actually led me eventually
internet person to leading the recommendations team there okay now you you dangled a big shiny object
Signals processing and machine learning
in front of my eyes and that is signals processing that was an area that I studied in grad school as well and I'm
curious well hey I'm curious if you could explain wavelets to me because
that was one thing that he's getting be a hard time but actually no we're not going to talk about that I'm wondering
if you see any parallels I'm wondering if there are any interesting things
happening at the intersection of signals processing and machine learning just out
of curiosity do you have you seen anything there's actually a ton of those
intersections there's there's more of like the principles and how they intersect that I would say probably more
interesting now there is the intersection and the application side of things right so if you think about it a
lot of the systems that are now being being that are being built using mmm
machine learning approaches particularly deep learning to understand things like
speech recognition or image recognition those were considered in the past like
signal processing applications and and for example although I didn't
professionally focus too much in speech recognition I did study quite a lot of that ran you know at that time where we
were using hidden Markov models and these other techniques that for us
in the signal processing world it wasn't you know they were just tools and means to an end so it wasn't like the most important
part of the system although you know it would really like the core of it but now that's moved towards some the
learning and RN ends and so on so there's always been an intersection right between machine learning and
signal processing and there's always a lot to say about how to interpret
signals wherever they come from and those signals again could be audio could
be meat speech music video images and you need to build system that actually
either understand those things or even able to generate them in some way and
there's always a well not always but at some point it's clear that that's evolved more into having a layer of
intelligence in the middle that it's going to be learned and that comes from a machine learning system that it's sort
of like at the heart of any of those systems mm-hmm right great so you you
Netflix
made your way from academia and ended up at Netflix immediately prior to where
you are now Quora and your focus there was on recommendation systems yeah I
started with a very specific focus on recommendation systems we which you
could consider there's a continuation and natural continuation of the Netflix
prize the famous 1 million dollar Netflix prize which by the way that's what got me connected to Netflix as I
was dabbling with it and also part of using that data set port for some of my
research ok so so yeah so I started with wood you could consider like the
continuation of that Netflix price but already working for Netflix and we eventually grew the team to be more of a
core machine learning algorithms team that was building not only recommendations but algorithms for
search and for different things where they two images and it was it grew to
sort of like being a core machine learning / algorithms team that was serving different purposes beyond
recommendations but recommendations is something that is very important for an
epoch right so that was really like probably the core of the team at any given time okay
Netflix Prize
so in terms of the you mentioned the next the Netflix prize am I correct that
the the winning prize entry was never
really implemented at Netflix I'm glad you asked this because I get this
question all the time and I I react to
it by saying it is correct the final entry that doesn't mean that it was
useless right so there's I'm saying that because people immediately when I say
that and we wrote it in a blog post at net when I was in Netflix at some point and even though it was very clearly
explained people still took away like Oh Netflix wasted a million dollars and they didn't use the outcome that's not
true actually net we've got way more than 1 million dollar back in research and in interesting stuff that is being
used and was used in different parts of different systems so so if there's a
difference between was the final entry used and the answer is no it was not used there were over a hundred and
thirty different machine learning models combined in an example most of the
different models that were there were adding just a tiny increase in accuracy and a lot of complexity and they were
not worth it so the reality is that two of the models on their own gave like
enough accuracy that the other hundred and thirty-some were not needed or they
were not worthy are I that's it doesn't mean that they were not useful to understand what they were adding and how
they were adding it so again the story is the final Prize winning entry with
the complex combination of all those methods in an example was not used as it was but the learnings were worth much
more than what was invested in the prize and part of the final winning entry the
most important method were actually used directly in production okay yeah this is I came across this recently
Economics of Machine Learning
in an interesting blog post by Josh bloom over at wise and he talked about
the economics of machine learning basically all of the various trade offs
that get you know that come up when real business is trying to figure out how to
put machine learning into production and that was one of the examples he used about how I forget how many pages or
something the the final algorithm was but a hundred Garrity models that's a huge that's a huge model yeah we're
friends okay and he knows a lot about so we've talked about this in person and
you know the thing is that story is so juicy that you can spin it in many
different ways I actually recently got this is pretty crazy but I did get in my
facebook feed and advertisement from MathWorks trying to sell me MATLAB that was using
that story and saying something like Netflix did not use their final winning entry we can help you with MATLAB and
all that was so I don't even I don't get
where they're going at all with that well I don't know but you know that's the point is that yeah the real story is
yes you do need to be concerned and I'm always say the same I mean you know you
need to be concern about system complexity and about making sure that whatever you do in research it's
actually be playable and it's and it's good too or easy to build engineering
around it but that's very different from saying that the Netflix prize was a waste of time or money
sure so can you maybe spend some time walking walking through some of the
System Complexity
various factors right so you mentioned engineering time and there's you know so
there's obviously like an implementable 'ti from a complexity perspective you know they're going to be data aspects
there's computational obviously you know when you think about
you know practical machine learning and the the issues of you know urine you're an engineering VP of engineering now not
a VP of machine learning research or some when you think about you know engineering these systems at large-scale
what are the things that you need to think about oh there's like a long list
of things and you mentioned a few of them system complexity is one which
actually spans into different sub areas
and different concerns are related to the system to the complex of the system one of them which is often overlooked is
simply cost right it's like if you can do something in a single machine which I
have this kind of infamous slide that I when I show people some people don't
like very much is I tell people that they can do probably almost everything they need to do in machine learning in a
single machine and I have reasons to say that but the point is that if you add
unnecessary system complexity first of all you're gonna have a lot more cost so you're gonna have this now huge number
of machine of machines that you and I have to maintain in a cluster or paid Amazon for hey double you guys cost
rahab so that's one and it's probably obvious and it's probably not the most important the most important one is
system complexity reduces your speed of
innovation and if you have a system that it's really complex from the get-go in the waiting on it becomes like a huge
pain right because then I'm trying to tweak something and it turns out that that's something it's just one of the
10,000 knobs that are in the system and it's hard to know what it did it's hard to understand whether it improve things
and if you keep your system as simple as possible as long as possible your
innovation is going to improve and your innovation speed because you're going to be in a much better position to then
change things dramatically improve them understand what you're doing and what is improving and at some point you need to
add complexity there's no way around it it's like complexity at enough improvement and either in
accuracy or basically whatever metric you care about that it's worth adding but the problem is you don't want
arbitrary complexity from the start because that mid term and long term is
gonna impact you're gonna be end up in a local optima so to sort of speak and
you're never going to reach that global one that you would be getting if you keep your options simple as much as
possible interesting to me the thing that it brought up was the the notion of
Algorithmic Debt
technical debt that's typically applied to code write code debt isn't has anyone have you come across anyone that's
thought this through in terms of algorithmic debt oh yeah there's this
interesting paper that was published actually originally was published in the works of that ICO organizing nips and
it's called a high interest credit car of machine learning depth and it's it's
a very good read it's by a couple of authors from Google by the way so they
know what they're talking about in terms of machine learning death and so it's
something that it's been discussed again even in papers right so all right so it's it's it's an something that any
organization will face at some point and it's something that it's really important and it's really important and
many levels not only at the level of the system itself but also and I would go
further that that's part of like the the core of the machine learning algorithm
algorithmic design right it's like it's a camp razor principle of you know if
you have a possibility of choosing between two things always to the simplest one and part of the reason is
because you want to minimize your depth as long as possible and only make things more complicated when they really need
to be and they're adding up enough so that goes back to the lesson learned
from the Netflix prizes like you know yeah sure you can have you can go for the more complex solution but it's the
Delta an improvement that is adding more the huge increase in complexity and many times the answer is gonna be no that's
Know Deep Learning
an interesting segue to one of the topics that I wanted to chat with you about you recently tweeted about a
natural language processing course and the hashtag you use was know deep
learning across a number of your public appearances you've maybe developed a
little reputation for mr. hashtag know deep learning and of course I'm being I'm being artificially you know
controversial here yeah I understand it this is you know it's a it's a tool in the toolbox but some of our earlier
discussion about system complexity I think is one of the issues that you have with deep learning maybe walk us through you know what your position how you
think of your position on deep learning and you know why you bring it up interesting when I talk about deep
learning I always start by having a few slides in my presentation that explain how deep learning works right so I want
to get that out of the way and say hey I know the deep learning works and it's great for a few things actually
particularly for natural language processing I think that it's getting to a point where it's the default tool for
many things and it's great so the reason I was using the hashtag is just to warn
people that if they were looking for deep learning it wasn't available in that course so I think it is it's very
important for people to understand what
is the right tool for the right task and for example we use deep learning at
Quora for several things but we have a lot of text and going back to the NLP example there's many things now in text
processing that RNN are you know they're actually the simplest solution there is
because you can you can find some of this ready available open source to to
kids that have already been trained and you can even use the model as it is you don't even need to have your own data
set or then you can retrain it but that basically becomes simple enough that
that could be your for approach to a an NLP task that you
have in hand but that's very different from saying that that's equally true for
all machine learning applications and you need to understand like what is the
complexity you're paying for defaulting to machine learning for everything you have and I've seen a couple of examples
recently where I think we're you know in a dangerous situation where a lot of
people especially like more junior researchers or engineers that they're
you know they've come into industry right at the cusp of the deep learning
bubble or wave or whatever we want to call it and their their mind goes straight into deep learning as the
default solution for anything and I've seen cases where I've had engineers in
some companies tell me hey I'm using this central flow architecture on a
problem where I have ten thousand examples and thirty features and I want
to ask you a question and my answer like why are you doing this to yourself I mean if you have ten thousand example
than thirty features do you really think you need deep learning model with a
bunch of layers and most of the time the answer is no and even if the classifier
you're building with that deep learning
architecture is let's say in the best case one percent better than the one you
could be building with a simple logistic regression you're still going to be better off going for the logistic
regression because what I'm going back to what I was saying before your ability to innovate on that initial model is
gonna be much bigger than your ability to innovate on a very complex the neural
net that you don't really understand what's going on in inside so I guess my the point that I'm trying to make when I
talk about quote-unquote know deep learning is that deep learning should be
another of the tools we have in our toolkit and there's a lot of other very interesting machine learning tools and
even research that is going on that it's we should still pay attention to there's
a problem also in the research world right now with deep learning is that because it's so new and there are so
many so much low-hanging fruit it feels like you know that it's the easiest way
to get a paper except that is to an incremental improvement or not so incremental an improvement on some deep
learning approach and that's why we're seeing all the conferences now dominated with deep learning things right even
when you go to a comfort like KDE or the
ACM recommender systems conference that I'm going to be attending in September
you start seeing like a bunch of deep learning papers because it's new it's easily innovating using deep learning
but we run the risk of like saying oh yeah this is the one thing that works for everything and we're going to try to
find all the nails that apply to this hammer and we'll think that they're all
they all look the same and and I think that's there is a danger in that so
Cora
you've touched a little bit on some of the things you're doing it Cora maybe tell us a little bit about you know tell
us a bit about your experiences there and you know what are some of the interesting problems that you face there
yeah sure so I that's a great question
one of the things that I love about Cora and one of the reasons as I said before
that we have a VP of engineering with this kind of background in machine learning and algorithms is that
everywhere I look on our product and our day shoes that we're dealing with I see
problems that are solvable and should be solved through machine learning right so
now if I sorry for interrupting but it's likely that most of the people listening
What is Cora
know what Cora is but maybe you can start with just an explanation of the
site and the mission sure that's yeah that's that's a very good point and it's a very
good point because also even people that know us and users frequently they have a misconception about what core is so core
is on the surface is a question and answer site and application mmm but our
mission goes beyond that so the mission of Cora is to grow and share the world's
knowledge and we think that the question/answer paradigm is really well-suited for actually growing and
sharing knowledge just to give a different example of the only other
quote/unquote company that has a similar mission which would be Wikipedia Wikipedia also believes in the spreading
or growing the knowledge but they believe in the encyclopedic format and that leads to a bunch of different
product decisions of course so we feel like question entering and a broader
notion of what knowledge is so Wikipedia is about factual knowledge we think that
for example an expert opinion is also knowledge and should be included in any knowledge base so all of that defines
our decisions and using question answer for now it's working really well and we
think it's the ideal vehicle but we are not close to trying different things and actually we do have even different
things as of today in our product that enable that knowledge growing and knowledge sharing so so another way to
look at and to understand Cora is the different sort of like networks that
overlay in the product so we do have obviously a knowledge network and even
another one that it's a topical network so we have entities of knowledge that are connected to each other topics that
are related to each other and then on top of that we add the social aspect
right so then we have people and we have people that are connected to other people and we have people are connected
to topics and to knowledge entities and this sort of like different overlays of
different graphs at different levels and the different connections between them is what makes the whole data problem
very exciting because we have a lot of applications that cross the different networks in different directions and we
have for example algorithms that are purely on the content space and they tell us how good is the quality of a
given piece of content we have other algorithms that tell us how likely is a person to answer a question on a given
topic we have different kinds of machine learning algorithms that their purpose
is sort of like trying to understand and predict different aspects of this dynamic system and the relations between
all these different entities so again examples of things that we do we do a
lot of recommendations you have initially in your home page you'll see a
feed of different stories that include questions and answers that we're
optimizing for you to be interested on and that's kind of similar to the
Facebook feed but has other implications and a different objective function so
recommendations like that recommendations that you get through email we optimize the notifications that
you get through different devices also using machine learning that's all on the personalization side of things then we
have content approaches to infer the quality of a content to do things like
ranking answers according to how good they are we have things related to a lot of the
text side of things automatic topic labeling hunting for a topic out of a given text how to find
similarities and questions and answers how to find duplicates and then also we
have the whole abuse side of things which also uses machine learning we need
to one of the things that cor is known about for is you know keeping high quality content and that's the quality
piece but also keeping a very healthy positive community and we do that with
very good Norm's and also algorithms that detect any form of spam harassment bad actors
and so on so forth and each one of them is a different machine learning algorithm so it's really exciting in
that sense because we have covering sort of like a huge space of applications and data tabs that go into this applications
interesting can you talk a little bit about the extent to which you use hybrid
Hybrid ML
machine learning plus human yeah obviously there's a big component of the site that you could argue as hybrid as
users are ranking different answers but are there ways that you're using hybrid
approaches behind the scenes yes we are so so one way to think about it is
initially all everything all of this was manual right then the first initial better version of korah
there were no algorithms in place and all of it needed to be manual so we do
have a team of moderators and people that look at content and there's always
a point where algorithms are not gonna be sufficient and you need somebody to look at the nuances of like is this
answer about this politician really violating our norms yet or no and it's
like really nuanced and we need to have person look at it so the way we think about it is there's if you think about
any content moderation issue there's always gonna be a high portion of the
stuff that you have on your side that is gonna be good and it's gonna be good with no doubt so you can have algorithms
that say hey above this threshold I'm totally positive this is good stuff we don't need to worry about it there's
always gonna be a huge another huge but at some part of your content is going to be really bad and there's no doubt about
it so there's another threshold that tells you below this threshold I'm just gonna remove this stuff because it's basically
crap and you don't want it that's how you keep the quality of your content in
the side right now there's this gray area between those two thresholds and the tricky part right so you have to do
two things one is there's you know you have to have people then look at this
gray area and decide yeah this is not really that bad it should where it should be okay with it and at the same
time you need to improve your algorithms to get those two thresholds as close to each other as possible and that's very
interesting right because it represents sort of like a research challenge for us to improve our machine learning algorithms say hey we won the gray area
of the things are uncertain too over time become as small as possible and
we're doing that and at the same time the gray area is still there and when when we have things in the gray area we
need to use some humans in the loop to understand what's going on uh-huh so if
Challenges
if Cora were to do a corner prize analogous to the next Netflix plot
surprise what would it be about what are some of the biggest challenges that you
face well there's in each of those dimensions
that I mentioned before there's there's challenges that are still not resolved
but I guess thinking of the Netflix Brydon is something that would be kind
of similar and I think it's very interesting and probably that an obvious
direction we would go is that for something like knowledge there's also a
problem which is similar to a network price of how do you get the right piece of content to the right person and
content is expressed in two ways right one is a content that you can consume so
that's an answer that you can read and you can enjoy you can learn from it and the other one is a question that you can
answer so both of those things how to route them to the right person and how
to optimize algorithm for those two things are at the core of what we're doing and they're very important for us
so I think we could think of like again drawing the analogy of the Netflix prize
of like question and answer recommendation being like a very interesting
topic that for us it's like a super interesting challenge it also connects
like many different dimensions on the different overlays that I was talking
about because it's not only about personalization but you also have to care about content quality right and you
have to care about those different aspects and how they feed into what the
users are going to be doing and reacting to short term but more importantly what
they're going to be reacting to long term I've talked about that in the past in some of my presentations like this serve
like tension between short term metrics and long term metrics and that's
something that a lot of companies have done the wrong thing and they've gone downhill because of that and it's really
important to understand for example in the context of content how to avoid
clickbait right and if you're optimizing for some things you're gonna get clicks sure but those clicks are gonna turn
into people not visiting your site ever again after a number of weeks so all
those things sort of like fit into this picture of like content recommendation
or knowledge recommendation how do you address the short term long term
Tradeoff
trade-off now maybe even in the context of a clickbait type of application so so
there's different things that go into it I would say that that that's one of the
most interesting research areas that I don't think it's been really solved even
in research literature because there's it's very hard to get enough good quality data sets to even do something
about it if you're if you're a research in academia and in the industry I mean
as far as I know from the people that I talk there's obviously different things that we're all doing but a holistic
approach to it is it's hard the one important thing is you do need to make
sure that you're running your a B test with right sort of metrics right because at
the end of the day you can be optimizing whatever you want in the lab and say oh it's a ranking problem I'm going to be
optimizing NBC G but the reality of that metric that you're optimizing in the lab
with your algorithm might not really correlate perfectly to what you want to
get and the product in that long term metric so first you need to make sure that you whatever you tune in you're in
the lab you run a b test long enough term with the right metric to understand
like what is the met what what are the effects that whatever you're doing have
on the users and then you kind of work backwards from that right once you have
the right metric on your a B test you know oh if I do this my users end up not coming back after two weeks what did I
do then you back you kind of work backwards from that and try to understand like what are the metrics in the lab that you
could have used to sort of like predict that kind of behavior in the kind of effect right so building regression
models from sort of like your easy to compute metrics which they're all gonna
be related to some kind of error or some kind of information retrieval precision
and recall whatever you will into the real world of usage I think that's
that's very important and then there's a there's a ton of other things that you can do once you understand those
dynamics in trying to define your training set in a way that actually mmm
defines the problem in the right way and and sometimes I have talked about this
also in the past people have this mistake of I need to use all the data
that I have and I need to use the raw data that I have and sometimes that's
not really the answer you might need to use some data and not others because some of the data that you might be
feeding into the into your model might be teaching them all the wrong thing or you might need to wake your data in a
way that some is more important than because it leads to longer term effects that you're interested on while other
might lead to a click but nothing else so there's there's a lot of sort of like
different details going into the recipe but again I don't think there is a very
holistic approach to it or not that I'm aware of okay one thing that that came to mind
Deep Learning
for me was and this is maybe going back to our discussion around deep learning
there is some research happening around our n ends and you know when the the
reinforcement or the score you know comes later and how the RNA and can optimize for you know this delayed
gratification so to speak and so you know maybe this is where you know if this gets sophisticated enough this is
where you get some benefit from the introducing the complexity of our n ends
where an otherwise simple model might come into play yeah that's definitely
true so multiples or approaches that have any sense of sequencing or time or
evolution over time to have some depth
some benefits and and and you can use them it's not only about a RN and another thing that comes to mind it's
some reinforcement learning approaches I mean the typical one of the typical ways
to deal with this is to use and some form of multi-armed bandit approach to
deal with the exploration exploitation trade-off it's it's more of like yeah you know I know that you're picking on
this but let me try to explore more things let me try to come up over time have you know my model converged to
something that is a global optimal rather than getting stuck on that local one where I am right now so yes you're
right I mean and some of the sequential RN ends with some form of memory and and
ability to sort of like remember different stages and sort of like end up
converging over time into a better optimal they're super interesting
before we before we get too far can you explain simply multi-armed
Multiarmed bandit
bandit yeah so the idea is pretty simple
I mean multi-armed bandit comes from this notion of you have the typical
image that people use is the slot machines in a casino you imagine that
you go into a casino and you have ten slot machines in front of you and you don't know which arm you should pull
that where the multi-armed bandit come from and you start trying one you say oh this one is giving me some interesting
prices but should I try another one because maybe the one that I have next to me it's actually better than this one
and how to deal with this dilemma of out of multiple arms that you could be pulling there's some that you have more
information about and you know with a degree of certainty how well they're
doing and there are others that you don't really know anything about them should you risk yourself and go into the
ones you don't know anything about them or should you just stick to the one that kind of works but maybe it's not the
optimal one so I think that's the whole point of the multi-armed bandit
approaches it's like they try to find a way in which you can have an optimal
policy to deciding whether you should continue pulling from the same arm or you should go to a different one and
there's there's a lot of literature on
on this in and you can read about it and
I usually joke about it there's a lot of literature about multi-armed bandit but
there's only one that actually works in practice but I don't know if I want to
give that away I mean it's it's it's pretty it's pretty well known in an
industry that comes from sampling is the easiest and sort of like more practical
approach to multi-armed bandit so I think that and I'm not giving too much away by saying that
right so what uh what what are you finding most exciting about machine
Most exciting thing about machine learning
learning right now obviously there is a ton of things going on there's deep
learning stuff there's the work that's happening around BOTS there's applying deep learning to NLP like you know given
everything that's going on like what what's the most exciting and and do you
get to apply that in your work and what's the most exciting thing that you're actually working on so I think
the most exciting thing for me it's almost a non-technical thing it's more
of a this thing coming from society as a whole that it's accepted as a given that
machine learning and AI is inevitably part of making a better future and I
think you know there are still so people that were argue about dangers and and about robots taking over and so on but I
think generally speaking society is convinced and it's pretty much you know
all bought in you know self-driving cars a couple years ago people thought we
were crazy about self-driving cars and now they're already being tested with people writing in them so so I think
this sort of like change in society and in mindset and people realizing that oh
machine learning is not really evil it can be it's a tool it can be used in my
benefit and it's something that I expect things to have to have so not very long
ago seeing something that was an algorithm or machine learning was like
whoa what's going on I'm losing control this is not something I like and now it's shifting to the opposites like you
expect applications you expect gadgets to have intelligence and to have machine
learning otherwise you're disappointed like oh my gosh I need to tell this phone everything I want the phone should
know what I want right so I think that's that's a very very interesting shift and
and it kind of connects a lot with some things were doing at Quora right in kora
we are very user focused and we want to we want to keep this warm feeling of
you're in the community you're sharing knowledge this is very important for you it's very important for the people but
you're gonna be surrounded by all this different algorithms that make your life much better and they protect you from
bad people and they protect you from horrible content that you don't want to read and they help you get your content
to the right people that want to read about it and they're going to be helped by it so this combination of silver the
warmth of community social aspects and knowledge but also surrounded by all
this different algorithms in a seamless way I think that's super exciting and it's something that you need to strike
the right balance but it's something that just a few years ago we wouldn't thought about because you know again
algorithms were the this cold evil thing that you kind of like wanted to stay
away from so I think that's that's a very interesting trend and something that I'm excited about mm-hmm we're
Favorite conferences
coming to the end of our time but I've got a couple more quick questions for you the first is you go to a lot of
conferences what are your favorite conferences in the space I would say I
go to a lot of conferences unfortunately especially now since my time as a VP of
engineering is pretty precious and I don't get that much time there's some
confidence that I have ties for a very long time and I keep going to them because I am very interested in the
content but also I'm interested in the community one of them is it's a small conference actually the it's the ACM
recommender systems conference that's a conference that is purely focus on personalization and recommendations and
I help start the whole thing I was a the general chair for that in 2010 back in
Barcelona and I kept kind of keep in touch it's in one of the interesting things about this community which I
think it's a little bit similar to for example kdd is that it's a very diverse
kind of audience and you don't get machine learning nips audience everyone
focus on the algorithm and you know squeezing 1% more or less pharmacy or
maa out of their algorithm there's a combination of algorithms but also
application and then user oriented research which I think connects to the
vision that I was saying right this connection between user orientation and algorithms it's very interesting so yeah
the ACM recommender systems conference which by the way is happening in Boston if anyone is listening from Boston or
wants to travel there this year is in the US and it's gonna be super interesting and when is it it's coming
up right yeah it's in September 15 so yeah in a few weeks we're gonna be there
and just to give an example I'm giving a tutorial with together with Deepak our
well from LinkedIn on all the latest research and all the evolution of
recommendation systems in industry and we're going to be giving a holistic perspective of me coming from Netflix
and now Cora and him having been at Yahoo and now reading machine learning at LinkedIn so it's gonna be sort of
like an overview of all this kind of machine learning techniques for
recommendations so that's that's an example of a small focus conference but
also with a very broad audience which I kind of enjoyed kdd which just happened
to be in San Francisco recently I like the community a lot and I think I can
find all of very interesting approaches in applications I usually yeah I'm very
application driven in my approach to machine learning so although I will I will read all the papers or not not all
sorry some papers from nips and ICML I I
tend to go to more sort of like application driven conferences and and there's also a lot
of small conferences that are organized now there are kind of local and focused
on the industry side of machine learning ml comp is one that comes to mind that I
attend regularly because I find the audience to be very interesting and very
engaging and it's a lot of practitioners from industry mixed together with a
bunch of researchers and that intersection I think it's it's really interesting mm-hmm great great and then
one more question that you're in a particularly good place to answer for us
and that is who are the people to follow the machine learning folks to follow on Quora oh that's a great question but we
have a lot of them so we've been doing actually a very strong push for this
product feature that we have with these sessions which is similar to an MA AMA and we brought in I would say like all
the top machine learning researchers to do some session in the past we've had
people like I mean most of the deep learning folks like young laocoÃ¶n and
joshua banjo and we've had Andrew Inc we've had Peter Norbeck we've had a lot
of different researchers and I would say most of the authors of the famous
machine learning books like Kevin Murphy from Google and so on or we we had Ian
Goodfellow the main author of the deep learning book also recently so there's
like a good you I would say 50 people that you would follow we've also had
people that leave machine learning in different companies like Amazon we have
my friend Ralph furbish from Amazon or Joaquin from Facebook
so there's like a huge machine learning community in kora that it's very active and very strong so it's one of our
strongest areas right now so I would recommend people who are interested in
machine learning there's like a ton of knowledge there and growing so yeah
great great well chubby thank you so much for spending the time with us I
Outro
learned a ton and I'm sure the folks that listen well as well anything you'd
like to leave us with no I mean thanks for having me and it was great to share
a little bit of that knowledge in this different format which it's also a way
of spreading knowledge and I look forward to interacting with people especially on Quora I myself write a lot
of different answers on different topics in doing machine learning oh that's a good point before we go where can folks
find you how can folks engage with you I'm pretty public on Twitter as you
mentioned you you had seen a bunch of my tweets so I'm they can find me on Twitter on
Chama at X am eighty or on Korra I'm also very active so you can follow me on
Korra and message me there I usually keep a very active public profile so
it's not hard to find me and I have a pretty weird name and last name so it's like it's really to go into the wrong
direction if you if you google my name yeah alright great thanks so much have you yeah thank you Sam
alright everyone that's it for today's interview before we go a reminder that
this week in machine learning and AI and O'Reilly have partnered to offer one lucky listener a free pass to the
inaugural O'Reilly AI conference which will be held at the end of September in New York City you can enter via Twitter
or the twill Malaya comm website by doing one of the following three things the preferred way of entering is via
Twitter just follow at twimble AI Twi la i and retweet the contest tweet that
I'll pin to the account and post in the shownotes do those two things and you'll be entered if you're not on Twitter you can
sign up for my newsletter at twill Malaya com / newsletter and add a note please enter me in the additional
comments field finally if you're not on Twitter and you aren't interested in the newsletter no problem just go to the
contact form on - Malaya comm and send me a message with that form using AI
contest as the subject the drawing will be open to entries through September 1st
and I'll announce the winner on the September 2nd show good luck and hope to see you in New York thanks again for
listening

**Introduction**

- Podcast: TwiML Talk, discussing machine learning and AI.
- Guest: Xavier Amatriain, VP of Engineering at Quora, former leader of Netflix's machine learning recommendations team.

**Xavier's Background**

- Initially focused on signal processing and multimedia research.
- Transitioned to focus on machine learning, especially in understanding user preferences through data.
- Shifted from academia to Netflix, leading their machine learning recommendations team, and later to Quora as VP of Engineering.

**Key Discussions**

1. **Netflix Prize**
    
    - Netflix invested $1 million but didn't use the final winning solution directly.
    - The prize led to valuable research and learnings.
    - Final entry's complexity outweighed its benefits, though some components were used.
2. **Engineering Practical Machine Learning Systems**
    
    - Importance of balancing system complexity with the ability to innovate.
    - Avoiding unnecessary complexity helps in maintaining a fast pace of innovation.
    - The concept of 'algorithmic debt' â€“ the cost of maintaining complex ML systems.
3. **Deep Learning Hype**
    
    - Xavier acknowledges deep learning's effectiveness in certain domains like NLP.
    - Warns against using deep learning as a default solution for all ML problems.
    - Emphasizes understanding the right tool for the task to avoid unnecessary complexity.
4. **Quora's Engineering Challenges**
    
    - Quora's focus on knowledge sharing and the importance of machine learning in their product.
    - Various ML applications in Quora, from content quality assessment to user behavior prediction.
    - The use of hybrid machine learning-human approaches, especially in content moderation.
5. **Challenges and Future Directions**
    
    - The balance between short-term metrics and long-term goals.
    - The challenge of optimizing content delivery to users.
    - The importance of using the right metrics and data in training models to align with long-term objectives.
6. **Exciting Aspects of Machine Learning**
    
    - Societal acceptance and expectation of ML/AI integration in products and services.
    - The blend of community warmth and algorithmic intelligence in products like Quora.
7. **Conferences and Community**
    
    - Favorite conferences include ACM Recommender Systems Conference, KDD, and ML Conf.
    - The value of conferences lies in their blend of algorithmic, application-driven content and community engagement.
8. **Influencers on Quora**
    
    - Quora hosts many ML experts and conducts AMAs with prominent figures in the field.
    - Xavier recommends following these experts for insights and discussions on ML topics.

**Conclusion**

- Xavier shares insights on the intersection of engineering, machine learning, and community-driven products.
- Emphasizes the importance of understanding the tools and methodologies in ML to apply them effectively in real-world applications.
- Highlights the growing integration of ML in everyday products and the societal shift in perception towards AI and ML.


----------
