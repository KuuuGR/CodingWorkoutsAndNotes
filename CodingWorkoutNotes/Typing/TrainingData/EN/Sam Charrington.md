
[The TWIML AI Podcast with Sam Charrington](https://www.youtube.com/@twimlai/videos)

-----
--99--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--98--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--97--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--96--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--95--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--94--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--93--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--92--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--91--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--90--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--89--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--88--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--87--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--86--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--85--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--84--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--83--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--82--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--81--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--80--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--79--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--78--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--77--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--76--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--75--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--74--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--73--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--72--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--71--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--70--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--69--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--68--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--67--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--66--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--65--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--64--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--63--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--62--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--61--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--60--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--59--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--58--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--57--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--56--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--55--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--54--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--53--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--52--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--51--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--50--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--49--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--48--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--47--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--46--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--45--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--44--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--43--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--42--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--41--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--40--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--39--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--38--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--37--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--36--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--35--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--34--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--33--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--32--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--31--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--30--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--29--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--28--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--27--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--26--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--25--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--24--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--23-- [The TWIML AI Podcast with Sam Charrington](https://www.youtube.com/@twimlai/videos)

-----
Date:
Link:
Transcription:

paste here

----------

-----
--22--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--21--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--20--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--19--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--18--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--17--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--16--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--15--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--14--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--13--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--12--

-----
Date: 2017.03.01
Link: [# Reprogramming the Human Genome with AI, w/ Brendan Frey - #12](https://www.youtube.com/watch?v=R5vDx5o9rzo)
Transcription:


Introduction
[Music]
hello and welcome to another episode of thermal talk the podcast where I
interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Carrington in earlier episodes of the podcast we've talked
about some of the many implications of machine learning and AI in healthcare but we've not yet had an opportunity to
dive deeply into this application area well that changes now I'm excited to
share with you a really interesting conversation I had with Brendon Frye professor of engineering and medicine at
the University of Toronto and co-founder and CEO of the startup deep genomics
you're going to love this show and learn a ton I met Brendan at the rework deep
learning summit in San Francisco a few weeks ago and I expect to share with you a few other conversations from that
conference over the next few weeks one of the questions I'm often asked is
which AI events are worth going to well there are a ton of them nowadays and it
can be difficult to separate the good from the bad so what I'll be doing regularly here on the podcast is sharing
some of the events on my radar I'll be sharing them on the twimble website as well for March I'm planning to be at a
bunch of events in the Bay Area the week of the sixth I'll be at AI by the bay
which looks to be a really interesting technical conference and next the Google
cloud developer conference the following week obvious strata Hadoop world which
is a great event for machine learning and analytics discussions with a particular emphasis on data engineering
and infrastructure and then the week of the 20th I'm attending another couple of
events by rework this time their machine intelligence and autonomous vehicle summits what I'm most excited about
however and I'll only share this brief teaser now is an event I'm organizing called the future of data
summit which will take place in Las Vegas in May stay tuned I'll share all
the details on a future podcast going back to the strata Hadoop conference for a sec you may remember that we partnered
with O'Reilly last year to offer a free ticket to their AI and strata conferences to lucky to mln AI listeners
well we're at it again we've partnered with the good folks at O'Reilly to bring you another
opportunity to win a free ticket to strata Hadoop world in San Jose California joint will and thousands of
innovative leaders and practitioners at strata Hadoop world to develop new skills share best practices and discover
how tools and technologies are evolving to meet new challenges one lucky listener will win a pass but everyone
can save 20% on registration with discount code PC twibell that's PC Twi
ml to enter visit our brand new Facebook page at facebook.com slash twimble ai
where you'll find full details of course I'll link to the Facebook page in the
show notes which will be posted at twill Malaya com slash talks last 12 since the
conference is coming up quickly you'll only have until March 3rd to enter winners will be notified shortly
thereafter and announced on the next podcast one more quick note before we jump into the interview towards the end
of my chat with Brendan I mentioned a sci-fi series that I like but I blank on the title well the series is called the
xenogenesis trilogy and it's by Octavia Butler it's also been re-released as
Lilith's brood so you may see that as well in any case I'll post a link to the
books in the show notes and now on to the show [Music]
so hello everyone I've got Brendan Frye on the line with me Brendan and I met recently at the
Welcome Brendan Frey
rework deep learning summit in San Francisco where he delivered a really really great
presentation called reprogramming the human genome Yai is needed Brendan was
kind enough to agree to discuss his presentation and work in the field here on the podcast so welcome Brendan hi Sam
thanks for having me on the show I'm really excited about this conversation
you know we've talked about deep learning and machine learning AI in
general and healthcare several times on the show not in a lot of detail but just
covering the news and there's been a lot of advancement in this area one of the
things that we talked about was Beth Israel Deaconess did the work with
applying deep learning to breast cancer detection google deepmind is active in
applying deep learning to eye disease and when I think about examples like the
ones that I've tended to see they are often they also fit into the pattern of
hey we've got a bunch of image data we know deep learning is great for helping us kind of find patterns and image data
let's apply deep learning algorithms to see if we can you know either augment or
replace you know the medical technicians that are you know finding tumors and
things like that but when I heard and thought about the your presentation and
the way you walk through what you guys are doing it struck me that and let me
know if this is fair but it struck me that you guys are applying or trying to
apply deep learning at a much more fundamental level like looking at the you know interactions between proteins
and things like that that create disease and that's not that strikes me is just
super exciting and that's why I wanted to really get have this conversation yeah so you know maybe to get things
How did you get into genomics
started and you didn't start your research career in genomics how did you find your way into the field yeah so in the 1990s
I was machine learning researcher I did my PhD with geoff hinton and we were
looking at image data and and speech and text as well we published one of the first papers on deep learning in 1995
which which appeared in science and so really it was those were good days just discovering new algorithms and trying
them out but we didn't have big datasets and we didn't have fast computers and so so that was really a big bottleneck of
course that's completely changed now but around 2002 by that time I was a
professor around 2002 my wife and I at the time discovered that the baby she
was carrying had a genetic problem we went and saw genetic counselor and and
had to deal with very difficult news it was the feedback was there could be it could be nothing or could be a really
big problem so that was a really difficult experience emotionally to go
through that and and really changed change my focus in terms of in terms of what I was doing as a researcher and I
decided to to stop working on on vision and speech recognition and text analysis
and really focus and sit on the human genome and figuring out how to how to connect what's going on in people's DNA
to to their health and also how to how to figure out how to treat treat disease
and so it was my assessment of what you guys are doing relative to some of the
What is your approach
examples I provided is that fair how do you think out the the approach you're taking yeah yeah that's that's accurate
so the a lot of other players in the field are essentially leveraging their previous experience on on the image
analysis to to then look at medical images our approach is very very different we're starting with the genome
as the input and the genome is just a long string of letters a C G and T 3 billion of them from your mom and three
billion from your dad and the challenge there is really to to figure out that
what the language is so so first of all the the language in the genome is not understood and how how words are put
together to to lead to to life essentially is not well understood
and so reverse engineering how the genome works is a big challenge and then of course once that's done figuring out
how you can manipulate the genome what you can do to to fix diseases is the second challenge so yeah I've been
working on that for about 13 years and applying machine learning techniques to crack that problem and in your talk you
Statistics
presented some pretty staggering statistics I think it was that the
lifetime risk for genetic related disease is something on the order of 65
percent and eight million births per year with serious genetic disorders yeah
that's right it's a big problem and you know we've we've been able to sequence
the genome and now we can sequence individual genomes for about $1,000 and in a few years it should cost less than
a trip to the grocery store to have your genome sequence and so we can read the text of your genome but the tragic truth
is that we are not currently able to accurately figure out what's wrong with you if you have a particular mutation
let alone figure out how to fix it and so there's really a big gap I called out the genotype-phenotype gap there's a big
gap between our ability to read the text of the genome and then make sense of it and an act on it and so those statistics
you gave like eight million births per year with a serious genetic disorder that's if it's kind of horrendous when
you think we can sequence their genomes we can find the mutations but we don't really have the ability currently to
figure out what's going wrong and that's what we're working on so both in my research lab and now at the genomics we're figuring out how to how to
understand those mutations or what their implications are so you know I'd like to
Biology
I'd like to understand all this better and I'd like the audience to understand
all this better you know what's the way how can you give us kind of a you know
push us off the deep end perhaps in the biology and genomics how you know how biology works and what are
the various issues and implications so that we can talk to have a conversation about this yeah sure so a common pattern
in the field right now is if people get a lot of data and then they kind of say well let's just throw in a big bucket
and give it to machine learning researchers know all at all yeah and I think that's a
really bad approach how so our approach is very much a systems approach and that
we try to understand biology we we bring to bear very carefully all biological knowledge that we can that we can
ascertain and then we build our machine learning systems to mimic that biology and so for example DNA is replicated
when a cell divides DNA is replicated so that's an important process the way DNA
is used within a cell is DNA is transcribed into RNA molecules RNA
molecules are chopped up and put back together again in a process called splicing the spliced RNA molecules then
they are translated into proteins and then the proteins go off and do things in the cell and one of the things the
proteins do is they bind to DNA so the proteins interact with DNA and they actually interact with DNA in a way that
controls transcription the proteins also interact with RNA in a way that controls splicing and similarly processes of
translation and so and so you can think of biology is these multiple layers of processing complex interactions highly
nonlinear and really the phenotype that we see whether it's maybe cancer or a
neurological disorder is something that's gone wrong within one of these processes and so and so that's a brief
summary of what's actually going on in the biology so in between the DNA and your phenotype multi multiple layers of
complex biological processes that are nonlinear and combinatorial and and so
what we do is we build machine learning models for each of these processes so right now in the biology community
there's just an explosion of datasets profiling what's going on with themselves essentially allowing
scientists here right inside of cells and measure the single molecule level what's going on and so there's a rapid
growth of datasets in the last few years and it's going exponentially and what we do is we use those those massive
datasets to train models to mimic these cellular processes and like to give you
an idea the kinds of datasets looking at we have trillions of data points that we used to travel oh wow so
Computational Biology
for any given one of these interactions you know before we even start talking
about the computational side of things just as a community of biologists how
well do we understand what's you know really happening in the end of processes
and when we have a data set that we're looking at yeah well that's that's one of the things that we spend a lot of time on
here at the genomics is basically taking known biology and then figuring out what
kinds of data we have that allow us to better model that known biology and then
also try to account for unknown biology as well which is one of the nice things that machine learning offers people in
the past have tried literally writing down programs computer programs to try to simulate what's going on in a cell
and the sending podcast those yeah you might guess that kind of approach breaks pretty easily first of all we don't know
all the rules second of all quantities in the cell or real value they're not they're not binary and logical and so that approach
doesn't really work that well people have also tried to write down stochastic differential equations describing the
concentrations of molecules in the cell another work for small small sort of very simple contained systems where
there aren't many molecules that kind of approach can work but it won't work for for living cells there's just too many
different molecules and the processes are too complex so the approach that we take is the machine learning approach we
can measure different data sets for these different molecules and then we train machine learning techniques to mimic the relationships between those
data sets that then emerge due to these biological processes is there some characterization for how many
relationships there are well we have a road map we have a technology roadmap at
deep genomics which lists all the different modules that we're trying to account for and we have a couple dozen
described in our roadmap but the number is much larger than that and growing
every year as well but having said that it's sort of a notion of diminishing
returns you can get quite a bit out of just modeling one or two processes for example
mentioned splicing where we're an RNA molecule was chopped up and then glued back together again and that process
depends on words essentially in the RNA molecule sequence so RNA like DNA is a
sequence of letters and the machinery inside of the cell recognizes little patterns of letters or words and those
words tell the machinery how to cut up the arms as letters those letters
What is RNA
represent proteins so the sequence of words is a sequence of proteins in the
RNA or DNA that right oh there's two kinds of words in there okay yeah so so one kind of word and the
RNA molecule is a sequence of letters does encode a protein so it corresponds to amino acids but make the protein but
there are other words in the RNA that are more like control commands so it's kind of like in a computer you have
print statements and then you have control logic right right the preak statements are like proteins but what's
even more important than the print statements is the control logic itself because that's that's what it creates the system that's responsive to its
environment and that can do different things otherwise we just keep printing the same thing over and over okay and so
the the ability of the system to respond to different circumstances and and be
dynamic is really crucial on that and that's achieved with these control statements if you like that are also
embedded within the RNA sequence so so we have a system that was trained to to
mimic this process of splicing and just to give you an example one of the leading causes of infant mortality in
North America is spinal muscular atrophy and there's a mutation in the DNA that
leads to this disease and and a mutation is in the RNA molecule as well and it
causes this process of splicing to go wrong it leads to - normally a certain
chunk of RNA is included in in the Croteam that makes up a certain gene called SMN smn1 or smn2 and if there's a
mutation then that chunk is left out and that leads to the disease usually those
infants die within the first year of birth recently at therapy was was was
given FDA approval and what's interesting about that therapy is that it doesn't target that particular
mutation it actually modifies another part of the RNA molecule which goes to
show you the importance of combinatoric right this is not the biology is not something where there's a correlative
effect or there's a particular mutation you just need to fix that Mon mutation quite often you need to change something
else in order to fix the problem that's occurring in the genome and so how I'm
What causes a disease
just thinking about the the scale of this system and the interactions you
mentioned a you know on the order of a dozen molecules or sorry modules for a
given disease is it typically only one
direct cause or they're all often you know combinations of things happening in these different subsystems that is what
causes the disease to spring forth yeah that's a good question and one thing
I've learned about biology is almost anything goes and so there's sort of the what's called the central dogma biology
which is very simple and but then you realize that a lot more complicated things can happen so yeah in the context
of diseases there's there diseases called Mendelian disorders which are you
can think of them as just a single mutation if you like in one gene and a very simple relatively relatively simple
mechanism still hard to find those still hard to figure out how to how to treat those diseases but but relatively simple
in the sense it's just one mutation or one gene but at the other end of the spectrum is a much more complicated
situation where you have many different mechanisms or many different causes that
combine together to result in the in the disorder or the disease so a couple
examples there would be diabetes or autism spectrum disorder and so if you take autism spectrum disorder as an
example the phenotype isn't even simply described if the whole range of phenotypes really and also the when you
when you look at the heritability of the disorder you find out that it can't just be pinned down to a single
mean as many different genes and even within those genes it's not just a mutation in a particular location as a
wide range of different genetic variability different mutations at different places within that gene and so
the kinds of systems our building can be used for both situations so we can pin down the single mutation is causing a
disease and we can also use their systems to understand the complex combination mutations but that is
involved in a disease okay let me take a maybe a little bit of a tangent there
What is CRISPR
were a couple of technologies that you mentioned in your presentation that I wanted to hear a little bit about the
first is one that I've heard quite a bit about recently but haven't really had a chance to dive into it and that is
CRISPR it sounds like well tell us about CRISPR what is that and what are the
implications of it yeah so CRISPR cap 9 it's a what's commonly referred to as a
gene editing system and it's a fairly straightforward idea you basically
program if you like a template into the into the system so it's a group of
proteins effectively and other molecules and you essentially program a template into those and then in living cells you
you insert these these molecules of system and then the template will find
us match within the within the DNA and then once it's found a match the system
will then edit the DNA according to your specification so it was different in the different ways that can happen it could
it could be as simple as the template finds a match and then it sticks there or it could be that the template finds a
match and then and then the DNA is actually edited so it's changed and
that's that's basically an example of gene editing which CRISPR casts nine is one instance and so how does that play
Problems with gene editing
into the kinds of work that you're doing in your lab and a company yeah so there
are a couple problems with with gene editing systems and one of them is off target effects and so and so the
template might not be perfectly specific which means the the system might bind to the DNA in
two different places and edit the DNA in two different places and so one of the places the correct one maybe there's a
mutation you're trying to fix to address some sort of a disease what happens is the system will also bind somewhere else
to some other region of the DNA and then edit the DNA there and that could actually lead to a problem and so and
that's called an off target effect the other one is that suppose the other problem with these kinds of systems that
suppose there's a particular mutation that you're trying to correct trying to fix it may be that the sequence
surrounding that mutation is just not good in terms of designing a template so
you can't actually design a template in your crisper cast mine system that will that will work properly in terms of
finding that mutation and then correcting the mutation those a couple different problems with these gene
editing systems another problem is delivery I just have to do with the fact that the machinery you need to get
inside of the cell it involves several different molecules and each of those
molecules has different properties in terms of in terms of whether or not it can successfully be be delivered into
the cell and so there's several different issues with gene editing systems and there's a few different ways
in which our technology that we develop a deep genomics can be helpful so one of them is because we we have these machine
learning systems that can mimic biological processes one biological process is this template matching so how
well the template matches the DNA that's actually a biochemical process that occurs within the cell so our systems
can identify off target effects and predict predict what might happen the
other example is when you can't actually edit a particular mutation so the
mutation of the patient has and you'd like to fix it you can't actually edit that mutation because the you can't
design an appropriate template so then what do you do well one idea is maybe
you can edit some other region of the DNA and somehow there will be some sort of compensatory effect but in order to
do that you actually need a model you need a system that can mimic how that DNA is going to be processed because you
can't you can't just fix mutation you need to introduce a mutation somewhere else that is going to
correct the problem introduced by the first mutation so for example the first
mutation disease mutation may cause a problem with splicing and what you'd like to do is introduce a mutation
somewhere else that will reverse that problem with splicing all right and again that happened that again that that
requires that you have some sort of a model for how the cell is going to process that piece of DNA to control
splicing and that's the kind of model we build okay so you guys the work that you're doing can improve these genome
editing systems and at the same time the work that you're doing around diseases
the genome editing system is one way that this work would eventually be
deployed if you will I wouldn't've into practice that's right at this point in
time indeed genomics we have not developed any products to address that particular therapeutic approach the gene
editing system approach but it is a research endeavor at this point mm-hmm and so why have I heard so much about
CRISPR you know maybe not this year but towards the tail end of last year is it just that it was yeah new or were there
new research results or is it better faster cheaper like what's the big deal
with all critters yeah yeah it's the leading to all sorts of breakthroughs in terms of research so the ability of
genome biologists to conduct different experiments different screens fabulous
tool for research and in terms of medicine is a lot of promise there are some issues that need to be worked out
but those are being looked at and I think it's very likely that it will it
will prove to be a useful therapeutic technique under some circumstances in any case and so the other question that
Cheaper genome sequencing
I had in terms is just the context in which you're doing your work is you
mentioned then you mentioned this in early in our conversation now is the
increasing drive towards cheaper and cheaper sequencing and at the
deep-learning summit you mentioned some new technology that you were expecting to drive the cost down to as little as
twenty dollars within the neck year or so what can you give me a kind
of a quick summary of the activity there yeah yeah sure there's just as I
mentioned before this rapidly growing diverse array of different bio technologies that allow us to measure
what's going on inside of cells and genome sequencing is of course an important one it's the one that kind of
gives us the software if you like for the basic source code of the person or the cell and yeah there's technologies
like the Oxford nanopore technologies and other companies have similar technologies which will which will allow
cheaper sequent much cheaper sequencing genome sequencing but that's not the only technologies helpful there's other
kinds of methods that allow you to for example look inside of DNA and see which genes are being transcribed and measure
how quickly they're being transcribed as a transcription rate of the gene
techniques that allow us to measure quantitatively how much protein is being produced and where the protein is being
located within the cell by the by the molecules that the shuttle proteins around so there's a lot of different
kinds of bio technologies being developed that allow us to essentially look inside of cells and measure what's going on okay so lots of stuff going on
Machine learning and deep learning
you know maybe let's I've kind of been just trying to satisfy some curiosities I've had about the biological side of
this but maybe let's kind of bring the conversation to machine learning and deep learning and maybe let's start by
talking through you know prior to the work that you and others are doing to apply deep learning
it sounds like machine learning is traditional machine learning linear regression and things like that were
applied to these types of problems well how was talk about the kind of the
standard to date approach sure yeah you know and I should emphasize that deep
snow mechs the word deep is is referring not just to deep learning but also these deep layers of biological processes that
get stacked upon one another mmm that relate the image of the phenotype and really that's what's most crucial and as
you might guess for each one of these modules the first thing we do is try a linear it's a simplest technique quite often
the best technique when you have a lot of data it does it does help to to look at more sophisticated methods and so
deep learning is a big part of what we do at deep genomics ok yeah so really in
in biology two approaches have been taken unsupervised learning and supervised learning and so wait way back
in the late 1990s people trained hidden Markov models on on DNA sequences and
those hidden Markov models were able to learn patterns that indicate the starts of genes and the ends of genes and the
locations of exons within the genes the exons where those parts of the genes that actually there like the print
statements they're the parts of the genes that tell you what the protein content is okay oh yeah that was back in
the late 1990s researchers were training hidden Markov models to tomorrow gene
structure and can you give us a 30,000 foot view into what a hidden Markov model is oh yeah sure so a hidden Markov
Hidden Markov models
model is like a machine and course simulated inside of the computer that
has several different state and the the mode can switch back and forth between
different states and so for example it might be in the promoters so the
structure of a gene is there's a promoter there's an exon and then there's an intron and there's an exon an
intron and that's just alternates until the end of the gene there's a couple other parts to the gene but for
simplicity that's just best to say those are the components of the gene so the hidden market model would start off in
the promoter state and then it was switched to the exon state and then back to the intron state and then back to the
back and forth right mm-hmm and so that's so that's a hidden Markov model
it has a finite set of states and then there's a model for how the probability
distribution that describes how them all switches between states so for example if you're in the promoter states and
there's a high probability that you'll switch to the exon state if you're in the x1 state the high probability switch
to the intron state if you're maint in the intron state high probability switch to the exon state and so on okay
so it's the probabilistic model it just allows this machine to switch and forth between the different states
and then for each of the states part of the hidden Markov model is also a description of what the data will look
like in that state and so for example when there's a transition from the exxon
state to the interim state the hidden Markov model also has a component or
probability distribution over what the DNA symbols will look like at that transition point and and so if you run
the hidden Markov model just just simulate it which means let it flip back and forth between these different states
and for each state let it generate some of the DNA sequence if you're on the hidden Markov model you'll end up with a
synthetic if you like a synthetic gene sequence okay and the way the hidden Markov models trained is to make the
synthetic gene sequence the output of the hidden Markov model match the real data as best as possible until in the
late 1990s researchers train these hidden Markov models using actual examples of DNA sequences and these
models were able to automatically learn what the structure of a DNA sequence looks like so they were able to learn
that there's a promoter with an exon and an intron and that there's alternation
between these exons and introns so that's one example that's unsupervised learning and there are lots of other
examples of how unsupervised learning has been used in genome biology ranging
from as I said modeling DNA to to actually just visualizations of
dimensionality reduction taking taking for example expression gene expression
measurements which would be say 20 mm gene expression measurements and compressing them down to a three
dimensional or a two dimensional representation for visualization so though a whole wide range of different
uses of unsupervised learning and then the other the other process been taking the supervised learning where you're
actually trying to solve a very specific task and probably the one of the earliest uses of supervised learning in
the context of genetic medicine of what was called the genome-wide Association study and in the genome-wide Association
study what you do is you measure for each patient a what's called the
genotype which is just a measure of of a verizon clothes mutations might be
measured using a microarray people might have heard of the name snip array and snip sensor single nucleotide
polymorphism and that's just a location within your DNA which could have a
mutation in it and so these snip arrays would measures say five hundred thousand different possible mutations in your in
your DNA another way you might measure genotype is whole genome sequencing so you've literally read out the three
billion letters or if you have if you can do it for both your paternal and maternal DNA you have six six billion
letters and so what however you however you go about measuring this genotype you
can essentially think of it as a vector so it's going to be a sequence of [Music]
letters AC G and T so for a 500 nucleotide array or 500 letters para
front for a 500,000 degree side snip arrayed have 500,000 letters and then
you can imagine encoding that vector as a binary vector so the letters a C G and
T you can encode it using one hot encoding so a is 1 0 0 0 right C should
be 0 1 0 0 different ways of doing that or what's often done is what you do is you compare to compare the person's
genetics to the reference genome and so then you represent whether or not they have a mutation there compared to the
reference genome something like that so so basically though you represent the person's genetics is a big long vector
of zeros and 1 and then we choose is use linear regression to try to predict the
phenotype so you've got a whole bunch of patients with cancer and a whole bunch of patients without cancer and and then
you just try to predict the whether or not they have cancer using linear regression that's what a genome-wide
Association study is so it's probably the simplest and and one of the original uses of machine learning in genomic
medicine ok and what are the challenges associated with that approach well if
Challenges
you think about it what that approach is essentially assuming is that your phenotype is a linear function of your Jin
headaches exactly maybe or not and and
so you know we already talked about these complex non-linear biological processes that relate your genetics to
your phenotype and we know from experience that this relationship is not linear and so it's a it's an assumption
that has been used successfully to find mutations that are involved in disease
but it doesn't really accurately mimic the biological process and so there are
a few consequences of that of that limitation and and one of them is that the at the genome the genome-wide
Association study is not guaranteed to find the colville mutation okay and so it may actually find a mutation that is
enriched for example that is common in patients with with cancer but it's not
guaranteed to be the mutation that actually caused the disease and that's a big problem if you're trying to find a
drug or a therapy to treat the disease because that mutation would be the wrong one now there are there techniques
called fine mapping where you we look for nearby mutations and try to try to
couple up those mutations with the one you found in the genome-wide Association study and that that fine mapping
approach has been used to to find the the causal mutation the one that should
be treated with the drug but it's still limited and doesn't doesn't solve all
the problems and I guess there are a few different other issues and one of them
one of them is the amount of data that's required and so because of the way the
genome-wide Association study works if you think about it there's three billion letters in the genomes rebilling
possible places it could be a mutation and preach of those locations you're going to try to use that location to
predict whether or not the person is cancer and then compare it against the experimental data so there's really three billion different places you can
look and the problem is if you don't have much data just by random chance one
of those locations is going to match up with the with the phenotype that is just even if the DNA is just noise even here
just generate a whole bunch of noise patterns you do that for your cases and your controls just by chance one of the
locations one of the positions in the genome you're going to get a good agreement between that me Asian and whether or not the person has
so-called cancer even though it's just noise it's meaningless and so and so I'm
going to get a lot of false positive that's right large number false positive okay and the
only way to get rid of that in a genome-wide Association studies collect more and more data and and that's the
way you get rid of those false positives but the problem there meaning you're not addressing you try and adjust it to kind
Deep Learning
of brute force statistics as opposed to a better technique that's right that's
right you're just trying to get so much data the overwhelm the the fact that you have involved with what's really going
on well so the approach we take which is this deep learning approach allows us to build models that take us from the DNA
to these intermediate molecular phenotypes are so variable if you like variable representing things like
transcription and splicing and those intermediate biological processes are
really what's crucial for disease and so by modeling those explicitly we can take
those big sequence of three billing letters and user machine learning technique to map it down to a much
smaller space that represents what's really going on inside of the cell and then we can relate that much smaller and
more compact representation with the phenotype whether the person is cancer or not okay and so it strikes me then
that basically what you guys are doing is feature engineering for this
particular type of system is that a fair way to think about it it's a it to some
degree yes it's a I would say instead of feature engineering its biological
engineering and the Chocolate word ten we're choosing we're choosing because these features that we're looking at are fairly complex
and high-level right um and and also it certainly doesn't do what you're doing
Feature Engineering
justice but if you think about you've got all this raw data that doesn't really Express or kind of model the
underlying phenomena and you guys are creating these meta models if you will
that does based on the raw data it's kind of feature engineering ish yeah you
will be the thing is we do have data for these intermediate variables so for example got it okay yeah
one of the one of the ways you might think of it as feature engineering is we actually model where a protein will bind
to the DNA right so so protein binding to DNA is a very important biological
process and understanding how a mutation disrupts that is really important for understanding disease you might sort of
say well what we've done is we've designed features that describe how the protein is binding to the DNA but the
way we actually account for that is we obtain training data for where the
protein bound so we've got a data set of DNA sequences and whether or not the protein bound to that DNA sequence and
then we train a model for that so if you like each of these features is actually a machine learning system and so that's
where that's where it's quite different from traditional feature engineering where you actually hand code the features so we don't we don't really
handle the features we obtain training sets and then we train them all to extract the features ok yeah but it is
you know it is it does have that you know that sort of confidential structure to it and we do there is this notion
where each of these features is validated we do carefully validate each of these we call them a bio module 3 we
validate each bio module to make sure that it's really accounting for that particular biological mechanism the
other way you can think about this actually is multi path multi task training and so in deep neural networks
one of the techniques that works really well is it's called multi task training that's where you you train your system
to solve multiple tasks at the same time okay so you might have you might have a very simple example might be a input as
an image and your training it to to classify animals and at the same time
you're also training it to classify some other some other kind of objects and the
idea is that they what it learns about those two for example faces so maybe trying to cross classify faces and
you're also trying to classify animals under some components to those two different problems that are that are shared value for example the detection
of body parts or the detection of eyes something like that and so by training
the systems with all these two different tasks at once that can learn sub components that can learn intermediate
variables if you like that are useful for the two different tasks and so you can also think about what we're doing
that way we have this these very deep multi-layer architectures and they're trained to predict genotype but they're
also trained to predict protein binding they're also trained to predict splicing they're also trained to predict transcription and these different
processes that are going on within the cell and by training them jointly to solve these different tasks they get
better at solving any one of them and in particular they can get better at detecting disease and also predicting
the effects of therapy are the different modules how do I ask this question or
Modules
they'd if you think about this is a if the model that we're talking about here is in a deep neural network are the
different modules expressed explicitly as layers meaning like the network
architecture or does the training process kind of cause the modules to be
expressed in the layers does that question make sense yeah yeah no it does it's a good question you're sort of asking what's the mapping between the
layers of biology and the layers of our machine learning systems yes yeah so so
we have two separate Maps if you like two separate networks if you like one network represents the biological
processes and for each if you like for each node and each arrow in that network
we train a deep learning system and so and so the biological network of ten layers deep and each layer is modeled by
a deep learning system with ten layers and there's an overall depth of a hundred that's one way you can think
about it and so because the system is trained in a modular fashion we can focus in on each component to the
biological network and then train a deep neural network to model that component
and sometimes sometimes use a shallow Network sometimes linear regression is sufficient so the complexity of each of
those biological modules in terms of machine learning is carefully selected
using the traditional machine learning types of techniques cross-validation and and perturbation analysis and methods
like that and does it ever make sense to then training these models as a stopped
Ensemble approach
neural network to think of it more as like an ensemble approach where your modules are more separate and you're
training them independently and then you've got some discriminator network you know it's sort of the overall idea
that each of these modules does have a place within the biological system but
having said that we do we do sometimes run into situations where there are fairly different ways we can conceive of
building each module just like in traditional machine learning you might have different types of you might use a
random forest in one case in a neural network and then you'd like to combine the outputs and see what happens that
sort of thing happens for forgiving biological module and I conceive of different ways we can build the machine learning system you know process and
then combine the outputs the the other the other interesting aspects of this is the sort of end to end training that's
once we put the system together then we can fine-tune it to make the overall
system perform better so even though each module is initially built using a
machine learning system could be a deep neural network independently of the other modules once we put them together
we can adjust all the modules so they work better together okay in your presentation you had a slide where you
Inductive vs transductive learning
were talking about kind of applying you know what you were doing and applying AI to these types of problems and you spent
quite a bit of time talking about inductive learning versus transductive learning and how that seems to be you
know something that's overlooked in practice can you recap that for us yeah
there's a big focus right now just on collecting data and I think not enough attention is being paid to analyzing the
data when it comes to genomic medicine and so there are a lot of private and
public efforts to just collect data you know the big genome projects the 100000
genome project the way the way success is measured is in the number of genomes as opposed to the information that's
being extracted right and and so I think more attention needs to be paid and
analyzed the data now if you look at a genome-wide Association study where it's this idea of correlating mutations with
the output that's what I would call is more more of a transductive reasoning approach where basically you're just
comparing you're comparing your mutation to the training data and then trying to
make a sort of a winner-take-all or taking a voting approach trying to make a prediction for that mutation mm-hm
now I that's one type of machine learning a different kind of inductive
learning an inductive learning what you do is you take your training data and then you build a machine learning model of what's going on and then you apply
that model to the test cases so you apply the model in the future and the
advantage of inductive learning is if generalization so for inductive learning
you can learn in sort of a in one way to view it as you're learning the rules of
what relates the input to the output you're learning more general patterns and this allows you to to take that
learning and apply it to completely new circumstances and so for example if there's a completely new mutation that's
never been seen before if you use inductive learning you might hope that your machine learning model can still
figure out what's going to happen without new mutation in contrast the transductive learning approach if
there's a new mutation has never been seen before the transductive learning approach can't do anything and so that's
- for genome-wide Association studies for example if there's a mutation in a patient that it doesn't exist in the
training set then the genome-wide Association study can say nothing about that mutation whereas with the inductive
machine learning approach you might hope that it could take the system particular look at that mutation and say oh this
mutation is going to cause something to go wrong with splicing and that's going
to lead to the disease and actually that's what we find with our systems so the systems we've trained a deep genomics were able to analyze mutations
that have never been seen before that don't exist in any database okay okay oh that's huge
yeah I'm really exciting actually so maybe let's let's dig into the data
Data landscape
aspect of this a bit in order to do what you're doing I'm imagining
your benefit you're benefiting pretty significantly by you new datasets coming
online all the time like how is that landscape change and what are some of the types of data that you're looking at
yeah it's one of the most exciting areas right now is biotechnology just the the
number of different kinds of datasets is growing very rapidly in the sizes of those datasets so 10 years ago we were
looking at small datasets consisting of a few thousand examples and now deep
genomics we look at datasets with billions of examples so we're the amount of data has just grown very very rapidly
and it's going to continue to grow there are publicly available datasets so these are publicly funded research
efforts from university labs and so there's plenty of publicly available
data and then is also different kinds of proprietary data data coming from patient populations or data that we
generate within deep genomics to to study particular aspects of biochemistry and sort of fine-tune or model if you
like and then in terms of the in terms of what the data is telling us as I
mentioned before it's these datasets are measuring all sorts of things that are going on with themselves so it's giving
us more and more accurate resolution higher and higher resolution in terms of
pinpointing different processes going on with themselves and relationships between those processes so I really do
think that in five to ten years because of this massive growth and data if we combine machine learning techniques with
all these data sets we're going to be able to produce models of these cellular processes they're quite accurate and
reliable hmm I took a look at one of their papers the paper that goes into
Data sources
the work that you're doing about around deep bind and one of the points that you brought up there was
a difficulty of extending results that are seen with in vitro analyses to in
vivo analyses and I'm assuming that well that's that's tied to this issue of the
or to what degree is this tied to this issue of the data sources that you're getting being primarily in vitro and
maybe we could talk through some of the issues there yeah yeah so so yeah
there's different kinds of data and in vitro is data that is measured under it's in the test tube and so the state
of this measured in the lab under very controlled conditions in vivo at the other end of the spectrum is within the
living organism and so it's the idea that they it would reflect more accurately was actually going to happen
in say a patient and and historically there's been a big disconnect between
these datasets but as we sort of fill in if you like if we fill in the map of all
the different kinds of things we can measure within the cell and we also fill in the different kinds of conditions
under which we can measure that data and so in vitro and vivo but also different
kinds of organisms different kinds of cell types different tissue types and as we as we measure more and more data for
these different dimensions if you like of different conditions in which we measure the data we get a more accurate
understanding of how different kinds of data relate to one another and so we're also building better and better models
that are accounting for confounding factors or experimental bias or or
example is a confounding factor in might be one oh yeah so there's different a wide range of different kinds of
confounding factors but I'll lump them all together into one group so
experimental bias is a big one and the experimental bias just means how your experiment was conducted you know what
what were they very what were some of the technical details that were used to obtain the data and those things can
have a big impact on the on the data itself actually one of the first projects I worked on in genome biology
we we used a particular kind of unsupervised learning method to analyze
the data we thought we discovered something really interesting and it turned out what we discovered who did the experiment on which day got
it and so that's an experimental type of confounding factor and then there are
confounding factors that are biological so for example if you're looking at let's take the genome-wide Association
study approach a very very simple machine learning technique and so if you're looking at a bunch of patients
that have a disease and a bunch of patients that don't have the disease the
problem is that those patients are not really independent and identically drawn from some simple distribution they're
actually related to one another in some way and so so maybe half of your cases
are derived from a single ancestor that lives a hundred thousand years ago or something like that so that kind of
structure in the population is going to lead to dependencies of course between these measurements and those
dependencies can lead you astray it's a little bit like you know suppose you're to one problem of all machine learning
researchers are familiar with is suppose you have a training data of a hundred examples and you take one of your
examples and just replicated a million times right you do not have a training set a bonafide training set of a million
and 99 examples uni is here yeah training data yeah just copied one of the examples a little bunch of times but
that's an example of a confounding factor that really does arise in human genetics and is really important to to
avoid and is the idea that your approach or a deep learning in general is has a
AI impervious to confounding factors
higher level of is more impervious to these types of compounding vectors yeah
yeah that's that's right and so because we're building the system to model these different biological components we can
factor out certain confounding factors and so as I mentioned for example our system could detect mutations that have
never been seen before which obviously means that it's not sensitive to the -
to the structure of the human population terms of the genetics okay but at the same time you know I
should add that of course our systems are being trained using data that has biases because basically all the rings
biology is highly pious right and so it's not like the problem is completely gone but yes they're more impervious
yep awesome awesome well this is this has been fantastic maybe any closing
Closing thoughts
thoughts on things that guys are working on or you know what you're excited about yeah I guess the the challenges for us
are so we have our systems are working well and we're making good progress in
terms of addressing interesting machine learning problems as well as having an impact in medicine but I think one one
area that's interesting to talk about is the kinds of problems that we're facing in terms of our machine learning techniques and how those relate to what
generally the the field is looking at and and and being challenged with and
one of those is is building systems that can explain themselves and so this is
going to people have been talking about this quite a bit recently is how do you build or train a neural network say or
deep learning system that in such a way that it can actually explain what's going on so we can explain why it makes
a decision right right and you know so that's that's really important for for
earning trust and so if we have a if we have a machine learning system that predicts that you you know woman should
have a disease-causing mutation in the context of say breast cancer in the system recommends a double mastectomy
then you really do want the system to be reliable and trustworthy and be able to explain you know why why it made that
decision why I made that provided that advice and so I think that's a really
interesting area for machine learning you know I don't have the answer to to how we do that but people are working on that area and a lot more work needs to
be done well on that point I did do an interview with Carlos gastrin not too
Deep Neural Networks
long ago and our listeners might remember that one if you're interested in this issue of explained ability check
out that interview but traditionally if you can use traditionally when talking
about deep neural networks I guess people you know when we're looking at machine one your models
people look to other types of models and not deep neural networks because of this
explained ability challenge that is you know particularly acute with
neural networks like do you see where do you see that going do you see light at
the end of the tunnel yeah that's a good point and I actually think that that belief is completely wrong minded so
here's the traditional argument for why you should look at simple techniques like linear regression or random forests
or something like that uh-huh so the argument goes like this to figure
out an explanation for why machine learning system made its prediction what you do is you should look inside of the
machine learning system you should look at the parameters okay so linear regression is really simple because for each input there's
only one parameter connecting it to the output and so you can just look at that parameter and if it's positive it means
that input has a positive impact on the output and if it's negative it has a negative impact and so that's that's the
justification for looking at simple machine learning systems right okay now this is why I think that's completely
wrong minded if you if you turn to your friend and ask them why did you make the
decision you just made you don't crack open their skull and look at their synapses to figure out the explanation
okay that's not what you do and yet that's what the traditional argument is
for why you should use simple machine learning systems you're going to look at the parameters and so therefore you need a simple system no you don't do that
so what you do is you ask your friend to explain themselves well why did you make the decision you made so I think the
future of machine learning is all about using complex deep neural Nets but training them in such a way that they
actually produce an explanation at the output so we don't crack them open and look at the parameters we actually train
the system so that the output of the neural network is an explanation as well as a decision going like that it does
MultiOutput Neural Networks
make sense so I'm thinking of the picture I have in my head is we talked earlier about neural networks that are
trained to produce multiple outputs and so in this case one of the outputs is the explanation and the other output is
the you know the thing we're asking it to make a decision so you got it that's exactly right you can think of this as a multitask
training problem yeah we're one output the decision the other output of the explanation and so really I think that
that's the future of machine learning in terms of explanation and there are obviously similarly challenging
technical issues for for how we get that to work and it's not really working well yet but I think that is really where
things where things will go in that regard and the other I guess the other observation I can make is as what what I
find really exciting about this area deep genomics is working on is the kind of artificial intelligence we need and
so if you look at some recent big successes like deep mines alphago or
Google Google's results or Facebook so if you look at some of the really exciting results that have come out of
those labs they're for things like games which humans invented or image
recognition which humans have evolved to be good at or my each recognition which humans invented or of all and over
evolved now these are all tasks that humans are good at whereas what I think really exciting about genomic medicine
is that the AI systems we build need to go beyond what humans are capable of mmm
so now a human is that ever going to be capable of understanding genome or how to cure genetic disease or no group of
humans will right it's so complex and so combinatorial and so really what we need is superhuman AI and so now it's making
me think of this making me think of a sci-fi book that I like black Savior Butler I forget the name of the book but
basically there is these races aliens that come down to an earth that's been
you know kind of ravaged by disease and this gift that these this alien race has
is to effectively repair a genetic disorder what that has to do with AI who knows but uh you're also I mean there's
some interesting things kind of switching the subject here there's also some interesting things happening up in Toronto right yeah yeah so you're asking
about things happening in Toronto so I can't talk a lot about it right now within the next couple weeks there's
going to be an announcement for a new type of artificial intelligence Institute in Toronto we have over 170
million dollars of funding for it and ideas to to build or rebuild the the AI
research capacity in Toronto and and ensure that we can use that capacity to
to foster innovation in the startup community and other and other bigger
businesses in the in the area oh yeah that's review announced in a few weeks and it should be really big big
news for Toronto I think big news for the AI community more broadly that's fantastic we'll definitely keep our eyes
open for that and so before we go where can people learn more about what you're up to and keep tabs on you yeah if you
can go to WWE beyond omics com also to Google me and we have various
papers posted online there where you can for example a tutorial paper that describes the approach and how you can
use machine learning not just deep learning which is just different kinds of machine learning techniques to to
approach problems in genomic medicine also well Brendon thanks so much this was an amazing conversation I really
appreciate you taking the time out you got family with a pleasure
[Music] alright everyone that's our show for today a huge thanks to all you listeners
out there I appreciate all of the notes and comments that you share be as a mailing list final form the show notes
pages via Twitter iTunes and all the other channels that you use to share your love for the podcast and don't
forget to visit our brand new Facebook page at facebook.com slash liminal ai and give us a like and register for the
Strada hadoop giveaway while you're there the notes for this show will be up at twill malay Icom slash talks last
twelve and there you'll find links to all of the resources mentioned in the show thanks so much for listening and
catch you next time

----------

-----

--11--

-----
Date: 2017.03.01
Link: [# Building AI Products with Hilary Mason - #11](https://www.youtube.com/watch?v=To0i4gpbzy0)
Transcription:

Introduction
hello and welcome to another episode of
twiddle talk the podcast where I
interview interesting people doing
interesting things and machine learning
and artificial intelligence
I'm your host Sam Charrington I want to
start out by wishing everyone a very
happy and very belated new year I'm
finding it really hard to believe just
how quickly the last few weeks of last
year in the first few weeks of this year
flew by needless to say I'm super pumped
to bring you this new episode of the
show before we get going
I've got a bit of a holiday gift for
some of you that's right over the last
few weeks I've received a few requests
from listeners who wanted to listen to
the podcast on their favorite home
assistants well it's taken a bit of
doing but I'm happy to report that the
podcast is now available on both Amazon
Alexa and Google home check this out
Alexa play the podcast this weekend
machine learning you'd like to play the
program called this week in machine
learning right yes this week in machine
learning and AI getting the latest
episode here it is from tune in to
Google play's a podcast this week in
machine learning learning in the AI
podcast twin will talk number 10
francisco webber statistics versus
semantics for natural language
processing note that for whatever reason
Alexa doesn't like when you ask for the
podcast using its full name this week in
machine learning and AI but this week in
machine learning works fine on Google
either works if you have any problems
just repeat the commands that I used in
the demo now I like to think that at
least some of you are listening at home
on your phone speakers and I've just
commanded your device to play the
podcast if that's the case enjoy it nice
alright moving along to our program
this time around our guest is Hilary
Mason who I interviewed last year at the
O'Reilly aí and strata conference in New
York City
I don't know that she'd refer to herself
this way but Hilary was really one of
the first quote-unquote famous data
scientists I remember the first
opportunity I had to hear her speak was
back in 2011 at the Strange Loop
conference in st. Louis at the time she
was chief scientist for bitly the
company that popularized short links on
the web nowadays she's running fast
forward labs which helps organizations
accelerate their data science and
machine intelligence capabilities
through a variety of research and
consulting offerings I tracked Hillary
down at the AI conference after hearing
from an attendee that her talk on
practical AI product development was
their absolute favorite session Hillary
and I had a wonderful although somewhat
brief chat that I'm sure you're going to
enjoy and learn a lot from of course you
can find this week's show notes at
twibell AI comm slash talk slash 11 and
now on to the show all right hey
everyone I'm here with Hilary Mason of
fast forward labs and we're at day two
of the O'Reilly AI conference the first
actual allow AR conferences we were just
discussing that's a and Hillary gave a
talk yesterday that I didn't get a
chance to see but I heard great things
about it so why don't we start by having
you introduce yourself and then you can
tell us what your talk was about sure so
I'm Hilary Mason and the founder of a
independent machine intelligence
research company called fast forward
labs and we look into approaches and
algorithms that are emerging in the
machine learning AI space but that are
not yet widely understood and we do our
own independent research to make them
useful to people so we write reports
that are a survey of the techniques from
a technical perspective at a conceptual
level talking about where we think it's
going to go any ethical issues that
might come up do a survey of the
commercial landscape and so what vendors
are out there what we think the
interesting application opportunities
are we also build working prototypes of
these things and finally we act as
Nicoll advisers to our clients like
their nerd friends and help them
actually build their and machine
learning products more effectively
because yeah that's what we do and
everyone needs a nerd friend yeah I mean
we all have that friend even you know if
you are a nerd you have your nerd friend
on your music nerd friends and your your
friend who's most likely sitting in
front of a computer at 9 p.m. yeah we
all have those people right all right so
you talk what was the title of your talk
What was your talk about
so my talk was practical AI product
development and what I was trying to
accomplish with this talk was that
coming into this AI conference there's a
lot of hype and a lot of lack of clarity
around what it means to actually build
an AI product what an AI product is so
what I was trying to talk through are
some of the challenges we've seen going
from the idea and the algorithm going
from the press release if you want to
say it that way to use a product so
being able to say we have a data set we
have a business problem we understand
and we have some you know we're willing
to invest in trying to make something
how do we actually do that and how does
it differ from data analytics and how
does it differ from software product
development there's a lot of people
today are trying to take a machine
learning product and sort of put it into
the software development framework and
they tend to run into a few common
friction points when they do that okay
The body of your talk
and so I suppose those friction points
were the body of your talk yes
so things like how agile software
development is really optimized for
building a product with commodity
technology um but that isn't how you
build a data product because you have to
understand that maybe even if it's a
good idea sometimes the algorithm you've
chosen won't work and you have to make
sure that the accuracy of the system is
within sufficient bounds there's a lot
of work to do around how you production
alized and operationalize these things
how you monitor not just that the server
is up but that the model continues to
return high-quality results over time is
the context and the data changes and
through all of those details are
something that
you really have to learn right now by
doing it and we have yet to really
standardize on a common accepted
practice and so in my talk I was sharing
what we've learned and what we do and
then hoping to have conversations with
people around what they do and what
they're trying to do and so yes that's
what the talk with it seems like agile
would be perfect for this environment
where you know things like working
closely with your customer the end user
of your your product you know failing
Why Agile
fast kind of at least the things we
commonly think about agile there's also
a whole software development lifecycle
thing which may be what you're referring
right so on the surface is absolutely a
compatible philosophy we do everyone
falls and be exactly because when you go
to implement the details when you run
into the problem when you have to say
you know how long do I will do I think
it's going to take me to find an
algorithm that can produce the useful
result and it doesn't take into account
the machine learning process of
developing really experimenting and
saying that you know I might try to
solve problem a but it turns out problem
a is really a lot harder than I thought
it would be but I can sell problem B
that's also useful in this product
context and it also doesn't deal with
the once you have something that sort of
works doing the simplification and
scalability work which is just as hard
as the initial algorithmic work but
often gets overlooked in a you know a
icon conference where everyone's excited
about what shining in me okay so to me
that says that it's not that agile
methodologies are fundamentally you know
ill placed in these types of problems is
that our sensibilities for estimating
and you know understanding the
development process that kind of feed
into an agile methodology are off like
we don't have all Salama feelings right
they agree but the the mechanisms are
second-class citizens so you can
allocate a spike time to
figure out an algorithm that that's the
hack and there's no first-class
mechanism for this sort of
experimentation in iteration okay okay
Stories
and so how did you with that being kind
of the premise for your talk what were
some of the things that you that you
dope into so I love to tell stories so I
talked about a couple of projects we've
attempted that didn't work out someone
was using a deep learning image
classifier to let you take a picture of
your plate and get a calorie estimate
which okay that sounds may be that
sounds like a good idea my team we
thought it was a good idea at least
worth trying we found a lot of data
there's a lot of food photography out
there and there's also a lot of data on
you know a cheeseburger has this many
calories did not work because that data
a cheeseburger can have anywhere from
300 to 2,400 calories and these datasets
just simply don't agree and we did you
know first were like okay we want the
actual calorie count from the plate and
then we decided on a more modest problem
which was can you tell us if it's very
healthy healthy or not at all healthy
eventually we decided that it was no
longer worth the time and investment
because the the quality of result we
could get was not actually useful and of
course this is a fun story to tell
because a couple months after we did
this whole process Google announced that
they had in fact solved this problem and
you know to me that sort of validates
that it was a good idea but we didn't
have the resources to make it work so I
talked about that story also went into
depth on that brief which is a
extractive summarization prototype we
built using neural networks for articles
so being able to take an article and
pull sentences out of that article that
are an effective summary of the entire
content of the article and that's
something where there's a product design
piece and there's an algorithm design
piece and they have to work together
well in order to make a usable useful
fun prototype and so so I went through
that whole example in the talk how can
you talk about that one in a little bit
more detail how did you go about that
and what was the process like yeah so
when so the work we do is always framed
around an application and so as much fun
as is it might be to say like okay we
want to spend four months using deep
learning to analyze text which is really
what we did we decided to focus first on
summarization and then under
summarization they're sort of two major
schools of system one is
extractive so pulling words and
sentences out of the body of text and
and there's abstractive which is
constructing a summary that may contain
language that does not appear in the
underlying text that's new language we
focused on extractive because again in
the product context we could actually
build something with a high enough
quality result to be useful whereas on
the abstractive side we're still as a
community very early and so the results
are kind of variable so again there was
that focus and then within that we
looked at a couple formulations of the
problems the one is can I take any
article and extract those sentences and
that's that the system we ended up
building is trained on about 18,000
human authored summaries with quotations
of news articles and it works very
effectively on those we also did a
second formulation of the problem around
multi document summarizations if you
have 5000 documents on the same topic
can you cluster them effectively and
then summarize each cluster and for that
we used LD a for that first step and
actually my colleague Mike Williams will
be at strata tomorrow talking about all
of the technical fun stuff underneath it
yeah if you're interested in that okay
and so for that for that example the
data set that you use was that a public
data set yes it's from a website called
the browser which is a terrible website
name and because of the ambiguity there
but and yes so it's a public data set
and one that turned out to be quite
effective interesting and LD a latent
latent display allocation
display allocation absolutely and how
does that I've heard that come up a few
times I don't really know how it works
what's the 30,000 foot on that case oh
the point conceptual overview is that
and it's a non supervised or
unsupervised algorithm meaning you take
the stream of text and it is able to
infer they're unrelated clusters
in the text fairly effectively one of
the limitations is that you have to tell
it how many clusters to look for which
you may or may not have an intuition for
going into an analysis which again means
that practically the way people handle
that on any given body of tax just to
sort of try ten clusters 100 clusters in
the narrow or their way in intuitively
and by clusters are we talking like
pet product reviews
engrams are we talking conceptual
clusters now we're talking to do
documents in this particular case so we
applied it to you Amazon product reviews
and we found particularly great results
in the pet product review category
because this is a section where people
are quite passionate about maybe I
didn't say it was yes I just do surprise
I guess um but we were you know a couple
of examples we ran into where things
like a dog toy that um you know 90% of
the reviews were five-star and 10% were
one-star and so when you look at the
clusters of those reviews you see that
you know most of them are things like
this is cheap I can buy it in Amazon
it's great this is really good for my
dog's emotional well-being and yes
people are very concerned with their
dogs of additional well-being and then
the the 10% were sort of like yeah my
dog ate part of this and had to have a
$4,000 surgery and so that's the kind of
structure you're able to pull out with
LGA and the utility there I think is
fairly obvious or rather one of the
things I mentioned in the talk is that
we tend to see these algorithms applied
to making things we already do more
efficient so if you can make that 20
page article down to two pages that's
making me more efficient but if you can
make me able to read five thousand
documents which I could not possibly I
could not possibly ever stand to read
five thousand reviews of the same dog
toy I can't worry about but now I can
get a similar amount of value and that's
sort of a really useful AI product and
optimization
when you say a similar amount of value
what was your what was your optimization
functional you how did you measure
whether the value is similar and so
that's a really good question
and in the case of our brief prototype
we had you know some human curated test
data but to be honest a lot of this is
really intuition which I know is a dirty
word in this context the world of AI but
I really do believe in the value of user
testing feedback loops and human
intuition and guiding the product
aspects of these the sort of work so
takeaways
what were the did you have a kind of an
enumerated list of takeaways for this
from the talk was it prescriptive or was
it so it was more laying out a shared
vocabulary and then sharing some
experiences but I'm not going to presume
to tell you how it's done for because I
think that where we are in the
development of the practice of AI
product building is still very early and
this is um you know I've been a data
scientist at the very beginning and it's
very similar to what happened with the
evolution of the profession of data
science where a lot of people are doing
a lot of different interesting things
that are all related and but there's no
one vocabulary no one process that
everyone has agreed on yet and so I
shared my point of view I got to talk to
people afterwards for an hour and a half
out here and hearing other people's
point of view and it's just we're at one
of those really exciting moments I think
yeah yeah have you done have you set in
what else do you think is cool
on anything else at the event like what
else do you think is cool and
interesting kind of in this realm so at
this particular conference one thing I'm
really impressed by is the different
perspectives in the room so most of the
conferences I've been to are either
technical or sort of business or sort of
product to design here we have everyone
in the same room which is great you know
VCS business startup people big company
people and you know software developers
machine learning professors all here so
that's really cool and I've heard a
couple of them
you know I always love the opening
keynote they were pretty great and then
there's been pox on everything from you
know tensorflow for mobile poets such as
Pete worden talk and he is a great
office if you haven't seen it all the
way over to the future of natural
language generation from the fix that
automated insights you know it's just a
few of the things I've been enjoying
yeah nice nice so how long have you been
outro
doing that for word labs the password
Labs is going to be two and a half years
old soon okay and we are eight people
plus two interns based in Brooklyn
oh nice warm Brooklyn we are actually
moving our office this week over to
Atlantic as Barclays Center oh oh yeah
you or any of your audience shoulda let
us know and come stop by if you're in
the Newbridge nice nice awesome well I
appreciate you taking the time I know
you've got a meeting to run off to Thank
You IRA to get an overview of your talk
yes great to have this conversation
thank you all right Thank You Larry
alright everyone that's it for today's
show a quick note for you guys tomorrow
I'm off to reworks deep learning summit
in San Francisco if any twimble
listeners are attending or will be in
the area please reach out to me I would
love love love to connect up with you
also please do leave a comment on the
show notes page at slim le I comm slash
talk / 11:00 or tweet to me at at Sam
Cherrington or at twiddle AI to discuss
this show and let me know how you liked
it thanks so much for listening catch
you next time
you

----------

-----

--10-- 

-----
Date: 2017.03.01
Link: [# Statistics vs Semantics for Natural Language Processing with Francisco Webber - #10](https://www.youtube.com/watch?v=kNnY7K_Fd2M)

Transcription:

Introduction
hello and welcome to another episode of we'll talk the podcast where I interview
interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Charrington once again the recording you're about to hear is part of a series of interviews I
recorded live from the O'Reilly AI and strata conferences in New York City my
guest this time is Francisco Weber who is the founder and general manager of artificial intelligence start-up
cortical IO Francisco's presentation at O'Reilly AI was called AI is not a
matter of strength but of intelligence to set the stage for my conversation
with Francisco recall that in the last interview Pascal Fung noted how recent
advances and natural language understanding have been based largely on ignoring language structure and focusing
on statistics well in this interview you hear Francisco argue that the next advance in NLU will come from shifting
our attention from statistical models to models based on a more sophisticated model of the brain a warning in advance
this conversation is very technical and moreover rather abstract don't be afraid
to listen to it a couple of times to allow the idea as an opportunity to sink in you'll find this week's show notes at
twin Olay Icom slash talks last ten you might be particularly interested in a
link to Francisco's presentation slides which are helpful to review alongside the podcast and now on to the show
Franciscos background
hello everyone here at the O'Reilly AI conference and I am with Francisco
Webber who gave a great talk earlier on AI is not a matter of strength but of intelligence
so welcome Francisco hello great to be here and to talk about my talk actually
nice why don't we start out by learning a little bit more about you and hearing a bit about your background yeah so I'm
coming from the Natural Sciences trained in medicine in Vienna but have since
ever saw a built-in affinity to technology and ended up sort of going into the natural
language processing information retrieval domain where I'm in for like 20 years now but I've been the sole
quota Clio is actually my third company I previously worked in the field of
patent information which is also a sort of complex natural language issue and
that was basically where I learned of the limitations of the current systems
and that motivated me to actually try and find something substantially
different okay so tell us a little bit about cortical yeah so cortical is all
about two things in fact so one thing is a theoretical framework that we have
discovered I would say and that we explore that is about how the human
brain to be more specific actually the neocortex supposingly handles language
information and the other is that we basically use this theoretical framework
to also create a real technology that we basically offer to the markets currently
this is mainly for enterprise customers they have a lot of problems out there
and so say the effort to revenue ratio makes us work there for a while but we
do also have a public API where basically everybody can play around with
our technology for free yeah so that's that's what the company sort of does and
we try to find a really alternative way
to deep learning and to the more traditional ways of statistical modeling
and machine learning for the moment our approach doesn't
actually use any statistics so which might not be the case in the future so
there are some motivations to maybe team this up with deep neural networks or so
so that's but in fact it's not our specialities or I leave this to orders to try out what we basically do is that
we have solved I would say or we have found a solution to the famous
representational problem that exists in natural language understanding since decades basically a very fundamental
issue is that basically says if you find out how to represent language which
means text at some point in a way that you can actually compute with it then
many of the big problems like ambiguity like vocabulary mismatch all the
traditional problems we have in NL you basically are solved in in one in one
approach and that's what I actually presented today is that by this little shift in how to generate the features
everything falls in place afterwards you know in a very convenient and and most
importantly efficient manner so tell us about this shift yeah so our approach
basically is founded on the work of Jeff Hawkins who is a researcher in in in the
area of cortical processing so he works on finding out how the human neocortex
actually processes data they assayed data in general because one of his
findings was that regardless what kind of data so might it be sound hearing or
seeing or touching all of that data when it comes to the neocortex looks the same
it is it has the same format which is a what is called a sparse distributed representation so
it's like a large vector of binary features where you have like two of two
to five percent of those features are actually set to one and all the rest is zero and everything is encoded into such
a such an SDR and that was basically our first goal is to find a systematic
unsupervised because otherwise it's not doable in practice a systematic
unsupervised way of converting text into such a an SDR ok now I've heard a couple
Neural nets
of times even at this event there were a couple of comments that were made that was one of them was even I think in the
keynotes this morning there was a comment about how you know what we've got with neural nets or don't have the
complexity and the nuance available to express what's actually happening in the
brain and and in another talk the the kind of follow-on statement was so
therefore we shouldn't try we should just use these as tools and now it sounds like you have a totally different belief system around this well I mean
fundamentally what we use as neurons nowadays has in fact very little to do
with real neurons right so it was an abstraction that was made
like 3040 years ago on a compared to
today on a very rudimentary understanding of what neurons actually do nowadays we know more we know that
for example the actual learning happens through the building and unbuilding of
synapses between them and if you actually model a neuron not not
chemically so it's not about sort of creating all the molecules that are there because that's something that
nature uses yes so nature you know in evolution you always have the components
from the previous evolution evolutionary state and you have to play with this kind of LEGO bricks and do
something which sometimes looks a bit inefficient but what is key on the other
side is what is the mechanism that those real neurons create and that is what
Jeff Hawkins actually has figured out and is about to even figure out in more
detail and so certain aspects like the the sparse binary representation are
actually key for this to work properly and by working on text so our approach
was basically okay if Jeff is right with his theory everything he says about the general way how the cortex processes has
also to be true for language as the language is generated by the cortex - and so we basically took his theoretical
framework as a set of constraints and we tried to say okay if that is the limitation how can I put everything I
know about language in this within this limitation and it took a while actually
25 years or so in general I mean not I know Jeff's work since since a little
bit over 10 years but everything that was sort of needed to me at least to
sort of understand and to operate at this abstraction level took a while but
then ahead at some point while I was listening to his talks reading his book
on intelligence and so and it was literally sort of taking a shower and in
a second I had this visual idea so to say how how this could happen because it
boils down to a sort of visual aspect in
the sense that as a necessity we have to find a representation where two words
that mean similar things have to actually look similar and when I say look there sdrs have to be similar and
literally similar so in fact and that's also what the brain is doing to put one
word representation on top of the other word representation and by measuring the overlap how many of the
bits actually stay at the same position you get two things one is how related
are those words and the second is by looking where the overlap happens within
this representation because this is a two-dimensional it's like a bitmap with
128 x 128 pixels and like 2 percent of
those 16 thousand bits are set to 1 therefore are like pixels dark pixels if
you want and so it's actually a visual thing and you can try this out on our
website when you take two words that are sort of have a common context or so you
can actually literally see that they look similar yeah and interestingly I mean it's it's hard
to know or maybe even impossible to absolutely decode what it means but if
you as I have done stare a lot into these representations you end up seeing
the differences like in a blink of an eye you might not know the details but to identify that two words are similar
in this representation takes a million seconds yeah and that is already a a
hint so to say that shows what the major
gain is of this approach which is efficiency and that's what our brain is
famous for so I know that on the deep learning in the deep learning community
things like precision and so are the key metrics and they are of course important
it's like having classes that are blurry nevertheless at the very end the choice
of algorithm is not so much on the precision but it relates to
down-to-earth energy efficiency yeah I mean the brain works with something like 10 watts or something
so I don't even want to know how much power the GPU servers eat up and that is
already a very good hint of how well a certain approaches if and
that's why I chose the title also of brute force because statistics
especially if you do statistics of large combinatorial spaces like like language
I mean you basically can create an indefinite number of combinations of
words to make meaningful sentences so to do a statistics on such an open system
it's a real hard work because you have to provide endless examples to have like
a micro bit of a semantic payload in
your representation yeah and it works up to a certain point no question I mean the statistical systems work but what
you see is that in order to make the model a little bit smaller or to gain a
tenth of a percent in precision you have to put a lot of effort in yeah so from
to get from sixty to sixty one percent precision you might even double the
effort like going from 1 to 60 is the same as from 60 60 to 61 yeah drag you
drag you back a little bit just it sounds like understanding the Jeff
Hawkins stuff is important to understanding what you guys are doing to some degree yes so they so he's defined
Real neuron
this sparse data representation this SDR and is there also a different concept of
a neuron that underlies that absolutely so he's modeling a real neuron but on
the functional level so he's also modeling a neuron if you want but he's modeling everything that is relevant
within the neuron for processing data is part of his model and everything that might be housekeeping building up
proteins and stuff like that is not part of the actual data processing layer and
therefore not represented there so he's basically tried to expand the simplistic
concept of neural net neuron to become a real neuron and sometimes if you face the
problem the way it is the solution is much easier to understand because it's
it's basically a model-based approach versus a model free approach as well so
if bringing in to one sentence yeah and so on this base of this more robust
State action
model of a neuron there's this notion of the SDR which is capturing you know and
I think of a neuron I think of the you know there's state plus action right and so this is capturing this even more it's
state action in time that is key to what Jeff is doing okay because his networks
have time built in okay so it's not only of deciphering a pattern of input bits
but it's rather memorizing a sequence of
patterns because in reality things are interconnected so to say they have a
semantics built into the system everything and therefore it is highly
unprovable if not impossible that by having an initial state a you can
predict which are the let's say physical possible next steps and that's what the
processing relays on the fact that not like in statistics after state a any
state could happen because I need to do the statistics for it but the reality is that after a step a there is a certain
set of steps which have all to be possible in reality and what we learn as
walking brains if you want is what are those potentials what are the potential
outcomes and how many hints from the initial state could point me to the
right next state and that's in the end what the brain is doing yeah the brain is nothing more than a sequence learning
engine that does prediction based on what it has seen so far and if you think
through the that on a let's say philosophical level you will find out that you basically can
The brain
solve or explain everything we do that basically follows this basic computation
yeah so there are two interesting aspects so this one is there is no processor so the brain does this by
being a memory system which is interesting I mean in computers it's exactly the other way around yes the
processing happens in the processor and the RAM is just a dormant store yeah and
the brain obviously does this differently and and and and the other
aspect is that the prediction is in fact the condensed intelligence because the
more I'm right in predicting the more I'm intelligent and by the way I mean
there you know there have been very behavioral ways of looking at intelligence that's the reason why the
dog looks intelligent to the dog owner because the dog owner knows the dog and
knows what predictions the dog is making about things and is right in doing so
and therefore the dog indirectly so to say looks it more intelligent to the owner than to everybody else yeah so
there's the SDR capture all of that are just the state so the SDR is all about
getting a an explicit representation of
the state so that's the other difference in in the world of brains and STRs you
only work with what is called semantically grounded information so every bit in representation of the SDR
actually corresponds to something real and concrete so for the visual system
it's pretty easy because in the end every bit of the image that is produced on the retina if you have two dots that
Visual system
are close to each other and have the same color or nearly the same color you are probably right in guessing that they
are part of the same item in the physical world if you have now a representation that
gives you the same phenomenon namely that two bits that are set to one stay
close to each other it's easy to guess that they are related and they are part
of the same maybe subunit of the system the only thing you have to be sure is
that the data that is provided is actually inherently semantic so it has
to be part of a system in a very abstract level so the world is a system
therefore any data that I can hear or see or so about the world is semantic
because there are rules of physics rules of biology and so on and the same thing
is true for language language is data that is inherently tied together by a
Language
framework of grammar of syntax and all these aspects we know since a long time
but we have we had the problem on how to
actually store these mechanisms and the realities don't store the mechanism but
just were the examples just or the detailed information the explicit information and are those words or those
yeah so in our approach we declare the
semantic atoms in language to be words I mean there are like smaller units like
full names for example but they have no meaning by themselves so the first time you actually have a meaning is when you
have a word and all the subsequent meaning of a sentence of a paragraph a
document and utterance even comes out of the sequence of those words and so what
Semantic Fingerprint
we do is basically we convert every single word into such a sparse
representation we call this because it's a hard to say we call this a semantic fingerprint and the interesting thing is
that through the way how we convert the semantic fingerprint you take advantage
of some of the properties that are inherent of sparse binary vectors for
example can make a union of as many sparse vectors as you want and you don't lose
information yes you can always say from an unseen vector if it was part of the
union or not if you try to do the same thing with the dense representation let's say the the ASCII encoding you
have 8 bits and every possible combination corresponds to another character if you make a union of a
couple of them no way to say what was the initial part and as I said the the
generation of this pattern is done in a way that every single pixel of our
Context
fingerprints corresponds to an explicit learned context and you can in fact
reward a context it's a context which is basically technically it's a bag of
words if you want it's a bag of words of utterances in which the word occurred
mm-hmm and so is a what's the scope of an SDR
in this model is it at the level of a corpus at the level of a language at the level of an utterance
well not another as an utterance in fact all of that so the what we do is in
principle we generate the atoms which is a fingerprint for a word but if I want
to create a fingerprint for a sentence I just convert every single word into its
fingerprint I make a stack and aggregate them together let me can over of all of them and then I have depending on the
Unions
location on the fingerprint you can do this because of this Union proper exactly exactly yeah that's the reason why we have to
stay on the sparse site yeah and if you make for example a union of let's say ten words that are in a sentence
you of course fill up the the representation therefore after making
the Union we we do what we call responsive Phi we introduce a threshold
to cut away everything that fills the fingerprint on more than the 2% and so
we end up with a fingerprint for a sentence that has basically the same topology that is directly comparable
a fingerprint of a word and we can do this with a sentence with a paragraph with a book yeah of course use the SDR
for book or for whatever let's say a
book is it n dimensionality where n is the number of unique words in the book no so there is the topology and there is
Semantic Folding
that's the name why why we call it semantic folding there is this semantic
space folded into the representation so the way how we do this how we generate
the word as the ours is that we take a collection of documents which are the
reference documents that's for a human that would be everything you ever read and heard all language elements that you
have exposed to and we digest them and
we do this of course using machine learning because we are not like humans we have not the time to wait 20 years or
so so that's in fact where we apply machine learning and what we do is that
we first of all cut the training material in little pieces and then we
define the size of our fingerprint which is a metric space so there is no
dimensionality if you want it's a two dimensional metric space and we position all of our training snippets on this
Semantic Map
space in a very simple rule two snippets
that are similar stay close together and two snippets that are different stay far
apart from each other and then it's you know one of these classical iterative
algorithms similar to have you're learning a bit like this local
inhibition mechanism and what you end up with is that you have all snippets about
animals in one region all snippets about family in another region and so on and
you get a semantic map and this semantic map is basically used to encode every
word because I can take all the words that are in my training material and for
each of the words I can say light up the decisions of the snippets where this word occurs in and then you get this
distributed representation and because you have to fold it in semantics so to
Representation
say to similar words like cat and dog look similar if you look them on the on
the semantic map representation of the fingerprint mm-hmm so you mentioned earlier a kind of a you
give an example of a 128 by 128 matrix at that size matrix like what are you
able to represent like is that a book all of the books I've ever read or what
it actually represents is a semantic space because it's it's it's it's the
the fundamental of the representation and if you just do the math of selecting
300 bits which is about close to 5% of 16,000 bits the number of combinations
you can do is like the number of stars in the Milky Way so it's a huge community oral space and as you know we
have not the same assumption as in statistics that in principle every word could be combined to every other word
Semantics
yeah so that's one of the central simplification methods is to say in the
language statistics that every word is independent which is absolutely not true if you have on the semantic level a
certain set of adjectives that you associate to certain um yeah so there is
semantic sort of glue between everything and in reality that shrinks the
combinatorial space and that's precisely what we need to learn the semantics of it mm-hmm okay okay that reminded me of
word Tyvek it's it's it's a natural
development that we have started NLP and information retrieval with so called
document vectors everything will sort of derived from a document
and we found out over the years that word vectors the representation of
individual words seems to be more appropriate nevertheless there is a
fundamental but crucial difference so were to act like other word embedding
mechanisms use they try to do dimensionality reduction and they end up
with a dance vector and to put even more on it a dance vector of double or float
numbers so sort of computationally expensive representations we don't do a
dimensionality reduction we might even to an increase the dimensionality at
some point if you want but we make it a sparse representation so we have sparse
binary vectors versus dense floating-point whether the double vectors yeah which already sort of gives
Visual Support
you a hint on where the efficiency will be all right right so I have we talked
through have we got to what you talked about in your talk or is this all been bad I understand I mean specially
because you are listening to this without any visual support and this is a very visual thing yes yeah typically
when I when I show this and people see the the fingerprints on the screen and how they interact and how they overlap
you can see in their faces ah I understand this yeah you don't need to know anything about machine learning or
so it's so intuitive but if I imagine to sort of follow a description that is
purely verbal then yeah so the the rest
basically was that I gave a number of practical examples where we apply this
and I can cite a few for example we do
Similarity
pro let's say we have certain prototype ways of solving typical problems and
what is the case is that we solve all of them with one unique operator which is
similarity yes so we only had the only sort of verb we have in our universe is is similar or
is not similar and so one thing you can do of course is search yeah
so you can and since you're operating on essentially these Mars vector
representations is when you hear similar like is it fair to think geometrically similar Geographic so we actually
measure this by calculating the overlap between two fingerprints which is the
most generic way I mean we we do offer a number of distance metrics as I said
this is a metric space so we have different ways of calculating a distant metrics like a Euclidean distance and
others but I have to say that in fact the pure overlap count is fully
Search
sufficient to get the result all of it and it's very computationally efficient
yeah so one of the prototypes as I said is search imagine you have a collection
of documents you convert each of the documents into a fingerprint you have a user who types in a language based query
I'm looking for information about red spot cause you create a fingerprint of
that query and you just match how much overlap you have between all the documents and the query and you rank all
your documents according to the size of the overlap very generic it's it's it's
a real search mechanism so what you get is really all the balanced aspects that
you have in a document so it's not just does a document contain the word sports car but it's about the aspects that you
might have developed in in a documents that make it match or less and in theory
the document need not even say sports car and exactly are doing theoretical arity to the this concern yeah so it
could be the race car it could be a text about the race car and my query could be about sport cars and it would still sort
of give a good match yeah and how does it apply to non-english languages I didn't hear anything
completely independent of languages so as I used to say
give me enough dictionaries and encyclopedias in Klingon and I put you
up a Klingon system no problem the point is that we have even brought this to a
step further because we were able to not only train in different languages the
semantic spaces but to also topologically align them and as a result
and I gave the example in in my talk we take the word philosophy in English has
a certain representation and the word philosophy in French has the same representation so the better patterns
are the same and what this means is you could have a system that contains
English documents and you can post French queries and it will still work
without any translation or or anything in between only for those words that
have a fair degree of overlap or well the word the words with the same meaning
regardless of the language have the same fingerprint right so a second prototype
where I could give you an example is classification so our classifiers
are actually just fingerprints I don't need to train my classifier if I say I want to get let's say all the tweets
about mobile phones I can take the word mobile phone create a fingerprint and
then compare the fingerprint of every incoming tweet to my fingerprint of the
Fingerprinting
word mobile phone and even if it talks about iPhone it will have sufficient
overlap for me to detect it and even if the tweet is in Chinese it will be
converted into something that I can filter with my English mobile phone fingerprint even simultaneously in
Chinese because your fingerprinting that based on its language representation and
there's the similarity is transferable
from one the the Chinese description of the new iPhone generates the same
fingerprint as the English description of the new iPhone why is that because the surprised at
that definitely definitely you should be surprised yes and no I mean people who
know two languages are able to do this in the same way yeah so there has to be a let's say mathematical way of doing
this and the point is that we aligned the two semantic spaces so we have one
topology that we generate in one language and we can then with a with a pure dictionary lookup mechanism which
is the dumbest way of doing a translation right we can convert all the
distributed snippets in the vocabulary of the other language and use the same
distribution that we have trained with for example the English methods and
therefore you have now the convenience to listen let's say to the Twitter firehose and regardless of what language
message comes along you can filter it with an English example and I've done
that just to give you a a feeling on efficiency I've done that in real time
on the fire hose with my notebook yeah so it was sort of running locally
through the I'm trying to run through the physical analogy or the biological
Transferability
analogy of this like in you know the the notion here is that you've kind of
extracted this model that you know more closely represents what's happening in the brain then and and you can you have
this kind of transfer ability across languages I'm is there some you know
again we're kind of way beyond you know the pale of what's actually gonna happen but like is there something like you
take the you know some part of the brain from someone who learned Chinese and you transplant it into a person you know and
then you know who has some other part of the brain that is kind of symbolically to English and they could then you know
translate on the fly yeah in theory in theory that would be possible the truth
is that there has been research for example comparing the brain patterns of people who have
who have been grown up with two languages they have they have a sort of
Brain Patterns
speech area in the brain that is actually intricately mixed yes so the
the two languages are represented in in a mixed fashion whereas people who have
been grown up in one language and who have then learned the other one they have added the second area so to say and
that's the reason why a native speaker of two languages can actually easily do
translation on the fly and can listen or read text in two languages without even
noticing that there might be two languages right in someone who has just learned another language has always in
his head to map from the one region into the other region now interestingly there
is and I showed this also in my presentation there is sort of new
research in in brain science that supports our representation strongly so
Brain Science
they were able to do an fMRI study to be precise so there has been an earlier
version of this experiment where people were exposed to words and then they made
like snapshots from their fMRI activity and what they found out it was in
encoding Ameland if i remember well what they found out is that you can actually
calculate starting with the picture an fMRI picture you can say what word this
person was hearing when this picture was taken yeah so this is as you say Wow but it comes better and they have sort
of trained a machine learning algorithm to make this transfer to correlate the
picture with the word that the persons have been hearing and the absolutely
unbelievable thing is that let's say you have been in the fMRI first the model
Machine Learning
has been trained on your images to map to certain terms if now you present this
very same model the images taken from my brain it will still recognize the terms properly and then whether we
speak the like same language no I'm this has been done in English I'm
I'm pretty sure that even if I would do this with with the Portuguese meaning of
your English terms it still might work out but but why not the fact is that
obviously if two individuals have been grown up sufficiently similar from a
cultural point of view yeah so we both went to school for more or less the same time we more or less read the same stuff
we've heard about the world in the same way the representation ends up being
similar across individuals and in the end it makes a lot of sense I mean just
imagine if we would really be wired completely different from one to each other it's it would be very hard to have
a simple conversation yeah and in fact if you if you do the the investigation
for example I'm pretty sure again this is just guessing but the fMRI pictures from I don't know some distant tribe
living somewhere in the Amazonian jungle they're the overlap between the two
representations is probably less because they have just not been exposed to a very similar kind of environment yeah
and and there is a newer publication which is I think it's from this year I
think it's it's it's from a lab in the MIT and there they were actually able to
create a map of about thousand words armed basically nearly the entire cortex
NLP
and and what it shows is that every it's not like every word has a specific
position but every word has a pattern of all sorts of positions all over the
cortex that lights up which is in fact exactly what we are doing with our fingerprints so I claim that we are the
first NLP algorithm the get support by fmri Wow
so this is fascinating stuff how do you help people make it practical like what
if I'm you know if what what are the problems that hey if I have this problem I should be looking at this as a
possible approach so so as I said earlier we are very strong with this
approach in doing similarity calculation and therefore classification and as you
might know in business natural language processing nearly all problems can be
reduced to one or several classification problems okay so we do all sorts of
Classifications
things yeah I mean companies who say we want to classify our inbound emails in
product requests complaints and I don't know looking for a person and individual
in the company and believe it or not I haven't seen any working machine
learning solution for that problem out there I mean I've been visiting like 150
companies over the last two years of course trying to sell our stuff right
but I haven't seen a working solution for simple I mean this is really one of
the most basic issues you could have and nearly nobody is actually using
technology for that because the the the statistical approach has a lot of noise
that comes in has false positives which is by the way the the biggest problem in
Use Cases
business and we solve we solve this in a couple of weeks you know so we we make
use of the efficiency of the approach in solving this kind of problems within
very short time for people so that's that's a specific use case are there is
there like a higher level characterization like you know in terms of problem yeah so we have customers in
the domain in a lot of customers for example are in the banking domain there
we solve problems like compliance monitoring or your customer activities or automation of business
processes that depend on some text input at some point we have consumer good
companies who want to know how to segment their customers for example we
have manufacturing industry where for example in technical products the
Manufacturing Industry
documentation that the manual of the product is so complicated good example is car industry for example modern cars
so complicated that if something breaks you need to visit the manual or to find
out what is this funny light meaning there is this dangerous or can I just continue and people can't find anything
because they have the problem that the person in the car doesn't speak the
technological language so an example that I've that I've learned is the query
where do I find a donut in in the US I didn't know that before but obviously
the donut is this fair wheel that is sometimes pretty good hidden so if you
look for donut in the in the manual you probably don't find it yeah and there is a lot of these issues yeah I mean to be
even a more extreme case a person speaking only Spanish driving a US car
LinkedIn
and being unable to actually find the the right answer could use our system to
sort of pose a Spanish query and be pointed to an English page for example
so as I said I mean in principle we have solutions all across the domain we can
do things like for example you have a LinkedIn profile you'll describe
yourself in your LinkedIn profile I can make a fingerprint of your profile and
if I do a fingerprint of my profile we probably have a lot of overlap as we are interested in the same kind of topics
traditionally to make matching of people in HR for example you need it to
actually if one person says I'm expert in GTE and the other person or the other job
description contains Java Enterprise there was no way of matching it yeah in our case we match this easily right Wow
Learn more
so the very fascinating stuff how can folks learn more find out more about it
contact you so basically on our website cortical dot IO what do I do RT IC al
that IO exactly you can go there you find a white paper where you get
basically a more in-depth introduction to the whole approach you find access to
a public rest api that you can play around it's trained on Wikipedia and
English Wikipedia data you can then even spin up an instance containing the
software on Amazon or Asia to play around if you have more proprietary data
so that you that you want to use and of course you can contact us if you need
help to sort of get started I mean the problem is that many of us who have been
struggling using conventional tooling sometimes it needs a little bit of help
Keywords
to sort of get the right angle on how to solve something yeah so we do for
example offer a keyword extraction functionality you can throw in a text
and you get like the ten most important keywords out of it and I've observed that many people try to systematically
extract keywords and then try to do some magic with that and I just told them
okay that keywords you need them if you want to show keywords at some point but you don't need them to make any
computation because you can compare the fingerprints directly so yeah it's a bit
of a change of mindset exactly well thanks so much Francisco ISM it was
great talking to you and amazing learning a little bit about what you guys are up to thanks a lot thanks back again
alright everyone that's it for today's show please leave a comment on the show notes page at twilly Icom slash talks
Outro
last 10 or tweet to me at at sam Charrington or at swim le i to discuss
this show or just reach out let me know how you liked it thanks so much for listening and catch you next time

----------

-----
--09--

-----
Date: 2017.03.01
Link: [# Emotional AI: Teaching Computers Empathy with Pascale Fung - #9](https://www.youtube.com/watch?v=-lH74mQO8Vk)
Transcription:

Introduction
hello and welcome to another episode of we'll talk the podcast where I interview
interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Cherrington once again
the recording you're about to hear is part of a series of interviews are recorded live from the O'Reilly AI and
strata conferences in New York City my guest this time is Pascal Fung professor
of Electrical and Computer Engineering at Hong Kong University of Science and Technology Pascal gave a really interesting
presentation at the AI conference focused on how we teach computers and
robots to understand human emotion and be empathetic she also had some really
interesting things to say about the theoretical foundations of the various
modern approaches to speech understanding and we dig into all of this in our conversation as always I'll
be linking to Pascal and her research in the show notes which you'll be able to find at twilly I comm slash talk slash 9
as is unfortunately the case with my other field recordings there's a bit of unavoidable background noise but it's
not too bad and now onto the show [Music]
alright hey everyone I'm here at the O'Reilly a conference and with Pascal
Fung who is a professor of Electrical Engineering at Hong Kong University of
Science and Technology and I sat in on her talk earlier on emotions and AI and
had we enabled computers to recognize emotions and she graciously agreed to spend a few minutes with us to tell us a
little bit about what she's working on so welcome Pascal thank you how about we
Background
start with talking a little bit about your background and the kinds of things you work on sure so my background I am
electrical engineer and computer scientist and I've been working on
speech recognition since 1988 and then move on to statistical machine
translation in the 90s and after I became a professor at HKUST I lead a
team working on speech language and more recently on emotion and mood recognition
or sort of using statistical modeling and machine learning methods that's my I
worked at different places before I was student and while working on my PhD
thesis at Bell Labs I was very lucky to work with some of the best people in the
area and in the early 90s when I started my thesis it was when the the field of
natural language processing was transforming from a heavily knowledge-based linguistics base feel to
statistical modeling and at the times that is modeling for language was very
controversial why she didn't believe you could learn language or study language or understand language was just
statistics but you know 20 years 20 years later and now that is the
mainstream approach everything we do is with machine learning statistical
modeling in natural language processing one of the slides you put up had a quote
Speech Recognition
from a professor who I think you mentioned you work with us and for every linguist I fire
for jellinek he's uh he's sort of the father of statistical speech recognition
so we owe the field a lot to him he passed away a few years ago and then
this quote of his was yeah his controversial but what was the quote the
quote was that every time my fire linguist the speech recognition accuracy goes up so it came from the at the time
at IBM he was leading the IBM group with a group of mathematicians and
information theoreticians to work on the problems of speech recognition ok which
was previously worked on using knowledge based approach in AI and actually his
group wasn't allowed to use that approach this somehow they did it anyways so there was always a little bit
of conflict between the knowledge-based AI community and the at a time the
statistical minded people so there were papers written about the empiricists
versus rationalists in the 90s and you know there was I remember my first
conference presentation on a statistical basis language processing paper I was
yelled at by some of the senior people in the field yeah those were the days
but now it's totally not controversial at all because you can see you machine learning is everywhere right and yeah
right so he said that because the approach he proposed his group opposed
was very radical at a time which is not looking at how to imitate human at all
I'm just looking at the input and output wear of the tassel so for example for
speech recognition the input is speech speech wave found are waveforms and the
output should be words right and for machine translation the input could be French and output should be English
right and they basically treated all these problems as a kind of information
theoretic problems to solve so a different mathematical approach from the traditional knowledge-based AI
approach at the time can you maybe give a 30,000 foot background of information
Information Theory
theory and how it plays into all this okay I'll try so information theory was
invented by the entire field came from a paper written by Claude Shannon 1948
called the mathematical theory of communication so it basically looks at
you know the information coding for example if you want to transmit
telephone signal through telephone cable there's only that much information you
can transmit and how many simultaneous calls can transmit in one cable is
limited by paths but by physics actually right and so the information theory
really is talking about you know how do you encode transmit and decode
information so the earliest application was of course telephone systems so no
coins so was no accident that closed and there was also at Bell Labs and then
later on information theory was then apply a Cheney at the time when caution
and came up with this information theory he already had paper on the information
of language all right yes yes he actually wrote a paper very early about
how language can be encoded and learned okay and so he was one of the earlier
pioneers of AI even though people don't think of him that way you had no idea
yes and he actually also had a sent if American paper on the the first chess
playing game chess playing algorithm or in the 60s so yeah so information theory
became an entire field of research and it's applied widely in many many
different areas most importantly in telecommunications and communications
then of course in all the statistical learning field we also use information
theory for example look at how how we
can learn how we can learn to model a
language my information theoretic point of views so it's more like you can think
of as message encoding kind of way yeah rather than linguistically motivated way
so it's a different way of looking at mathematical approach of looking at problems right you mentioned that in
your talk and I found that fascinating and it never occurred to me I think you you pose the idea of thinking of machine
translation problem as you've got this you've got this message that is you're
trying to translate French to English you've got this message that's in noisy English yeah English so for example is
exactly that so for example the word orders are different in different languages so it can be you can think of
that word reordering as a kind of a distortion right and then some of the
words are actually in French English some of the words are the same like 40% yeah but other words are different so
you can again think of a word that's different a different language being a kind of noise I mean are kind of
distortion I'm sorry and if you can learn that distortion that you learn the translation so and that is some
information theoretic approach that's what Google Translate still based on mm-hmm
interesting so how you starting from what are some of the you know the
Algorithms
algorithms or approaches that kind of come out of the information theory
background like how is it applied more concretely to that problem okay so for
example all modern speech recognition software is based on the noisy channel
model okay so the whole idea of you can train a speech recognizer with lots of
data so let's say voice search and all that so based on what other people have said
and it's based on these days millions of hours of speech today like Siri for
example yep it's trained on these data and then it uses different kind of
machine learning method and but all these speech recognition methodologies
based on one big paradigm is still the noisy Channel model which is speech in
words out yes I saw that through this channel now the latest approaches has
turned part of these of these methods into using deep learning to replace some
modules for example replacing how phonemes can be modeled or replacing
part of the predicting what will come after it which word so some of this is
now enable by deep learning but the whole paradigm is still a information
theory approach similarly for for things like Google Translate it's still based
on the what I just talked about the fancy channel model okay and recently
there's some research work on using new or not but we haven't seen any
commercial application that's that claims to be using neuron that four so
end-to-end neural net for for machine translation yet okay mm-hmm okay so how
did you how did you kind of get in not to mention all the encryption software
all that is based on information theory right yeah okay that is not my field but that's actually a main application for
for this kind of yeah okay awesome awesome how did you get into the study
Emotion
of emotion how did I get into the study of emotion basically we noticed that for
so I've been working on spoken language understanding force for a long time yeah and I've participated on
different efforts from different generations of virtual agents when we call virtual assistants today okay so
the earliest system in the 80s was funded by DARPA which was a ticket
booking system while you call and say I want to go from here to there and then it's trying to to to book a ticket for
you okay and from that time on that we have seen different generations of dialo systems up until today we have Siri and Cortana
right and working on these dialogue systems I've noticed that we've always sort of just look at literal meaning of
a user query so the machine just pays attention to you know what is the
destination the origin city how many takes do you want you know what kind of
restaurant you're looking for so a very sort of literal interpretation of your
query which means that your query has to be very clear these days people always complain all Siri doesn't work well and
all that to us researchers we can see why doesn't work
well because the system assumes users to be very clear and say explicitly what
they really want to get right and you know unless the users are lawyers I mean
very few people talk that way you know we talk naturally we expect you to understand what I mean right
and for some you just laughed because you know I was trying to be funny and that kind of information is completely
lost in our dialogue systems of previous generations and but it is very very important for to communication if we
want to go back go past the current sort of sort of a plateau of understanding we
have to also incorporate the understanding of emotion intent and all that in addition to understanding the
meaning of the words so that's what I started working on incorporating so I do
not recognize I don't work on recognizing emotion for emotions sake it's really recognizing emotion for
communications so it's well I that's what I call empathy module and then when I look at
the future applications what we do and some of the most immediate applications
that immediate in the sense that in the next 20 to 30 years you will see widespread application will be health
care and elderly care because by the year 2050 there will be more old people
than young children in the world so and so elderly care is a big area and
governments will be running out of resources human resources the cake take care of this elderly so they we're gonna
need a lot of machine assistance now my mother lives spends a lot of time alone
she's very independent she's in her seventies she doesn't want to live with me and she
wants to be left alone to do her own thing and I'm always worried and in her
independence you know I worry about her health she seems healthy when I see her but at
her age I want to make sure that she's fine right now for example I want to know she's fine right right so this kind
of I want to know her her health conditions but also her mental
conditions so if she doesn't want to live with her with me now how about if I
have if we build a home robot who can be there at her you know at her home or be
around her all her and converse with her sometimes just to get a sense of how
she's doing simple things right and then sends me a message so I know how she I
mean all sends me a curve of her vital signs in addition to her emotional state then that will help the guilty children
busy guilty children but also help the elderly because in a lot of people have
emergency problems like a heart attack or something all right that could I be they could have been saved if someone no
and then there are others who can get lonely and depressed
and then they can also be helped a machine to some extent not completely by
machine living with just machines is also very sad but but when there are not
enough humans around people around then the machines can help so this is why I
want to work on pathetic robots indicating people in the case of a
Empathy
crisis situation like a heart attack where does empathy and what does that
come in I think heart attack it's it's basically its emergency then the robot
has to basically alert call the ambulance right that's the first thing but what what empathy comes into play is
that so daily reminder to take medicine right in some of the aging studies
people have found that a lot of elderly they don't want to take medicine to some
unless somebody talks to them okay like sometimes you have to sort of entice
them I mean some elderlies are like children so in that case so the Machine
doesn't just say take your medicine and repeatedly insisting that you take your
medicine like with the same command that would be extremely annoying and it will have the opposite effect right so the
Machine needs to know that the elderly is hesitant or resisting and how is the
person's patient's emotional state and to know whether now the machine consists
or it's time to call a doctor or a nurse or whether you know telling the patient
a bedtime story will soothe them so that requires empathy if you think about all
the nurses you know in the hospital love their job a lot of their work and their
tasks are very repetitive and sometimes the pattern nurses are the ones who
really have a very good bedside manner right and what is the best time manner other than being empathetic just being
empathetic you know for both doctors and nurses so if you want doctors and nurses to be unperfect obviously you want the
healthcare robots to be empathetic right right you talk I interpreted its focus
generative empathy
on empathy recognition but your description is also talking about what
you might call generative empathy right right so empathy has two sides it's the emotional recognition right and then the
the appropriate emotional response okay so empathy I only in my talk I focus
mostly on the emotion recognition Parker because that is hard until we can recognize the emotion we don't know how
to react right right so I focus on that and but the response part I talk to
Lipper at the end that we're trying to learn the appropriate response as well from data so also using deep learning
use cases
you mentioned healthcare as a use case I think there's all often for awhile now
people have talked about a customer service use guy sure sure you know the the hold line will recognize when you're
getting frustrated in fact at AT&T Bell Labs in the 90s
already there they a group worked came
out with the system called how may I help you so when you call the AT&T line it's it's
a virtual assistant virtual operator that talks to you first and says how may I help you and you basically say
whatever you want and then it goes to different categories yeah and that is the intention classifier okay and also
at a time 1818 AT&T had the internal programs to analyze all these call
center data or see whether people upset they're happier and all that so that is already the beginning of emotion
recognition that was my first contact with emotional recognition and it was war for customer service indeed it is a
big area but I don't get the sense that it's widely deployed or at least not in
a way that I would see yeah yeah I said back-end so this is the
thing because it's not consumer facing it's really for it's really in the in the area the realm of data analytics
yeah so it's more of a tool used by corporations to improve the efficiency
of their call centers in a performance management performance yeah they do do that there are companies that provide
technology - right so you also talked
CNN and emotion
about the use of convolutional neural nets and recognizing emotion and kind of
do some interesting correlations across you basically that the cnn's were able
to functionally approximate a cochlear functionality yeah the human perception
system indeed that's what we found that so so I think my talk I started out by
saying okay of traditionally speech recognition look at these human human
some perception system and try to imitate that but we sort of hit the bottleneck and we had to move away from
that and go with the information theoretic approach where we actually try
and though we don't try to imitate humans model at all and what's interesting with CNN is that a lot of
times CNN or other deep neural net are being used as a black box so we know it
works we don't know why and humans being humans are always interesting knowing
why and in fact there's a a practical reason is that if you ever
want to commercialize the technology like that right to provide to your customer they want to know why what it's
learning so so we then look take we then took a look at the CN n different layers
of CN n and saw that it was actually as I mentioned they my talk that it's
basically approximating the future Bank in our cochlea that's connected the
auditory system and then we also saw that it's picking up on the amplitude the peaks in the amplitude that
correspond to different emotions so we thought that was very interesting that we can actually see what's going on for
example it was always CNN was first applied to image recognition and they
used like seven eight nine layers of CNN
to achieve that purpose and image recognition they were able to see that each layer so for example one layers
recognizing the edges of image and the other layers looking I think maybe
something else some features on your face and all that so it's all very obvious and really nice and we were
never able to explain how deep neural networks on speech and language so it
was interesting to see why works on emotion we still are not able to figure
out what it's doing on languages so what each layer is recognizing we were hoping
that each layer will correspond to some linguistic functions such as syntax or
semantics we haven't seen anything that that that neat yet so yeah so it's not
blind mean learning learning some it's learning something which has a physical meaning but you made the point that it's
Perception
it's also an error to correlate it too tightly to brain function because that doesn't really a dozen it's not because
no so even though I said approximates humans perception system it's really the hearing system right it's not the
understanding part it's the hearing system and we know exactly how hearing system function we know very
very well we don't guess but how how our minds functioning and understanding the
meaning we don't know we're activating our you know I mentioned 100 billion
neurons and 100 trillion synapses to get that until we have new network of that
size it's hard to come up with something similar to human cognitive ability so
it's not we don't have that so that's why I say it's no coincidence that we
can use we can explain what CNN is doing for perception so speech recognition and
emotional condition are both perception problems perception is actually easy
because we understand human perception how our skin feels the temperature we
actually understand the physics of that very very well but once you go into an understanding which is language
understanding which is the trance you know if I want to use a noisy channel model it will be from words to meaning
right once we're getting into the realm of that we are kind of clueless as to how humans so there are a lot of
linguistic theories about how humans think we must understand but you know
for every language Theory there are other people who say no right so there is no no there's no scientific truth
that we all share right now about how human mind understands language or
understands anything else how do you know a video of a cat is funny or not
how does our mind interpret humor we actually don't know a lot of marketers I
know so yeah how do you predict for example one thing we were asked since we could classify music we were asked by
companies say hey can you predict whether a song is gonna be a hit or not
or not be amazing no you know because you know even we look at big data of all
the past hit songs yeah I can learn it cannot predict what the
next one not yet maybe so all we can do right now is use engineering models to approximate input
and output you know what I mean it's really a mimicry like just looking at this input can we come up with output
that's similar to to the truth so we're not no we're knowing anywhere near to
imitating human minds and that would be even though so even the term deep
learning is a new term for something that has been around for a while so that's a particular kind of machine
learning right it is it is no deeper let's say there are the kind of machine
learning methods is the terminology and also your network it's a very very
rudimentary kind of neural network you know for speech recognition that might be tens of thousands of neurons for
emotional recognition much much fewer so that cannot compare to the human brain
Emotion Recognition
one of the things that I noticed in your presentation is the when you're doing
the emotional emotion recognition you're mapping it to kind of the you know these
names common names we have promotion angry sad yeah is that model even too
simple or is there an underlying more nuanced model for emotion yeah well so
there's a lot of research done on the underlying model for emotions such as
valence arousal you know and there are models that try to protect that first before they
predict the final label and to interrupt you because I saw the slide but these folks haven't valence arousal those
valence is like the strength of the emotion and arouser is the string okay valence is the positive and negative of
the emotion and so the various you know angry sad cool yeah it's a combination
of they're a combination of different values of valence and arousal this is
one emotion theory so these are models the psychologists came up with
to try to organize what we know about emotion and so it's just human minds
were symbolic animals you know we need to have names we give them so everything yeah so it's just easier for us to give
a name to emotion so we know what we're talking about yeah rather than give this vague you know number right valence
arousal if I tell you all this is valence or also this and this number you wouldn't know what I'm talking about if
I say he's you know he's showing happiness on his face you kind of kind of know so it's kind of a emotional
recognition is kind of like speech recognition when it was only recognizing isolated words it's oversimplified for
sure yeah we're not good at all with emotional conditioning you saw my slides the
performance is nowhere near the ability the performance of recognizing words
recognize the emotion is much harder right right now one reason is you pointed out is that it's hard to define
what emotion is you know for example maybe it's easy to see if somebody's
happy but is he smiling because he's happy as he's smiling because he's trying to be polite right and also what
about emotions like frustration how can you tell sure some some some things are obvious
you know when someone's frustrated if they're rolling their eyes or something but there are other times you know from
the voice from the tone of your voice we can tell a lot of things right but we
cannot tell everything all the time so it is a long ways to go you saw the
accuracy the accuracy these days even the best commercial systems is just like 60 percent you know at most 70 percent
and speech recognition we're talking about above 90% right so there's a long
way to go for emotional recognition and especially some more complex emotions like humor sarcasm sarcasm yes sarcasm
and humor and and deceit you know is
this person lie right and there are colleagues in the field who have come up with systems that
there has some nice um percent accuracy in detecting deceit and literally performs better than humans turns out we
are now very good at detecting receipt we're not good at all and humans are not very good in recognizing emotions yeah
you know what we found with when we did some human subject studies is that we
found that women are better than men
there was actually quantifiable and then women can detect motion across languages
language in language they don't know interest also better than men in the same language so you can talk about why
the reason you know we're programmed to be mothers we must recognize the emotion of the baby early on and I think there's
some merit to that soon although I am NOT an anthropologist I cannot prove
that but yeah and then the reasons you
know you see there are a lot of women who are nurses doesn't mean men cannot be but just happen that way alright a
lot of the caregivers are women you know
kindergarten teachers you know just nannies you know so if we want to build
robots we want them to be more like that one pathetic all right great well thanks so much for
Outro
taking the time to sit down with me it was a great discussion I really appreciate it thank you would you like
to share how folks can find your research or are you on any of the social media that works well it's easy to
google my name okay which is Pascal faux pas al e phone is fu ng if you google my
name you come to my website which lists all my our publications our projects and
all that and they interesting and they can email me via that website as well great right great well thanks so much
thank you alright everyone that's it for today's show if you enjoyed this show or
have something to add to the discussion please leave a comment on the so notes page at Twilley i.com slash
talk / 9 or tweet to me at at sam Charrington or at to MLA I to let me
know what you think thanks so much for listening and catch you next time

----------

-----
--08--

-----
Date: 2017.03.01
Link: [# Deep Learning: Modular in Theory, Inflexible in Practice with Diogo Almeida - #8](https://www.youtube.com/watch?v=DIVfEMf3dfg)
Transcription:


Introduction
hello and welcome to another episode of twit talk the podcast where I interview
interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Charrington the recording you're about to hear is part
of a series of interviews I recorded live from the O'Reilly AI and strata conferences in New York City last month
I'll be sharing these interviews on the podcast over the next several weeks and I'm sure you'll enjoy them this time I
interviewed Iago Almeida senior data scientist at healthcare startup and lytic Diego and I met at the AI
conference where he delivered a great presentation on in the trenches deep learning titled deep learning modular in
theory inflexible in practice Diego and I discussed the ideas he presented which
are centered on the data software optimization and understanding issues
surrounding deep learning Diego is also a past first-place Cagle competition
winner and we spend some time discussing the competition he competed in and the approach he took to win it Before we
jump in a bit of a listener warning our conversation gets pretty technical pretty quickly I do try to make sure to
summarize key points from time to time and I really think that if you hang in there I'm sure you'll learn a ton of
course let me know how you like this level of detail I'll be including links
to do go and a bunch of the data sets and other things that we discuss in the show notes which you can find at Twilley
i.com slash talk slash eight also as is
the case with my other field recordings there's unfortunately a bit of unavoidable background noise sorry for
that and now on to the show
Meet Diogo Almeida
alright hey everyone I'm here at the O'Reilly AI conference and I'm sitting
with Diogo Almeida who just did a really interesting talk on deep learning and he was kind enough to sit down with us and
talk a little bit talk a little bit about what he talked about Diogo why don't you introduce yourself cool
I'm Tiago Mehta I work at this supercool medical deep learning startup where we
work on giving like really accurate really fast really safe medical diagnoses and this is something that we
hope will completely change the world before that in past life I was a
mathlete so I broke a 13-year losing streak for the Philippines in the international math Olympiad was in the
top team in the world at the interdisciplinary competition in modeling and there's a website for
machine learning competitions called kaggle that I won first place on in one competition as well one was that this
was in 2013 because the cause-effect Paris challenge tell us about that oh
it's just a very weird challenge where in most machine learning you have like
tabular data so you know like you have columns of features rows of observations and in this problem your data was pairs
of sequences so you have something like altitude and like one exact one observation is like altitude and height
and you have like a pair of sorry a sequence of pairs of like which altitudes correspond to which heist in
some unordered manner and the idea was given this you're supposed to predict whether altitude is causes height or
height sorry the Alton Hydra the same thing I'm Ehren altitude in temperature right so you were supposed predictive altitude causes temperature temperature
causes altitude and obviously that alidade causes temperature right for us uh-huh but there's a lot of like very
complicated tasks that we don't know the answer to and it's kind of like the basic task is to if you know the saying
correlation doesn't imply causation right it's supposed to do the opposite of that so you're supposed to figure out
how the correlation implies causation okay which is extremely useful because you have like lots and lots of observational data right it's very hard
to have like a controlled study so the more accurate we can get a view of the world from purely observational data the
more we can either have informed priors before running the control study or figure out
how to order the controlled study in an appropriate way okay and this is also the kind of analysis you would use for
Diogos background
like a root cause analysis or something in like an IOT use case where you've got all these observations and you're trying
to figure out what the the underlying condition is or I'm not as familiar with that there are there were there was
traditional statistical work and there was a that actually was a background for this topic but I kind of didn't paid
much attention to that because I kind of went my own way and it was much more for fun than for winning and winning was a
very nice side effect nice and I I went to a much more like software oriented
way of just like build a really complicated powerful model and have it
solve this based on like rather than like hand engineering stuff why not just like automatically engineer a lot of
informative variables and then solve it with that okay so can you walk us through the process like how do you how
Diogos methodology
did you formulate a methodology for attacking that was this your first cattle competition or had you guys my
first serious one I I've done like one or two before that I didn't really like really spend much time on yeah but like
you know you quit like after two days because it turns out your teammates weren't useful or something like that so
I have like played with it before but I've never really gotten all out until this one so my methodology was well some
background is that there are like statistical tests with people use you know that did very well in this task
okay and or sorry that people used to using this task and put it roughly in
perspective these godlike points six ish a you see so you see a paper in nature or science about a new test for
causality probably gets around point six a show you see okay you see for those that don't know is area under the curve
and that's a performance metric yeah so we were we were solving a ranking problem okay we were trying to rank the
outputs given that we know which ones were which ones caused each other just a little bit of complicated metric because
we actually had three output classes so we did like a bi-directional AUC but that doesn't really matter much
and so these tests would like point 6hc they're roughly a single feature because it's just the prediction you extract it
directly from the data the most of the other competitors in like the top ten
had you know tens of features or something like that and the second
placer I think had like a whooping like 100-something features okay and I had
fifty thousand so okay so what I did was I found like a very simple way of
determining causality which would be the rational B if X causes Y then Y is a
function of X you know there's noise in there somewhere right so roughly you can tell how good one is a function of the other based on
how well they can be approximated by functions and this is kind of like a very vague like recipe for how to create
these features but the ideas are rather than you know hard coding statistical tasks like you know like add a Gaussian
integrate this thing out whatever I just figured that we have an entire field of curve fitting which is called machine
learning right and these are often like built after natural like very natural priors so the idea would be try like a
ton of machine learning algorithms all of the ones through computationally feasible try different metrics for what it means because v is this is it's kind
of like a not like a very exact term right like throw like these are all the features now throw them all into like
big boost decision tree um train this thing for a week on like a
fifty core machine and then you know take a nap the entire time so that was
roughly my solution Wow and so the solution was that was
primarily based around the boosted decision tree as opposed to some super complex ensemble or some yeah actually
it's a weird story that for this competition I was so far ahead for
almost all the competition I didn't even try so the what was it like for
basically everything beyond the last week like you know like maybe a month or a month and a half before I even saw the
competition late I was like so far ahead that the between like me and second place was like the equivalent of like you know
second and like 50th or something like that oh wow so I was like feeling really confident and I actually stopped pay
attention to this because I felt that like oh this is gonna be easy right but
then during the last week you know someone you know people started sharing their solutions like I only got 10th or
something here the features I used and all of a sudden like everyone started rising okay and this is definitely by
creating ensembles of everyone's different solution like people likely kind of hinted at with it but I think
there's only one person but like they had like a lot of good stuff in there that other people started using okay and once people were getting performance they like to make more of it or
something like that so people are signing to rise right right and like I didn't haven't even in sümbül this far
and I unfortunately had a model that took out like a week to try and think I said yeah so and I only had a one week
left for the competition so I decided that I tried like a few last-minute
attempts at since Tom Blaine uh-huh but nothing beats my like my super big one week long model okay
and so I just stuck with that thing and that ended up actually winning and it actually was very scary because people
ended up passing me on the the trainings on the validation leaderboard yeah but
in testing your body was like was completely flipped because by day overfit yeah they oh they like they had
like hundreds of submissions while like my best submission was like my sub tenth it was like it was Labor's very like
hands off competition for me I cared about it a lot and I like I wrote like lots of software that was I thought nice
um but like I was really I really really thought that would have been like an absolute slam dunk okay so it's exciting
though okay so where did the 50,000 features come from so you can imagine
like exponential growth when you're just trying like every combination of this with every combination of this you know there was like every combination of
metric that I can think of every combination of machine learning algorithm that was like computationally tractable there was like symmetric
features so you could like augment your thing with like different features because like it doesn't matter if your
eye is right there was a a new instill a
n-- when i talked about the competition we is not all of the input was numerical
some of it was categorical okay and like it you just can't like throw categorical data into a numerical algorithm right
right so it becomes actually a complicated problem how do you compare those different ways of calibrating your baguette or something like that well I
mean either you can it's very easy to convert the maracle's categorical yeah but you lose a lot of information from your ID so what I did was I did
different ways of converting from like like this is like a categorical
numerical pair metric you know this stuff like compare you know compute sorry convert numerical to categorical
via like clustering or bidding or something right and then you know when you want to convert categorical to
miracle you do something like the PCA you know like get the first principal
component or something like that in projection to the first principal component and basically are just looping through all of these things so you can
imagine like a lot of less than four loops okay in the end I had a bunch of them so like if it ended up with like 50,000 - and I almost skipped a detail
there which is I also used a feature selection algorithm and I'd like to make it a little bit smaller my gosh help
performance a bit but it ended up not being important okay I usually omit but for the sake of clarity that was also
done okay okay wow that sounds pretty cool and now that was a little bit of a
digression interesting story though yeah
absolutely absolutely generalized to new problems as well I believe the competition organizer was applying it to
some sort of biology problems and they were showing that it actually predicted causality on that as well oh really so
yeah hopefully that kind of thing could be really useful oh nice nice but what
Deep learning myths
you were talking about here was deep learning yeah and I didn't catch I
didn't catch all of your talk I caught the last bit of it but it seemed like
what you were going through kind of a bunch of war stories lessons learned like you know you hear a lot about deep
learning you know but there are a lot of things that people broadly believe about deep learning that actually are false
and why don't you explain kind of what your intent was for the talk and I kind of
walk us through you know an overview of what you presented cool so the way I see it is like there's these two competing
these views on deep learning like upstream views which is deep learning will solve all our problems and learning
is complete garbage or less sorry it's all hype yeah an exaggeration with maybe for exaggerating views you can say that
and there's evidence for each of these views you know like there's some amazing results of deep learning there's some mate like extremely poor results on deep
learning right and the idea is that like these are not as informative of the stuff in the middle so the idea is that
you draw all this evidence in like this one dimensional plane you know you'd like try to like draw like a max margin hyperplane you might
get like you this interesting decision boundary cuz like this is where the interesting stuff lies like this is the stuff that's going to be moving slowly
over time if deep learning is doing well all right or the other way if people are starting to like find all sorts of failure cases
and the idea would be if we talk about like these examples and like the edges
of our understanding or the edges of our everything or like edges of you know like all the things are limiting deep learning
nowadays and like keeping us from solving all of our dreams that can hopefully give people an impression of
like what everything else is like because it's like just very extreme on the other ends of the spectrum and I feel like that's just not very much
talked about because when you said like a lot of people are on the deep learning hype train or kind of being sat at home
and like being grumpy because now all of the all of the questioners are silenced dry hmm
so if we kind of map out what the coroner cases are and the failure modes
Understanding deep learning
and things like that it'll help us push forward our understanding of this thing is the basic premise and kind of like acknowledging it also helps I don't
think what I did was the greatest acknowledgement of it but I think it was a more thorough one than I've seen before uh-huh and realistic especially
in that I think that sometimes just understanding your problem really well um really helps you to solve that
problem so I know now that I mean like I do research as well and this stuff's
very important to me and by looking at it from like a kind of a higher level I
can kind of see better like this seems like something that looks really promising to me or this
doesn't seem promising at all right like for example one of the problems with
deep learning nowadays is everything's very local right like um you can all come in Watsons you use the gradient
right or maybe higher order derivative e things but they for practical purposes
use the gradient and this can be insufficient for some applications right and it makes you like life like going to
a higher level maybe I can start with a lower level right like SGD doesn't work
for my spatial transformer network this is unfortunate like let me try ad and
let me try our miss problem but if you go to a higher level you realize that the problem is the local learning the spatial transformer network not
necessarily the gradient descent so to tell us about spatial transformer networks are yeah so this is just one
example I use of a kind of network but it's very easy to see the issues of
local learning with it's very nice because it's a differentiable Network it's very easy to see exploration
problems in reinforcement learning domains this is one that you have a derivative of and it should be easier to
optimize and it is but you sometimes don't get what exactly you you it doesn't like to feel its full potential
so are you kind of seeing that there are a lot of people coming into the space that you know that you know try to throw
Spatial transformer networks
deep learning at a given problem the common way of solving it is using stochastic gradient descent and they
don't really think about you know how that's working in that it's you know finding a local optimization and there
are some problems that you know for which they get kind of stuck in that local and that is unfortunately the case
like I have seen many people introduced to deep learning who think that let's stitch together and architecture that's
differentiable in a bingo bango collar day we've like solved problem X right like they there and they realize the
latest that the limitations of requiring large data sets but they they think
that's what it amounts to and I think often times very often times it doesn't so back to spatial
transformer networks what they are is basically instead of like a single metric that learns how
classifying image you have to networks one of them learns which part of the image to look at and the other part
takes what that network looked at and does the classification on it and this is a huge advantage because a lot of the
times your input image might be really large and you don't want to run the network overall all of it it might have
like unnecessary information it might be really useful to like co-localize so
like have the where as well as the what there's really good reasons to use this and in fact if for medical problems if
it worked well I would use it for everything number one number two is if it worked well I'll use it for every computer vision problem because what
these spatial transformer networks can do is not only find the region but it can also transform the region into a
canonical location so rather than having to learn filters of like cats at every orientation you might have to learn
filters of cats at only one orientation which like would reduce in resulting like much better date and parameter efficiency but back to the issue here is
that you have these two networks that are they're not competing but they're working together but they only using the
current network the current other network as its source of signal basically so if your classification
network gets really good early on in training your localization network gets stuck in this Optima right because like
if it changes anything at least a little bit your classification network will do worse so like the gradient tells it's like hey hey just just stay where you
are you're pretty good or move you a little on the small region right which might be very far from the intended purpose right if like correctly like
zooming all the way into the thing you care about and like rotating it a lot and on the other hand if the spatial
transformer network converges early so imagine the classification network is garbage it might the zoom into like
regions of the image that are just independent of the class but makes the classification network tends to perform
a little bit better on so it might like for example if you're trying to classify
kinds of dogs or like imagenet and it turns out like your classifier starts
out like just being good at telling grass means dog and the localizer noticing like just zooms into the grass
right like to zoom-zoom-zoom grass and basically you've cut the dog out of the image in the moment you can cut the dog
out of the image you get no gradient signal and when you have go no green signal you're stuck there forever
and this is a problem that people just don't really like to acknowledge in networks right like that's that's
actually a very complicated relationship because now you need to like maintain a balance and all of that and I don't
think people even know how to do that like people don't how to do it - the generative adversarial metrics either another example I gave of this yeah yeah
Talk structure
so what was the what was the overall structure of your talk so the title of
the talk was deep learning modular in theory and flexible in practice uh-huh so I want I first wanted to talk about
the successes of deep learning not should not get anywhere rather to show that deep learning is very modular and
it can do a lot of things and you know get them into the mode like wow we can solve everything and I actually think
that I had a somewhat bold claim than that first part which is that deep
learning today steep learning components can solve any problem any like
computable problem if you ignore the practical aspect which would be I mean I
think it's interesting to point out right because then now that you I salute that you know like the back glass specs
are the issue right right and those Pecha aspects are data software optimization um in probably order of
difficulty of how to understand them and the latter part of the talk I talked
about these issues with deep learning like specifically data software optimization and then a final section
and understanding just because I wanted to point out that while understanding is not necessary for like getting things to
work which maybe is what we care about understanding is very necessary to make progress and we just it's amazing how
little we understand about anything well let's come back to that maybe walk through the different sections so data
Data walk
walk us through the points that you were driving home around that okay so from a
super high level it's that neural networks are extremely data deficient Maran they don't have to be that way and
data efficiency is the root cause of all problems because if we were day deficient the size of data sets went
matter right the data sets we use are kind of flawed in that like they have
known issues that you know researchers know about and they new nations like they're noisy or like what kinds of known issues like
penn treebank is a very small data set therefore making bigger networks is not very helpful because the overfits
therefore you should generally only publish regularization research on it or
something like that so you're referring primarily to kind of the known that kind
of thing it's the kind of things that you know like the mainstream deep learning researchers published on to write in to them hey I have something
cool use my thing right and that is I mean it's important right like the
alternative is publishing they said no one knows about which is also very hard to get any information from but well one
Reproducibility
has kind of a is almost like a reproducibility kind of issue where there are elements that are inherent to
the data set that you know drive towards or require a certain class of solution
yeah it's a horrible state of affairs where like you need to like if you you
know you read a paper the paper usually has a high level it doesn't have all the little details have some notes for and you implement the paper exactly as it
says and it gets not anywhere near close to what they were they had right and
you're like yo what the f and then you know you maybe you email the authors
maybe they eventually reach the source code and you run the source code cuz you won't believe them and you're like wow they just reproduce exactly what the author said it turns out like it just
has like a bunch of magic hyper parameters like you said you know LT regularization to news you need this learning rate schedule for
sure use this optimizer and also preprocessor your dataset in this way and sample it in this way mm-hmm and
like these are all things that you really want to be robust to right and you just you just aren't right like that
is it's it's a very unfortunate like aspect of the world right like uh huh
you're you're put into this position where if you don't do you know if you
don't play the game you never get stood up the odd results and people don't listen to you you do play the game I
mean some people listen to you but some don't because they know the game mm-hmm but then like it's the only way to get
people to see your thing mn by the game you mean in terms of the researchers like there publish you know you know winning the
Research
competition's for whichever data set there is usually it's usually like you
want to get people interested in your papers yeah and it's very different if you just didn't care and you wanted to publish interesting things right right
but if you want to get eyeballs sometimes like unless you're already a respected person it's kind of what you
have to do yeah right so like ID like it so sometimes that
kind of thing is important I think that it's kind of very qualitative thing and which is unfortunate in the data world
that they gets to get a feel of a data set like Wednesday's that's starting to get like really overfit mm that perhaps
it's not useful anymore and I feel like some research it's like qualitatively feel that about like so far 10 and so
far 100 especially so far 10 I'm not represent sure but if I'm 100 isn't watching a data set this is a data set
of 32 by 32 RGB images okay it's a popular use baseline because it's
a very small baseline and images of anything in particular see four 10s 10 classes okay 10 Coleman classes and they
are the popular data set because it's a really small data set 32 by 32 images you barely see anything and it's not in
mist because people have like basically decided like and this research is not enough mm-hmm so like they just don't
listen to em this research at all all right it's starting to be that way pretty far down mm-hmm just because we're getting to be so good on it now
okay yeah there's known limitations that
makes it it makes it hard if you have a genuinely good result to tell people if
you have a genuinely good result mm-hmm especially because like as you scale up like it's also very computationally demanding right so and you you describe
Data Overfitting
the the data sets is being over fitted oh for sure which I explained elaborate
on that because I tend not to think of data as being inherently well the
community is over fitted they said not even the algorithm itself oh there's actually this cool test that someone did I can't remember who where
they showed us like four pictures you know images and they asked like these
are these are the four days that's the are sorry maybe not they said like do you know what they said this pictures from this pictures from this pictures
from this picture right like many people did like CFR is a very canonical dataset um there's a places dataset my large
teen understanding one right and there's image nap which like more general so and so you're basically saying that if
someone can really know these datasets so well they are not generalizable yes
like people have actually reported like native results are generally not reported as much right just so much of
it writes a very empirical field so maybe this is uninteresting now but this just happened so much like people
have noted that the inception architecture you know seems to work much better an image in it than it doesn't
other tasks and it is a pretty complicated thing right right so maybe
maybe that makes sense or I I've had friends that I talk to I'd hate that a
lot of my references are friends but there's like the field moves so fast right that like sometimes even archives
can't keep up which is I think super awesome for being a bit where and anyway they chat about sometimes how ResNet I'm
oftentimes don't work for the computer vision architectures right or one of the the best practitioners of using
continents a friend of mine standard yellow man he works at deep mind he has
not been able to find batch norm to work for him and I find that to be really interesting like is it because all of
his other parameters were too into batch norm is there something that he solves the Batchelor solves also that is not necessary
mmm is this easy just wrong honestly I don't know but I think that there's a
bunch of cool stuff there that maybe we can figure out right and and is this
Generalization
inherent issue inherent to deep learning or is it just the approach we've taken I
mean I would argue it's not even an issue in deep learning it's actually like maybe we can look at the bright
side of this I was like it's a miracle it even works so going to the
understanding topic right there's as far as I know no practical theory in deep
learning like there's nothing that can actually guide us understandings like there's what I call stories
like every paper has like a high-level story if this is why I think it works yeah and if you like really try to vet
the story really well you can like very easily like disprove it and I know of no story that's like 100% bulletproof so
I'm willing to make that claim and so we have these stories and like the the
guide people but they they rarely work out just useful tools unfortunately so what we have instead is empirical
results what we do is we want generalization generalization is kind of like a lofty concept and we don't really
know like it's not like you can in like traditional statistics you can kind of do that but like deepen it's just much
harder to see if so many parameters like you can't really measure it well you can't measure the VC dimension but it's really it's so big that it
doesn't matter there's a lot of things that what's the VC dimension its I
probably would screw this up but I'll give you like my best like first-order approximation what it is it's roughly
how powerful your model is so it shows
it kind of corresponds to like how much data you need in order to get generalization okay so like very curvy
powerful models have like a high VC dimension which means that you need a lot of data you see doesn't stand for very curvy does it know it stands for
I know the V stands for vapnik and the C stands for another person's name yeah
okay sorry so generalization like in a
you know like in the very old-school machine learning sentence a sentence that I don't think we'll come back to personally like you could have bounds
and like how much data you need in order to get like this epsilon difference between trainer vests and stuff like that and that's just not something
that's gonna happen in deep learning and as long as we keep using deplane we're probably not going to get that right so what we have is empirical results and
with these empirical results we just have a bunch of experiments and a bunch of datasets and we show like it seems to
work in the datasets we've tried hopefully it works in everything and so
like this is where you might see it as a pro but I sorry as a con but I see this as a huge positive deep landing right
like it's actually super cool that it generalizes right like you can get a new computer vision to ask um I use computer
vision because like that's one of the easier domains and you know kind of a ton of data you can just generalize you know you can
use it to generalize you can use imagenet features to generalize on that but that's just not something that makes sense right um
I mean like if you look at it from like a really strict perspective of like there's no guarantee that this should
work but it runs to work and that's really interesting alright and I think
that there's something about deep learning that allows it to generalize so well you know you can even generalize to
domains that you've not even trained on I think there's been some work on generalizing imagenet models to cartoons
and like even like cartoon drawings of the things that they were classifying sometimes activate or there's something
related to that yeah so yeah it's a wonder of deep learning I actually there
are some experimental result to try to explain after the fact why things work but without being falsifiable it's
questionable how useful it is so perhaps maybe deeply exploiting some
of these kinds of explanations there was a recent one on physics recurred that
deep learning is the more like deeper the kind the class of things the deep
learning is very good at fitting are a very like in very natural class of functions therefore since deep deep
learning models only can fit like a sufficiently fit a small subset of the function space but that happens to be
like a very common like based on physics kinds of functions that would occur okay
so you started out talking about data and that overfitting problem and then
Data
tools was that the network software software software I there's two more things in data though okay is that the
data we have which is problematic there's a data that we data we have and we use like datasets data we have that
we don't use and there's like tons and tons of data that we have that we don't use but I think that we just don't know how to use well unsupervised learning
multi task learning transfer learning we kind of use but we don't do very smart things I think and even like this
implicit stuff like the trajectories of the networks that you've passed through maybe there's some interesting information there and the last kind was
the data that we don't have that we needed like for example measuring these things
that we really care about that we are just missing right now like we have we have no way of measuring
long-term dependency like how well networks capture longer in dependencies we don't have like in general RNN benchmark mm-hmm we don't have a good
benchmark for visual attention you don't have a good benchmark for hierarchical learning like how do we even know if
we're learning hierarchical stuff right do we want to learn hair cool stuff I don't know but like if I would think
that if we want to learn something having benchmarks were to be really good right so that was roughly it for data um
I'm a software perspective it was more about like how the tools we use nowadays
really limit what we can do and like every tool is flawed in some ways this hits home for me personally because I'm
a software engineer okay and I want to use really good tools you mean tensorflow doesn't solve every
problem in the universe no I think they introduced some really good ideas they
definitely brought something to the table but it alone isn't enough um it
might like the the like I think better things could be built on top of it I don't think that it's the low-level
components that are a problem and I actually don't think that hardware is that big of an issue like a speak of an issue that people make it out to be and
in in theory in practice if you really want to get the art results I think sometimes that's needed but there's like
higher level problems that you can solve without hardware so the idea with behind software is that you can like very like
easily see situations where like the
software we have actually prevents us from doing what we want to do so I I
think I have like two examples that really resonated with me where that an
example of bad software is when it's
easier to explain in words the technique benefits with code because ideally you want to like Express ID that you wants
like the flow from ideas to code to be very easy and the flow from ideas to words is generally pretty good and that
just means that you have a bottleneck and like words to code and maybe it's a reality of life we'll never be that simple did you
provide a specific example yeah I had like a list of like many examples okay
of like different kinds of tricks that are hard to do in various frameworks so
the depending on the framework you do some things can be kind of difficult so like for what is it for
Tricks and frameworks
so when you say tricks and frameworks the the basic idea being you know kind
of the at you know the research I did see that you thought put a lot of paper
you were just showing a lot of papers which is great documenting kind of where the ideas came from so in the research
you know we're introducing all these various tricks to improve solvability of the of the deep learning networks and
it's not what I'm hearing is the tools are you know on the one hand you know
great there and create they're raising the level of abstraction and making this stuff you know more easily adoptable but
you know that also prevents us from implementing some of these tricks which have to be plugged in at lower levels
yes so when I mention trick I use that as a general term of like this like one
unit of thing that you do to a neural network like you can think of layers as
tricks but ranks being more than just layers like for example an additional regularizer might be a trick right or
doing like they could be pretty complicated I think like doing unsupervised pre-training might be a trick and the argument that I would have
is that no framework makes everything really easy and easy in this sense is
that I would I would ideally like it such that everything just gets solved
for me like I'll be able to like yeah like this is probably not going to happen but we can get closer right like
I would like to express like very declaratively like what I want this neural network to be like literally like
take this neural network in this database apply this transformation run this transformation do like train on
this training set like I want it to be that simple and I added like I don't think it can be
but like striving towards that I think is good sure and like a lot of the frameworks like
doesn't support a bunch of the thing like it makes it a large number of lines
of code in order to do something rather than few mm-hmm so what should be an example like batch normalization is like
a pretty simple thing right or sorry so it's actually not a very simple thing
in terms of implementation but like many frameworks can do batch normalization very very well like my torch can do
batch normalization amazingly because like they can just implicitly keep it state and in torch like each of the
nodes applies its updates on its own like when flowing through the Grad and like applying the update so that's very
good but tensorflow for example like in
order to apply Bachelet you have to have to do quite a few things right like you need to create like some state for if you're doing the rolling mean
approximation you create some state for the mean some state for the variance you need to make sure to like apply the
updates to this thing you need to only apply the updates at training time and then it becomes like much more
complicated and just like calling a layer on something right um depending on
how you wrap it of course but it's like this this kind of things used to layer in Torah tried mm-hm and like every framework has its
trade-offs but I just don't think that we are at like the efficient frontier yet of like this is like we like I think
we can get benefits for free basically and I actually have written a few libraries that the try to get these
benefits for free and I think they've been pretty successful I'm still experimenting with them because I think
there's just so much to do there but it's a it's an open problem and are
these libraries are these standalone frameworks or libraries to plug into other existing frameworks mostly they go
on top of yeah no right answer flow okay cuz I think that there actually are both I think that they're both like very good
baselines I'm a big fan of the computational graph I think the design of Theano is actually like quite
excellent I'm a huge fan of Theano and its developer is it has the downside of distributed computing but I think that
it's abstraction of was actually quite good but you can capture that abstraction level very well it's
optimizations are like thing I probably wouldn't do by hand anyway so you get them for free it's it's a it's a
very I'm more focusing on piano test flow similar but kind of has a mix of abstraction levels so I'm focusing in
the low level aspect I think those lower aspects are actually like quite good like they might actually be on an
efficient frontier of trade offs you know trading off like usability versus
usability versus like um flexibility yeah yeah flexibility performance and I
think that that's like there's that just one of you right like merluza you know have computational graph have like all
the basic operations and they are optionally use an optimizer in order to do that like another view would be like the torch it all right catfish view
where you bundle up the pieces of functionality that have a lot of like
the highly optimized pieces right and like that's the view you go for a hat max performance I think it's also very
different philosophically there's nothing wrong with either of these views I'm fine building on top of that there's knowing what you're using it's more of
yeah it's more of the level and how you construct the computational graph which I think should be independent of piano
or tensorflow like these are just different levels right like you could have like a really nice low level thing
but change the high level thing on top of it and it should be fine which is why I not the biggest fan of tensor flows
like many different abstraction levels and I think most well all of the best
people I've talked to who use sense flow they kind of only use a little bit of it and they think that a bunch of it is
like it's not the greatest but I I don't care I'm not using it okay and like it
is those higher levels that I think is very interesting and like that's also where the user interacts with it right
like if you're having Kody interact with Kody doesn't matter you can have like the ugliest interface anywhere in the world like your compiler can just you
know switch things around and all of that stuff okay so data software was the
Local Learning
third fixation optimization so I touched a little bit into it with local learning yeah and Andre carpathia had a great
quote which I can't remember off the top of my head but it roughly goes along the lines of that neural networks only do
memorization they don't do thinking mmm and this is problematic because this
already not as good but this brown line because we'd ideally like them to think we want them to do like cooler complicated things right that like blow
our minds in a coolness right and they do blow our minds already but perhaps those things were simpler than we
thought yeah and what's gonna happen when you want to do something pretty darn complicated yeah right like we'll see right like
there's some tasks that we think that would require some pretty complicated levels of thinking in order to do perhaps playing Starcraft you need to
like think many moves ahead and imagine what the opponent's gonna do in order to like take actions and Nolan Reese are
not very good at imagining what to do yet maybe that will change but we'll see
and and doing likes to say that um as a heuristic of what neural networks can do
is anything a human can do in less than one second but I mean if that's a hard
limitation mm-hmm then there's a lot of tasks so take more than one second for variable to do and right will this solve
generally I for us maybe not like when you phrase it that way right so and it's
like it should be possible right like it's modular in theory like you can't just just have architectures that give
it a magic set of parameters would solve that task so smashing is how do we do that right and it's just there's just
many tricks on that and I talked a little bit about the downsides of local learning how we don't pay attention to
exploration in supervised learning and like mostly its attention reinforcement
learning but we treat it as like obviously the plane like there is some implicit exploration because you're you
know you're using stochastic gradient descent so your gradients noisy but roughly it wasn't noisy you'd you know
be plopped on a point and you just kill climb down some direction and be stuck there and like you don't even know how
good of a solution it is right all right so that's that can be I don't know like
that that can be a very unsatisfying because if the answer is I mean this
goes back to I was talking my life in terms of limitations like maybe local learning just can't solve this right and
that would be super duper and satisfying cuz local learning is like our most a learning algorithm we have right like
using gradients is really really good for trending lots of parameters like we're gonna have to have make like a lot of plant like a lot of different plans
we want generally I without great this merit so yeah we're gonna have to figure
it out so we're gonna have to figure out tricks and how to do this better maybe tricks for more principled exploration and maybe this will make it
such that these won't be problems anymore or at least our will find much harder problems right though hopefully
always be problems and that keeps the field going right yeah yeah but hopefully they're not intrinsic to the
way we do optimization and people are making better optimizers yeah you know
it's quite slow the progress right so data software optimization and
Outro
understanding and we talked a little bit about that earlier are there are you gonna post your slides up somewhere
probably I think that well the I think I think that O'Reilly people but the
slides up somewhere okay but they haven't asked me for the slides yes I think I supposed to do it after the presentation okay which is probably good
since there was like last minute editing going on but it'll almost certainly be
up somewhere okay and how can folks that folks want to learn more about what you're up to or find you do you have a
github or Twitter or I had do have a github it's even though that's probably
not a great way to come on what's the github.com slash do go di oggi o 149
okay and probably email would be the best way this is something I love
chatting about it would be di oggi o at oh god my company names hard to spell
analytic which is e NL i TI c dot-com
okay great cool thank you awesome hey thanks so much
alright everyone that's it for today's interview please leave a comment on the show notes page at Twilley Icom slash
talk slash eight or tweet to me at at sam Cherrington or at twilly i to
discuss this show or let me know how you liked it thanks so much for listening and catch you next time

----------

-----
--07-- 

-----
Date: 2017.03.01
Link: [# Explaining the Predictions of Machine Learning Models with Carlos Guestrin - #7](https://www.youtube.com/watch?v=MEJTxcfzvqQ)
Transcription:


Introduction
[Music]
[Applause] hello everybody and welcome to another episode of we'll talk the podcast where
I interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Charrington the
recording you're about to hear is the first of a handful of interviews I was fortunate enough to be able to record
live in New York City from the O'Reilly AI and strata conferences that I attended last week I'll be sharing these
interviews on the podcast over the next several weeks and I think you'll really really enjoy them I'm especially excited
to lead off this series with an interview with Carlos Estrin now if that
name sounds familiar it's because I've discussed Carlos's work on the show a number of times most recently when I
discussed Apple's acquisition of his company Tori back in August in addition
to Carlos's new role at Apple he's also the Amazon professor of machine learning at the University of Washington earlier
this year Carlos along with one of his PhD students Marko Ribeiro and postdocs
Samir Singh published some very very interesting research into the explained ability of machine learning algorithms
my conversation with Carlos is focused on this research and the paper that the
group recently published called why should I trust you explaining the predictions of any classifier this paper
has been on my reading list for a while and I encourage you to take a look at it of course you'll find links to Carlos
and the paper in the show notes which you can find at twimble AI comm slash talk slash seven a quick
note about the background noise and this and the other on-site recordings they're not too bad considering the noisy
caverns in which they were recorded but some of you might find the murmurs and bumps a bit annoying if you find
yourself in this camp please accept my apologies and now on to the show
[Music]
so hey everyone I'm here at a strata conference in New York City and I happen
Meeting Carlos
to find Carlos gastrin who we've talked about on the podcast before he's the
Amazon professor of machine learning at the University of Washington and we've known each other for a bit so Carlos say
say hi hi thanks for having me here and it was great running into you it's great
event yeah absolutely absolutely in fact I think we probably had a briefing like
right and at this very table a year or two ago yeah and I think we method this event not very place right right that
room over there yeah yeah yeah so I guess I'll say very briefly to the
audience we're not in the most convenient spot for podcasting so if there's the occasional trolley rolling
by just try to block that out because if you want some lunch is right behind us right but I'm sure you'll you'll won't
About Carlos
remember that at all because we're going to have a great conversation here first of all congratulations on the the
acquisition of Tori Nadeau a graphlab yeah abiding people I mean that was
amazing yeah we're very excited to work with Apple it's great awesome awesome so
why don't we why don't we just start with introductions like introduce yourself talk a little bit about about
your background I think a lot of people kind of know what you've been up to but sure I'm happy to share so well I'm
Carlos Carlos guess me working with Delaney for a long time so I was a
professor at Carnegie Mellon for about eight years and then the University of Washington since about 2012 and been
excited about machine learning for a long time and worked on many areas of machine learning
most recently copper has been exciting to me are really around dealing with big
data and the two sides of that so when one side algorithms for machine learning that scale to very large data sets so
how can you scale up to deal with tons of tons of tons of data and the second side is what I think about is the human
side of machine learning so how can a human understand large datasets how can a human understand and what mission and
the algorithm is doing and bringing some kind of human perspective into the mix so I think about those two sides the
computer perspective and the human perspective of machine learning in large datasets and I imagine is also a fair
Big Data
amount of overlap and intersect between those and of course right so the bigger your data in a sense the harder it is to
figure out how to make it work but it's also hard to figure out what's going wrong with that so debugging a machine
learning algorithm that requires it to run in a cluster with tons of machines is just almost an impossible task and
and honestly the way I think about it is that there's no machine learning without humans in the loop right now we're
trying to build this incredibly intelligent application they're going to be self-sufficient and but we'll always
have humans be part of the process at some point and so making that more human
is a very important part to mission and it's very understudied in my field but
it's something that I'm very excited to engage in as well yeah yeah so there's there's humans in the loop and lots of
Explainability
places actually and one of the places that humans are most certainly in the loop is you know on the the back end of
a machine learning recommendation and your group has done a lot of interesting
work very recently at least on explain ability can you talk a little bit about
how you value arrived at that yeah so so just again in one sentence we're
interested in being able to provide more transparency to machinery Meo to explain why emotion or any model makes a
particular prediction or why behaves a certain way now we fell into this
thought that kind of interestingly in various ways so for example in academia were working with various folks in
application domains and we said oh come use machine learning solve this problem is gonna be awesome gonna change your
life and their response was like sounds great but why should I trust this model
what is it doing right and I was like it's got great accuracy so that's one
side but somehow unsatisfied it satisfy no it's not enough it's really not enough okay talk about why it's not
enough and then on the other side once we build a company around machine learning you know three data graph we
start working for other companies that brought machinery into production and there was always a step that nobody talked about what was very fundamental
you trained the model to do something recommendations or whatever predictor and predict fraud right anyone deployed
into the service that every time you swipe your credit card it makes a prediction about fraud right you don't just make that happen on the Box you
know make sure that model is working well and is doing things for the right reason because if it's not you're gonna get fired right you really want to
understand why that thing is behaving the way it is so we by talking to notice that I we've talked about on the podcast
Legislation
before in Europe there's legislation that's coming down the line there's mandates explain ability for machine
learning and predictive applications yes there's legislation Europe and not just that even in the US for certain
application domains in the financial sector they mandate certain models you
are allowed to use versus others because they believe those models to be more interpretable or more believable or
something and so for certain tasks in that sector you you have to use a particular kind of model okay
it's and and that just you know blocks a lot of the high accuracy models you
might want to use so it is a real issue and I think that issue gets a bubbled up in three areas so one is discuss general
use of a model how can they gain trust that service is doing things for the right reason so if I go to some movie
content recommendation see Netflix I want to know that I got recommend the Lord of the Rings because I also like
Star Wars that gives me a sense that thing is doing the right things it recommending things that make sense to me and they can begin to gain a
relationship of trust with that artificial intelligence system underneath so that's a kind of a more
personal consumer thing but if you think about from see a decision that's really
important the really life change like a doctor making decisions about the treatment of a patient right there you want transparency so if you
Transparency
have a system that says the patient's gonna have cancer with 90% probability
most doctors are gonna ignore that system because they might not trust it and also because there's a holistic
approach to medicine that we want to have where it's not enough to just make that one prediction but if the system
were to say you know this patient is likely to have cancer because if you
look at their MRI results you see this lump and if you look at this related
cases they were diagnosed in the same way or and if you look at this latest study this all corroborates the evidence
then I can gain a more holistic view and I can gain trust in the system so that's the second piece it's kind of gaining
deeper insights as to what's happening in that prediction and the third way which is more kind of personal for me is
as a data scientist I want to be able to make the models always better and I want understand when it's working what's not
working so they can improve it and so those three areas public perception of
machine learning making more informed decisions not just a prediction for machine learning and improving the
models through feedback and so transparency and explanation gonna be indispensable to make that happen
and unfortunately as a field we haven't invested enough in that topic right right but you guys have started to invest in
New paper
this and you was it a month or two ago you published a paper there yeah the paper came out just yeah a couple months
ago and it's a resistant called Lyme that my student McCrory Bader and postdoc Samir
saying wrote some years now a professor UC Irvine um and we wrote this paper
based on the feedback - were hearing and the need to do something more in this area and it means some other works in
the kind of explain ability machine learning but what was unique about the perspective that you know Marco and
Samir brought into the world is how they approach the problem so a lot of the
work in machine learning has been about finding models they're transparent or explainable I talked about in the
financial sector so this models have to be simple so that somebody can understand it but the problem with that is that a simple
models than to be inaccurate right and so you're compromising accuracy for
explaining variety and that's what is the wrong compromise to have in fact when you are making the comment that you
drawing the analogy with Netflix earlier I was thinking to myself you know it actually kind of rather that Netflix
Netflix analogy
recommends movies that I want to see and not tell me how it got that then recommend movies that I kind of know
really yeah yeah it says it's read this is the reso right so actually do it but
but for me the way I'm thinking about is accuracy is number one right you wanna
have high accuracy for the right reasons but that's the main thing otherwise you're not gonna be able to solve the
image processing tasks that we've seen solved really well deep learning today yeah if you don't have the most accurate models if I want to allow to use a very
simple model like a shallow decision tree you're never gonna be able to detect objects in an image right so
what's the point right you know we're not gonna build that kind of deficient telogen system so what we did was say
okay let's take our accuracy as a requirement right and so that means that
we want to be able to give a data scientist the flexibility to choose any mother they want yeah and the question
is can we provide an approach that can explain the prediction for any model right that was like the question and I
think it's a really beautiful question and you know the way that the work came
together was was really interesting of course it's only scratching the surface of the possibility but what basically
Marcus Muir did was come up with a system that says okay I want to explain a particular prediction why did you like
the movie or why does this patient have cancer and the way we're going to explain it is in a simple way it's good
just for this prediction and we're gonna do it by highlighting the pieces of the input that we believe the model was most
important for the model to make this decision right so for the doctors example it is this particular area of
the MRI this particular studies they're going on this related cases so that's
kind of a small explanation for the recommendation system Netflix that you are unhappy with it might be that the
underlying systems complex and very accurate by the explanation were to give you it's kind
of very simple but some hearts to be faithful right it has to say it behaves like the model for this particular
prediction it's not like the model only uses you know Lord of the Rings for
everything but for this prediction that's what was what my right right so that's how we we went about doing that
and you know we we did a bunch of user studies that really showed that this can
be very powerful and can be used in various ways so it's pretty cool what's the nature of the user studies so one of the really cool user studies that Marco
design let me let me just step back and say it's really hard to evaluate explanations because it's a subjective
thing right how do you even figure out that's doing anything interesting and so Marco and maybe if it's if it's better I
How it works
want to get into kind of how it works and what the what the research actually showed if it's better to do that first
we can do that first and then circle back to the users is it is you're the boss so let's let's talk about how it
works okay so the way it works is to say I want to have a particular particular
prediction I want to explain why was made why did the model make the finish in the same way and the way when explain
it is let's look at the behavior of the system the behavior of this complex model around this prediction so not
everywhere but around it so for this particular patient I'm going to try to explain why the model thought he had
cancer so let's look at patients around that some that were predicted to have cancer by the model assume they were not
predicted to have constant by the model and fit a simple explanation that
explains the difference between those similar patients it doesn't explain the difference between every patient just
patients are kind of like you so patients that had most same characteristics as you that have cancer
had this things going on and paste in the smoke actresses you that don't have cancer and these other things going on
and that gives me a lot of sin sites for this particular decision and that's how okay more or less how it works
so as a coarse approximation of what I'm hearing take the example of
that you use in your course you know predicting real-estate prices based on a bunch of different variables it almost
sounds like you know you might have a model that's a regressive regression
model that's predicting based on house size right and the the the the lime
system is almost a reverse regression that's going the other way like predicting what the inputs might be
based on the output so one way to think about it is house prices are very
complex it depends on the neighborhood you live in yup the the characteristics of the house everything about it there is a simple
explanation why your house costs showing you five million dollars right that's how much your house got and so X can you
give me a raise so so uh you know why
did your house cost I mean why did the house cost certain amount who knows really complex the models can be very
complex right but your specific house can I explain why the model presents five million dollars that's very doable
I can look at your house and I located similar houses the like it and I can fit
locally a simple model with only a few variables they were most important that's basically let's say linear and it
says around your house the variables that were most important were square
footage and zip code and the number of
bathrooms mm-hmm that's real important for $5,000,000 house mm-hmm for for uh you know five hundred thousand dollar
house it might be a different thing is more important mm-hmm and so that's the kind of thing that the model will say
right so in so that's that's kind of how it works it provides the key pieces of
the import they were most distinctive for a particular prediction and as I
said this is one way to do explanations which scratching the surface there's all sorts of other ways you can imagine doing it and I think there's a lot of
opportunity to do even more but one of the challenges is how do you figure out if this makes any sense whatsoever
if these kinds of explanations are good if the algorithm is working at all like there's so many dimensions that this can
go wrong yeah how can you even figure out if this is at all reasonable right
and Marko Samir and I spend a lot of time banging your head against the wall
to figure out how could even test this how can you even measure it I mean
explain my explanation and a marker had a couple of brilliant ideas that really
surprised me and so let me let me give you one of them so he wanted to know if
explanations are good and intuitive that would mean that somebody who is not a machine learning expert a layperson
could look at it and make good decisions from it okay so that's it that's what that's we thought so how can we test
that hypothesis mm-hm so he the two two tests validated the hypothesis in a really brilliant way so
the first one was if I can interrupt you yeah it sounds like the that the aim of
Human readable explanations
the research was not just to spit out like an ordered list of features in
terms of you know weighting or importance in the output but more generating human readable description am
I reading too much into this or is that it was a human interpret about human interpretive all necessarily readable
okay that's one way to explanations can be many things right we explored the human readable explanations in the
visualizations we explore different ways to explain things okay
but yeah so so here's the experiment that he did which was pretty brilliant so he took a data set and just a little
background which is kind of a funny story that I just told there's this famous data set called the twenty
newsgroups data set but why like twenty newsgroup state news groups okay a dataset that's been around for about
thirty years in the machine learning community and it's from newsgroups which you might not
know what they are but they're later called forums yeah now called Facebook pages right the way they have a topic
and people talk about the topic and they post things right and the data set was famous because that it was given the
text of the posting can you predict whether this was about Christianity or atheism or hockey or computers or
whatever the topic was and basically any more emotional approach gets 94%
accuracy so everybody use this data set in their in their classes like I use in
my class I said oh my Sheila is so cool gets 94% accuracy when Marco run his explanations on that it turns out that
the main features being used are things like the email address of the poster okay like so Sam at gmail.com
always post in the I don't know what your interests are and let's say podcast
news brings up podcasting yeah rector forecasting clearly and and
obviously it's a great predictor right but it doesn't generalize to somebody else right and so it's not a
good feature it's a good feature for you another good feature for the world right and so if you remove that those kinds of
features there are Christian went down from 94% to only 57% yeah so the state
has said that everybody has used for decades a mission listen well actually
so well Wow anyway about CDF and explanations so the question that he starts coming back to the user study was
as an expert he discovered this with explanation can somebody who's not
machine expert discover this and improve the performance of machine learning system we took this data set and any
there was one getting fifty seven percent accuracy and then clean the data as much as this curve scrub the scrub
scrub to remove all this bad features and retrain the model and use able to get about 70% accuracy okay by removing
all the bad features like samma gmail.com and now and then coming up
with a new model or new model train on that train on the same day that's the go sound the clean data and
there was the dirty data yeah and the question was using explanations could mechanical Turkish who know nothing
about machine learning and identify bad features we don't we said look at explanation just cross out things that
you think should be relevant for this decision are interesting we didn't say anything else just cross out things that you think are irrelevant and we thought
okay good crossing it out get performance of clothes to Marco's gold
standard mmm from known experts meaning so you ran the you ran the lime system
Explanation system
the explanation system begins the dirty data set yeah you came up with these explanations and included things like
emails that should be irrelevant and you asked if turquoise yes turquoise if they could figure that out figure out what
parts of the explanation right what what in this sense what features you thought should not have been used as part of
this decision and how did they go so after just three rounds of Mechanical
Turk occurs crossing things out they were able to get better accuracy DeMarcos gold standard dataset Wow
so they were able to clean the data better then marketed what is around in
the round was showing the so so I get into details but we showed the
explanations to a number of Mechanical Turk occurs yeah they were able to cross it out and retrain the model okay then
we showed the new model to mechanical different said Mechanical Turk is the cross some things out we should again so
we just did that three times three iterations yeah we've known experts and by looking at explanations they were
able to find all sorts of problems in beta clean it and get better performance in mark or did like sitting down and
like trying to clean the data himself Wow which was surprising that means the non expert this is just an example
suggest a non experts are able to understand explanations of complex
machine learning system and provide some feedback to that system that can be used
to improve the performance that's it yeah which was really surprising wow that's that's very cool it was very cool
and then the second users so so then they know Marco is bold and then want even more interesting still mark and and
the second one was also really exciting so here's what he wanted to ask when you
train a machine learning model usually training or some data and you evaluate it on some data you hold out it's called
the Nessa sure so that you don't kind of get a biased prediction of how well the
model do mm-hmm so you can imagine some models might do well on the training set but don't don't do well on the test
dataset so we want to throw out those models yeah and some of those will do all the tests it set and and then you
want to keep those models so that's what you typically do if you go back to the training newsgroups data set if we had
just looked at the trainings groups with the email addresses net in the death set you do well on the test set so you think
you're doing well but it wouldn't ever validate way well right but if you had tested on some other data set they
didn't have some as you mail.com any would have done badly then you will be able to throw it out so here's the
experiment that marketed which i thought was brilliant he split the data into a
training setting at Assad right and he trained a bunch of models a lot of
models and different random subsets of that input date of the training data yeah and some models did well on the
training data and some of those did badly on the training data okay he threw out everything that the body nutrient
datum because we only want to keep high accuracy models okay so he kept only things that were accurate in the
training data right and then he looked at the test dataset and some of those did well the test dataset is the models
did badly on the test dataset right and then he said oh the mom up till now I
Data scientists
mean this is pretty standard what any data scientist would do yeah yeah yeah priest on run a bunch of models see what
sticks against the wall yeah exactly yeah now he he did the following took mechanical turkeys would know nothing
about machine learning yep and show them explanations for the models that did they don't say anything about hit
processes and it was all randomized and blind and what everything right so the explanations for models they were doing
well on the test set yeah and models that did badly on the test set okay and they both look equally good on
Results
the training set yeah and the that's what you mean the
validation actually was a hidden in this case it was a hidden asset so something
that you wouldn't do as a data scientist but he held off some additional data
assimilation they run lots of models they did well on the training set and he picked out some that did badly on the
test and did well on the test mm-hmm and then showed explanations for
all those models to make up called Turkish and US which model do you think is gonna be better in the real world
based on the explanations of why they're making the predictions yeah and they they were asked to pick between
one between yeah between two and you are comparing them with a coin flip yeah right so yeah so a coin flip gets 50%
accuracy 87% Wow so totally untrained unwashed Mechanical
Turk masses are basically creating you know feet doing feature engineering on
models in there so the first part to situation II the second part it was like model selection basically right they
were labor to look at explanations and figure out this model is stupid yeah even though on the training set it
looked great yeah but in the real world it's gonna be bad wow that's pretty amazing and that was amazing to me and
the fact that we can do that as I said we're only scratching the surface here but the fact we can do that to me says
humans will be in the loop run there is there are sites who have humans in the
loop because even kind of the statistical problems that underlie this question like if we discussed for a long
time we talked about why this irrelevance officially humans might be able to pick those out and they'd be
able to do better future engineering they'll be able to understand problems they're going in the data even on trained folks and now if you imagine
doing this to get more insight for doctors or more systems in the real world I think you
could do really amazing things so it's pretty exciting to me to start you know exploring this further and further that's very cool let me ask you this
this is kind of in the weeds question but were the features what you might think of as natural features or
engineered features yeah so so he that's that's the the really interesting or a
really interesting question so the underlying models use the engineering engineered feature so for example he
also showed this was good for deep learning models for images which is really you know learn complex features
of the data but the way that he explained was from pieces of the import so there's something that he made was
the input is interpretive all okay and by selecting pieces of images or part of
the text a human to look at that and say oh this makes sense if we looked at like the seventh player
for neural net where I can say oh this is a like a human be like what is that yeah and why do I care you know and so
that's why we we biased towards this approach doesn't mean that in the long run we want to invent something better that looks at the features because the
problem might be down in the future might be down in the weeds and that's kind of where the research should go but
as a first step we looked at pieces of the input and it's totally model independent model network models yeah
yeah we've done this with deep neural networks we've done this with booster decision trees we've done this with lots of different kinds of Mao Wow
so I know we need to get you off to your next session where can folks learn more about this so if you just search for my
name in lime you'll find our paper it was a kdd this year line Li a me L IME
okay you can also find a github project that Marco has been putting together we
open sourced some of these ideas but yeah it's been a pretty exciting working
just keep tracking know Marco Samir there'll be a lot more in the pipeline
this is a really cool thing that's awesome that's great and if folks want to reach out to your on Twitter or I was
already get in touch with you I'm on Twitter gastrin it's my last name window and so reach out awesome back we're also
as you know on a Coursera teaching machine learning that's another place that I interact
with folks yeah and it's a great course I highly recommend it very case study focused I really enjoyed it okay thanks great all right thanks so
much yep a handshake here on the idea Thanks
[Music] alright everyone that's it for today's
show thank you so much for listening and for your continued support a quick story
if you follow me on Twitter you know that I recently called out an iTunes review that I'm actually particularly
proud of in this review a user that originally rated the podcast a 2 out of
5 based on their disappointment with the switch to the interview format came back
and revised that review to a four noting that the interviews were getting better
and that the format was really starting to grow on it now don't get me wrong please I really really really appreciate
those of you that left five star reviews on iTunes and I hope the rest of you go run and do that right now but it also
felt great to see that in spite of his initial misgivings the shows just kept getting better and the user eventually
came around that kind of feedback is great to read Thanks to everyone who
stuck with the show through the transition and I hope you're continuing to learn a ton please join the
conversation by commenting on the show notes at the twilly Icom website or by
reaching out to me on twitter where you can find me at at sam Charrington or at
Twilio alright everyone thanks again for listening and catch you next time
[Music]

----------

-----

--06--

-----
Date: 2017.03.01
Link: [Generating Labeled Training Data for Your ML/AI Models with Angie Hugeback - #6](https://www.youtube.com/watch?v=WhYrLA-fOK0)
Transcription:

My guest this time is Angie Hugeback, who is principal data scientist at Spare5. Spare5 helps customers generate the high-quality labeled training datasets that are so crucial to developing accurate machine learning models. In this show, Angie and I cover a ton of the real-world practicalities of generating training datasets. We talk through the challenges faced by folks that need to label training data, and how to develop a cohesive system for achieving performing the various labeling tasks you’re likely to encounter. We discuss some of the ways that bias can creep into your training data and how to avoid that. And we explore the some of the popular 3rd party options that companies look at for scaling training data production, and how they differ. Spare5 has graciously sponsored this episode; you can learn more about them at spare5.com.


[Applause] [Music] hello everyone and welcome to another episode of twill talk the podcast where
I interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Carrington so I'm recording this intro in New York City
where I've been attending the O'Reilly AI and strata conferences I did a ton of
great interviews here at the events and I'm really looking forward to getting these posted over the next few weeks
today though I've got a show that I know you're going to really enjoy my guest
this time is Angie huge back who is principal data scientists that spare five a company focused on helping its
customers generate the high-quality training data sets that are so so crucial to developing accurate machine
learning models in this show Angie and I cover a bunch of the real-world practicalities of generating training
datasets we talk through the challenges faced by folks that need to label training data and how to develop a
cohesive system for performing the various labeling tasks that you're likely to encounter we discuss some of
the ways that bias can creep into your training data and how to avoid it and we explore some of the popular third-party
options that companies look at for scaling training data production and how they differ before we dive into the
interview though I really want to take a moment to acknowledge spear 5 who stepped up to sponsor this episode of
the show now I'm not going to spend time talking about their service here because Angie and I do cover that in the course
of the interview but I will say these three things first what spare 5 is doing
is really cool and if you have a training data problem and you know who you are if you do you should definitely
take a look at what they've got to offer as you explore your options second they've put together a great offer for
25 lucky twimble talk listeners will hear towards the end of the interview and third I'm just very grateful to
spear 5 for helping to make this podcast possible for all of you and I want to really encourage you all to show them
some love so please hit them up on Twitter there at spare 5sp a re
the number five and just thank them visit their website sign up for a demo all of these things let them know how
much you appreciate this podcast and their support for it as always I'll be
linking to Angie and the various things we mentioned on the show in the show notes which you'll be able to find at
twill a Icom slash talk slash 6 and now
on to the interview so hey everybody
welcome to another episode of twit Moll talk i've got Angie huge back the principal data scientists at spare five
on the line Angie why don't you say hi hi everyone hi Sam hey so happy to have you have you on
here today digging into some good stuff awesome awesome so why don't we get
Angies background
started by having you give us a little bit about your background and how you got started in machine learning yeah
sure so so I started out as a math major in college and I took a stats class I
was at the University of minnesota-duluth and I really fell in love with the idea that you know it was math but it was
applied and you could learn about the world around you you know through math so I I got really interested in
statistics I ended up getting my masters in PhD in statistics I got my PhD at the
University of Chicago and and when I came out of school so I had worked with
my PhD advisor was really big on teaching me how to creatively solve a
problem do you know creative algorithm development you know just start from the basics you know what are you trying to do and construct from there and I was
really interested in in doing that I had a strong interest in machine learning types of topics so when I came out of
school I had this idea you know that I want to work in machine learning but I want to do you know the creative algorithm development and trying to find
that and at the time you know the term data scientist didn't exist yet but
that's essentially what I was interested in doing um so it just took me some time from there to kind of blend between the
traditional definition of Titian and sort of the engineering end of machine learning and find a good
balance and this is where I landed awesome now what were some other kinds of problems that you were interested in in
Other grad school projects
grad school yeah statistician by looking into machine learning yeah sure
there was when I was doing my master's degree had a master's thesis problem that was really fun I was you know there's the game master mind where you
have the little colored pegs and someone has a code which is the an ordering of the of colored pegs and you're trying to
guess through making proposals of you know what you think the code might be and getting some feedback right and so I
had a lot of fun playing around with I ended up building like a metropolis Hastings important sampling style
algorithm to solve the game mastermind in a very limited number of steps and
you know the game is you know fairly straightforward when you're dealing with six pegs in the traditional sense but
then I was taking it up you know well what if it's 14 pegs or 50 pegs and the space of the problem becomes incredibly
complex and I was really interested in the metropolis Hastings algorithm is kind of like I stimulated a kneeling
algorithm where you're you're able to speak to explore a really high dimensional space very quickly and kind
of rapidly move around in this space and figure out where you're making progress and work toward an optimal point so so
that was fun I I also worked on a lot of problems in astronomy so astronomy was something I'd always
been interested in but never had a chance to learn in school so my advisor
was awesome mark coram he invited me to you know come on over into the astronomy
department at the University of Chicago and talk to the professors there and figure out what kinds of problems they were working on where I might be able to
help out so I did some work with quasars I got to do some work on some solar
science research got to do so I had a internship at NASA working on some solar
research continued to do some consulting with them for a while and and then the last project that always sticks out out
in my mind wasn't really part of my thesis work but when I was in my ph.d program Netflix announced they're not
slick prize competition which was it was a competition on predictive modeling to do
movie ratings right so so they released this publicly available data set and it
was all these pairs of a movie ID and a user ID and then a rating and then you
were supposed to be able to predict how certain users would would rate certain movies and I played around with that
problem for about six months that came up with a great solution and that I
would that was competitive in the in the contest but then through that I actually
came to a different understanding of what I what worked better in terms of
actually making recommendations so I built a movie recommender out of that and had it up on the web actually up
until about six months ago I was still using it to recommend movies for myself
but so those those those kinds of problems were the things that I was generally interested in Wow I went
through school mm-hmm so does that mean it takes you less time to pick a movie to watch then it takes
Movie recommendations
the rest of us yeah the best the best
feature was being able to combine two movies so you say you know I want to see something that's like you know I want to
watch a movie that's like Footloose meets fatal attraction or you know something like that and then it would
come up with some you know some great recommendations crossing between those two that was really fun oh nice you
mentioned a whole bunch of really interesting stuff in there yes I want to
I want to mention since you mentioned astronomy I don't know if you've had a chance to hear the last one we'll talk
about that I just posted is with Joshua Josh bloom who's a astronomy professor
at Berkeley and also the CTO of a company that uses machine learning I think you'll find it super interesting
yeah oh absolutely Thanks I mean you've given us an entre
Metropolis Hastings
to go deep kind of quickly here metropolis Hastings importance good
question I think it's you know it was sort of sitting in the in between the two fields and probably a little bit more in ml yeah so metropolis
Hastings is some is one thing important sampling is another yeah and so yeah so metropolis Hastings is it's really just
a optimization technique you know for you know maximizing a function in a high
dimensional space where rather than say you know following the derivative or
doing something more mathematical in that sense you use a random component so you say you start with a space in the
high dimensional field do you say okay this is my initial starting point and then you propose a point around there
that you may go to next and so you you come up with a proposal function so you
say okay maybe my proposal is I pick one of my dimensions at random and then I
perturb that that value a little bit with some random you know in some random
distance in some direction something like that and you say okay that's my proposed point and then you compare your
function on that proposed point to the function on the initial value and if
you're in a better place and you say oh yeah this is a better place to go to you'll you know you'll always move there
but if you're if it looks a little bit worse than your initial position you'll still move there with some small probability and so what that allows you
to do is it allows you to move away from you know local minima things like that that you might get stuck in yeah and and
it just it yeah in in many many problems it it provides a rapid way to search
through a very high dimensional space would it be fair to say that if your proposal function was was your slope in
What is a proposal
the N dimensional space that it metropolis Hastings with kind of approximate gradient descent um not
exactly know the proposal is always another it's it's basically a sampling you're gonna sample from the collection
of all possible points that you might evaluate okay so I mean I suppose I suppose you could create a proposal
function that says I always select the point that follows you know thus the slope but you know that sort of
thing it's good you come up with that but typically you yeah your proposal is supposed to have a random component and then there's the additional random
component that you may choose it even if it moves in the wrong direction okay nice so that that raises a question for
Data Science Philosophy
me coming from strong stats background
you know what how does how do you feel like this guide your perspective as a
data scientist data science has come to mean a whole ton of things for many it's
you know heavy programming for others it's heavy data engineering you obviously it's heavy stats have you do
you have kind of a philosophy on data science and kind of what that all means to you yeah I mean yeah definitely
definitely data Sciences is a big umbrella covering a lot of different things and you know I think those though the whole field is is evolving right and
you know and there's there's more and more applications in this area many more people going into this field yeah
so I'd say now there are a lot more people coming out of a computer science
background going into this type of work you know whereas back you know when I
was coming out of school I you know it's almost 50/50 it would like machine learning was really sitting in between statistics and computer science at least
that's the way that it was it at University of Chicago um yeah and I do feel like I think I have a I tend to
approach problems more from the predictive modeling viewpoint I I do a
lot with just constructing probabilities likelihood estimation things like that
that maybe wouldn't be as commonly used coming straight from a more computer science engineering machine learning
perspective where it may be more about deep learning and you know specific types of algorithms so so I guess I see
a little bit of a difference there but I you know and I and I have background in more traditional statistics you know
with just doing you know I don't also you know experimental design and doing
you know more more just classic kinds of testing issues and distribution comparisons and things like that but I
would say that's a small part of my daily work it's one of those you know yeah we do a/b testing we do things like that and
I'll participate in assisting with those kinds of experimental analysis but typically I'm doing more
yeah just constructing from probabilities from likelihoods you know working with predictive modeling things
like that to you know to solve our product goals other things that you see
commonly in the industry that you think would be different or approaches that
folks take that they might take differently if more people had a stats
background yeah I don't know I I see
them I see them blending and I see you know in people today that are coming out of you know strong computer science
programs with machine learning background really there's quite a bit of overlap in terms of you know the
materials that's been taught the skills that are there so yeah so I'm not sure not sure okay okay
What are you up to now
and so what are you up to now yeah so right so now I'm here it's fair five um
yeah and yeah so what we do here is four five we do we collect training data for
computer vision and natural language models so other companies that are building out AI building out their own
machine learning models and computer vision and natural language types of problems need really good labeled
training data in order to power you know the algorithms that they're trying to build and so that's what we do
so um I got really interested in this space because at my at my prior company
we started we were building out to natural language models there and we had some really good stuff it was working
really well but we wanted to push it to the next level and the thing that was preventing us was just getting that really good labeled
data um and so I was thinking a lot about that problem and then I heard that
Darren who's our CTO here I heard that he was at spare five and I heard about what they were doing and I just saw a
huge opportunity in terms of I was like you know you know AI is getting really
big machine learning it's getting really big you know it's no longer kind of a fringe thing that a few companies are trying out it's that you
know it's like to be in the in the space you know to be competitive companies need to be building building these
things out and as far as I can see the real bottleneck is in the training data so you know that was where I was excited
to to jump in and and be a part of that be a part of that business nice I think
Availability of training data
there's growing recognition to that the availability of training data is one of
the biggest issues that new entrants to you know the folks that are trying to
apply machine learning to various problems take on and you know for a lot of people they look at it and say and
I've heard this you know I've heard this coming from several different angles but you know something along the lines of
you know soon if not now it'll be very difficult for a start-up for example to
you know compete with Facebook or Google or you know large company and Industry
you know X because they'll have all the data and they are not well you know will
not be able to gather it and and label it and all that I do agree with that in
general oh yeah oh yeah absolutely absolutely I mean the way that I see you
know I think for quite a while though the focus was on the algorithms themselves and you know how do we get
better algorithms better algorithms but at this point you know we have so many sophisticated algorithms a very flexible
algorithms right for solving so many different types of problems that that really the defining factor becomes the
training data that you have underneath it to power it in terms of what you can actually do so yeah I think that's a really valid concern um although you
know I would say you know I mean it definitely depends on what you know what industry you're trying to jump into if
you have a start-up and they're trying to do something new I think yeah I definitely think you know if they have a
specific problem that they're trying to tackle that you know requires a very
specific type of data I think there are still a lot of opportunities to get into that space if you have access to right
there would be ability to get that get that label data that you need right which is which is where we come
okay so walk us through specifically what you guys are doing to help help
Helping companies
companies sure absolutely so I guess yeah so I would start by saying you know
when so when customers come to us you know typically the number the number one
thing that they're looking for they're looking for high quality data right and they need that data at scale and so and
I would say you know traditionally companies may you know initially they
may start trying to label that data in-house right and they may say you know everybody take a few hours and you know
look through this data add some labels that sort of thing and then quickly find out like okay this is gonna take forever and we don't have the resources and then
a next step that companies sometimes would go to is trying to use these you
know publicly available crowd sourcing things like Mechanical Turk where you know yeah there's a crowd out there
maybe you can you know put your data out to them and they can label it but that can be just incredibly painful in terms
of you know you never know the quality of the work that you're getting back it's like you you end up having to design an entire workflow around just
trying to QA the data that's coming back to you you end up having to send the data out many many times over again to
multiple different people and try to assemble and make some sense out of the results that you're getting back I'm so I can be a real headache so by the time
customers get to us so what we're doing instead is we handle all of that
headache for you so basically the customer comes to us what all they need to communicate to us is exactly what
correctly labeled labeled data constitutes to them right so so they
usually the customer will present us with some examples and you know these are some images here's some example
annotations that would be the correct annotations for these images that sort of thing and from there we do all the
heavy lifting in terms of we we can we will take on the task we will create and
generate that labeled data using our own community which is like a thoroughly vetted community we do all of the QA
and we guarantee the level of quality coming back to you and your data right so if you say these are the specs this
is exactly what correctly labeled data means to us we want ninety-five percent
of the data coming back to us to be correctly labeled to spec right then that's what that's what we can guarantee
and that's what we can provide and so yeah so definitely it's it's quality is
a major challenge that we're providing just speed and scale and then one other important piece is that we we provide we
can provide diversity among the annotators and so in particular if
there's a specific audience that the customer is interested in so say they're building an AI model and this AI model
is going to be used by you know women age you know 20 to 30 typically right in
the US we can target annotators from that audience so that you know the
keywords or whatever the the labels that they're providing are relevant and the types of things that that audience would
typically use which is really going to improve the performance for the models themselves you know in those types of
settings so I would say you know those are kinds of three three pillars of problems the customers have that that we
were able to solve for them okay let's come back to the diversity yeah because
Augmenting data
that's super interesting but even before you get there if I'm a company and I
want to solve a given problem in my industry do you are you able to help me
find the right data or augment the data that I do have with other data that might help me drive better
predictability yeah so we don't we don't um go out and find like publicly
available datasets for you you know we are in the business of generating the data but we definitely do augment data
so we've done data verification sometimes you know validation sometimes companies have already gone through you
know some steps of a process to assemble data on their own but they're hitting the point where they're realizing like the quality's just not there and what
they need you know what they need our community to help with is just going through and validating which ones are
correct which ones are incorrect so that they can increase the quality there we also we can definitely augment data you know
maybe they have images that have certain annotations maybe the images have tags
for objects that appear in the image but the customer now wants to identify where
in the image does that tag appear so the team might be dog and they need to know exactly where in the image the dog is
and you know we can have our community either do pixel level polygon annotation
around the dog in the image do a bounding box imitation around the dog of
the image those types of things so yeah there's there's a there's a really wide variety of things that we could do in
that sense okay how much of this is best thought of as a services or consulting
Services vs Platform
engagement versus some platform that you guys have built up that that automates a
ton of the you know the second work I mean it's really both and I guess it
depends on I mean the platform I think is really core and central to what we're doing and what we are able to do here
but I would say the consulting aspect on the front end of you know making sure we design the tasks precisely to you know
making sure the customer is gonna be very happy with the data that they receive and then it's going to do what they needed to do for their models is is
also absolutely essential so I would say you know we do have some customers that
come to us that that have you know teams that have been working on these these problems for a long time and they know
exactly what they want and they've already got an idea you know you know exactly how to kind of organize the
logic you know what what a correct annotation looks like and in those cases it can be relatively straightforward for
us to move that right into our platform in other cases you know we've got customers that you know they know the
types of models they want to build they've been struggling and they may even want you know some initial
consulting on you know what what types of data are available to us you know what can we do to to improve you know
improve the the quality of their own models and we can we can do some initial consulting there as well so it really
just defined depends on the customer but but basically you know that yeah the consulting aspect is is all setting up
all the work making sure we design the task correctly going through an iterative you know process in the
beginning with the customer where you know we say okay we think we understand your specs you know we've designed two
tasks the way we think will work we run you know maybe we do a thousand annotations for the customer return that
back to them and they verify yes this is what we're looking for you know yeah you're you know yeah the annotators are understanding
the tasking and this looks good before we go to scale okay so you mentioned a
Crowdsourcing
bunch of things that customers have typically tried before they come to yeah
are you doing all those things as well plus some other things like for example you know farming the labeling out to
multiple people and doing some kind of quorum or voting or something like that right so so no so yeah so we we are not
doing crowdsourcing so first of all we don't use Mechanical Turk we don't use any external community we use our own
community through our spare five app through the web that are you know
everyone is fully vetted we have great detailed information on our users we get Facebook and LinkedIn data from our
users we do lots of survey and skill assessments we're continuously monitoring their they're tasking
behaviors in real time monitoring the quality of you know of the tasks that are being submitted and so but we are
not crowdsourcing so on the flip side what we do is we get really really good
at understanding the quality of the work that the that the community members are able to provide so that we are targeting
we're targeting the right users from the beginning right and then so we've got
predictive models in place to identify when a new task comes in we can identify who are the users that we believe are
most likely to do well on that task and so we can initially target the right
subset of our community then we turn the task on and we have a real time
monitoring system so we we turn on that process and as soon as the task is live all of that
real-time monitoring is feeding back into our predictive models for quality assessment on the user and so we're
making real-time decisions about when to potentially remove access to a
particular to a particularly for a particular user because we're not seeing the level of quality that we need and so
we have a kind of that user quality model running in real time and then we
also have an additional layer which is an answer quality model so depending on the task for some for some task types
there are other types of data and information available just based on the
answer that itself that's provided and so we have that additional layer just to make sure that the answer itself is is
meeting our quality bar okay that's interesting my initial reaction was uh sounds like crowdsourcing but yeah you
know semantics here but it sounds like the key distinction your that you guys would make is that you know with
crowdsourcing you kind of put the task out there and anyone can kind of take it and what you guys are doing is you know
targeting it to specific people yeah developed a relationship with over time
is that the right way to think about it that's true and I think you know it is a little bit semantics and just culturally
how how people use the terms but I think that crowdsourcing often connotates that
you're going to send a question to multiple users and then assemble a correct answer from those multiple users
right and so that's that's kind of the main distinction that I made so a lot of you know oftentimes if we're talking to
customers especially if they've already got a lot of experience themselves working with Mechanical Turk they're really interested in you know well what
are the metrics you're using to decide whether you have enough consensus on a specific answer to move forward and how
many users do you need to ask a question and that's for thing and in our in our setting it's actually irrelevant that's
not that's not the approach we use that's not the perspective that we follow okay so is it fair then to think
Mechanical Turk vs CrowdFlower
about this space as like Mechanical Turk is an API on top of the people but you
have to build everything you're yes figuring enough figuring out how to get them your test you're
figuring out how to do all this voting stuff so that you can get decent quality data whatever you're writing yeah you're
writing the instructions you're doing yeah you're filtering you know which users are you gonna use try and trying
to track their quality all those things yourself right and then CrowdFlower who I think a few months ago announced some
specific as some specific offerings around labeling that I covered on the podcast like they're kind of taking they
actually originally were on Mechanical Turk but I think that's like their own community now and they they're kind of a
slightly higher level of abstraction that's doing a little bit more of the
stuff but still fundamentally this we're gonna take the task and push it to a bunch of people and that's great that's
the results and you guys are you know we're gonna look at your problem and
design a solution to get you quality data and you know that's exactly right
okay and can you talk a little bit about you know as a statistician like what are
Challenges as a statistician
some of the interesting problems that you've you've come up against helping to
build this for four spare five yeah I think I mean the the most interesting
and challenging problem for me was just how how do we design a system in a
platform that can work in a general sense at scale so you know when I would
not when I started out you know we were still doing there was still a component
of manual review internally just to make sure the various processes that we had in place were working as expected and we
had a lot of tailored you know for this task type we managed it in this way for this task type of management in this way
and so when I came in that was one of my initial goals was how do i how do i come
to understand this entire system with all the complexity for so many different types of tasks and we had objective
tasks we have subjective tasks we're looking at images we're looking at text we're doing all these different types of
problems how do we how do we develop a cohesive system
to attack and address all of these different task types and ensure the right level of quality so that was that
was really exciting for me and the you know that's been the focus you know over
the last several months and and now we're there and and so that to me is has just been a really exciting
accomplishment that sounds pretty huge can you talk or can you talk to whatever
Tech stack
level of detail you can oh sorry can you
could you clarify the last part oh you're you know data science pipeline slash technology stack slash you know
kind of anything that can help folks get a sense for you know as they're building
labeling platforms what are some of the things that they need to consider sure
well so I can tell you just for the tech stack that we have here so on the back end we're using Ruby
we use our Postgres and then we have a lot of different AWS services and then
on the front end we have a web client as well as a native iOS application and
yeah and so let's see yeah so I think
that there's a little bit sorry maybe you can give me a little more guidance on exactly what direction you want oh
yeah so from so that that is very helpful on understanding the tech
platform it sounds like you guys are taking advantage of the cloud in AWS in particular as you you know when you
thought about this challenge of oK we've got all these different types of data
how do we unify this into a single platform that eliminates like a lot of
the manual steps that you described are there any lessons that you've learned
about building data science pipelines that helped you achieve that goal that
you think would be transferable to other people yeah and I don't know that this is specific to data labeling I would say
I would say one thing that I've learned that that's worked really well both here and
in previous companies that I've been in in terms of integrating data science with the existing tag an existing
product is I think what's what's really essential is you want you want your data
scientists to be focused on prototyping like rapid prototyping you want you want
them to be really nimble in terms of you know hey if something about the product changes next week we want to be able to
like dig into the guts of our models make our changes really quickly and be able to push that back out into
production in a seamless way and and you don't want your data scientists to have
to be spending the majority of their time you know maintaining and these larger systems and really having to do
having to be so focused on the engineering side in terms of you know let's make sure everything's staying up
and stable and doing what it needs to be doing so so one solution in the team that I led previously at my previous
company one solution that we came to which worked really well it was we developed so we still wanted our data
scientist to be able to to prototype as quickly as possible so I would say you
know our is fantastic in terms of prototyping you have your you know the graphics and visualization component is
you know on it was unsurpassed by any other software you know you have Python
has lots of packages that are fantastic you can definitely do some good data visualization there but our team was
focused on our and so we wanted our our teams to be able to to do their modeling
and we had a lot of predictive modeling kinds of work going on there we wanted our data scientist to be able to do the
predictive modeling in our and hand off the actual model component to the larger
system in such a way that you know it was sort of a you know plug it in so
that you have you know these are the inputs coming into the model these are the outputs coming out of our and we
want that we want to be able to just take that that chunk of code that is the
model pass that over into production and and get that plugged in and going at scale and and that you know what what
worked really well what we've done in both situations is we used actually used our serve it doesn't
have a fabulous amount of documentation out there but it works really well and there's a yeah so anyway so so building
out a system with our sir building out some software around that to allow kind
of just a kind of input-output portion to be handed over and so that so that you can have you know other standard
systems you know picking it up and hitting your models and you know moving that all into the cloud so that you can you know if you start getting a lot more
volume hitting your model than you originally anticipated right you just spin up some additional clusters and manage that traffic is our serve
open-source yes mm-hmm okay so what I think what I'm hearing is
that you've got the the exploration and
the model development that's all happening in our and then you're able to take those models and essentially deploy
them into our serve and use that for prediction as opposed to having to throw
that over to an engineering group - exactly exactly exactly and and what I
would say to is I mean it really depends on the algorithm that you come up with if you're prototyping in at the end you end up using something that's
mathematically very simple then you know then maybe you just pass along you know
your pseudocode you know what I mean I'm like okay these are the steps you have that reimplemented in a faster language
but but if you're using you know there there are many many fantastic predictive
modeling packages available directly and are and if you if you get your system set up correctly you know you know we
were using and we had predictive models with you know thousand features in in
real time returning results in about 100 milliseconds right coming straight out of our if all your hitting our for is
the actual modeling component it once it's already constructed so you know and I think and I think keeping the bottle
code in our and that sense just makes it all that much easier for you know any modifications down the line that needs
to be made and you know and and I think another layer that you can out on top of
that you know which can really well is is if you can start to integrate some automated model
rebuilding mechanisms right so you've got got one system going on that's pulling in new data continuously
updating your models and then you've got another system that's just plugging into the existing most current model to
actually get the results that can work really long it's interesting I'm glad you raised that I was thinking about
Model Drift
that as well and if you what you've done to address like model drift over time
and if you've been a build in like 360-degree feedback loops that kind of thing yeah so just some what I haven't I
haven't gone too deep on those sorts of things so what well I have some tricks that I like to use in terms of kind of
monitor model model monitoring you are doing automatic updates and things and so there's various kinds of sanity
checks there's a there's a paper out of Microsoft that was just fantastic in terms of like these are all the things
that can go wrong when you put your model on autopilot and right and these are the things that you need to be
monitoring and so with just as they're like seven step-by-step suggestions in terms of things that you might want to
get implemented and that's fantastic I can I can try and pull up that paper
for you oh that would be amazing that sounds like a great resource yes I
Computer Vision Natural Language
believe it was in the last couple of years yeah I've got to look it up just
making it up yep and then you guys are
focused on computer vision and natural language is that right yeah that's right
that's our focus right now and how do those domains influence the approach
you've taken um yeah that's an interesting question I mean so so clearly with computer vision we're
dealing with with image data right and so so I think it you know it's had a
huge influence in terms of the the types of tooling that we're building out to allow our users to you know correctly
annotate the data you know our our end resulting data is only as good as the
tooling allows for user accuracy right so so it's
definitely had a major influence in yeah the the types of tasks the types of tooling things like that that we've been
designing um but but I would say at this point in the you know in the in the
general system for QA that we've constructed that that is general it's
not it's not specific in those areas but what we would like to do continue I
guess delving into is internally you know what can we do in terms of natural
language and computer vision modeling ourselves internally to improve the quality of the results itself and and
and that's something that we're just now starting to dive into okay so in
Deep Learning
providing the services that you guys were provide are you needing to get into
things like deep learning and other things or are these more the things that
the customers would use to train to train with the data that you're
providing yeah that's right so so we we purely handle delivery of the training
data so we can do we can do some consulting in terms of yeah you know if you use data like this you know this
this is how you know that may affect the models but it's religious more of a consulting aspect we're not doing any of the model training ourselves we're not
hosting any models nothing like that okay okay interesting do you have a set of you
Training Data
know they're like a top three list of things that you would want everyone to
know about training data oh let's see
yeah I think yeah I mean I think anyone who's got you know any experience in the
field knows that you know your model is only as good as your training data if you're if your training data really only
represents you know a subset a specific subset of the space in which you expect
your model to to function then you know you're gonna have you're definitely
gonna have low accuracy in in areas where you haven't provided as much training data
um so definitely you need good good coverage in terms of you know whatever whatever the inputs that you're going to
be anticipating that will be coming into this final model let you build you want to make sure that your training data
represents those inputs as closely as possible in order to get the best results okay one more one more well and
I think I guess one interesting thing to think about is it you know it depends I
think in the in the computer vision and natural language applications that were focused on the quality of the data data
is really essential but in in other in
other areas there are situations where you can get away with you know less with
lower quality training data and the model is you know as long as the as long as the patterns are there and the
patterns are present you know your model model may still be able to pick up those patterns right and so I guess just
keeping in mind what level of quality is important or essential for the type of
model that you're building the type of methods methods that you're using you know there can be some variation there
okay how would you characterize the scenarios in which you can get away with the lower quality training data um I
Lower Quality Training Data
would say definitely when you're when your model is more about summarizing and
generalizing the data then you know having a few odd observations and there
isn't going to dramatically affect you know effect that that summary um are
there any sequels that come to mind of customer basics let's see um well I'm I
Customer Examples
know there's some well that was so here's an interesting examples a little bit on the edge of what you're talking
about here so so sentient is is a customer of ours the that is there a
provider of AI we have a task that we've been running for them for quite some
time now what they're interested in understanding is user perception of
similarities between shoes so so they're building out you know that you know they
have models that they're building out that are trying to decide what shoes belong together not from some specific
taxonomy or you know hierarchy that some human person wrote down but you know like Oh first you go by color and then
by size about nothing like that what they want to understand is what a human person looking at a collection of shoes
you know if you say here's one pair of shoes now look at these other ten pair of shoes which one is most similar and
they're really trying to understand the human's perspective of you know which of these ten shoes do you think is similar
so it's a very ambiguous task it doesn't actually have a right or a wrong answer but but when we throw this task out to
our users and we return the data back to sentient they are able they've had very good success in terms of improving their
models at which you know in terms of what types of results they're able to surface you know as a result of getting
a very deep understanding of what similarity means at a human level so you
know there are many many varying degrees of you know of going you know the ambiguity versus you know essentially a
very precise you know correct answer and and it just absolutely depends on the model that you're trying to build
exactly what you okay and that example are they ultimately trying to make
Other Use Cases
recommendations or do something I believe so I believe so okay and I feel
like I should have asked you about customer examples are there other
interesting kind of use cases that you guys have taken on there sure yeah well
so here's one we did recently this was kind of different for us and it was a lot of fun so there's a company called in it in it AI and they're building AI
chat bot technology and so they they're interested in building these chat BOTS
in specific contexts and so they really they need conversation data like they
need you know text of conversations in these contexts and they're having a difficult time going out and you know
finding that data publicly all things like that and so um so we were talking with them um you know and
you know we've already typically done lots of categorization of text we will do what's called like aspect opinion
linking of taxi of various kinds of and I'll tasks but what what we ended up
doing for in it was we actually helped them produce the conversations and then
took those conversations and labeled the data the text of those conversations so we actually ended up designing a task
that we put out to our community which was you know pretend you're selling flowers and you know and now you'd be
the be the person selling flowers and then to another user we're saying you know pretend you're buying flowers and maybe we you know give some guidance
things that the customer is interested in learning more about and we actually send this in the tasks back and forth to
collect a complete conversation um yes I think we assembled something I forget
our there was something like ten thousand conversations that we assembled and then for each of those conversations we went back and we did the labeling
that they needed in order to understand the content of what's going on you know in those conversations to help train the
AI models that they're building so that was that was fun that was fun for us in terms of just designing it out but it
was also fun for the community we got so much feedback from people saying like I love this I could do this all day you
know because you're just and the conversations were fantastic so I mean it was it was really it was a lot of fun
that reminds me of someone had a month ago or so set up basically these three
Diversity
kind of conversational chat BOTS you know in a chat room and they were talking and when they got stuck he just
throw out some random thing I don't know
that I'll be able to find that but if I can I'll stick it in the show notes so
one of you talk one of your top three things was subsetting the space which
reminded me that we needed to talk about this diverse any point oh right I want you to kind of
start us off in this discussion with you know the examples that come to mind for you of like where it's been done really
poorly well well actually I have an example this is something we actually
had an article in TechCrunch about this recently so we've been thinking about this a lot just since you know it's
obviously the benefit that we can provide to our customers to actually get the diversity and the you know the community that they need but so we did a
little experiment in-house so we started thinking about the question of you know how different are the results like how
different does the training data look if you're sampling you know various types of populations and so you know and and
it turns out we really didn't have to dig very deep so one of the first datasets that I went to analyzed we had
we have a task that we had put out to our users just as sort of a fun mental
break once in a while which was called rape the puppies and so we just show you pictures of puppies and then you rate
them from one to five stars and you know and you know cute or okay maybe not so
cute and and so we'd collected we've been collecting that data actually over over quite a long period of time just
you know a few puppies to rate you know here and there for our users and so the first thing I thought was okay let me
just take a look at the data and see how the ratings differ by gender and so I split the data by gender and it was just
dramatic and obvious difference that the women were rating the puppies as tutor
consistently across the board across all puppies and with and there was a wider
gap on the cuter end of the spectrum and the gap was more narrow on the not so cute end of the spectrum but it was but
it was still there and yeah it was just striking right and you know it's a simple example and you know okay well
how does it matter you know how cute the women are the better rating the puppy is okay probably not but it's such a it's
such a clear example of the difference differences that you can get in the
training data itself right just by sampling a different population and of course the training data is what's
guiding your model in terms of that output that's coming up together end mmm-hmm yeah but yeah so it's art that's article
in TechCrunch we can put a link to that article as well okay nice nice and so do
Challenges
you run into any challenges and in identifying diverse communities to to
target like what are the challenges generally they come come up for you guys in trying to solve this problem for
people sure so each of our customers is gonna have you know their own their own
demographic that they're that they are targeting right and so at this point we
have a broad international community we have many many users in the u.s. we also have many you have a good presence
across just internationally and we have good data in terms of who our users are
because we put out surveys to collect demographic information we put up you
know many different surveys through our users you know that they're compensated for for completing these surveys to be able to collect that information so we
have a good understanding of our our current user base which is you know which is very diverse but we we
occasionally still have a customer come in and ask for something very specific that we haven't targeted before so they
potentially could say you know we have this task and we need who knows we need
experts in bird identification to label these images of birds you know and tell
us precisely what species of bird you know this is you know something something like that right and so when that comes up you know we can first of
all we can go out and survey our community and find out you know in our in our broad community do we have people who are able able to do this type of
classification already we can identify them but and if we find that we don't have enough members in the community yet
to meet the velocity needs or you know the volume needs for the customer then
we can go out and do specific targeting to bring like you know online marketing to bring those individuals into our
community and we've had good success with that okay so we've talked about demographic targeting primarily thus far
have you ever done anything with psychographic targeting like I don't
know for whatever reason I'm thinking hey we want this to be answered by Terry Briggs ENTJ
absolutely we've definitely talked about it there's yeah there's you know there
are all sorts of interesting just personality profiles out there and that's what's kind of lovely is because we do have this stable community but
that's working with us is if we put a survey out to the community right you know and we compensate them for their
time in completing that survey we can get any any data that we need so it's actually it's actually very easy for us
to target the user that the customer is interested in okay okay this is a little bit of an aside but I often think that
Uber Ratings
in the example of like uber ratings you know I think that there's probably well
not probably there's you know there are you know for average Raiders and there are three average Raiders like you know
hard graders and easy graders I don't have the impression that you know an uber for example would normalize you
know a person's rating against their average rating has a strict
interpretation right three stars four stars that's what the user said yeah dude is it does anyone do something like
that or you know how towhat it agreed it to what degree to your knowledge to
folks think about that in you know thinking about like rating schemes and what's the current kind of thinking in
the industry around that kind of stuff yeah so yeah this is a little outside my my area it's totally really yeah I was
working on the Netflix prize competition because in that case have the 1 to 5 stars rating system and so you know so I
went really deep and it's just an understanding like you know what are these different user profiles or you know what types of users are out there
and you know one of these distributions generally look like and yeah it's like on a 1 to 5 star rating system you basically get ones fours and fives and
occasionally a three you almost never get a two and you know so so I have some
interesting too but you know your thoughts about that in general but you know it currently at spare five from
from our perspective you know we occasionally do ratings tasks it's not
one of the it's not one of the common customer requirements and we'll talk with the customer
whether you know is a is a binary answer you're gonna be more informative for them or a three-level and services a
five-star answer and and so we do have some experience in background and thinking about what type of data is
going to come out of those different kinds of rating systems and you know and besides just the rating system itself
you know the wording of the question is so important in terms of you know you know the words that you use if you if
you have a three-layer system do you say you know perfect okay terrible you know
or do you make it more you know more nuanced right and you're gonna get you're gonna get a different data based
on different wording that you use which thing yeah which actually that too just to segue
into you know another like a whole whole another area of things that we are thinking about constant we here at
square five is just just in the the wording and the framing of of the
question it is just such an essential piece of what we do here whether you know even if it's not a rating question
even if it's a totally open-ended you know writing writing captions for an image or free text keywords versus you
know very objective taxonomy categorization and things like that the wording of the question makes all the
difference right and so in our case you know we actually take our users through or there's a there's a complete process
so when when a new task comes in right where we're iterating with the customer trying to design the task when we start
preparing our users to complete that task we will typically put up a tutorial
we'll start with a tutorial so the users just working through the tutorial they can work through it as many times as they want it's giving them direct
feedback on whether they're you know doing what's appropriate for the task the next stage is a qualifier which is
more like a quiz you don't get the feedback as you're doing it at the end you find out if you passed or not and
typically we won't allow users to continue into the task without completing the qualifier right and so so
we have instructions for the task that we're writing we have the tutorial that we're writing we have a qualifier that we're writing and then we have to task
itself from the questions that we're designing inside the task right and all
of the wording logic the orientation and design of that information on the page it is
all part of the you know the whole formula of how do you get the right data
coming out at the end it's oh so that's that's a really important piece that I think people that are new to this field
are just finding out about it for the first time don't realize what an intense amount of work and thought and effort
goes into getting that right that's really interesting to what degree are you relying on or do
Research
you have the benefit of relying on other folks research to figure some of this
out or is it all empirical analysis on your part great so there is definitely a
lot of great learning already out there you know that's been published so we we
have Dan weld is a computer science professor at UW and he consults with us
regularly he's been fantastic he's a crowdsourcing expert and he's been
fantastic about pointing us to you know all the good research out there about different things that have been tried in
terms of yeah in terms of instructions and tutorials a designing tasks and all those kinds of things so so there's
definitely a lot of learning there but I would say you know given given that you
know we are working in a somewhat this is somewhat different space and that we
are not doing traditional crowdsourcing we are not you know farming answers questions out to multiple users at a
time and so there has been a lot of just individualized learning on our part in terms of how do we work with our users
and what level are our our users at and you know how do how do we how do we bring them through the training process
to get them to the level that we need so there's definitely a lot of internal learning and I would say you know each
time we do another task you know each you know we have certain task types that we've done again and again and again at
this point and we're learning each time we do them along the way how to make refinements how to optimize that process you know how to make it even more clear
anything that we can do to me to make the instructions more clear it just saves in terms of efficiency because we
have that many more answers coming through they're actually accepted and allowed to the deliverable for the
customer nice nice I think we're coming up on an hour anything else you'd like to share with
the audience yes absolutely well I would say yeah if anyone is interested in
getting in touch with us you know there's a couple of ways to get in touch you can always head over to spare five com you can also email me directly I'm
Angie at spare five comm and I guess one other thing that we wanted to let the
listeners know about so we have a blog series that we started a couple of months ago it's called conversations in
machine learning and it's just all about any interesting new applications in AI
and ml things you know that are popping up all over at various companies that we're watching in this space and for
your listeners were offering a fantastically fabulous spare five t-shirt to the first 25 people who
subscribe to the blog series and if anyone's interested they can sign up it's a spare five comm slash podcast
well that's awesome that's awesome and I'll include a link to that of course in the show notes ok wonderful thing
awesome well thanks so much Angie this has been a great conversation and I really enjoyed it
catch you next time thank you yeah absolutely thanks Sam all right thanks bye
[Music] alright everyone that's it for today's
show thanks so much for listening and thanks once again to spare 5 for
sponsoring the show please don't forget to sign up for their t-shirt offer at
spare 5 comm slash podcast and of course we both want to hear your feedback on
Twitter I'm at twiddle a I Twi MLA I and
spare 5 is simply at spare 5 reach out to us and let us know what you thought about the conversation thanks so much
for your continued support and catch you next time

----------

-----
--05--

-----
Date:
Link: [# Machine Learning for the Stars & Productizing AI with Joshua Bloom - #5](https://www.youtube.com/watch?v=B6gbGB_fWkM)
Transcription:

My guest this time is Joshua Bloom. Josh is professor of astronomy at the University of California, Berkeley and co-founder and Chief Technology Officer of machine learning startup Wise.io. In this wide-ranging interview you’ll learn how Josh and his research group at Berkeley pioneered the use of machine learning for the analysis of images from robotic infrared telescopes. We discuss the founding of his company, Wise.io, which uses machine learning to help customers deliver better customer support. That wasn’t where the company started though, and you’ll hear why and how they evolved to serve this market. We talk about his company’s technology stack and data science pipeline in fair detail, and discuss some of the key technology decisions they’ve made in building their product. We also discuss some interesting open research challenges in machine learning and AI.

hello everyone and welcome to another episode of twiddle talk the podcast
where I interview interesting people doing interesting things in machine learning and artificial intelligence
I'm your host Sam Carrington I think you all are really going to get a kick out
of this show my guest this time is Joshua Blum professor of astronomy at
the University of California Berkeley and co-founder and chief technology officer of machine learning startup wise
i/o I was in California last week and Josh graciously agreed to host me in his
company's office for this interview we had a wonderful discussion and as you
might have guessed if you happen to have noticed the length of this episode we covered quite a lot of ground but I
promise you that you'll find this 84 minute interview to be jam-packed with great information ideas and war stories
in this show you'll learn how Josh and his research group at Berkeley pioneered the use of machine learning for the
analysis of images from robotic infrared telescopes we talk extensively about the
challenges they faced in doing this and some of the results they achieved we also discussed the founding of his
company why's that IO which uses machine learning to help customers deliver better customer support but that wasn't
where the companies started and you hear why and how they evolved to serve that market we talk about his company's
technology stack and data science pipeline in fair detail and discuss some of the key technology decisions they've
made in building their product we also discuss some interesting open research challenges in machine learning and AI of
course I'll be linking to Josh and the various things we mentioned on the show in the show notes which you'll be able
to find at twimble a i.com slash talk slash 5 that's Twi ml
AI comm slash ta LK slashed the number 5
and now on to the interview [Music]
hey everyone I am here at the wise that I owe offices with CTO Joshua Blum and
Joshua Bloom Introduction
we got a great conversation lined up for you and we'll start with Josh why don't you introduce yourself to the audience
here great so this is Josh and I am CTO and one of the cofounders of Waze IO I'm
also a professor at UC Berkeley in the Astronomy Department one of the I think
important things that we'll touch on today is how does somebody go from astronomy and teaching to building a AI
application company it's I think a big part of the origin story of of course so
the company is is is my history but I think it also has some interesting
lessons for how we think about AI in production systems and and why having
diverse backgrounds is is pretty important these days yeah that's a lot
Joshua Bloom Background
of good stuff to talk about there why don't we start by learning a little bit more about you and your background and
how you got to where you are so I was trained as a physicist and an astronomer
went to Harvard as an undergrad and caught a bit of the research bug over
over the summer was working in Los Alamos then went to Cambridge England to
do a master's and back to Caltech where I did my PhD all in the context of
astronomy and then back to Harvard where I was a postdoc and all the while
working on what we could broadly term time-domain astrophysics understanding
the variable sky and why things do what they do explosively cataclysmically or otherwise
and while there's a deep interest in understanding the origins of those
events and how they're connected to other things that we study in the universe
I got more and more interested over time in the informatics of actually just
doing the the science statistics on variable sources in the presence of noise and then as I became a
faculty member at Berkeley started looking ahead to really a series of new
surveys particularly imaging surveys of large swaths of the night sky and one of
the great interests for myself and many others was in finding new events essentially new explosions or a new
variable erupt of stars that hadn't been known about before and doing that as quickly as possible now the traditional
way in which that was done and in some cases is still done today is that as you
acquire more data you linearly hire more grad students that scales with the total
number of the images that you're getting and you need to sift through and as I
was becoming involved in some of those projects we'll end up realizing that as time went on that really wouldn't scale
anymore and we we needed to find effectively a replacement for domain
experts who otherwise would have been looking at an ax pining on data using
alternate techniques and about 79 years ago I stumbled upon machine learning as
a real interesting potential Avenue and at the time machine learning really
hadn't been applied to anything in astronomy 'men the variable sky sort of
in a time domain context there had been a number of studies in using machine learning to do special types of
inference on the static sky and understanding you know sort of
demographics of stars and galaxies and their distribution in space so we really
felt like there wasn't a lot of precedent in us applying some of the
capabilities to this data but we wound up realizing it was sort of an imperative and one of the things that
people who know astronomers would probably say about them is they tend to like to use tools that help them and
seek those tools out be those new types of detectors so seats see CDs for
instance we're something that astronomers adopted almost as soon as they were invented and
obviously statistical techniques and computational techniques astronomers are willing to try things out to solve to
solve their problem that the classic example I go back to is Galileo who said you know hey there's this new thing
that's been invented to look at the horizon for ships coming towards us what
if I just took it and pointed it to the Stars what could I do with that and so our use of the telescope was essentially
a co-option of the use of a technology that had been built for other purposes and so that kind of precedes a pace
throughout the history of astronomy and so the idea of bringing a essentially a fairly new technique into the fold is
not at all unusual do you remember how you stumbled across machine learning yeah so part of it was just asking the
Discovering Machine Learning
question if I've got a if I've got a bunch of data and I need to decide you
know is this part of the sky interesting or not as is this event new or not what what type of event could this be very
quickly you wind up realizing this is a classification problem of some sort and
you know talking to people at UC Berkeley and the stats Department as I was starting to introduce some of these
interesting challenges became very clear that machine learning and particular
particularly supervised learning would be a you know a fertile ground for us to
start exploring but one of the challenges that I saw is that even though we're in a very rich and fertile
environment in at UC Berkeley and there's a lot of crosstalk between departments and individuals within
departments it was very hard to get even the kind of the language on both sides
up and running where both sides understood the methodological folks who deeply understood what machine learning
was and what it could be used for and then people like myself from the physical side of even learning to ask
the right questions so thankfully wound up getting a group from the stats
department and and those folks from computer science together with me and my my postdocs and
we were able to get a proposal together the National Science foundation-funded that allowed us to basically start
building out new ways of doing inference on on astronomy data that turned out to
be a very fruitful place for us for me in particular to learn about the landscape of what other techniques were
out there that we hadn't been taught in school mm-hmm can you tell us a little bit about from that initial discovery
Research Arc
what the research arc looked like what were some of the first things you started exploring and how that evolved over time yeah so I really just started
looking at you know toy amounts of data that we already had in the can and we
could start applying these different techniques to and looking for tools that would be useful for us and really the
best thing out there the time that we started with something called Weka which was a and still is a collection of
machine learning algorithms that one can apply in a sort of GUI graphical way all
kind of written in Java and really that was our original playground and
benchmark and used that as a launching off point to start understanding what are these different modeling techniques
that are being exposed here what is a support vector machine what does it mean when people say random forests and use
that as a way to sort of educate myself and those in our group I mean started seeing some interesting results right we
started seeing accuracies that were better than what you could get from random and then as we poked farther and farther wound up seeing how far can we
take these algorithms how well does one of them work relative to the others to
get the kinds of answers that we want how do we build in a loss function which turns out to be very important to get
good answers because in the case of what we did when we're discovering something
in the sky it's not easy to articulate that loss function and and and by that I
mean what is the cost of missing an interesting place in the sky it means
that you don't get to do new science versus what's the cost of saying everything in the sky
is interesting which means you burn all of your follow-up resources and starting
then to think about context-aware classification now just not in the in
the context of really just resources but now time constraints making making an
inference about something that could be of interest may be more important than
waiting to get another couple of data points and saying something with even more confidence so understanding how to
calibrate confidences and probabilities doing this in the presence of sort of
missing data and irregularly sampled data and time all of these also started
wind up showing to us that there were parts of the machine learning sphere in
the at least in the academic world that were not often exposed to the kinds of
data that we were exposed to and so noisy data for instance now whenever
when they talk about the iris dataset they don't say you know the petal of this is red plus or minus purple so even
just having uncertainties in your features let alone your labels became an interesting challenge and we wanted
realizing perhaps there were some new techniques that we needed to start innovating on to even do the kinds of
inference we wanted to do with our data so you mentioned the go loss function
Loss Function
and needing to wrap your arms around what that means can you succinctly
describe how you grappled that what did you end up how did you approach it and what did you end up coming up with for
the types of data that you were looking at one of the things we wind up realizing is that one person's loss
function is not the same as another person's loss function and so to get traction on on your answers one needs to
at least be clear about what it is that you're optimizing for and at least give
people the ability to imbue their own loss functions if for instance you're producing a catalog of different types
of variable stars on the sky we have a specific notion of what it means to get
something wrong about saying very minority class versus a majority class and I wouldn't say that we solved
that problem by any stretch but at least we were trying to be clear about what our assumptions were of the loss
function and articulate you know what it is that we're optimizing for you know
when people are doing AI or machine learning in a production environment there is always going to be an
optimization of some sort and the typical one people will go to without
knowing exactly what the kind of business value is or scientific value is
of the answer is you go for some notion of an accuracy and then when you get a level deeper and that you say well what
I really want to do is I want to minimize false positives at a false negative rate of 0.1 and then that is an
implicit statement of what your of what your loss function is and you hope that
by defining it that way and by optimizing on it that way that you're
actually getting very close to an optimization of what is you know the result of what you're what you're
emitting out of your out of your modeling mm-hmm and so you're you're
Other Fields
primarily looking at image oriented data over time are there other fields where you've seen
them adopt the same types of approaches to what you were working with well one
of the nice things is you can work at the sensor level data which is effectively photoelectrons in a CCD and
counting those up as a function of position and X Y and then trying to map that back in the sky so that's what you
might call noisy image sensor data and we worked at that level but then we also worked at a metadata level which was now
let's use traditional astronomy techniques to extract the brightness of a star as a function of time and so we
got ourselves out of the out of the image plane and into the time domain and
then we were basically working with effectively tabular data and again you
know there are lots of different models and feature engineering approaches that one can take to all of that I wouldn't
say that there was a common thread in our work other this across a bunch of these different sort of sub questions
other than say that over time we wind up realizing that there were only really a
couple of different machine learning models that did as well or better than
everything else and so even though for instance support vector machines are very popular because they have some
great sort of theoretical provable properties they tend to be kind of unwieldy and for dealing with the kinds
of data we were working with which is heterogeneous noisy dirty sparse missing
and multi class where you needed to also get probabilities out that you could
then calibrate models like support vector machines really fall short for
practical purposes and so we wound up recognizing in our group and I think that was validated in a conference that
we ran at at UC Berkeley on essentially streaming inference with machine learning it was a sort of week-long
conference that involved folks from Netflix folks from Google and then
domain experts you know everything from biomechanical to to physics a number of
people would stand up give their talk and say yen we wound up realizing that decision for us pretty much always won
now this was in 2012 before the resurgence of deep learning I bet if we ran this conference again half of the
talks would be about how that's a better algorithm as it were but it was pretty
eye-opening and it was one of the things that we took to heart as we wound up starting the company is a recognition
that to exceed to produce value sort of
very generally the algorithm itself is not necessarily the key in some sense
the way I view this now is that algorithms and their accuracy that they can produce and their ability to to
optimize them around a loss function is really only table stakes for the utility
of these in in a real and real environment and so yes you need to use a model that's very very accurate and
potentially can be retrained and get slightly better but as most data scientists or most people at work
machine learning workflows we'll say almost all that work is in feature engineering and if you're a deep
learning person you'll say almost all that work is in figuring out what the shape of the network should be in order
rating over that all right on that note before we jump into what you're doing at
Results from Astronomy
the company today what were some of the results you saw out of your research on the Astronomy side so we we looked at a
couple of different realms one was looking at large catalogs of variable
stars and coming up with probabilistic classifications of what type of variable
stars they were what was the physics that that drove them and we did that in a bootstrap way starting with
effectively a few hundred known classes and few hundred or a few thousand known
labels and then extrapolated that to tens of thousands hundreds of thousands of variable stars and produced
probabilistic catalogs one of the things I became adamant about as we were doing
that was producing a catalogue where you say hey this object in the sky is of
this type with this probability is effectively useless unless it's then
used for some new kind of science and one of the things that I became I won't
say frustrated with but I noticed often is that people started using not just in astronomy but in many other fields
machine learning as an end to itself saying I'm going to apply machine learning to this data and I'm going to
get a result until that result itself is novel or until that becomes a stepping
stone to another result which becomes novel you know it was it's sort of an
empty exercise and so what we wound up saying is what can we do with with this probabilistic catalog that couldn't have
been done with any other means and so one of the things we did is we looked for very strange types of stars that had
certain properties and then followed those up with big telescopes and actually wrote science papers with with
those so we use that as a launching off plane in a real time environment where we actually were looking at images as
they were streaming off of telescopes in Southern California off of pal
Mountain every 60 seconds or so it would basically get transferred up to Lawrence
Berkeley National Lab and we'd apply our machine learning to that to find new interesting objects in the sky and then
populate databases of you know for tonight here are the interesting objects and then also add another machine
learning code which would go into those databases and periodically make statements about what types of objects
those window might be what what they could be and we went up having I think
of order a hundred maybe two hundred papers that came out of that refereed papers which again the machine learning
part of that was really the stepping stone to discovery the other parts of the machine learning were the stepping
stones to initial inference and obviously in the end you needed people to to actually write the paper but don't
try it out to the grad students what's that it all goes back it all goes back to grad students exactly but I really
thought about kind of removing people from the real-time inference loop and getting as far up the inference stack as
we could we even got to the point where we were finding interesting objects in the sky without any humans in the loop
identifying that not only is it a new object but it's something we probably should be following up and we were
issuing alerts to robotic telescopes to go follow those up so by the time people woke up in the morning we not only had
the discovery we not only had the initial inference we then also had real
follow-up scientific follow-up one of the I think great achievements of the
work that we and others did and one of our collaborations was to build this
production system that you know had real consumers on the other side of that and when it was broken or was wrong or
didn't take feedback properly you know we'd get nasty emails from our collaborators and say your thing didn't
work for an hour you kind of screwed my science while I was at the telescope so having having an end user really keeps
you heavily focused on making sure the things that it needs to do does it right
and robustly but because we were discovering things even faster than a whole
I mean grad students wouldn't been able to pour over all of these images we're able to find for instance the nearest
type 1a supernova that had been found in 25 years and get a whole bunch of people
looking at that part of the sky and taking lots of data that led to a bunch of papers in nature and science Wow not
because that object wouldn't have been found by even amateurs because it got so bright you could have seen it with
binoculars eventually but because the interesting science happened hours after
the event blew up right the event happened and so it wind up also driving
home for me the need for not only something that's working and is robust etc but where it's able to make
statements quickly and do it in a way that's reliable interesting interesting
so I'm sure that that has led you to some interesting perspectives on the
AI and Jobs
relationship between this technology and in society and jobs and stuff like that
I I'm hearing parallels to you know a lot of people kind of projecting that as
AI is deployed shifting shifts in the job market will take place that put a
lot of people out of work but I'm also hearing in your example kind of the
counter-argument you often hear that really what it does is and empowers people and allows people to do different
things that aren't it don't necessarily want to go deep into the society stuff at this point but yeah it's it's it's a
AI and Customer Support
certainly a valid concern what we do in our company at waz IO
is help customer support agents become more efficient at their work by
suggesting answers of how they can respond to an incoming inquiry by automatically triaging incoming increase
in or tickets emails etc that is getting them to the right person or the right
group who's going to be the best at answering that question and then in some cases we will automatically respond to
incoming increase so when you write into e-commerce site and say my package didn't arrive there's a
growing chance that us or somebody else may be answering what looks like a bespoke question of yours with it looks
like a bespoke answer but in the end it's just a templatized response that
where we ourselves are using for us to be able to do that obviously we can talk
more deeply about how that works from an AI perspective we have to get very confident in what our answers are but
what does this mean on one side to your question about labor displacement
companies don't need to hire as many support agents so where would they have gone the other side of that is that the
agents that they do have become better and more tuned at working in some of the
harder parts of what their own products are about and where their customer complaints are about in a way they
wouldn't have been able to because they would have been distracted by the mundane so if you say how do I reset my
password and there's a person or sets of people to have to look at that email and decide how to respond that's time that
those people are not spending on really complex problems where empathy is required as well so we think of it as
our product and and what we do as a way of freeing people to work on the things
that people are uniquely suited at that machine's really aren't going to be that good until somebody solves you know the
Turing test chatbots are not going to be able to understand people in the subtle
ways that they need to but we can take a lot of easy stuff off the table we have and so there was certainly a concern as
we were starting to roll this out that this was we were part of this labor displacement movement but we heard time
and time again from our customers that their support agents became more and
more happy the more involved we were there was an entire team in in Asia who
had been tasked with basically reading an incoming inquiry or a ticket and then deciding who else should be reading this
to the problem and because our our triage capability came into play they
effectively deprecated a 20-person team one of our clients off of triage because
we're effectively automatically triaging now and we were worried what's gonna happen to these people they had they
have families to feed and we got a whole bunch of really great quotes from them saying because they had been reassigned
to actually work these support tickets rather than push them along they were much more happy in their job
that's fantastic so we jump right into what you're doing it was that oh that I
From astrophysics to software company
oh but the the transition is a fascinating one as well how do you get
from astrophysics to a software company doing CRM stuff and I know there was an
intermediate step there as well yeah so so going back to the original part of
the conversation we had recognized in the team that that I'd build in the
people I'd work with that a we had some great sort of technical orthogonality
some were good software engineering somewhat good at ml some UI etc and be
that what we had learned to do of recognizing the importance of putting AI
into production and having real end-users give real feedback in potentially a real time loop was
something we at the time didn't see anyone else doing we knew obviously that the googles and
the LinkedIn's and the Netflix is the world had this kind of baked into their overall data flow but we certainly
weren't seeing companies helping other companies do it and one of my now
co-founders had more or less while he was between jobs figured out how to make
one of the algorithms that we liked a lot for our scale very very well at least on a single machine in a
multi-core environment and so we realized that we might have some interesting firepower and given that
there seemed at the time to be so much emphasis on massive scale machine
learning it was it was certainly a pre spark era but was very much in the new pay day it looked
like most of the interesting ml that was starting to come out and some of the other companies that were coming out we're really focused around helping the
you know I won't say exits gala but certainly petascale level Google scale amount of data companies bring machine
learning into their into their workflows so we thought about sort of skating to a place you know using the analogy that's
heavily overused to the part of the where the puck was going to be which was helping smaller companies and midsize
scale companies bring machine learning into production environments and that
was the impetus for starting the company what the domain was going to be we
didn't know who the customer was going to be and who the buyer is going to be we didn't know we were I'd say
blissfully ignorant about all the business challenges that we would wind up encountering over the next couple of
years in bringing this to market and when we emerged out of our first
accelerator I gave a talk in that our demo day was the alchemists accelerator
where I said we're going to be github for data scientists and produce some
interesting you eyes of interactions to help data scientists like ourselves more
easily build models that they could then push into a production environment we wound up seeing over that over the next
couple of months the challenges of selling products like this into data science teams first data science teams
were few and far between and the ones that existed were either too sophisticated to believe they could
build it all themselves or not sophisticated enough to get a large enough budget to pay for the things that
we wanted to provide them from a tooling perspective all the while we were
building out our underlying platform to be able to do exactly that to be able to
build templated machine learning models against certain types of data for
certain types of use cases and then even though you built it at a laptop level push it into the cloud and have you know
in in Amazon or other compute frameworks the scalability to be able to serve
large numbers of customers around those same use cases we're what's emerged for us is the
difference between a customer is not new code it's just a config file if they're using that same use case so all of that
to say that we evolved you could call it a pivot if you'd like but I think of it as a series of of pivots to a place
where we wind up seeing in customer support a lot of data a lot of manual
work and some really nice CRM systems
with open api's and a fairly thick schema so the sales forces and Zendesk
and service now's of the world really are the data Lake and the transactional
layer for doing customer support and related activities and we thought if we
could build now an intelligent system on top of that and do all the things that I mentioned empowering these agents to
become more efficient of their job and make the whole support desk more efficient at serving customers we would
solve a bunch of being pain points and that as we wound up going into the market and started leading with products
that could be more or less installed by a non-technical user and could be used by a non-technical user wind up getting
a lot of feedback that indeed we were solving some pain points there's obviously the efficiency question of
needing less headcount but there's also some really interesting customers of ours who were growing very quickly and
one of them said to us that if the CEO had given him an infinite budget he wouldn't be able to hire good customer
support agents quick enough so helping them capture all the internal knowledge was something that it turns out machine
learning actually does quite well at what were some of the biggest challenges
Biggest challenges
in going from product direction focused on generalized tools and platforms to
one focused on a very specific application area I mean interestingly it
was all the things that we hadn't thought about which was product management and how do you get structured
feedback from customers what what does it mean to build an MVP roll that out
iterate on it etc a lot of kind of lis startup 101 stuff was something that we
hadn't really been thinking of when we started the company and certainly didn't have a you know frankly a lot of
expertise in and and then as we started scaling it was a recognition that there
are large parts of a machine learning pipeline that don't naturally scale so
figuring out ways to containerize the parts that need special attention from
you know Phe levels and data science and abstract that away from other parts of
our engineering group that don't need to know about what's happening deeply but need to be able to ask predictions of
some other part of the stack restfully in a services oriented way we just wind
up realizing that what worked for us from a scaling perspective compute
scaling perspective also wound up being what we needed to do from organizational and HR perspective when we hire
front-end engineers and and middleware engineers who are great at writing
scripts against data bases and managing Redis cues etc those folks don't need to
know about machine learning they need to know that there is a contract between their part of the stack and and
somewhere deeper in the stack that if I ask you for a prediction for this client for this model I'm expecting to get it
back in this format on this timescale and if I don't then our contracts broken
but likewise I'm going to hand to that deeper part of the stack that's going to be providing those predictions you know
effectively some data and JSON or otherwise that will have a fixed schema so that the group that built that
machine learning pipeline knows that you know this column is going to be of type date/time this column is going to be you
know an int and it's going to join using these four indices on some other data so
once we wind up realizing that we could lock down the schema for a given use
case it meant that we could write data science pipelines against data we hadn't
seen before well you need to see it once to make sure it's all working and make
Crosse validates in an offline sense and has the kinds of accuracy properties that you want but then it means that we
could basically start spinning up new customers where they get the sort of base template that operates on their
data and when we need to make changes those can happen really from a more or
less technical person than you know somebody with a PhD in statistics so
there were a bunch of challenges around that and as we kind of started solving those it just sort of fell out that our
stack really mimics what our organization looks like okay okay can
Customer data
you talk a little bit about the the data that you typically see in a customer environment I'm you know imagining just
loads of trouble tickets but I'm but I imagine as well that there's ancillary data supporting data as well and you
mentioned that there's lots of it can you talk about the size you typically see those kinds of things yeah so our
typical our typical customer is doing of order five to twenty thousand tickets a
month okay and we need to be working with companies that are that are sort of
achieving that level of volume a because the price points are reasonably high and
so it's typically the companies that have those large volumes that are willing to we're going to pay for what we do and and be because the machine
learning models are built specifically for and on their their data and we don't
use a common model for instance across our customers so we need a lot of training data for a given customer
now this isn't again this is not petascale amounts of data we're talking sort of tens to hundreds of gigabytes
you know at the per month level per customer the data is indeed a lot of
human a human interaction from emails web forums even chat and there's also a
lot of metadata so what is the value of this of this customer what products are
they using how often have they been emailing so there's a time series component to this as well and you know
we've had to build these pipelines that are generic enough that we can then apply them to other use cases but specific enough
that they give you no good enough accuracies that wind up rivaling what you know what humans can do and so
oftentimes our our goal is to get to naught we don't call it accuracy we call it matching capability because
oftentimes when humans labeling something and saying it belongs to this bucket or this person should answer it
or it should have been answered with this template they oftentimes can be wrong so we want to get our we've all
had that experience we want to get ourselves to that kind of level of quality let's say so it's mostly from an
from a feature ization perspective we're doing natural lots of natural language processing getting it to the point of
you know sort of rectangular eyes data where each row is a different instance
in each column is a set of features and then we have a bunch of labels of what
the answers are so we're working almost entirely in a supervised sense where we know from past data to particularly
close tickets what the actual right answer is quote unquote we've got a couple of unsupervised
models that we also wind up running where we wind up discovering for instance that there is a grouping and
semantic space of outgoing tickets that is how agents are responding that don't look like templates that are sanctioned
by the by the company which means that they're coming up with their own templated responses and potentially even
sharing those with other agents so we have a dashboard for instance we show our customers than one of those running
the support desk of potentially new templates that they can use obviously if
there's a new issue for instance with a product then agents who are on the ground have to figure out a way to
answer it and if it's a recurring problem within a couple of emails that went up essentially having the right
answer that they've already pre formulated so that's an unsupervised problem and do you see in that last
Future of generative AI
example a future place for generative types of AI approaches or is that more
do you think of when you hear that is that like the technology you know looking for chasing the problem kind of
thing yeah it's a good question we've shied away from the generative component and
in fact we make that a big part of our sales pitch of to say you are a
potential client really know the voice that you want to speak in and speak with
your customers and it's who is it for us to come in and say we're gonna Auto
generate at the character level you know CN NS or something a bespoke answer the
way that you know Google inbox does well if it gives you you know five five different words sure all sounds good
I'll see you then or how about Friday those are fine but because we're we're
really focused on not just getting results into the hands of agents where
they can actually see in a UI sense within the dashboards they normally see what our predictions are I'm consuming
in a way that they like to we also want to take a lot of these tickets off the table in an automatic sense the only way
our customers get comfortable with that is if we're basically showing to them in an offline way here's our accuracies for
these types of templates so we're gonna send every now and then somebody says I'm very unhappy with what you've done
we're gonna send thanks for your feedback when it should have gone a different path but we're only going to
do that you know one percent of the time at this at this level of false false positive and once we can do that then
our customers essentially can turn on a specific macro for us to auto respond with the idea that we're going to auto
respond without any agents in the loop to something like that to potentially irate customer is is pretty challenging
so we don't certainly we'll fill it out but we certainly don't think about ourselves as producing generative
answers in a bespoke way we're just more or less turning all of our problems into
multi-class classification problems of what of the hundreds or potentially thousands of canned responses is the
right one to answer with okay and just so I understand the comment that you made a second ago in terms of sending
Exploration vs Error
out a given response a small percentage of the times are you describing an error
type of situation are you describing a feature where your explore like an
exploration type of feature good good point so we try not to do we there's an explorer
exploit component to what we do in a multi-armed bandit sense that's
typically not you know exposed or a knob that's tunable by our customers so that
will happen and some of that will happen naturally in the case of auto response we hold back 10% of the ones that work
we know what the answer is or we believe we know with a certain threshold of confidence and then compare after the
fact whether an agent who wound up now having to see it because we didn't know how to respond gave the same response we
did so there's there's an exploration where the agents job is now to do the exploration implicitly know that the the
one I was pointing out is ones where we are essentially wrong yeah and that gets
back to the question of the loss function of what does it mean to be wrong right if if one of the can dress
Ponce's is I'm so sorry for your loss I will refund your entire vacation you
know in the amount of $10,000 the cost of being wrong of that is very very high
but if somebody is mad and says my vacation got ruined because of something you did I want my 10k back and we say
thanks for your feedback it's being wrong on that side is not nearly as bad
as being wrong on the other side of that and so we give and empower our customers to basically make the decision about you
know let's do the easy stuff where the cost of being wrong is not a big deal but because and that's for the automatic
response but for the recommended types of responses if our first canned responses here's your money back and an
agent looks at that and says no that's crazy the right answer is farther down the list they'll select that and that
becomes the feedback that our model you know our models wind up getting better as they wind up learning over time what
Challenges
are some of the most interesting challenges that you've run into in
putting together this kind of hybrid you know ml + human solution like in one of
the things that pops to my mind is you know just user experience user interface like are there challenges they
that are interesting or you know what surprised you the most in trying to feel
these types of solutions certainly because this we're getting in we're
getting into the space and the face of agents who do this all the time when we
first started releasing our products we didn't have a good training program for them and so when they would see even
though what we thought was an intuitive set of responses in the form of widgets that would show up on their on their
desktop you know they didn't know how to consume it and they didn't know how to use it as effectively as we thought they
should you know there's all the mundane stuff around you eyes like responsiveness and somebody's saying
well doesn't look like your products working because now there are no responses and we'd say well that's because you've already responded and
you're bringing up a new ticket you bring up an old ticket that already has a whole conversation and we're only
getting involved in at least for now in the first part of the conversation what's the first response you should do
okay so then we weren't showing the results and so how can we you know modify our widgets so that the agents
understand we're not showing it for a purpose it's not that our system is is broken and then realizing also that many
agents wanted parts of our UI that and
and UX were more generally that doesn't have any anything to do with ml so they
wanted keyboard shortcuts because we thought everyone would just click on stuff but high velocity support desks
want to just use the keyboard so having to build that end for a set of customers
because the essentially is like the keyboard had some disease or the mouse had a disease on it they didn't want to
touch it getting feedback from the UI itself back
into our system making sure that we're getting the right metrics back making
sure that the KPIs that we're measuring or that were aligned with the KPIs that our customers wanted I think one of the
hardest things for us and it frankly continues to be a challenge is really
just thinking about how fault-tolerant ml needs to happen and again Google going back to
in box for those of you that have used it it makes a couple of suggestions
about how you could respond to an email if you don't want to use those you don't use it so I would call that a great
fault tolerant all right ml experience and the same thing in this you know spam
filter within your within your mail system it'll say we think this is spam if it's not move it over and then later
on we'll figure out how not to call these things spam anymore that are like that that sort of fault tolerance where
you're also getting feedback either implicitly or explicitly it's just something we've had to build up over
time but I think more broadly that that kind of approach needs to be built into
any AI system in a production environment unless the AI outputs that you're building are going to be consumed
entirely by machines you need to have some level of understanding of who it is
it's going to be consuming it what are their concerns and how can they give you feedback so that your model is one of
getting better over time can you talk a little bit about the algorithms that
Algorithms Pipelines
you're employing and the toolchain the pipelines what does all that look like
yeah so we we stay out of what we call the our algorithms arms race internally
a we're not really selling the the platform to other data scientist so it gives us the freedom to focus on parts
of the the pipeline that we find most important all of our algorithmics are
learning parts and then prediction parts are built in C++ and then surface back
out into Python which is where the data science team winds up working we have
our own notion of what a pipeline needs to be and the data science team works
entirely within that the confines of what that pipeline ought to be which is some sort of pre filtering so for
instance if a ticket is from a voicemail don't predict on it or don't use it for
a build so you get rid of those that have this in this column this value then
there's the data transformation parts of that in the joining multiple across multiple data sets if
that's needed and then the feature ization which will often use open source
tools for that in the Python ecosystem pandas is a we use very regularly and
then once we wind up realizing that we've created a bottleneck which typically will happen not so much in time but in RAM usage
will wind up rewriting other people's algorithms or code so that we create you
know a RAM efficient pipeline and then once the features a ssin happens basically the learning winds up
happening in the c++ layer and we've built a whole bunch of hyper parameter optimizations and feature selection
capabilities and then post process capabilities to get calibrated
probabilities out of a multi-class problem so we have a bunch of pieces
that we've been building up that are not in the open world and it's something that we've decided not to open source
for now that allow us to work efficiently so we think of as you know
high velocity data science and building out a template for the first time but then because the models have to rebuild
every single day for every single customer on you know cloud infrastructure which is not super cheap
we needed to make the cost of doing that as small as possible and what we wind up
realizing is that open source tools you know that that many people use like the
psychic learns and and the tories slash datos of the world and even spark and
now i'm we're vastly more costly to run even if you could do it in the same
amount of time which we think we're much much faster than most of those tools because of the RAM requirements needed
on multiple machines or even a large single machine in Amazon the costs of
building a model just was X percent higher and X being you know in the thousands so having a ram efficient
speed efficient and obviously again getting back to the original conversation about table six highly
accurate said algorithms which produce the kinds of answers we want that we could then get
into and modify if we needed to was kind of where we went up settling is where we
needed to spend our kind of our de / engineering time now one of the areas
Data Science vs Production
that many of the machine learning platform companies are focused on is
trying to close this gap between data science and production yep and in
essence eliminate the hey I've got this model that kind of works throw it over the wall to developers and have it
implemented and it sounds like you guys have maybe embraced that and you're
using that as a way to build out the models in C++ presumably for a
performance are there ways that you've then compensated for that in terms of
automation tooling or do you just accept that or you know even you know we just
have you know the best people on both sides of that fence that can deal with the you know the existence of the gap
like how do you maintain a level of efficiency and innovation in terms of
the development pipeline and not the machine learning pipeline so that it all works for you
yeah so there's definitely this separation of concerns which again is
both an organizational one and then is also a computational one to the level
where we think we often talk about what we call the organizational API of who
within this stack is the customer of who and so for instance the people who are
the sort of core ml and algorithms folks in the company you're working in C++ and
surfacing the great results back in into a Python layer their customer is the
data science team the customer of the data science team is the the people
working on our architecture who have to make maintain you know this scalable
robust infrastructure and you know their customers are the people working in the
middleware and their customers are the ones who are the in the UI and so each of them have a set of
contracts of what it is that each part of that stack is looking for and and how
in fact they're supposed to engage with each other and that's become very very helpful for us because you know what you
find is that when you put somebody in a box they figure out a way to innovate very highly within that box so if there
is a very strong contract of what data is expected to come in and what data is expected to come out and everything in
between there is really up to you to decide how to do this well and efficiently that's where our for
instance our data science team and implementation team will wind up working and building at a new template they can work at their laptop or glorified laptop
level on a toy dataset get some confidence that the pipeline is working and offline accuracies look good and the
whole thing is going to work and they once they're comfortable with that they literally are just pushing a new version
of a docker image into our registry which then farther upstream from
something they ever have to think about from a production sense once a new-build winds up getting kicked off for that
customer for that type of template then you the new image will just get pulled and it will just get built with the
config file for that customer and so the data science team can wind up working
within their confines and of course we have a whole testing suite to make sure that if they build something new they're
not going to break something downstream from them they gain confidence in that and then they're literally just pushing
the results of what they're doing on a semi weekly basis into the docker registry that becomes the latest
template for let's say triage and then all the customers in production are automatically migrated to that so having
the data science team be able to pushed up into production without having to be on the ops side of things nor have to
think about the architecture has really freed us up in great ways I think to
innovate and likewise when they need a new Bell or and or whistle from the core
algorithm folks because they say this part of our entire build chain is really
inefficient they can ask of the people working on that to improve it and they go through their own
testing suite and I think read 300,000 regression tests in our core ml we're
also testing against every open source algorithm on customer data to make sure
that we're staying as efficient or more on all these different axes before we
cut a release then the data science team can just pull essentially python egg from our registry and and use that in
their system so having those separations has been great obviously if you're
abstracting everybody from what the ends use cases are there can be a huge danger
but it's the job of people like myself to make sure that everyone is focused
and innovating towards the right set of goals oh great great I'm glad that docker came up you guys
published you published and maintained a set of docker images for data science
tools I've come across that my impression is that in general docker
adoption within the data science machine learning community is not particularly
high is that yours as well I certainly haven't heard of many other companies
Containerization
using it in the ways that we are but it seems like such a natural way to
literally containerize and abstract the work of one part of an organization from
the other so long as they you know that container will respond with a slash build predict endpoint you know feedback
endpoint etc in the way that everyone expects it to I think that's a I think
that's a wonderful way to do abstraction so and then it obviously also helps you
wind up achieving scale because for us scale is not you know can we serve you
know a billion of our customers with the same app it's instead well we've got a new customer that we just spin up more
containers to do the builds and the predicts for that customer and we need more compute capability that's elastically scaling for us for free on
top of Amazon so I think of as very natural way to separate concerns you
know from a stack perspective and an also very natural way to do what is for a company that's serving lots and lots
of customers a very embarrassing li parallel type of of compute interesting
I got into a conversation on Twitter or reddit or someplace where someone was kind of griping about just the
dependency hell with Python and pandas and trying to come to terms with
managing different versions of you know different tooling versions and things like that and I suggested I might have
even pointed to your docker repository and the response was now I want to make
this simpler not more complex and obviously you find it to be simpler can you give folks that aren't familiar with
docker in containers like your 30 second you know docker for data science pitch
and where they can learn more about it yeah so docker is a way of basically
Docker
explicitly specifying what not only your let's say Python requirements are which
you can do with a simple file but also what the entire OS shall be for running
whatever scripts you're gonna you're gonna need and once you build that and
you you've you're you gain confidence that that image is doing what it what it
ought to you can essentially very rapidly turn a container on that is the
almost instant instantiation of that entire OS plus that script and all of
the dependencies built inside of that and you can hand somebody a link to the
docker hub registry or if you maintain your own private registry explicit URI
to that explicit version of that explicit image and more or less
guarantee that when they run that with whatever data is contained inside of
that or whatever will be pulled over so long as it's the same you'll get the same answer out so I tend to think from
a data science workflow and then getting back to you know just doing science more
generally docker is a very nice framework for reproducibility and so the
idea that now I'm I'm not I don't have to share a machine with you or an Amazon machine image or with you I'm just
handing you effectively a docker file and says if you run this you're gonna wind up getting the same answer that I
got but again because I don't think doing the types of work that we do and
wise and in some cases what we do on the science side of things as the you know
the the final result is not what comes out of the docker image or container
it's not okay here's a report of what my ROC curves going to be my false positive
versus false negative curve and then let me write a paper about that it needs to be for us at least in a production
environment just now I've produced a prediction that now needs to get consumed by something that's farther
downstream so docker is quite nice in that sense as well because you can also
now connect docker containers explicitly using something like a docker compose
there's many other tools out there as well so that containers talk to other containers and you allow each container
to have again its own separated concern from the other ones but still pull the results and push results to the other
the other ones around in addition some containers can just contain data and you
can build databases around that data so now it allows you to build up a very
lightweight version of what might be your entire stack and do this in a way
that's programmable so we've found that to be incredibly useful for testing purposes so your github the place that
someone can go to learn more about what you're doing there yeah so we've got a public docker registry that you can go
to the docker registry and search for wise i/o or you can go to github slash wise i/o and see our other public
projects that we've pushed out so there's one around docker and data science
which in that case because we're not releasing any of our internal tools we've built we're basically building up
a container with open-source tools that we find are really useful for doing lots of different types of of data science on
the other major project which we have up there in github that's open is something
we call paratext which started as just sort of a weekend
hack from one of our engineers Damian EADS who wanted to see what it would be
like to read data from disk in parallel mm-hm just to see what kind of speed ups
you could wind up getting and it turns out pretty much every open-source tool out there doesn't read in parallel and
the ones that do are explicitly parallel lies like over multiple machines but if
you just made well mult use of the multi-core environment how well would you get and we went up getting 100,000 X
speed ups over some of the other tools that are out there and importantly also use vastly less memory that parrot X is
not yet in our production environment but we thought it would be a good example of kind of showing off the
philosophies that we try to adhere to within the company of creating
efficiencies that isn't just the one thing like around accuracy but you know around how fast can you read data in how
big is your model on disk all these other aspects of what it means to do machine learning that it has nothing to
do with the algorithm once you're happy and you've reached some level of plateau with the algorithm accuracy all that
you're left to do is optimize all these other pieces of that pipeline and so a
lot of our engineering over the last year in particular has moved away from just optimizing accuracy to you know
things like creating interpretability around the models that we we build making the model smaller on disk making
the other parts of the features Asian pipeline be more RAM efficient and once
you start sort of playing whack-a-mole with let's just say RAM usage you wind
up finding really interesting parts of your entire pipeline that very few people went to
talking about just you know again reading data which should be the easiest part of your entire tool chain is vastly
inefficient and you know whacking that mole turns out you save a whole bunch of
Amazon costs because now you need a smaller Ram machine that's great that's great
you mentioned interpretability have you spent a lot of time working on that and
what were the drivers for that we have spent a lot of time on that you know it's sort of one of what we think of as
our trade secret one of our trade secrets around getting back to the
question of UI and UX for end users we we were asked often at least in the
early days well why are you getting the answer that you're getting and you can't say well you know it's a thousand
dimensional feature space and there's covariance between all of these and you know the model importance is over the
entire thing you know says that this is the most important feature I don't know why we said for this one what the answer
is but right that answer is what what's called in in the financial services world reason
codes turns out to be really important so some some places it's actually regulatory ly required that you tell
somebody why you got the answer that you got even if it's a machine learning black box and so some of our early R&D
effort was around how to make at the instance level so an individual prediction level how do we make these
models interpreter by saying these are the important features and these are what's driving this specific prediction
so as an example if you're working on a customer churn and you want to predict
somebody going to churn in 90 days from now it's a use case that we've we've also used on our platform but not
something we go to market with necessarily to customers can have an
identical probability of churning but one of them may be churning because they haven't really used your product and
they haven't done the training videos and the other one may be churning because there's a high they're gonna go bankrupt in the first
case that's something you can do something about and the second case you know you're kind of Sol and so even
though they're identical and what their predictions are and they're and the probabilities of those predictions
coming to pass one is actionable and one isn't and so it's not just people
gaining kind of a warm fuzzy about why did you get these predictions and does it jive with my you know feeling about
why that that could be okay which is critically important it also then starts tying into next best action and because
I think again a important part of machine learning and production is to
drive value if the value isn't the prediction in and of itself then the
prediction and of itself is really just there to drive the next thing that happens and so next best action is
heavily coupled with you know the the importance is around which features are
driving the the prediction okay you mentioned value and that's a great
transition the one of the things that I really wanted to dig in to with you and that is the this blog post that you
wrote about cost optimized AI that I've incidentally mentioned on the podcast a
couple of times do you have time to go into that of course so I guess the first
quote come up several times in our conversation already this notion of cost
and value but was there a specific thing that prompted you to uh I really got to
write this down now what drove that so that was a bit of an intellectual journey I I was wondering to be really
frank why the hell are all these companies building these neuromorphic chips and all these specialized hardware
to do deep learning where we're you know because I think much of the world's data
and much of the world's value in data is tied up and I'll use the word or quote
unquote small data or medium data non massive scale Google scale data Facebook scale data
I was wondering why all these people are starting to build these very specialized pieces of a hardware when you know deep
learning I think magnanimously one could say or charitably is incredibly good at
a large number of of inference problems but not very good at a large probably
even larger space of inference problems that may be changing over time as people start applying it to these new realms
but the place where deep learning winds up shining is in really large amounts of
data right because effectively what you're doing is turning millions or even billions of knobs to optimize a model
and to do that credibly without overfitting one needs lots and lots of data so so I wound up asking this
question of myself why are people doing this and why isn't what we already have
out there and even just the GPU land good enough and if you look at the a
plot which I have in my blog post of the efficiency sort of gigaflops per watt
right which is something of if I put this amount of energy in which has this amount of cost
how many computations can I get out that efficiency has been growing over time
but it's nowhere near what some of these other chips are the specialized pieces of hardware can do for these specific
types of calculations and those themselves are nowhere near what the human brain can do right which is of
order if I remember right about ten to the five gigaflops per watt so your your brain is a you know 30 watt
supercomputer unrivaled at least for now by anything else it's out there and anything that else is out there is
likely going to take megawatts or hundreds of megawatts to get anywhere close the the computational capability
incidentally I don't know if you've come across it but there's a parallel to using DNA for storage and the storage
density per per unit energy is incredible in DNA yeah something like
you know the the drop of a you know in a teaspoon or something it
can take all the world's data as it's in it's incredible so so getting back to
this you know that's an obvious that's an obvious one and I started thinking about it when alphago had its big set of
results the national or international championships and you wind up looking at
the computational capability that it took to win those those competitions is
just huge thousands and thousands of computers thousands and thousands of GPUs the amount of power required there was
several orders of magnitude larger than what was going on in you know the the
the champions head that they was playing against so I was thinking about that sort of vast gulf and wound up realizing
that the companies that are pushing towards these specialized pieces of hardware is because they realized that
for a given amount of time and a given amount of data because these algorithms
are all basically saturating on your perfect answers the only thing left to
do is to get more energy efficient machine learning for building and that
the step after energy efficiency when it comes down to it is really cost efficiency and and so my my takeaway on
on that part was that people are building these chips because that's sort
of the last frontier of squeaking out and eking out the last amount of dollars
coming out of the system for the number of dollars going into the system and
then it's taking a step back from that it wound up realizing that or at least
realizing for myself it's probably obvious to most out there that because machine learning is optimization that
you're a good optimizer will find the optimal answer by definition that if
you're not writing down your your function that you're trying to optimize to get a minimum of our maximum of in
terms that actually matter then you're creating by definition a sub-sub optimal
answer or system and now that system doesn't just involve you know as my
algorithm more optimal at getting an accuracy better than yours but now translating the accuracy into well let's
go back to our loss function what's the cost of being wrong you know and saying this thing is a and this thing is B
translating that to a business term is something that's critical and almost everybody knows that that's that's
important but then you wind up realizing well if I'm going to build a model what
if it takes me 12 days to build one of these models to get an accuracy which is only epsilon better than one that takes
me to 10 seconds and what if you know I can build a model that may take 12 days
and the accuracy is much higher than one took me less time but the labor costs
are very different so I had to spend more data science time building one versus the other and what about the opportunity costs of
those data scientists not working on another problem in your business that may be more important and when you wind
up couching the problem that way you get out of just again focused on accuracy
and the algorithm - what is my cost of doing the entire pipeline and now the
entire pipeline isn't just running a machine learning model in production for
the specific use case but how does that couple to all the other things you're doing in your business are you hiring a
data science team to do this and then paying pensions or are you gonna do a third party to do this and just write a
check one time and then you know what are the societal benefits of all this and you know becomes unwieldy at some
point if you're actually being very honest about what's the cost of doing this but at least if people I just
wanted people start thinking about as we started thinking about within our company that accuracy is the table
stakes and let's assume that you all have your good algorithm that's going to do well is it going to have strong
scaling properties so that if you needed to get the model built you know an X amount of time that you could just have
n number of machine that get you X / n amount of time on the clock because maybe you need that model
bill very quickly very often right um and then you know questions around the pipeline and and RAM usage and AWS costs
in the end as a small start-up when you
start getting down to the brass tacks of what's our revenue and what's our cost
of goods sold with our cogs the cogs component is really what does it cost to
build a model and predict and until we were able to boil down the fact that the
cost per prediction for one of our customers is X and we're going to be making x times some number you know
everything else is sort of moot if you're losing you know every time you make a prediction effectively
hand-over-fist then you've got something wrong that's unsustainable so I started
thinking about it as we were going through the exercise of what's our cost of doing business and the cost of doing
business is running an AI system in a cloud with real customers and the labor
part we can get but all the other pieces there in the end there's an Amazon bill and because we put it all inside of
Amazon you know and we know how much money we're taking in we can see how those two things relate to each other
mm-hmm so you started with the question and and I've ran through the thought
exercise what's next there whether it's you or someone else that does it Dean do you see this evolving or you know Co
evolving with someone else thinking about you know analytical frameworks for thinking about this or you know tools
you know whether that's a spreadsheet or you know it almost lends itself to
machine learning algorithm yeah and to figure out how to deploy resources to you know do the machine learning yeah
it's a great question and you know what a nice things about blog post is you and made it out to the world and you hope somebody writes you know if somebody
runs with it it's been helpful and focusing for me and my own thoughts and as we drive
those sorts of efficiencies in in our company and then again more broadly you
know in doing science doing astrophysics choosing the right tools I'm choosing
the right skill sets and people choosing the right problems to work on or not work on those those are very obvious
sort of outcomes from me having thought about and framed it that way one of the
challenges is and I think people may wind up being able to pick pieces up of
this and work with it is coming up with articulations of essentially what is the
conversion term between that item in the
in the entire optimization equation and dollars so one I'll throw out there that
I don't know the answer to is what's the dollar value of interpretability and
once somebody starts getting some handle on that then optimization takes it's
wonderful you know toll or approach or at least shell lead to a great outcome
which is you know once you once you can really put a dollar cost to all these different pieces then I think you can do
a real honest-to-goodness optimization so I know what the dollar cost for instance of needing a ram machine of
this size versus that size on Amazon okay great but what's the real dollar cost of and
can I know what how much time it's going to take for a data science team to build up this template from scratch and then
push that into production and how many people do I really need on that is it good to have one data scientist or
multiple ones right and so all those things I think wind up becoming really
interesting over time once people want to potentially even wind up agreeing upon what this equation what's in band
for this equation what's not obviously out of scope is you know what's the
probability that you know my machine learning algorithm is going to start World War 3 probably not worth talking
about right right something smaller than that smaller and scope at the company level
is probably worth starting to get some clear understanding around so now we've
maybe come back full circle to graduate students a lot of interesting research project
questions in here for a PhD student or something yeah I think in the you know for for those in computer science
thinking about systems optimization who are also interested in machine learning this is hopefully some fertile ground to
start to start thinking the other statement which hopefully is clear from
what we've been talking about is that doing machine learning for machine learning sake really doesn't make sense
it's it's probably the last thing you want to do if somebody hands you data
Madhu it because you have to do it it's painful and to run it in a production environment given all the crazy Bugaboos
that many many people have talked about there's a great paper from the folks at Google by Dee Scully is the first author
called machine learning is the high interest of a credit card of technical
debt yes I'm not surprised it's an important paper it's got I think no
equations in it but it's a whole bunch of important lessons about how machine
learning systems tend to be very different than typical engineering systems so so so there's a lot in there
to get right a lot of Bugaboos there that people who haven't done this before tend to get wrong but when you wind up
realizing is that once you realize machine learning or more broadly AI is the right set of tools to apply to the
problem that you have what you'll often wind up finding I think is at the
graduate student level in terms of graduate student projects they could be working on is that it's still very much
early days for that for the types of algorithms pipelines etc in dealing with
real world data I've often said to my my colleagues on campus that real data is
not doing sentiment analysis on Twitter right and yet many many many papers
saying my scaling algorithms better than your scaling algorithm will wind up using that as a toy dataset the real
world is not tweet yes we need to have benchmark data to
have a lingua franca of who's doing better and these different axes but when you wind up getting exposed to real
questions you wind up realizing that all the stuff that people know out there in the academic world that people write
about and do Cagle blog posts about are not what you really need if you're being
truly honest about what needs to get optimized mmm that's great so how does one manage being CTO for you know the
high-growth startup and you know being a astrophysics professor it's becoming
increasingly common to see folks particularly in the machine learning community have professorial posts and do
academic or do you know work in these research labs and things like that but
yeah so I I been on a what's called an industry leave for a number of years and
so it's allowed me to have also that separation of concern so so not getting
not getting paid by the University and not having healthcare has made it easier for me to spent all my time as need be
on the on the company while still maintaining the kinds of links that I
think are important as I you know start thinking about coming back into the university setting obviously a number of
things I've picked up and management they you know ideas and and capabilities and then also thinking about how to
evaluate new technologies when is it appropriate to bring this into your toolkit or when is it appropriate to
wait those become really practical uses that you know I can take I can take with
me but then also again recognizing that as I was saying before there's a whole
interesting set of problems out there that are not being addressed by pure
academic Rd research means that I can also start you know looking for those
white spaces to actually do some pure academic research and around those I'm particularly interested in questions
around interpretability and how you put metrics on interpretability and that's something that I think I benefit from
having come from you know felt the pain of custom is asking about that right that you know
at least have a fresh lens on that mm-hm doesn't mean I'll solve any of those problems but at least I'll have a
direction of potential interest so it's a it's certainly a challenge but I think
despite the challenges the the benefits to both myself the company and the
university and my students at the university are far outweigh all the gray
hairs that I wound up and getting I'm teaching a data science class
essentially a Python ecosystem data science class right now it's aimed at
graduate students and the things that I've seen in the in the business world
have really helped me hone that that class and I'm very directly giving back
to the students from that from those learnings and is that a MOOC or is that available only to it is not a mooc other
incarnations of that class that i've done in the past are probably online somewhere in the itunes here or
elsewhere that can also be found on github all the material and then we'll
hopefully post some of the of the lectures online as well okay right so if
folks want to learn more about the the company or get in touch with you what are the best ways for them to find you
guys easiest is drop me an email and you can find that by googling around so I'll
add that as a little bit of a bar that if you really fun if I maybe you have a little bit of work you can tweet at me
so that's prof jsb as my twitter handle and and we can do a direct message maybe
that's probably the best way to get at me right great well I really appreciate you taking the
time it's great to finally meet you in person and I really enjoyed the conversation I think folks though will enjoy it as well
and get a lot out of it great well thanks so much thanks for your interest
[Music] alright everyone that's it for today's interview thanks so much for listening I
haven't asked you all to do this in a while but if you enjoyed this episode of the show please please please do these
two things first share it with your friends on Twitter Facebook guild email
or however you share cool things with your friends second reach out let me
know how you like the show who you'd like to hear on it and how I can make it better for you you can reach me on
twitter at at twitter i and at sam Charrington and you can email me
directly from the contact page on the Twilio comm site thank you so much for
your support and catch you next time

----------

-----
--04--

-----
Date: 2017.03.01
Link: [# Interactive AI, Plus Improving ML Education with Charles Isbell - #4](https://www.youtube.com/watch?v=CyiLyMdplP8)
Transcription:

My guest this time is Charles Isbell, Jr., Professor and Senior Associate Dean in the College of Computing at Georgia Institute of Technology. Charles and I go back a bit… in fact he’s the first AI researcher I ever met. His research focus is what he calls “interactive artificial intelligence,” a discipline of AI focused specifically on the interactions between AIs and humans. We explore what this means and some of the interesting research results in this field. One part of this discussion I found particularly interesting was the intersection between his AI research and marketing and behavioral economics. Beyond his research, Charles is well known in the ML and AI worlds for his popular Machine Learning course sequence on Udacity, which he teaches with Brown University professor Michael Littman, and for the Online Master’s of Science in Computer Science program that he helped launch at Georgia Tech. We also spend quite a bit of time talking about what’s really missing in machine learning education and how to make it more accessible.


[Applause] hello everyone and welcome to twill talk
the podcast where I interview interesting people doing interesting things in machine learning and
artificial intelligence I'm your host Sam Charrington we've got another great interview for you this
time around but first a quick update on the drawing we've been running in conjunction with O'Reilly Media as you
know if you've listened previously O'Reilly media is holding their first ever AI conference on Monday and Tuesday
September 26th and 27th in New York City the conference will span both low-level
talks on implementing AI and high-level talks on the impact of AI in society and
I'm personally looking forward to speeches by AI luminaries such as Google's Peter Norvig Facebook's Yann
laocoön and Intel slash Nirvana's Naveen rau and we're giving away a ticket to
one lucky winner here today in addition right after the AI
conference on Wednesday and Thursday the 28th and 29th is the O'Reilly strata
plus Ahdoot world big data conference which is one that I've been attending for years now you may have heard me
mention this one before strata is a much bigger event and while it's not strictly focused on AI there are tons of really
interesting AI machine learning talks at strata as well along with talks focusing on what I consider to be the core topics
of the of that event data infrastructure and data engineering and O'Reilly has
been kind enough to offer us a ticket to strata as well which will be giving away today so about that giveaway if you went
ahead and entered into the contest via either Twitter or the Twilio comm website before the cutoff date your name
or Twitter ID went into a spreadsheet and you actually had a pretty good chance of winning as far as giveaways go
I chose winners using a random number generator to pick four numbers in the
range of my spreadsheet rows the first winner who was lucky number 17 is Lance
Poole who entered via Twitter Lance gets to choose Eve conference ticket for his prize the
second prize winner is Jenka also from Twitter and he gets the ticket remaining
after Lance's choice I've also chosen to runner ups who may be called upon to
fulfill the duties of one of our winners if either of them can attend the event our first runner up is Samuel W and our
second runner up is Dennis a who both happen to have entered via the twilly I comm site if you hear this any of you
please reach out to me to claim your prize now if you didn't win it's not too
late to save 20% on your registration for either conference you can do that by using the registration code PCT WI ml
when registering and I'll include a link to the registration page in the show notes on behalf of the podcast and our
partner O'Reilly thank you to everyone who entered and now onto the show
alright folks I am super excited to bring you this interview my guest this time is Charles Isbell jr. professor and
senior associate dean in the College of Computing at Georgia Institute of Technology Charles and I'd go back a bit
and in fact he's the first AI researcher I ever met his research focus is what he
calls interactive artificial intelligence a discipline of AI specifically focused on the interactions
between AIS and humans Charles and I spent a good chunk of time in our
interview exploring what this means and some of the interesting research results in this field one part of the discussion
I found particularly interesting was the intersection between his AI research and the related fields of marketing and
behavioral economics beyond his research Charles is well known in the ML and AI
worlds for his popular machine learning course on Udacity which he teaches with Brown University professor Michael
Lippmann in addition Charles helped launch the online masters of computer
science program at Georgia Tech we spend quite a bit of time talking about what's really missing in machine learning
education how to make it more accessible of course I'll be linking to Charles and the
resources we mentioned in the show notes which you'll be able to find at - male Icom slash talk slash four and now on to
the interview alright everyone so I'm here with Charles Isbell Charles's
senior associate dean and professor at Georgia Tech and actually Charles and I
go way back so this has been a weird conversation because we're already like 20 minutes in and just getting started
with the interview Charles say what's up to everyone and we'll get started what's
up everyone how are you doing I'm happy to be here I'm happy to be having this conversation awesome well thank you so
much for joining us for this interview now I think we figured out that that
it's been like 20-something years since we met and that was pretty interesting
in that we were roommates during a I guess summer internships at Bell Labs
and that was when you were at MIT and studying AI tell us a little about your
experience at MIT and and what you said you're in the famous AI lab there right
I was although the AI lab no longer exists it merged with the laboratory for computer science and is now known as
csail so I loved my time at MIT and I loved my time at Bell Labs and
eventually you know 18t labs sort of my journey through through AI is a I don't know it's it's a bit of a wandering one
so here I'll just give you my entire history up to now in like 15 seconds and we'll see how that goes
so as you can tell by my accent I was born in Chattanooga Tennessee but my earliest memory is arriving in a moving
truck at the age of three and a half in Atlanta so I think of myself as being from Atlanta but very very early on I
cared a lot about computers and computer science and I knew when I was eight years old that I was gonna do computer
science although I didn't know what it was I knew I was gonna be a professor although I didn't know what it was and I knew I was gonna do AI even though I had
no idea what that was something about building robots yeah at eight years old you know it took me a very long time to
realize that not everybody thought they knew what they wanted to do when they were eight years old I think I was probably a senior in
college before I realized this but I had always sort of wanted to to build intelligent things although I couldn't
have articulated it that way when I was eight years old but I always wanted to build smart things I always thought I thought that computers were great at
least what I thought computers were and I basically just wanted to build you know an intelligent friend that's
basically what I was into at the time and so everything I kind of did from at that point on was about that my actual
first encounter with Bell Labs long before we met I was the summer before
ninth grade so I was 13 years old or so and I built a computer at Bell Labs as a
part of this summer science program what I say I built a computer I mean there was a kid and another engineer did all
of the work while I stood there and watched it yeah it felt like it was a Timex Sinclair t1000 and I remember yeah
it was a little Chiclets thing and it didn't have an on/off switch so when you turned it off you had to unplug it great
and the first program I ever wrote was a piece of code that would fill up the
screen with inverse spaces and it ran out of memory before it could finish doing it and that was my introduction to
real computer so um you know that that's what I figured I needed to fix that and so that whole summer we spent well the
two or so weeks that I was there for that program I spent a lot of time trying to figure out how how to make computer smart and how to make them do
what you wanted to do and it just verified for me that that's what I wanted to do for all of my life so I
kind of dove in from there and I kept getting bigger and better computers and convincing my mom that you know an Apple
2 GS was the right thing and it was the best thing she could do for my education she kind of nodded politely eventually
gave me the things that I wanted and I sort of moved through and one of the advantages of knowing what you want to
do with your life is that you sort of move towards it there's some disadvantages we can talk about those
but really admit that you know I knew I wanted to go to Georgia Tech because I wanted to stay in Atlanta and I thought
that it was the best place for me to be so I went to Georgia Tech as an undergrad I completely dove into AI
didn't do a lot of research at the time because that you know in the 1980s it was a little there
in as many places where you could do the kind of research that you can do now as an undergrad but no matter
sort of what you're into and then decided well there was basically one place you're gonna go to grad school and
I applied to MIT and I went to MIT and I wrote this long essay about building
robots and and trying to make them smart and and and trying to make certain that they wouldn't run out of memory and it
was a it was a lot of fun so I ended up going to MIT immediately started diving
into machine learning which at the time was sort of new for me I knew about AI and I knew I wanted to build robots but
it didn't occur to me that you needed to do something separate to make machines learn and I decided almost immediately
once I was exposed to it that this was the central question you couldn't be smart unless you could learn right and
our machines were never going to be able to do the interesting things that I wanted them to do when I was 8 9 years
old unless they were smart enough to learn how to do them on their own and so I dove into that became a part of the AI
lab went through a couple of advisors I'm still good friends with with all of them and eventually ended up where I was
the side story where we met is I at the same time that I was going through grad
school I got to go to Bell Labs every summer so part of this this fellowship program you know all about this of
course and there I did a lot of really interesting things in AI that had
absolutely nothing to do with what I was doing in grad school but it was so
interesting what they were doing they're trying to build these knowledge representations and kind of really understand how it is you could think and
you could represent thought that I just you know at the time it felt okay that I was at making progress in grad school
because I was still getting to do these cool things and so by the time we met I was doing six months out of the year at
Bell Labs and six months out of the year at MIT more or less oh wow I don't think
I realized that at the time yeah because I take four and a half months over the summer I'd start before everyone else
and I would end after everyone else and I would go back during the winter breaks okay okay so that I think the time that
you kind of came up in AI was during the quote-unquote AI winter is
that right more or less yeah sort of at the tail end of the ad winner nobody told me that I didn't figure that out
until much later so how is that impacted your and your contemporaries perspective
on AI and and the work you've done and how do you like what do you think about
the current popularity of AI and where it's all going
so I think basically what it's mainly done is it the people who are about my
age and a little bit older who lived through the AI winter I think basically spend a lot of their time wondering when
the next AI winter is going to come so a lot of us are very very sort of naturally and reflexively worried that
we're overhyping what's going on right it was it wasn't it was difficult to get funding it wasn't it it was it was
difficult to do work it wasn't that there were weren't people interested in the problem that we were interested in it's that any minute now the federal
government would take away all of the funding and we would you know we would go from having 10 graduate students to having two graduate students and I kind
of think that little fear is always there in the in the back of our heads and we find ourselves thinking please
stop overhyping deep neural networks or you know getting people convinced that we're gonna we're gonna build the next
data or that you know the next Android and self-driving cars and that any minute it can all kind of go wrong so I
think it's probably made us somewhat more cautious at least it's made me somewhat more cautious and trying to think a little bit of about the hype
that sort where it's kind of driven me but you know the other advantage of
being a part of your part is sort of AI when it was during the winter is that you knew that you and the people you
were talking to her in it because we were truly passionate and motivated about solving the problem as opposed to starting a company that would make you
really rich or you know this is the hot thing you were doing it because you you actually cared about it and and I think
that you know that's important right certainly when you're when you're doing research you have to be passionate about the the things that you're doing and
really believe that somehow it's gonna get you someplace interesting and so do you think fear notwithstanding do you
feel like the is the industry structured in the same way such that the risk is
the same or is it different and in particular I'm thinking about is there you know that the funding
source is more distributed now is the level of industrial activity you know more greater now or is it all you know
from a research perspective all still fundamentally the government funding everything and you know when they decide
to change their when the winds change there it all collapses well I think
structurally two things have happened one is computers computing and that sort
of way of of crunching things and data are now ubiquitous they're everywhere so industry is deeply into this it's not
going away Google exists right and everything is driven by data and it turns out that the
parts of AI the parts of computer vision the all the sort of pieces of building intelligent things they're driven by
data now and since we everyone has access to data and everyone has access to computing everyone has access to
really fast machines I'm not worried about sort of it structurally going away in fact the problem is sort of the
opposite it's that everyone has a piece of it now it's driven as much by commercial interest as it is by sort of
pure research and so really the difficulty in some ways is that there's so many opportunities to do what I would
have thought of is AI what we would talk about is machine learning and those kinds of related things that it's easy for things to become diffuse in a way
that wasn't true 25 years ago I don't think this is a bad thing I mean the the
fact that Facebook exists the fact that Google exists the fact that everything is about you're about data and about you
know sort of modeling what people are doing and what things are happening is definitely a good thing and it does mean that there's always gonna be funding for
some piece of it even if it's not being called AI there's not being called machine learning the kind of ideas
metastasized so I'm not really worried about it going away the only thing that worries me is that people are concerned
that bad things will happen because of what we're doing and for good reasons right they're concerned about their
privacy we now have all these abilities its ability to track everything that you do I guarantee you Google is well aware
that you and I are having this conversation right now they probably know what we're gonna say before we say it you know they've got more data on us
than you can possibly a man truthfully we I'm not entirely sure that we mind facelet knows everything about
us there companies out there neither was ever heard of who know kind of everything about it so people worried
about privacy they're also worried about cars running off the road and killing other people they're worried about
robots you know rising up and Terminator style killing us all so they're the kind
of fears is the hype has actually gotten to the point if not what you haven't given us what we promised it's that
you've given us more than what we asked for I think that's where the danger is coming from now but in terms of funding
in terms of people being interested in these problems know that that's driving everything even things you don't think of as being a are being machinery mm-hmm
it's interesting that in in some ways it's in some ways the the industry is
given more in some ways like we're still waiting like you know if you if you serve a sci-fi and you know even the
Jetsons you know we're we're sci-fi thought we would be in you know 2016 and
a lot of ways we're we're not there yet right like a lot of movies would have had the self-driving cars all over the
street but some of this stuff it takes longer it takes longer to develop then
you think and some of the stuff is happening quicker than you think I think well some of the things are
happening that nobody ever thought about I mean you go back and you start thinking about sci-fi it wasn't self-driving cars or self-driving
jetpacks right I still haven't gotten a ship I'm still waiting for that and it's true we we haven't gotten the flying
machines we haven't gotten the the really smart Butler's that are that are taking is everywhere although they and
we've gotten a lot of other things right we've got access to information that we've never had access to before we can
ask questions and we'll get the answers back we can look up anything we want to we can teach ourselves we've gotten a
lot more of things we never thought about than we thought we would and we've gotten less of the kind of obvious
things that that I think people sort of hoped that we would one day get so you know it's a mix I I'm okay with that I
mean I people ask me all the time you know when are the computers gonna achieve sentience and and take over the
world I think the answer is probably never or at least probably not for a very very long time not in the way that people
think about it but we're gonna have very smart machines and we already do doing a whole lot of things for us that we never
sort of expected them to do and the interesting thing is we won't even notice and it won't seem like that big
of a deal I mean for example with the Tesla autonomous cars uber and all the
things that they're doing that's amazing have you ever been in one of these cars if you ever did do this that's amazing
that you can sit in that car and it can drive you through traffic on a highway at 65 miles an hour that's it's amazing
i if you had asked me how you would do something like that 25 years ago I don't I don't even I can barely figure out how
human beings do it and in fact being on the road it's pretty clear to me lots of human beings don't do a very good job of
it but that's a more that's a miracle and we barely notice right every time you get an airplane right the pilots not
flying the airplanes flying itself right and we just take this every day miracle is just the another little thing in fact
you know one of the big complaints if you're in the a I write is that you never actually get credit for the cool
things that you do right ai is kind of the the science and the engineering
making computers act the way they do in the movies right but one of the things
that sort of tied into that is if it's got to be intelligent then it's got to be like humans and if it's got to be
like humans it has to be mysterious and something we can't understand so the problem is every time we do something
even if it's amazing once we know how to do it and we understand it well that can't be real intelligence and so we
don't give the credit to AI so AI sort of has this problem where you you can't
ever win because anything interesting you do well we understand that and that's not real intelligence so it's no longer AI that's just learning or just
it's just computed right it's never this thing right where you succeeded it's just oh that's not the real part the
real intelligent part is this thing and then when you can suddenly do beat you know people at jeopardy well that's
that's not really intelligence the real intelligence is other things so you you basically just keep you know innovating
your way out of out of business and so AI get sort of smaller and and smaller and what it's allowed to call itself because the mystery gets
smaller and smaller is it smaller smaller or further further well it's always sort of infinitely far away all
right it's it's it's something that we can always look for but we can never quite get to sort of Zeno's paradox of
AI but there's not like a you know there's not some finite set of things
that we need to do to figure out AI and we're chipping away at it and it's getting smaller and smaller it's like
the goalpost is moving yeah well so I think both are both oh I think I think
both of those things are true I think there are a finite number of things we need to do we're definitely chipping away at it and so the stuff we need to
do sort of get smaller and smaller though it's still really big but the goal posts keep moving right we've got
cars that can drive themselves more or less and now that's no longer remains it so it's got to be something else but
that was that's amazing and by the way it's not just amazing it has an amazing impact on the world have you seen this I
know you're on Facebook you remember this map that was going around for a while that showed the the most common jobs in every state
you remember this hold about a year ago yeah and do you remember what the most common job is in almost every state in
the u.s. truck driver right yeah truck driver a delivery person taxi driver right that's something like 42 or 44
this diagram the right number but it's over 40 for some reason in other states it's Elementary School teacher I don't
know why but but mostly it's truck driver well you know we're five years away from all the cars delivering
driving themselves right right uber is it's not going to have people involved anymore
my old advisor one of my PhD advisors you know is heading the work at prime
air right so things are gonna be delivered to us by drones and people aren't going to be involved anymore well
that's the most common job in the country and it's going away mm-hmm
so the the goalposts are moving the the things we have to do or getting smaller
or not and people have this sort of feeling what AI is but whether or not you want to call it a air or not it's gonna have a massive impact on our
day-to-day lives it's gonna have a massive impact on the economy it's gonna have a massive impact on sort of how see ourselves and how we interact with
one another and whether you decide that it's a Arnaud or it's intelligent or not whether you move the goalposts or not
it's changing everything around us in deep and profound ways yeah absolutely absolutely so I want to talk about a
couple of really specific things with you and we'll take these in in turn the
first is in the realm of education and the second is in the realm of your
research focus area and reinforcement learning but let's start with the first
of those we got we got through year grad school experience at MIT then you went back to Georgia Tech and most recently
you've been doing a lot of work in online education and around machine
learning maybe walk us through what you're doing there and in particular I'm
curious and maybe as a bit of background here I I didn't go through your entire
course but I took a look at the course that you did with Michael Lippmann and it was I really enjoyed the the
presentation having gone through a number of ML MOOCs and it made me wonder
like what you know what unique views do you bring to you know teaching and/or
learning machine learning and AI that you surfaced in the course work as well
as you know which of the you know are there any views you have that you think kind of go against the grain of the way
people other people are approaching it yeah so so I'm glad you enjoyed the the
classes Michael and I had a ball just a total blast doing it and if you haven't you should watch the the Michael Jackson
parody video we did about machine learning you get to see getting to see Michael dressed up as Michael Jackson
and dancing which is well worth the price of admission which is freak so the
you talk about this kind of interaction we have one of the things that Michael and I tried to do is we decided that
we've been wanting to do things for long together for a long time but you know these are one part of the country I'm in another part of the country we wanted to
do this this machine learning mukhin and this gave us the opportunity to do it and the way we decided to come at it was it's much like
we're doing this now we said you know what education like this should be more
like a podcast you should have a conversation so every time we did one of these these lectures one of us would be
the professor who would try to present the material and the other person would try to be the student so the professor
would do all the preparation and and come up with a sort of lesson and get everything together and the student would do no preparation at all in the
coming cold so you know in that way it's just like regular school and we would
just talk and of course he's an expert I'm an expert and this is what we do all day so it's not like we didn't really
kind of understand what was going on but it turned out and I think this really does come out in the conversations that
we had that we actually have very different views of what's important right so Michael is much more of a
theoretician if you asked him what a I and machine learning is he might say something like computational statistics
I'm much more interested in thinking about it it's kind of practical applications and you know sort of what
you can do as a practitioner to to use these tools to make them work and get synthesis I want people to see that this
thing over here is just like that thing over there which is just like this thing over there and they're all tied together and I'm much less interested in proving
in the abstract what it is that that you actually can learn and what you actually can't look it's not that these things
aren't important I just you know I'm just less interested in them then then Michael is and so we would spend all of
these times kind of arguing some of them sometimes obviously sometimes not about what's going on and what I vote came out
of that you can tell me if you think it's true or not is that the student was drawn into this conversation and at
least got the feeling that not only were they learning some equation or getting ready for some test or doing some assignment but that there really is a
deep conversation going on about AI and machine learning and there's lots of different ways to think about it and and
really that kind of gets to my larger philosophy about the way education works and why I'm so excited about the online
education that that we've been able to do to me what's really missing in education is access right the ability
for people to really to participate in the in the Commons that is education
that is research that is learning and one thing that I think is important for people to understand is that when you
say access some people turn that into affordability you know is it cheap enough you know
tuitions too high you know and that is a part of access but access is actually very different access is just the
ability to be able to participate in the conversation and that if you're capable of getting through it being able to have
the real opportunity to get through it for debility is only a small part of that so one of the things that we've
been doing and and I'm actually quite proud of this over the the last three years is we decided that we wanted to
push on this idea of access and affordability and that online education in MOOCs were one way of doing it and
while we were working on this this machine learning class we wanted to make it a part of something bigger and so Georgia Tech when and when I was there
in my senior associate dean Rowe I guess I still am and am a professor role we wanted to build an entire degree a
graduate level degree that anyone who could get access to the internet and
then who had the time and had the desire would be able to get through an actual full fledged course a whole fledge and
not just a course a full fledge degree a real program and so we created this online MS program it's exactly the same
as our on-campus program same requirements same degree you get through this you get a you get a Master of
Science computer science from a top ten department and it's indistinguishable from the the one that you get on campus
and here's the thing that we did two things just sort of push on this notion of access one is we decided to make it
as inexpensive as possible so the entire the cost of the entire degree is something around sixty six
hundred dollars Wow depending upon how fast you you get through the program so somewhere between six thousand eight
thousand dollars sort of depending upon what you do you can get an entire degree that's pretty inexpensive if you came on
campus and you were out-of-state student it would cost you more like forty six thousand dollars so that was the first thing that we did make it affordable but
the other thing that we decided to do is we decided to admit every single student we believed who could succeed this is a
pretty big deal right if you if we think about our on-campus degree we accept about 10% of our applicants mm-hmm why
do we accept 10% of our applicants because it's all the space we have right I'd estimate somewhere between 60 and 70
percent of the students who apply to our graduate program are above bar but we only got room for 10% of them so only 10% of them get in
and by the way it's it's it's basically a lottery I mean you know when when
you've got your place like Stanford and you're accepting 4 or 5% of the people coming at your undergraduate program
there's no way that that 4 or 5% really better than the next 4 or 5% the 4 or 5% after that you're almost closing your
eyes and just picking people and this is about what we were doing the graduate level we don't like that for our online
degree which again is the same degree as our on-campus degree at this point we're accepting about 60% of applicants ok we
have gone from zero students 3 years ago to 4,000 students this term 4,000
currently enrolled students or is that accumulative 4,000 currently enrolled students okay wow that's pretty good
many on campus uh about two or three hundred okay in fact I by the way it's
not just that we've got 4,000 students they're performing as well as the on-campus student mmm oh by the way it's
not just that we have 4,000 students who are behaving who are performing as well as the on-campus students they look very
different so if you look at our on-campus degree about eighty-five percent of the applicants or our
Nationals vast majority of whom were from India following behind China so about 15 percent of US citizens for our
online degree it's the compliment about 85 80 to 85 percent of the applicants
are US citizens or permanent residents okay right they're in their early 30s early mid-30s not in the earlier mid-20s
most of them are working full-time they've got you know jobs mostly NIT
though not all of them they've got mortgages they've got kids and they're trying to sort of get
through their day but they can't take the time to get further education or to
do that thing they want to do because again they've got mortgages and kids I've got responsibilities right so what's interesting is we've done studies
of this we partnered with Harvard and looked at it we think that of the people who are coming through our program almost none of them would have pursued
an advanced degree otherwise they weren't because they just simply didn't have the option they couldn't take two
years off from their lives to go and pursue a degree because they had too many responsibilities and things that they
had to do but this gives them the option of doing that and so in fact the overlap between them and the people who normally
would get education is almost zero current estimate is that we'll add between eight and ten percent every year
to the number of graduate IT workers in the United States then we otherwise
would have seen and have you looked at what what they're doing afterwards how long is the program been in place and
how long have you been tracking that and to what degree so it's been about three years in fact I think we're beginning
our will be we're just ending our third year now we'll be starting our fourth year so people have just begun to
graduate we had twenty people graduate two terms ago and this semester we're
expecting to see closer to about 250 and we're expecting to see a steady state of closer to a thousand people graduating a
year most of them already had jobs so you know usually the way you measure
success you say okay do people get jobs when they graduate well most of these people already had jobs so they didn't
lose their jobs I guess it's a good thing but it's hard it's hard to know what that it what that impact is because
the usual measures don't really make sense but they're all they all seem to
be happy 97% of them said that they would you know recommend this to two other people many of them do get jobs
while they're in the middle of the program a lot of them get promotions and they move through so you'll have to ask me in five years with what the real
impact is but right now it appears that people are happy they're getting a lot out of it some of them are able to
change careers get promotions and to do things they wouldn't otherwise be able to do because they just couldn't take
the time off to do it so I'm very happy with that and happy with the sort of impact it appears to be having on
students let me ask you this a lot of people who listen to the podcast or you know somewhere along the progression of
learning and and entering machine learning as a field as a profession and
I'm wondering what what do you think the right set of set of educational tools
that take advantage of right MOOCs are a piece of that but there's obviously other pieces that go into making a full
kind of a well-rounded student of machine learning and AI how
do you recommend that students approach that or do you have a philosophy around that well so I sort of do and I do think
it comes out of my in my class if you actually take the class as opposed to watch the lectures you get my
assignments and I'll just describe my first assignment to because I think it actually captures a lot of at least what I believe matters in becoming a either a
machine learning researcher or a machine learning practitioner or even AI or more broadly speaking so here's my first
assignment the first assignment is go find two data sets I don't care what they are so long as they're interesting
they have to be interesting by themselves and they have to be interesting together and you have to convince me that they're interesting
then I want you to implement these five or six algorithms and when I say implement I mean steal the code I don't
really care you'll get any credit whatsoever for implementing and running the code you'd still libraries you know
go get your your favorite implementation of KN or boosting from somewhere else I don't really care and I want you to run
all of those algorithms on those two datasets and I want you to do analysis and explain to me why you got the behavior that you did
why did some of those algorithms we should all work why did some of them behave better on some date on one of the
data sets than the other what sort of things did you learn by applying those algorithms and doing the data analysis
convince me that you've thought about it convince me of what experiments you would need to run in order to really get
the answers to the questions and then run those experiments do all of that and then write it up in 12 pages not 13
pages about 14 pages 12 pages why do I have an assignment like that I have an
assignment like that because I think much about machine learning much about the field that we're in is really about
the practice of doing it you know theoretically all of these algorithms is
particularly in supervised learning they're all very similar they all can learn the same kinds of things you know but there's no free lunch right so there
has to be built into what you're doing deep assumptions about your data what is you're trying to accomplish and you have
to be able to surface those things so if somebody want asked me if I wanted to really do machine learning what do I
need to learn I give them two answers one you need to learn the foundations and the fundamentals yes you need to know the math you understand information
theory you need to understand you know what linear algebra is you need to not flinch or somebody mentions an eigenvector and eigenproblem to you you
need to get the math you and you need to get the computing because it's a fundamentally computing pace discipline and computing is not
math computing is not engineering computing is not science you need to internalize the computing part of
machine learning but just as important and in many ways more important I believe is you have to really dive
deeply into the empirical side of it you have to get dirty with data you have to understand what the difficulties are in
answering the questions you want to answer and you have to really realize that the questions you're asking aren't necessarily the right ones
most of what traps us in machine learning and in lots of other things we do are the unspoken assumptions you have
to surface what those things are and I think that the best way of doing that is by getting your hands and your feet
dirty so my classes are designed to do that to force you to get into a messy
ill-defined situation and to work your way out of it so if you want to do data analysis if you want to do machine
learning that's great it's wonderful I can think of nothing more interesting to do but you have to get out of the
textbooks you have to play through the data and understand why it works the way that it does why the algorithms have the
effect that they do why you can learn some things you can't seem to learn another thing and that I think is actually really missing I think people
either dive down the empirical side and just try to get stuff working but with no understanding of the fundamentals
doesn't even know how to ask the questions or they get so caught up in the fundamentals they don't worry about
whether it actually works in practice or how you would actually apply your ideas and you have to do both especially in a
field like machine learn
they use all the tools of social media tools that are out there to build community to talk to each other to talk
to the faculty to talk to advisors they really build an entire community around what they're doing and really the people
who are in that community do well and the people who are not a part of that community do poorly so one of the things
that's important about the trips that I've been taking in the traveling around the world I've been doing is making certain that we provide the tools so
that people can build local community that makes sense to them because that's how the learning happens
you guys might be single-handedly propping up Google+ well there's no lag
because no one else is using it so you got that nice nice nice so let's switch
gears a little bit and talk about your research your research is your research
focus as I understand it anyways primarily around reinforcement learning or maybe you tell me tell us what your
research focus is nowadays and how you think of that area yes so I I you know
like I said earlier I really been into AI and machine learning for a very long time and it took me a while to figure
out what it was about it that I really cared about and it was easier to see when I was reflecting back on it and
what it is that you know I found interesting what I didn't the kind of machine learning that I care about the
name that we we kind of given in the field is interactive machine learning and or interactive AI I I sometimes
referred to as interactive AI because I care about the AI problem as much as I do the the machine learning problem and what it really is is about what happens
when you instead of just saying oh look here's some data and I'm gonna look at that data and then I'm gonna build a
function and now I can do some prediction you know that you're gonna have a fundamentally incremental and interactive process so I want to model
human beings because I actually care about messy data and there's nothing messier than people so I want human
beings to be a part of the story of how I learned when I say that I think that people learn only through social
communities or they learn best through social communities I think that's actually true for our machines as well so that ends up looking a
like and I spend most of my time worrying about reinforcement learning so you're right about that and the reason I
care about reinforcement learning is that reinforcement learning is really I think trying to do something big and hairy which is actually model what it
means to be an autonomous agent so when people ask me for the one sentence description of what it is that I what it
is that I do I said I care about interactive machine learning I care about building intelligent agents that
have to interact with other intelligent agents perhaps hundreds of thousands of them at a time and some of those intelligent agents
might be human they don't all have to be human but some of them will be and since some of them are human you can't just go
around sending XML packets back and forth you have to actually engage in conversation you have to worry about the
fact that human beings change over time they're inconsistent their error prone they're highly non Markovian there's all
kinds of interesting things about people and you need to be partners with people and you need to be long-lived
in order for you to make progress in the area so that's what I really care about I care about building a system that
doesn't just predict whether you know a car's gonna run into the side of the
road or not but actually deals with the fact that there are several million other people on the road at the same time and you have to interact with those
other people and you have to learn by talking to them and interacting with it and so reinforcement learning is a
subset of that yes and that's right I spend a lot of my time worrying about
game theory I spend a lot of my time worrying about marketing believe it or
not about social behavior and how people tend to interact and and I work with one
another and how you can convince them to to work with you or how you can deal with them if they're trying to work
against you so it's the whole gamut of what it means to interact with other intelligent beings that have their own
set of goals and an interest that might not be the same as yours so something
you mentioned marketing tell me more about how that plays into your research or maybe even give us an example of some
of the research topics you've been looking into recently so I like the marketing question so so I spent a lot
of time with a friend of mine with one of my students is now a professor in North Carolina on something called drama
management so the short version of drama management is well you know you've played video games right uh yep and you know the
thing about video games is the interesting ones are ones where you're you know involved an entire world and an
entire story so what's actually going on is that you're the person building the system for you is trying to build a
story but most stories you just read in you're a passive participant of it and things like games you're actually an
active participant which means there's this trade-off between your sense of autonomy and agency on the one hand and me making certain that you have a good
experience or a good story so you can actually think of lots and lots of things like this you can think about conversations that you have in the
interviews and the podcast is like a story where you're negotiating back and forth and trying to trying to figure out
how to tell the story that you want to tell while still allowing people to say the things that they that they need to say or that they want to say you can
think about all kinds of examples like those can kind of go on for a while but the the thing that the thing there is
that it turns out that because your player or the person who's participating
in building this story with you has their own ideas they might take your ideas off track they might turn your
murder-mystery into a horror story they might turn your interview where you're supposed to be going back and forth and
having conversation into a series of you asked me a questions and I say yes or no and it's not much of an interview for
you right so you have to influence what the player is doing with the human participant is doing or otherwise you
don't end up with a good story that you want to have so there are two ways of doing that one and I think you know you
and most your listeners if you ever heard the expression a game that's on Rails sure so you know that's where well
I'm sorry I'm just not gonna let you go through this door because if you do it breaks the video game or breaks the story and so you're on Rails and the
thing about being on Rails is it takes you out of the story takes you out of the experience and that's what a lot of
people do and a lot of the drama management stuff is about that as well but there's another way of doing and in
fact the right way of doing it if you can make it work is you get the other person the person you're interacting
with you're trying to learn with the story you're trying to get to participate in the story to actually accept your goals as his or her own and
it turns out marketing is very good at this so we build this kind of system where you get people to do the things
that you want them to do by putting them in situations where it's just natural for them to do those things so rather than lock every
door except one - we're in a room so you go through it I make something happen maybe some noise or something
interesting that makes you want to go through that door right so the more like themes of behavioral economics and
incentives and things like that coming into play here right oh that's exactly right so in fact the example of this
that everyone's familiar with is one called scarcity so that's where it turns out that people if they believe that
something is going away they suddenly find it more valuable right right so anybody with kids knows certainly anyone
with kids ten years ago know that Disney has this habit of saying oh we're gonna release on DVD Beauty and the Beast and
then we're never going to release it again uh-huh and so everybody buys it right because it's about to go away or I
mean Black Friday's like this right you're gonna every year at the day after Thanksgiving you go to the store to buy
a bunch of stuff it doesn't make any sense whatsoever there are not any things you want to add but they're going away you're gonna get a price right now
it's on sale and so people react to that they can't help themselves it's a scarcity is just one of is one of
the particular that's very easy to understand there's tons of others of these or something called liking which is it turns out people will do things
for you if they if they like you people react to Authority actually my favorite
example is a something called consistency where if you can get someone to say something out loud that they
believe something they have an almost pathological need to be consistent with it over time so you know you have
anybody in your neighborhood won't mow their lawn here's the way you get them to mow the lawn you wait till it's winter right and so
all the grass is you know kind of dead and it's all the same height and you start up a conversation and you say man
you know it really looks great around here when it's like this you know everything's the same color everything's the same height and you get the person
to agree with that yeah it looks really nice of this like this the next summer they'll know the lawn because they
basically believe that's the way it's supposed to be and you know what's really nice about it it's not that you got them to mow the lawn it's that they
believe that they are in complete control of the idea that they're the one
who made the decision are in charge so that's a long story but the short we built systems like this that
basically convinced people to do the things that we wanted them to do we influenced them so I'm not using machine
learning just to predict your behavior I'm using machine learning to figure out how to intervene to get you to do something and what I really want to
happen is for you to believe it's your own idea so we built this little story just a quick example we built this little story where the whole goal was to
get you to buy a fish at a market not the most exciting story in the world and there are lots of ways we can influence
you to do this with scarcity and various other things and it and so we had people play this game and the people we tried
to influence were much more likely than the people we didn't in buying the fish
and doing the things that we tried to get done sure sure now that's interesting but what's more interesting
is that when you ask the people whether they felt manipulated or not the people
who are not men that manipulated were much more likely to say they felt they were being manipulated than the people who actually were manipulated that's
interesting why is that because the whole the whole way this works the whole way the kind of psychology works is you
feel as if you have agency that you're making the decision when something goes on sale and you decide you're gonna buy
it you don't feel that you've been tricked into buying it you made the decision to do it right and so what's
really interesting the data and doing machine learning it's actually understanding about human behavior its
understanding behavioral economics it's understanding the way marketing tricks work it's it's all about getting the
person to make the decision you know themselves that they want to do this
thing and then they have agency they have control and they're much more likely to see it through the fact that
you kind of tricked them into doing it is neither here nor there so a quick note for listeners anyone that's
interested in digging deeper into some of these ideas there's a great book called influence by Robert Cialdini that
is super accessible and as covers all the things that you you talked about
consistency and scarcity things like that but this brings up a question for
me and that is a lot of the a lot of the work we read about
reinforcement learning nowadays is your training these agents to navigate a
world right and the work you're describing is you've got this world that's essentially training the human to
navigate it and there's an interesting complementary nough stew it and I'm wondering if if that complementary
niché's Plourde at all like the things that i'm thinking around like adversarial networks like can you have
the ones rating the other thinking it's training the other does that make any sense is anything happening there oh yeah that's actually very common way of
doing it so the way so the thing that really got me into reinforcement learning when I was a young graduate
student a couple hundred years ago was actually playing games so there was this a guy named sorrow who had built
something called TD gammon which was a particular way of doing reinforcement learning to learn how to play backgammon
and the way it learned to play backgammon was through self play so it it played both sides of the game and it
learned by playing itself how to get better and this is a well when I think of pretty well understood so the
technique for learning right you it's difficult to it if it's too hard you can't learn if it's too easy you can't
learn you need to be just about a little beyond your current level of understanding and so yeah this kind of
thing happens all the time now it is true that a lot of people who worry about machine learning do not think about the kind of complementary nature
that rather than there being an agent that's training in an environment the environment could be in fact training
the agent and people don't always see that in fact my biggest complaint or complaints not the right word but my
biggest I don't know let's say complaint my biggest complain about the way machine learning is portrayed is that
it's portrayed as a supervised learning problem you know I'm gonna give you a bunch of input-output examples and
you're gonna learn the function that map's input/output and that's interesting but I think reinforcement
learning is more interesting because it's this bigger problem you don't have inputs and outputs all you've got is actions you can take and feedback you
get from the world and then from that you have to figure out how to behave that feels richer to me even though in
some sense they're equivalent the unsupervised learning is a very different way of thinking about the world even though again sort of
mathematically they're all kind of equivalent and that kind of Brett what machine learning and AI is is something that I don't think we spend
enough time really thinking about I think people tend to focus on the supervised learning part instead of the
reinforcement learning and the unsupervised lying probably stin the kind of popular press ok hmm so maybe
taking a step back how do you think about the current state of reinforcement
learning like can you characterize the the major research efforts or even is it possible to characterize the major
research efforts into a handful of directions and kind of who's doing what
so I think there's so the answer is no it's way too broad but there's a couple of things that I think are really
interesting one is all this work on deep networks and deep neural networks which you know is the the current thing that
everybody is really into and by the way it's really good work you know I know the people who've been pushing on that for years and years and years and and
they've really been able to to do a lot of interesting things they got the kind of fundamentals right with the math and
they're taking advantage of the fact that we have insane amounts of data so that we can actually sort of take
advantage of those algorithms to do cool things a lot of what's going on at least in my world that people are paying a lot of attention to is figuring out how to
use the stuff that we know from deep networks and even deep learning and applying it to reinforcement work okay
and and rather than doing the supervised learning take where you said well okay here's the state of the world tell me what to do you're actually treating it
the way you treat a reinforcement learning problem you're talking about building value functions over what the
states are in the world and what things are better and then using that to figure out how to make a decision and use what
you learn from making the decision to affect your view of what's valuable in the world and kind of having each one
feed into one another and so recognizing that there's at least two parts of that problem instead of one part of that
problem is a really big deal and being able to marry the kind of math that's come out of supervised learning has been
I think really important that I think has been really interesting as has pushed forward a lot of a lot of what
we've been able to learn in the last couple of years for sure the second thing which I think is interesting in
part because it's my own work and and place where I lived is very related to what we just got through talking about and it's this interactive machine
learning it turns out you know I mentioned earlier that there's no free lunch right so for those of you don't
know the no free lunch theorem basically just says that all algorithms are equally good and in fact not only are
all algorithms equally good but none of them are any better than behaving randomly and the reason that's true is
because overall the infinite number of problems that one could encounter you know any algorithm has just as good a
chance of doing well as that as any other algorithm but it turns out in practice we don't care about every
possible problem in the universe when you care about a small set of problems in the universe and what allows us to
get leverage over that small set our built-in assumptions about that about that world so the problem of learning is
difficult and in some ways impossible but it turns out people are really good
at solving the problems that people are really good at what they're really bad about is explaining to you how they do
what they do but they're really good at solving these problems so a lot of what's been going on in the reinforcement learning world in
particular is taking advantage of people learning from getting people to tell you something or to demonstrate something to
you about how to do something so that you can learn much much faster than you ever would and really what you're
getting out of it is you're getting human beings the human beings assumptions about the way the world
works and you're taking advantage of those assumptions to narrow down the genero down the search space sorry I'll
give you really I'll give you a really quick example so it turns out that people do not think about things in
atomic actions they tend to think about them in these big temporally extended views of the world so they take
something like pac-man right if you asked I've asked you to explain pac-man to me you wouldn't be describing in terms of up-down left-right or what you
would say things like oh well look you need to get the power pellet you need to avoid the ghosts you need to you know
you need to do these four or five things and we run experiments on this where we ask people to create buttons that they
would use if to make pac-man go faster and they come up with these interesting buttons these sort of long term things
but dividing the world up in like that not from up-down left-right but in to get the power pellet avoid the ghosts is
something that is very difficult to learn from scratch but people have already figured this out so you build
systems where people are able to express to you those those shortcuts those assumptions about
the world and then you can learn so much faster than you would ever be able to do
on your own and that's kind of where we're getting a lot basically taking assumptions from the world and getting
them automatically from humans I think that's incredibly important and one of the reasons I think it's important by the way is because so many of the
problems that we actually care about involve people right they involve other people they involve interacting with
people and so you have to understand the fundamental assumptions that people are living and and you have to take advantage of them if you're ever gonna
learn so those are two areas that I happen to think are are really cool and the reinforcement learning space right now are we also learning how to enable
the machines to make the assumption the assumptions themselves like what's happening there yeah but the way they do
it is they kind of do it they do it by dint of observing the world right there's a Michael Lippmann always says a
couple of things that I really like and one is that you know if the person who's doing the programming is doing all the
learning and writing down the data structures then you're stuck right you need the machine itself to be able to
learn its own data structures through observation it needs to be able to to build its own assumptions and its own
models if you're always giving it the model then it's always depending upon you to give it the model it has to be able to to build it's its own model so
fundamental to that is this idea that that you're going to learn these you're gonna build in your own assumptions
you're gonna learn new assumptions and you're gonna build models that you're willing to adapt and so yes yes that's
definitely built into it it's definitely a part of what's going on but the problem is absent nothing
perhaps in anything you you can't know where to start and so this gets us back
full circle to this idea that learning is a social exercise right as human beings we interact with other human
beings that have a bunch of assumptions they built the world together and they kind of know how it works and a lot of
your job is to figure out what it is they've built into the world as assumptions so that you can begin to
learn and our machines have to be able to do the same thing or otherwise they're not actually living in the same world that we're living in hmm
interesting interesting one of the one of the papers that I pulled up of yours
on archive is a paper perceptual reward functions which is pretty recently
published and that goes into I think the the former of these two areas that you
mentioned where you're trying to map kind of the deep learning to you know a
broader set of problems can you describe that the paper there's a bunch of new
things that are coming coming out about that student miner Ashley Edwards is really buying into the fundamental idea
there is that you know people have people's reward functions so if you're a
machine learning guy right there particularly reinforcement learning guy you start talking about rewards and you start talking about States and you
divide the world up into the abstract space and you know that's how you solve problems but we spend most of our time never actually worrying about where
these things come from they're just given to us and this paper is a part of actually a larger body of work that that
I've been I've been paying a little bit of attention to the last couple of years of trying to figure out where those things come from are there principles
about where reward functions come from their principles about where state come from at least with respect to the way
human beings deal with it so that you can actually solve these problems in general and be more robust to small
changes in the environment one of the things that that's true about reinforcement learning is you know
there's a nice little math equation that you need in order to figure out how to learn and determine value and it's very
nice and it's very elegant but it's actually quite fragile so if I were to build a system let's say a robot and I
wanted this robot to get from one end of a hallway to another and along the way
it might do some other interesting things I can construct all my little alphas and my my learning rates and I
can put everything together so that eventually it will learn and that it will do exactly what you want it to do and it won't get so scared that
something bad will happen that it won't move and will get so distracted by some interesting thing over here to the left
that it'll never get to the end of the hallway I can actually do that pretty well but then if I take that robot at
all that it's learned and then I make the hallway five inches longer it will stop working right because the math is
very everything is set up just right so that everything kind of touches one another and what you want to do is you want to
build systems that are robust to that you want to build systems that adapt to that and it turns out that human beings
are very good I mean in fact optimized in some ways for dealing with you know it's still a niche environment right we
do pretty well on earth we don't we won't do pretty well on Mars right we don't do pretty well in space but but
you know it's still a rich environment that we're in and you want to build systems that can do that and so the
perceptual reinforcement learning stuff is about using what we get from our
perceptions directly as the notion of state and as our notion of reward that
we try to get things to look like what we see we try to imitate the things that we see through through our perceptions
rather than you know build simple or actually complex optimization functions
that tell us you know whether this thing actually is like that thing no you just think about what it is that you see what
it is that you're what it is you're perceiving and there's a sort of larger philosophy around that I'm actually quite excited about the work I think
what it allows us to do is to stop thinking about reinforcement learning as
five you know a five tuple where you have to set the values and start thinking about it as a larger
programming problem where the whole thing is it's reinforcement learning is not the thing that you start with it's
the mechanism by which you happen to solve the problem it is itself a programming language is itself a way of
viewing the world and you've got to step back to the level of task and problem instead of thinking about solving this
particular equation interesting yeah I thought the example that was provided in
the introduction to the paper was a good one that was cheap training a robot to
to fold origami like you know what's the state of an origami and how do you how
would you represent that traditionally you know whereas the what's natural for us as humans is to see a picture of the
final result and you know how do you define a you know a score metric or a
distance metric from you know a given current origami to this target yeah it's
and it's a rich problem too because it as soon as if I ask you to explain to me how to do origami we probably have
absolutely no idea how to do something magic that's your hands paper you do a flurry of things and then suddenly there's a dragon I really know that
happens but you know you start saying oh well you start thinking about folding and you start talking to this very high level just like with pac-man right and
the way of dividing up that world is actually important because if you don't divide up the world in the right way you
will never in a million years a billion year in the lifetime of the universe actually solved the problem because
they're just too many possibilities right this goes all the way back to language learning and you know it turns
out that people do not actually correct their children right so you don't get
any negative examples hardly at all in your kid and yet somehow children learn to speak their particular language even
though nobody's telling them when they're what you think you are but you don't actually correct your children and we can prove to you mathematically that
you can't learn under those circumstances so the only way it can be happening is if the world has been divided up in the nice little ways and
there's only a few possibilities and you're searching over those few possibilities because the worlds already been divided up for you if you have to
go through the trouble of dividing up the world yourself and you're just there isn't enough time there aren't enough examples there isn't enough time yeah
yeah yeah so okay I'm just gonna say so
to me if you pop up to the AI level instead of the machine learning level all right that's really the interesting
thing right but what's really exciting about AI right now what's really exciting about machine learning right now is that we finally have enough
computing power you finally have enough mathematical sophistication and we finally have enough data that we can
actually start solving really hard problems where we're gonna be forced to move beyond you know the equation that
we wrote down in 1965 that hasn't changed to thinking about bringing in all of these other things whether it's
marketing and behavioral economics whether whether it's game theory whether it's well engineering whether it's
control you know we're actually gonna have to bring in tons of other things in order to solve the problems we're now at
the point where we can actually do that so we're actually meeting in the middle so that so this is why this is an exciting time for me nice nice so at the
risk of asking a question that we've kind of touched on in a couple different ways already for someone who wants to dig
deeper into the kind of stuff we were just talking about interactive machine learning and AI and reinforcement
learning are there any places that you would point them to get started well I
would start with just a basic machine learning class particularly one that covers reinforcement learning if you really are interested in reinforcement
learning as a topic I mean you know rich Sutton's book is freely available online
it's a great place to start to kind of understand what's going on the class that I teach with Michael Lippmann is
freely online there's lots and lots of lots and lots and lots of examples out there I would actually start with that
and get the basics there's survey papers I mean Google is your friend in this case but if you're the if you're the
kind of person who wants to have someone give you a nice brief overview what's going on then you know hey start with my
class just pick Michael Lippmann oh you can go to Udacity you can get it for free just sort of skim through it and watch through it and you'll you'll
figure out from there where to go and I would really I would really encourage
people to pick a problem that they find interesting if you games are the things for you and
start looking up the deep learned the deep reinforcement learning work on on games there's a there's a bunch of work
done recently on solving most of the Atari games mm-hm using deep learning that's really interesting stuff the problem was
starting there though is that oh now you have to know about convolutional nets are and you know you're gonna find yourself distracted for nine months
while you learn enough math to figure out what's going on I would actually start top-down I would start thinking about the problems what
the issues are before I get so deep into the to the math that I get lost you don't want to lose the forest for the
trees here and it is very easy to lose the forest for the trees because there's so much kind of interesting and very difficult math that's underneath all of
this but really you want to keep sight of the goal right which is to build something that can learn over time can
adapt over a year can live for 20 years and continually learn and adapt and think about what that would mean think
about what it would mean to you as a person and then start asking what kind of background you would need to have in order to build a system that does that
that's great and I'll include links to a bunch of the things that you mentioned in the show notes
I thought I would let me add one thing to the show notes you mentioned the the
book influence I would also recommend the media equation immediate is evasion that is a fantastic book it's one of
these it's a short book about how human beings actually behave and how it turns
out that people will treat machines as if they're humans even though they know better because they'll treat anything
that acts like it has intention as if it has intention and I think that fact alone should influence everyone who's
thinking about building systems that have to interact with humans interesting we're not even all that good at ascribing intention other people the
thought of applying it to machines is and we're gonna have to work on that I
actually think it's the other way around I think the problem is we're incredibly good at ascribing intentions to other people it's just not always the right
intentions ah yeah yeah yeah great so I think we're this has been a great
discussion I appreciate you getting together with me for it especially on a Saturday morning and don't want to
monopolize your Saturday so we'll wrap things up here anything else you'd like to toss out no just I really enjoy this
and we should have this conversation again absolutely absolutely and then for folks that want to get in
touch with you have a fine do you want Google+ the one guy who's still there no
just send me an email it may take me a while to respond that I'm more than happy to respond just cool Val let's see
detect that e to you okay and are you on Twitter or any of the lesser used social networks I have I have a Twitter account
and occasionally I even use it but email is the only way to really get to me
unless you have my cell number and I'm not giving you my cell number nice nice all right great well thanks so much Charles really appreciate it and next
time look absolutely awesome
all right everyone that's our show for today thanks so much for listening if you're
one of our lucky winners or runners-up please reach out to me at sam at twilly
i comm a bunch of you have asked hey what's up with the newsletter no you
haven't missed anything I've just been crazy busy and haven't had a chance to get one out I'm so sorry about that I'm
still working on it and I'll keep you posted thank you so much for your support and catch you next time

----------

-----

--03-- 

-----
Date: 2017.03.1
Link: [# Open Source Data Science Masters, Hybrid AI, Algorithmic Ethics & More with Clare Corthell - #1](https://www.youtube.com/watch?v=YgQxlKPeC-g)
Transcription:

This week we interview Clare Corthell, Founding Partner of Luminant Data, recorded live at the Wrangle Conference. We cover her background and what she’s been up to lately, the Open Source Data Science Masters project that she created, getting beyond the beginner’s plateau in machine learning and data science, hybrid AI, the top 3 lessons from her time as a consulting data scientist, and, a recurring topic both here on This Week in Machine Learning and AI and also at the conference: Algorithmic Ethics.

hello everyone and welcome to the podcast if you're looking for this week in machine learning and AI you are in
the right place but if you're a regular listener you may be wondering what happened to the teaser and the awesome
intro music well this is going to be a different kind of show so I've skipped the usual intro to avoid confusion this
week we're going to take a break from the news format and I've got a really interesting interview to share with you in its place as you may recall I spent
Thursday at the Wrangell conference in San Francisco which was organized by cloud era who was kind enough to sponsor the past
couple of episodes of the podcast I'm really glad I went to that event the program was super solid and I met a
bunch of great people one of those people was Claire Cornell whose work was
discussed in one of the very first episodes of the podcast and she was kind enough to agree to be interviewed for
the show we had a really fun discussion and touched on a bunch of interesting topics including her background and what
she's been up to the open source data science master's project that she created getting beyond
the beginner's Plateau in machine learning and data science hybrid AI which is of course the topic of the
article of hers that we talked about on the podcast and a recurring topic both
here on this weekend machine learning and AI but also at the Wrangell conference and that is algorithmic
ethics Before we jump into the interview a few quick logistic notes first what
about the news well if you've already signed up for the email newsletter that I've been talking about on the past few
podcast episodes you'll be receiving a summary of the week's news right in your inbox on Monday morning if not it's not
too late to sign up at two malaya comm slash newsletter second if you're
excited about machine learning and AI and you've got research or writing skills I'm looking for correspondence to
contribute to the podcast and or the Twilio comm website shoot me a note at
Sam at to Amelia comm if you're interested finally I had a blast doing
this interview and I want to know what you think about it and the interview format in general as always you can reach out to me at at
Sam Carrington on Twitter SI m CH AR RI ng t ln with your comments questions or
suggestions alright let's get to it on to the interview alright so I'm here
with Claire Cornwall at the Wrangell conference in San Francisco hey Claire
right to finally meet you in person hi great to meet you in person too Sam yeah so what's particularly exciting
about getting to talk to you is I talked about your post a few I guess it was one
like the second the second podcast I did you wrote that post around the same time
on the hybrid AI mm-hmm and I thought that that was a really interesting post and it was one of the
things that I talked about on the podcast so I'd be looking forward to catching up with you on that as well as
kind of getting an update on what you're up to and what you've been you know digging into so maybe we can talk a
little bit about your background and kind of how you got into data science and machine learning yeah not a problem
Open Source Data Science Masters
so I I'm a little bit weird because I'm not a theoretical physicist or you know
some of some of us in data science are applied physicists too but I'm not in that camp I actually started with
product design and came into it from that perspective so I was actually
working on a very small product kind of a start-up within a start-up when I
decided that I wanted to understand more about our users and what they were doing so I went to the parent company and I
said hey I I don't know much about this but I think I should look into our user logs and try and understand more about
how people are accessing the product and what might be happening and do some basic analytics and the head of
engineering kind of looked at me and goes I what logs I thought well this is
this problematic how am I going to learn about my users if I don't have that and it kind of started me on this long
rabbit hole which has now turned into my career and I actually very much overshot that I I did not end
up in analytics I work on machine learning applications and that problem space now but I came at it from that
perspective and at that point I was really looking for a new direction and
decided to invest the next 7 months in learning everything I could to prepare
myself for a career in data sided data science so I built a curriculum because
at the time I think insight had just started so this was early 2013 ok and
there weren't any academies that focused on this so I built a curriculum around that and published it on github and
that's become a popular resource for people who want to get into data science and understand what it's all about
because there wasn't really a road map at that point but and obviously the open
data science masters is that what the open source data science masters yes be the very descriptive unimaginative name
that I gave it so it's exactly what it is though so that was a really
challenging project to work on and I'm really happy that I was able to put that
out in the open source world and to give you a preview ma'am I'm working on a second version I there were breaking
changes that floated up from Coursera who took a bunch of their courses offline and that's so yeah I'm working
on bubbling up those dependency problems and fixing them ok so after that I went
to a company called meta mark they're actually around the corner from where we are now and they tried to measure
private company growth so very similar to what Bloomberg does for public market companies and sure um at the time when I
joined that company the CTO was ready to divest in machine learning he was convinced it was not going to solve the
problems that they were facing it wasn't going to pose a reasonable set of
solutions for their near term and midterm goals as a company and I on my
first day designed the key components of how we would move that strategy forward
and that company has a I think it's a 10-person team
working on data analysis and machine learning and they're going strong and I
I moved on from that to consulting about a year and a half ago and have been
working with companies of various stages and sizes on getting started with data science or getting started with new
functions within data science that they want to spin up on so helping them understand how to get from A to B and
what it's going to cost them for a solution space that they're investigating there's a lot there to to
dig into on the open source data science master's that was that a little bit of
kind of building the building the parachute as you're jumping out of the plane or building the airplane as you're taking off that kind of thing right you
were learning you're collecting laundry as I was falling out of the sky something like that absolutely it was
perhaps most challenging because I had to rewrite it as I was going so I would
continually check in with people I knew in industry and try and navigate to figure out what what skills were
actually applicable what kind of depth I needed to go in on particular particular topics what was actually key to
understand and to this day this is something that I hear a lot about from people who are hiring managers that when
they try to hire people who are very fresh to the field sometimes they don't have the wealth of an intuition
distributed into the right places so they may know how to build a model but
they don't know how to validate it and they don't know perhaps how to test
data or work with data sets that are very messy there there are various kind
of drawbacks to having a self-guided
education and and having to retarget that as you go is certainly challenging
so and were you did you learn yourself through a self-guided kind of approach
did you collect all this by you in the process of learning it or what was I guess what was the background that you
brought to your getting into data science where did you start yeah I will be so I have a degree from Stanford in
product design but it's through a department called science technology in society and it's actually a hybrid
engineering program so you take two engineering tracks at once and then it ties together with this X component
which is the reason that I talk a lot about ethics publicly which we had an
STS at RPI alsa oh that's great that's great they think it's not get into various places but it's a sister program
to sims's which became a more known program recently because one of the
Instagram founders came from that borough in any case very small program at that point in time now it's one of
the biggest and I focused in computer science and product design through
mechanical engineering so it was very product focused but through those two lenses of engineering so I came into
this with a background in web stack engineering and UX and full digital
product design but I I wasn't coming at it from having no programming experience
so moving into a Python workflow and using tools and technologies like sequel
that I'd seen before was not not the primary challenge for it so I differently wasn't starting from zero
like some people do yeah yeah and how about the stats component where did that come from
I had taken some stats classes in college but there was actually one that I loved and the professor thought I was
the weirdest person in his course I'm sure because it was a bunch of people who wanted to go into management
consulting it was an operational statistics class like how done and
understand how cars pass through for toll this when you have you know two of
them open at any given time long you know he's kind of convex optimization problems and I thought it was just the
most interesting stuff and I couldn't come up with an application that was anywhere close to a career that I
thought I might have I just had no idea how this stuff would be applicable same thing with linguistics I for a long
time would read linguistics textbooks and read a lot of no Chomsky when I was
in high school and more behind it love that stuff my parents
thought I was gonna major in it and I said it's not applicable I can't use it yeah of course flash-forward to several
years later and I'm actually working quite a lot with unstructured text and that's actually the biggest request that
I hear in the market as a consultants how do we work with text and understand
it through a lens that works for us and isn't just a word cloud or a count of
various themes coming up how do we understand it from the perspective of
the customer service industry or we saw it talk here earlier about data science
and HR and understanding feedback those types of applications are becoming very popular and widely requested right so
things always come back right one of my favorite designers has has this well
this thing that he paints on Billboard's this guy Steven Sagmeister he says
everything I do always comes back to me and I think about this all the time because there are always these these
little things these old vignettes that you take and you never quite know when
you're gonna come come back to that and it's going to be relevant to what you're doing yes oh yeah so you've built the
open-source data science masters for folks that are starting at zero and trying to work their way to or starting
someplace and trying to work their way forward what it what are the things that you find folks struggle with the most
Biggest Challenge for the Curriculum
the biggest challenge for the curriculum and for people going through it right now I will say this is the two-sided
problem is that they can't people can't find problems that are appropriately
scoped to showcase their talent and the curriculum can't necessarily provide that right now I have investigated how I
might go about providing you know sample data sets and questions mmm alongside
them that would kind of give you a take-home package of something that would showcase your skills but it it's
actually a lot more work than you'd expect and it's very difficult because it's it's a scoping
problem at its heart you have to have something that has enough depth but isn't overwhelming and can showcase a
bunch of different skills so it's it's a big challenge for people to sell themself selves through providing that
type of portfolio piece yeah and at this
point I think Kaggle does a really good job of curating datasets and providing conversations around analysis and
modeling and predictive algorithms and ways to approach problems and I usually
direct people I was going actually if cadigal was one of the places that you point people yeah yeah I think they do a
really good job at that so they I think they're their entire model is built around that type of that type of work of
probably scoping scoping questions around a set of data and allowing people
to work on it and sometimes rewarding them for that yeah yeah it's interesting
it the podcast has a lot of folks that are you know somewhere on that curve yeah I hear from folks every once in a
while you know asking about how they might apply you know how much how might I apply machine learning and yeah uh you
know healthcare or some problem they have an interest in and it's difficult to to manage that scope but as a
beginner in part because you don't know what you don't know right but at the same time you a lot of times when you go
to some of the public forums where people are asking you know how do i how do I learn this stuff you know people
will say whoa go you know take this course or a course that Coursera course and then go work on a project and the
gap between take this Coursera course and then go work on a you know a pet project is actually pretty huge yes yes
Building Analytical Intuition
and it's a gap that you have to fill with building analytical intuition which
is something that it's very hard to teach but is very learnable so there's that counter intuition there that it's
something that you can learn and it's best learned from other people but it's very hard to learn it from a book so I
do encourage people to to use that practice and you know for example looking at a kegel
competition around healthcare data and taking a stab at it without seeing what other people are working on given a
question and then coming back to see how other people address that question very useful workflow and does provide you
some of the asynchronous communication that you would otherwise have yeah in
person in a company the other thing that I the other key component there that I think is really helpful is to have
questions that are actually appropriate for the data and to be very strict about your own workflow when you're when
you're answering that question because you can get lost in the weeds everywhere and in fact I'd say most data science
teams there their biggest struggle is not necessarily with structure but with
the rigour of having questions that they can actually test in hypotheses that
they can actually test against you know and I certainly do know teams that have
a more Rd approach and that can lead you to interesting places but it doesn't necessarily help you answer a question
because you're not necessarily restricting yourself to that path yeah yeah so you've got a teaching bent you
put together the set of resources and the natural step consulting right we're here teaching fines that are actually
trying to build these teams yeah exactly exactly it is very natural and I I think
Working with Product Managers
the biggest reward that I get in consulting is when I work with someone
who's a little less technical or more distant from the data science and they
start to understand and Intuit as people who would be new to data science they
start to intuit about what's going on with the data and how you can answer the question with the data and why it's
appropriate or not appropriate and what manipulations they need to make and what you know what type of data they need to
make in in intuitions about what they can do with it so that's really
rewarding I've had the pleasure of working with a couple product teams and product teams are great because they
they have a vision for what they want as an outcome and that outcome is really helpful much like a
driving question or hypothesis to guide you through a set of possible solutions
with a lot of rigor and direction so that's been really rewarding to see
product managers saying hey I think this will work because I know that it worked in this other case and we learned about
that a couple weeks ago and it's it is very much like teaching not knowing a little bit about your background now I
can almost imagine the context out of which the hybrid AI blogposts you know
came you know product teams telling you oh can we just throw a tie at this and not have any humans in the loop
yeah you hear a lot of that I definitely heard that like well we know that we
Hybrid AI
have to have some mostly human approach and we can use some predictive
technology alongside them for a while but we're really shooting for 100% at the end and that ultimate vision is very
problematic because as I explained in the post and I can summarize that
briefly but hybrid AI in cases where you need people to look at data where you're
not certain how to predict an outcome or or classify or whatever your your
objective is have them look at that poorly or less confidently predicted
data and make their own judgment about what should happen allowing that to
happen incorporates the possibility of future outcomes and and future inputs so
in cases in cases where you haven't seen everything that you could possibly see
because in the future there will be new and different options it's only necessary that you would always involve
people because you have to incorporate those new those new opportunities and
those new those new possibilities so I I
think tempering our expectations about how much work computers will do and what type of work they will do is really key
to building the right solutions because otherwise we we don't have a good Pareto
8020 approach to our problems where we can say hey let's set
aside this part of the problem because we know it's always going to be too hard for the computer it will cost us 80% of
our time to solve the many percent of the problem and it doesn't actually make sense let's just route that to people we
might learn more about that problems faced in the future but we also know that there's there's prove slop that we
always need to account for and that's important and it sounds like you don't think we're anywhere near you know
getting closing that gap getting to the humans out of the loop it depends on the
application you're looking at mm-hmm absolutely we used a human in the loop system at
matter mark for various tests on machine learning team and we actually had people
in-house in addition to some systems where we had outside labeling done and
we had used vendors for that type of thing there are a couple good options
there but I think the the pragmatism on
the shape of the solution the solution space that's possible to achieve in a
reasonable amount of time and any sort of reasonable cost for a solution all
drive us toward this hybrid case and you
also discover pretty interesting things when you use people or services that
that have people labeling data or providing you feedback because they will
they will give you more information than you asked for in some cases and we actually have had people in the past
come come back to us find our email addresses I don't think they were given them so they they actually went out and
did research now know how to email us and said hey you asked me this question I actually think there's kind of an
issue with how you phrased it it doesn't fully address this other issue have you thought about that I'm worried that I
answered the question wrongly meaning these are people that were on your labeling team yes felt so compelled
these are actually people that weren't weren't even on the team they were an outsourced group of people that were
paid to work on that data so mm-hmm you you learn a lot because you get more
perspectives and more eyes on the data which is which is always a good thing especially when you're thinking about
blind spots that you might have yeah even one of the simple things that I
thought was pretty interesting about that post was you presented some kind of broad brush stats and I don't remember
the specific stats about something along the lines of you know AI by itself right
now you know a machine learning solution can get to 90% accuracy for you know
generalized speech interpretation but in order to really be usable it needs to be 98 or something like that I forget the
numbers but I think you know I don't think people think about that enough
Data Perception
they don't they don't so it's funny when
humans look at data they have a very different perception of it than when they look at the metrics about the data
so for example if you have a classifier for five classes and you look at a
seventy-five percent accurate classifier over all of those classes it will look
like garbage to you as a human even though that's that's pretty high mm-hmm relatively speaking and you probably did
some work to get it to that point right you would probably still call it a
unacceptable option or a non preferable right classifier because that's it looks
like garbage to you and I think the cases where that garbage matters is
where we have to worry about the way that we build hybrid into solving that
last that last component and getting to 99 percent or 95 percent or whatever we
need to feel good about the application for example if we're predicting the
health outcomes for a person that's a very high stakes prediction and we would
probably want to skew much further in in the hybrid direction or in the human
Augmented direction otherwise because the stakes are actually very high so I think when we
start to discriminate between types of application that's where we see this coming in but even for consumer
applications like Google knowledge cards things like that people still curate a
lot of that information it's not necessarily summaries that are generated
by a computer sometimes there are people that are taught to create that data in a
particular way and I think we saw a great example of this a couple weeks ago
when news about how Facebook curates news articles came out and that's a very
good example of how your definitions of taxonomy is your acceptance of how
things are classified and your incorporation of new information all
impact your end user and sometimes in in
very critical ways they might sway how someone votes it might give someone a
perception of of the world that they otherwise might not have in that case so I think we're starting to see the
impacts as well from consumer applications that we thought were not so high in terms of risk and I look forward
to seeing what they invest in at Facebook because I think I wager that
they have people working on this that have an eye on how to make this better but at the end of the day you do end up
in in a semi political discussion about what what fair and balanced means and journalism and it becomes very
domain-specific so I I think it's it's healthy for society to grapple with that
and for us to think very critically about how these things are actually working instead of just engineering them
away and having a hundred percent machine solution are you aware of anyone
any groups working on the problem of hybridity they're from an academic other academic research
topic areas in there somewhere or tools platforms or is it you know everyone
kind of figuring this out on their own building their own custom thing and that's just the state of the art right
Custom Platforms
now um so the short story is that a lot of companies do build their own custom
platforms for for doing this right they usually leverage some sort of
marketplace for data entry data annotation a question-answering
and broader products like Amazon Turk is
a very broad products you can arbitrarily give people tasks and you place a bid on how much you would pay
people for those tasks and they can choose to accept it so a lot of companies will use that platform and
build on top of it and do a lot of integration of that type of system on
the backend so in some ways you know they call this artificial artificial intelligence in some ways the that
component is actually a technology interface itself which is very interesting to think about because there
there are people on the other side of on the other side of the technology but
there are a couple other vendors that do things to support hybrid CrowdFlower is
one in san francisco it does some some of that to my knowledge they do in our
observer validation basically to give
you multiple multiple sets of eyes on a given answer to a question to ensure
that it's correct so you don't have bigger sets of errors or unmeasurable
error and you know where things are going to be more ambiguous that in
itself can be very valuable too because you can you can basically say here's this big set of data or this big set of
questions let's say how would you answer these questions and give it to multiple people and you'll find out where people
disagree and that tells you more about the ambiguity of of the problem space
and where you're going to have to make stronger decisions about what you think is right so that's been a really helpful
thing for clients to understand in the past and I think they have a pretty good
understanding of how that works and we'll see if they build more products around that mm-hmm
do you have a other you know top three takeaways that you know client you found
that clients you know as you look across a set of clients you know these are the
top three things that you know they all you know either learned or need to learn in order to be successful at this stuff
hmm I can tell you the first one is always know what your question is be be
Questions
very precise and know exactly what the answer would look like if you saw it so mm-hmm if you see the answer you'll
you'll know that the right thing is happening I certainly worked with companies that say hey we have all this
data we want to learn from it and I say great what do you want to learn and they say anything and so that's that's a
perfectly healthy and normal place to start but at that point you don't have a question where you can build anything so
you have to formulate questions and decide what's actually valuable for your business which is more of a business and
product space question formulation task so that strategic involvement has
necessarily become part of the business coming from a product background I can
appreciate that I think there are a lot of other independent consultants and
people I know who work solely on questions after they've been fully formed and they say you know once you
have the specs ready happy to work on it but otherwise it's it's not it's not
what we do right and that initial step of defining your question knowing that
it's an appropriate question for the data it really is the space where we we
thrive and help our clients succeed so if they can come in with a strong
understanding of what they have in what they want that's all better I think that's true broadly in
business survive um what else so I was just having a really good conversation
over lunch with a couple people about how one of the things that we don't see
as often in data science machine learning land is a strong leadership
that knows how to market really well so a lot of what I've seen data science
team struggle with is marketing themselves internally or marketing themselves up and managing up to be
sweet or the VP of engineering whoever it is and it's it's really important to
develop those soft skills and understand what your value is relative to the
company sure and I can say that but at the end of the day it's actually extremely difficult to define that value
because your systems maybe giving some
feedback to a business team that allows them to make better decisions but really
they're making their own decisions and they're supporting them with data in some cases but you don't know what the
investments would have looked like otherwise and so comparing the alternate universe that you might have been in had
you not had the technology that your team is building can be extremely difficult to quantify but that is part
of the work of leadership right clearly so I look forward to seeing more
breakout leaders that are really good at that and I think it'll necessarily be
something that we see in the next few years I wouldn't call myself a pessimist
but I would say we're kind of high in the hype cycle right now and I'm not optimistic that we're going the market
will continue going up so to speak goes in a cycle of companies saying hey
we're gonna make this big investment data science we think that data science is a very valuable investment for us for
these reasons and then a couple years later they come back to the team and
they say so have we done and at that point the team really needs to sell what they've
done ideally they'd be selling them along the way as well and I think we're
coming to the end of one of those periods where companies expect to see those big wins and teams really need to
justify their existence and and be able to move the needle and describe how
they're moving the needle yeah yeah soft skills yes yeah take that Venn diagram
of all the things you're supposed to be as an ear I just add like four more things to it no problem yeah nice nice
so that's two but they're big so yeah well on the third eye third is probably
just managed expectations right relative to any other you know any other number
of sets of things in terms of expectations always you said you said it exactly right
Managing Expectations
yeah managing the expectations is probably the biggest thing I do with
clients the first thing I say is I can't do anything to pull a big one out of the Hat I won't be pulling a big one out of a
hat for you if you still want to talk about this and you want to find out what
this technology can do for you and how it can incrementally improve your business and create new opportunities
for products let's talk about that but it's not going to surface anything that you don't know about your own business
because frankly you you know about your business new businesses in existence so you must have some deeper understanding
of what you're doing and when I look at your your deal flow your best customers
or your best customers I'm not gonna tell you that there's there's a there's a sleeper whale somewhere deep inside
Salesforce and that's okay I can give you better confidence to make decisions
and and understand the differential value between things but no promises you
really can't make promises yeah absolutely so you know going back to
this conversation around hybrid AI we started to talk about
you know the role that human humans in a loop play relative to you know their
biases and and you know quote-unquote algorithmic bias and things like that
which actually that was the kickoff panel here it and wrangle conference is
that that's something that you're spending some time looking at now as well right yes so I things are all inter
Preexisting Bias
woven in some way the active learning and human loop patterns of hybrid are
certainly ways to combat actively
reinforcing pre-existing bias if you construct a system to to expose that or
amplify I or both you do not you don't do either and both it depends on what
you know about what you're doing right so if the example I give is a model that
was built at one of my previous employers where we wanted to predict who
would start a startup leave their job and started startup within the next six months and the company had an intent to
build this model create a list of people that were going to start companies soon and sell that list to investors as a
type of pre-crime basically algorithmic pre-crime for seed stage funds and it
could get in early before people even knew that they were going to start companies branch is a which is a like fascinating concept so they used a
number of factors to make this prediction like where you had gone to college what kind of degree you had what your
job title was what your previous employers were there was a bucketing for
the prestige of your college so you know the IV's were at the top and kind of cascaded down through bigger
institutions and that included age so
what we ultimately saw when we predicted who would be a founder in the next six
Bias from the World
months was pretty interesting because all of those factors seem to be directly relevant to how a person's career
which developed them to be a founder in the future and interestingly a lot of
the people in the list were thirty year olds management context management consulting or X I bankers who were white
males and it didn't deviate too far from that and at the time it I thought you
know this is this is pretty uncomfortable but I don't really know why and it took me it took me about a
year to examine that emotion a little more deeply and underneath is actually a
very good reason to be concerned because though you are making a prediction on
characteristics that you you think are fundamentally predictive of an outcome they have bias from the world rolled up
into those factors so all of the all of the decisions that were made to allow
people to get to where they were and become founders in the previous state of
the world and the training data is is your prior for your prediction of who
will be a manager founder and if you don't explicitly observe that you know I
think I clicked through on LinkedIn - maybe that top 120 people on this list
and that's the only way that I knew that there was a certain split of like where
people came from in terms of home country or home state where people came
from in terms of in in terms of age all
of these other characteristics but even name can be ambiguous for what gender
you are so I didn't get a sense of who was actually in this list until I went looked at it and we didn't have columns
that said male female we didn't test against that we didn't predict on that but there were only 13 women and uh-huh
at the top of the list or near the top of the list and I thought well that's that's somehow
unfair right and I think looking back on
that at the time it was we were not talking about that specific type of
diversity in the market for founder's now that that conversation is happening
more it's become a more unambiguous case where you can say all of the prior is
all of the pattern matching so to speak literally coming from VCS as is being
encoded into the algorithm that's making this ultimate prediction and that's not okay so the question then becomes what
do we do and there are a couple of people doing really great work on this I think there's one department at Carnegie
Mellon where someone's coming up with validation metrics that will help you test against the the characteristics you
Algorithms
know you might have a bias outcomes on so in this case you would say how many
men and women are there and in our outcome does it fit our expectation for
what we would want to happen and I think the the real key insight here is that we
want to build algorithms that will construct the world that we want to live
in rather than a world that existed in the past we know the flaws of our current society
to a large extent and some people more than others but as long as we can be
vulnerable to one another and try and
validate that we are not reinforcing unjust actions from the past and just
perpetuating them with algorithms in the future that that is actually key to our work and that's really important for us
to carry as it towards going forward so I'm very excited that we had actually
one talk so far and we will have another talk today about algorithmic bias and and harm and
how these systems affect users and I think it's a conversation that needs to
gain more traction in the practitioner space and we need to examine our own
practices much more closely and know what we're doing perhaps the most
egregious example of this in the press lately was a an algorithm that police
police stations were using across the country to predict recidivism which right this is the one that was exposed
in the Pro Publica article yes exactly and they they did a bunch of work that
Data
they put up on github along with the data set to to explain what what was happening and how they had analyzed the
the outcomes and as far as they they could see what was happening in that
technology and I believe the company still they're still holding it as private IP so even even the police
departments don't understand how this model works the journalist did a really good job of saying this is a big problem
here's here are the metrics and here's the full explanation with the data what's so wrong with this beyond the
anecdotal evidence of this is predicting that one person who is has like
committed one petty crime is more dangerous than someone who's a repeat
criminal and it has been violence it's it's a I think it's a really egregious
case but I don't want to say that it's good these things happen but I think a
few high-profile cases will push the regulatory system to to become more
serious about this so insofar as like it has to get worse before it gets better I'm I'm hoping that that we can get out
ahead of that as practitioners but regulation will certainly be getting there as more and more these cases are
uncovered do you have a vision for how regulation can play here without you
know overly suppressing innovation which is a big concern that you hear the other side it is and a good example of where
Overly suppressing innovation
you see that is in loan assessment and the finance base
where there are very strong regulations about how how you make decisions about
what credit lines people will get so bright get in again like things that got
worse before they got better redlining in the past and other actions that have been taken that were that were deemed
not legal after that that environment has responded extremely strongly to that
there's a there's a pretty good
understanding of what it what is important in making that work
transparent so that you can actually give someone feedback on why they were rejected for a loan or why they were
given a certain loan amount or a certain credit line and wow that's important I
think there are improvements further to be made so I would expect that if the
Regulation and innovation
regulation comes down really hard in the way that it has on that industry if it comes down similarly on others as I
think we're seeing that you it's becoming interested in or then it can stifle innovation and probably grind it
to a very slow pace but we're resilient we'll figure out ways to justify our
existence and how we do our work and I think that's very healthy in the
ecosystem and the ebb and flow of of these factors and regulation and
innovation are always battling it out and run ends the faster we can get out
ahead of it and say no no we actually know what we're doing and we actually know how this works and we are justifying these things and we are
taking the appropriate precautions and trying to be a self-critical as possible
and doing so honestly if we can do that then the regulation will be the regulate
the regulatory environment will be very different when it finally comes to bear
in these other areas that aren't just creditworthiness yeah great
right we've got additional talks here too yeah go check out anything you want to leave folks with point folks too I
would say keep watching the algorithmic harm and ethics arena there's a lot of
Outro
work being done there and there are people that are finding great solutions
and people that are also of course always coming out with more critique and
interesting philosophical perspectives to consider so yeah stay involved in that conversation because it's a it's an
active one and everyone can can be part of it yeah that's that's great and I think the you know your comment about
you know AI encoding the future that we want as opposed to the past that we you
know that we know I think it's a great one very very optimistic yes yes if you
care about that stay involved in the conversation and and yeah be a part of it
great all right thanks so much quick thanks am
all right everyone that's our show for today I really hope you enjoyed the
interview and thanks so much for listening of course you can find the notes for this and every show at the twiddle AI
dot-com website Twi m.l.a i.com the notes for this particular show can
be found that Twilio comm / 11 the number 11 as always I really appreciate
getting your tweets and emails and newsletter subscriptions and iTunes reviews so by all means keep them coming
of course we'd love to have you join the conversation you can tweet me at Sam
Carrington Clare is Clare Cordell and I'm also increasingly using the twimble
AI Twitter handle Twi MLA I looking forward to hearing from you and catch
you next time

----------

-----
--02-- 

-----
Date: 2017.03.01
Link: [How to Build Confidence as an ML Developer with Siraj Raval - #2](https://www.youtube.com/watch?v=aI0vwl2c3qs&t=1s)

Summary:
In the podcast, the host interviews Siraj Raval, a well-known machine learning hacker and educator known for his engaging and informative YouTube series on machine learning and AI. The discussion covers a wide range of topics, including Siraj's journey into machine learning, his advice for aspiring machine learning developers, and his thoughts on the future of the field.

Key Takeaways:

1. **Siraj's Journey into Machine Learning**:
    
    - Siraj was captivated by the potential of robotics and machine learning during his time at Columbia University.
    - His startup, Lucid Robotics, aimed to create home robots but faced challenges in object recognition, a problem now being solved by deep learning.
    - Disillusioned with traditional education, Siraj moved to San Francisco to immerse himself in the tech culture and focus on AI and robotics.
2. **Machine Learning Education and Content Creation**:
    
    - Siraj's YouTube channel started as a side project while he was working at Twilio. It aims to make machine learning accessible to developers.
    - He follows a structured process for his videos: research, coding, technical writing, production, editing, and marketing.
    - The channel serves a diverse audience, including research scientists, developers, and non-technical enthusiasts.
3. **Advice for Machine Learning Aspirants**:
    
    - Siraj emphasizes starting with Python and building simple projects to gain confidence.
    - He advises against being intimidated by the math in research papers, focusing instead on the abstract, background, process, and conclusion.
    - Diversifying learning sources, including videos, articles, and conversations, is crucial for a comprehensive understanding of machine learning concepts.
4. **Future of Machine Learning and AI**:
    
    - Siraj is optimistic about the potential of chatbots and envisions them replacing apps, citing the popularity of platforms like WeChat in Asia.
    - He is concerned about the ethical implications of AI and supports efforts by organizations like OpenAI to ensure the safety and benevolence of AI systems.
    - His future projects include collaborating with big ML to create a video series on pragmatic, real-world applications of machine learning.
5. **Closing Thoughts**:
    
    - Siraj encourages perseverance in learning machine learning, highlighting its potential for lucrative opportunities and intellectual growth.
    - He underscores the importance of machine learning in solving complex problems and believes mastering it can enhance cognitive skills applicable to various domains.

Listeners are encouraged to subscribe to Siraj's YouTube channel for insightful content on machine learning and to consider participating in the O'Reilly AI conference ticket giveaway through Twitter or the podcast's website.

Transcription:

Siraj Raval is a machine learning hacker and teacher whose machine learning for hackers and fresh machine learning youtube series are fun, informative, high energy and practical ways to learn about a ton of machine learning and AI topics. I had a chance to catch up with Siraj in San Francisco recently, and we had a great discussion. Siraj has great advice on how to learn machine learning and build confidence as a machine learning developer, how to research and formulate projects, who to follow on Machine Learning twitter, and much more.

hello everyone and welcome to the podcast if you're a regular listener of
the show I want to start out by saying thank you so much for your support it's been really great to get your notes and
feedback about the show I won't go into the backstory here but going forward I'm going to pivot a bit in my approach to
the show and focus on interviews with interesting folks in machine learning and AI and to accompany the podcast I'm
still going to bring you the news but now via the email newsletter if you'd like to know more about these changes
hop over to the show notes after listening which can be found at twill Malaya com slash talk ta LK / - the
number two okay so about the interview you're about to hear if you've listened
to a few of my previous shows you've probably heard ly mention the name Suraj Ravel Suraj is a machine learning hacker
and educator who's machine learning for hackers and fresh machine learning YouTube series are fun informative
high-energy and practical ways to learn about a ton of machine learning and AI
topics I had a chance to catch up with Suraj in San Francisco recently and we
had a great discussion Suraj has great advice on how to learn machine learning and build confidence as a machine
learning developer how to research and formulate projects who to follow on machine learning Twitter and much more
I'll include links to Suraj's shows and some of the things we discuss in the show notes
a quick note before the interview if you're new to the show you should know
that I've partnered with O'Reilly to give away a ticket to their upcoming AI conference I'll talk about how to enter
after the interview and in the show notes and now onto the interview
all right so I'm here with Suraj Rawal Suraj it's great to meet you in person I've been talking about your YouTube
videos on the podcast for I've talked about a couple of them and like I wanted
to talk about you like every week oh because there's so many great videos but I've held back a lot you know I got to
spread the love so it's great to get a chance to meet you in person and you know I just wanted
to spend a few minutes kind of talking about what you're up to and how you got how you got here sounds good yeah I'm totally down
appreciate you coming over nice so you know let's start there like how did you get into machine learning I so I mean
ever since I was in college like I was I was looking for something to really put
all my energy into and what it was for me was the robotics lab at my school at Columbia and the robotics lab was my
first foray into machine learning and I found that there were all these problems that I wanted to solve that at the time
deep learning wasn't really a thing that deep learning would then solve later like in two years and so I was looking
into like the initial types of machine learning like support vector machines and things like that and just gradually
over time I realized like hey neural Nets deep learning this stuff is like going to solve so many problems so yeah
I've just always been into intelligence and solving intelligence that's that's pretty much my main driver in life like
I want to help humanity solve intelligence because I think it's the most important thing we can do so
Columbia is you know not San Francisco and we're sitting here in San Francisco I what was the path how did you get
how'd you end up here and what are you up to yeah so yeah I was at Columbia and honestly I didn't feel like I really fit
into Columbia I I was you know I fit in really well here in San Francisco and
like Silicon Valley culture I think because I'm you know I'm I'm not so much into like going to classes in person and
just like studying subjects that I don't care a lot about like I just wanted to just study robotics and AI so once I was
at the robotics lab I felt like okay this is this is like my thing I'm going to keep doing this but that only lasted
like a year and then I had a startup called lucid robotics where I was trying to create a robot for home like a platform where each app
would be a physical task so you'd have an app for like cleaning the dishes and stuff clearly this was way out of scope at the
time but at a time you couldn't tell me that I had to see the computer science problems myself what actually ended the
startup I mean we raised funding from severe bhatia the founder pop mill we we had a team what ended the startup was we
couldn't get the robot to pick up a simple novel object they had never seen before mmm deep warning now solves this mm-hmm
so then so then after the starter failed I dropped out I dropped out of Columbia I just was so disenchanted with so many
things and I felt like San Francisco was a place where I could go to rediscover myself and it's been it's been a you
know quite a journey and there's been a lot of uncertainty in my life about what I should be doing the path I should be
moving towards but I'm lucky enough to have come to the conclusions that I have that intelligence is the most important
thing for us to solve in our lifetime because if we don't solve it then some
other catastrophe could wipe out our species whether it's biochemical terrorism or some natural disaster or
you know any something like that we have to solve intelligence yeah so how did
you how did that bring you to doing a YouTube channel yeah so I so I you know
I had a few jobs here as an engineer at CBS Interactive and at Twilio and they were they were incredible I love these
positions but engineering itself just I don't know I felt like I could I could
be having more impact at Tullio I mean Toyota was a great place they I was doing a company it was a great company i
was doing developer education and like that was my full-time role so I was doing technical writing it was the first
time I hadn't just been doing code and I found like okay this is this my thing like technical writing this is awesome I
guess I get to combine my writing ability and my coding abilities yeah but I think for me like the reason that I
left was that I wanted to do video documentation I believe in the future of video documentation and I feel like
tolya was going on a different path so I decided okay you know what I'm just going to do this full-time and so I
started the YouTube channel on the side while I was at Tullio okay so I was making one video a week but the quality
wasn't at the level that I wanted it to get in have enough like the production equipment wasn't good enough yeah I
would I wasn't giving enough time to the technical writing to the only option I had was to quit and do this full-time okay
and so then I was like all right here we go and now how many have you done video so far yeah I think it's like it's at
least at least like 28 videos now Wow almost 30 it's one video week every week
since like January 1st Wow nice nice and you've the original show was called
machine money for hackers is that right yeah machine money for hackers and you you just launched a new one yeah fresh
machine running and does that one replace machine money for hackers or they like to parallel tracks that
continue ongoing you know it's interesting because the idea with machine learning for hackers is that
it's meant for developers and fresh machine learning was also meant for developers but it was like a different topic subset it was like newer things
but what I've noticed is that I have so many subscribers I have three different
types of people who are watching me I have the research scientists the cool kids who are like developing the novel
algorithms then there's the developers who are honestly they're also the cool kids and those are the people I really
want to you know that they were my main motivation from the start like I want to make things for developers yeah and then
there's actually the third subset which I'm learning about which are people who are not really technical but they really
want to be mmm so it's like I have to make videos that are catering to each of them so I'm still kind of trying to
figure out like you know because sometimes my videos cater field research times you sometimes the developers
sometimes to the you know people who are not very technical so I think for now I'm making videos that kind of cater to
all three eventually I want to get up get to the point where I have channels dedicated channel for each of these subjects yeah and for that I have to
grow a little bit more okay right it sounds like in a lot of ways the parallel path to mind with this podcast
I am you know my initial vision was you know I just couldn't get enough machine
learning information like I you know spend the week like opening up a web browser tabs of articles that I wanted
to read or papers that I wanted to take a look at and I end up in a given week with like 80 to 100 of these
ABB's open and I'm like this is ridiculous not and then you spend some time going through it and half of it is
crap and like if everyone's doing the same thing then you know people would appreciate you know something that tries
to figure out what's good and what's not and just spend some time talking about what's good so hey I don't have to spend my we collecting this bag of okay
and you know it's been super rewarding but it's like a ton of work it's a ton
of work and then I wait at your stuff I'm like I can't imagine what goes into you know your videos because you're like
going deep into a topic and then you know you're writing code you're like you know publishing code up on github what's
the process is it is it the same every every week or you like still experimenting yeah so I've developed a
methodology for this over time like I'm building the process so what it is is like the first part is research like
what is the topic I want to talk about and let me just learn about it the second part is the code like programming
it like I'm going to program some very very simple what I like to call the quick start of X to the quick start of
autoencoders a quick start of support vector machines yep then it's the technical writing so research code
technical writing then it's the production so the actual video like shooting it and then it's editing and
then there's marketing and release so
yeah it's like five or six things in sequential order and and I can I manage
to fit all these things into a single week and it takes around 40 to 60 hours
for a single video generally it's closer to 60 hours now I have clients so I'm I'm increasing the
output from one video week to two so that's like 120 hours a week that's a lot this is actually the first week
where I have to make two videos in one week so I'm hiring yeah I'm hiring a
video editor a technical video editor which is like a new role because they have to be a video editor who also knows
kind of like how to code right because I have code and acted like you know they have to point those red arrows at what
I'm talking about this would be I'll leave a middles know what's important then you know what's important right and they have to know that cards you know
the car I'm talking like what I'm saying like oh this is irrelevant what he's talking about like support our Commission's or
whatever yeah so I'm looking for unicorns basically yeah yeah our wheel
well so so you're doing it all in one week you're not like you know researching one week and producing the
you know researching in next week's video one week and then producing it's all self-contained in that week it's all self-contained that weekend and how do
you determine what's you know what you're going to talk about next that's a good question I i yes
so I browsed the machine running subreddit look at what's hot what whatever interests me I look at hacker
news I look at Twitter like Twitter is actually a great learning tool for me I just follow people who I think are
really smart and you know young laocoön and stuff like that and I think my the other data source is Facebook groups I
mean a lot of machine learning Facebook losses so yeah whatever is like new and hot and the intersection of what's new
and hot and like what I'm into generally I can figure that out in like one day mm-hmm but it takes all day yeah yeah
yeah the curation part is it's hard I mean as well just there's a lot of stuff
out there you know like I said before and there's a lot of stuff that you know looks really there's a lot of clickbait right it looks really interesting
anything you get you dig deep and it's just nothing there totally or it's like
just way way too technical and you didn't even think it would be like a lot of mess like ah here we go is there an
example of you know something that you thought you wanted to take on and then you just you know found out that it was
just that the math was just too ridiculous um I think uh well I can't
well like if I've decided I'm going to do it I'm just like I literally don't have time to like not do it because I
mean because I have to keep going but I can tell you that the closest I was to like not being able to finish a video
was generative adversarial networks so I was a video thank you that was the hardest video I've ever had to make
because make because that stuff yeah that stuff was pretty hard and I was
that's a video where you're like well this really should be two or three videos but I'm just going to you know cram it down to one and see how it goes
there was one of those where you said something like that I thought yeah no no I could definitely have more than one on
again yeah so what you know that's the topic that's come up on my podcast quite
a bit why don't you talk a little bit about you talk a little bit about Gans what you learn there in doing that
project yeah so yeah Ian Goodfellow
who's now a research scientist at open AI he's the guy who authored the paper but it's a generative model that can
create so if you give it some input data it's going to it's going to have some
output data that's similar to the input data but different so if you feed it
like a collection of faces it's going to generate faces that look similar but are different and at first I was like well
how is this going to be useful but it's a tool for any kind of engineer to design so if you feed it like you know a
collection of living rooms it's going to be able to generate novel living rooms that look photorealistic
which is super cool so it's a tool to help engineers like envision their ideas better and yeah yeah I like the idea of
two dueling entities mm-hmm you know how the discriminator is always trying to
full full full pool its counterpart or the counterpart is always trying to fold
a discriminator which is always trying to detect like oh just if it's false or real right it just keeps doing that
until eventually you know it just gets better and better it's a brilliant idea and like and you know deep mind has done this stuff with
like alphago when they trained to do old neural neural nets against each other to play go so it just got better and better so I think this idea of you know of
having this adversarial nature can be applied to a lot of other things in machine learning have you seen examples of that I've been
looking for that as well I've come across least ideas of
you know where hey if we can fit one machine moaning out where one a eye against another you know and let them
train each other have you seen besides from the the generous stuff that was covered in the
papers other examples of that yeah I think there's a lot of potential for
like game AI so if you have you know a bot versus a human or just two bots personally I think so deep mine is like
really into games which is cool and I think there's a lot of potential for
combining adversarial work with what
they're doing in 3d games mmm just as like an I mean it's kind of like a
suggestion on my part I'm sure they've already thought about this but if you
apply Dan's if you were to apply Gans to games I think that would be really cool
I haven't seen a paper don't we talk about like first-person shooters that getting's or the types of games that
they're playing you know in deep mind the Atari game no no yeah okay so okay
so like what I think is really cool so opening I just yesterday I think release this call for research scientists on
four problems there was number four was what was it it was like create a simulation that where all the entities
get better and better over time like you create an entity in the simulated world and then it learns to like what kind of
food it needs what kind of nutrition it needs to survive better and better like I think there's a lot of potential for
adversarial algorithms there are two entities forcing each other in this in
this simulated world so maybe not necessarily just a game but any kind of simulated environment where you have a
set of constraints and you want you want it you want some kind of AI to get better over time I think we're going to
see a lot of a lot of adversarial algorithms in the future and a lot of
one-shot learning I'd like to see more of that because right now you know all this machine learning stuff is
is kind of siphoned off to these big companies like Facebook and Google and
Apple but with you know if we advanced in one shot learning anybody who is going to be able to create these models
and learning algorithms from sparse data startups for example that only have like you know 100 users but they want to
apply machine learning to that right all right so you dig into a topic like this you
know ganz there's you know research papers how do you how do you how do you
make the leap from that to code to getting code up you know a lot of the
folks that listen to my podcasts I've you know heard from you know are in the process of learning and they're trying
to figure out projects to work on and you know getting from some of the things that they're reading about to you know
some working example like and you've got that down to a science right so at this point yeah term repetition how do you
approach it yeah so I think for me it's
a lot of it is what I learned from Twilio like the idea of having a quick start like a bare bones skeleton that a
developer can then build off of what is the minimum viable product for for demoing this this idea that you have
however simple you can make it do it so if I read something like you know a
paper on like for example there's so
much autoencoders what's the simplest
thing I can do with an autoencoder an autoencoder take some input compresses it and then
reconstruct it it's only it's a three layer it's a very simple neural networks what's it's the most simple demo I can
make with this and I just think about it and I'm like okay compression Oh compression just compression alone so
just use it as a compression algorithm so like a zip you know zipping yeah zipping and unzipping so then I was like
okay so then I'm like okay so how do i code it so what I first do is I search github so typing like very you know
auto-encoder and I look under Python because python is awesome and I see what's been done before
usually usually something has been done before and so I'll take that and like kind of like strip away the unnecessary
things and add documentation and that's going to be the demo okay and the rare case it's not then I have to go it myself okay yeah how often does that
happen the dad let me get yourself myself like entirely I'd say like off the top of my
head probably like 15 percent of the time okay yeah so one of the you know the two lessons I got from that are you
know simplify simplify simplify like you know you know whether it's the actual
coding or the you know trying to parse the research is like figure out what
this thing is that it's bare essence and focus on that and then like reuse like
figure out what's been done and trying to use that is there anything else and like any other pieces of advice that
you'd give to folks that are trying to work this process
yeah just like don't be intimidated by papers like there is a lot of math and
papers but like really like when I'm reading a paper it's the abstract and the background the
process and the conclusion which matter the most to me and there's really not a
lot of math it's when they start describing you know certain aspects of
the process that it can get really really confusing if you don't know math notation but math notation itself is in
serious need of an upgrade so it's more human readable right now it's kind of siphoned off to
just these research scientists who look at this stuff every day so I think you know we're going to start
to see innovations and how we publish scientific research so that anybody can
read it what that's going to look like I'm not sure but they're just they're just too much coming out right now
and it's too important for few people for only a few people to be able to read
it so so I would say if you just read
the abstract of a paper and you feel as you get the gist that's fine I you can go start searching github with
just that don't feel like you know guilty or or something and definitely look at
videos and and what I try to do whenever I'm trying to learn something is I try to get as many different types of data sources that can into my brain that
always helps videos articles conversations with people you know
there's a lot of content out there it's just going to increase exponentially mm-hmm yeah and I find the same thing
and find also that sometimes it doesn't work out like you expect like the I did
a review of the Google research wide and deep learning paper and you know they've
got this cool YouTube video that you know simplifies everything but I want something that in I didn't get it but then I went through the paper and it
made sense and then I went back to the video and like oh yeah I don't know why I didn't get that before yeah so I agree
that you know having lots of different types of input can make a big difference
so what's like what's your roadmap for for upcoming topics and research yeah so
in terms of like the topics themselves I kind of decide them week to week but the for the the larger vision is to just
focus on machine learning kind of be like Khan Academy for machine learning hmm and I'm going to start needing help
and from other people so I'm hiring and yeah I just try to get I'm just
optimizing for subscribers I want to get you know I want to get every developer on the planet to at least to a little
bit of machine learning I think it's super important there are about 10 million developers in the planet right
now and not nearly there's not nearly enough that are even aware of how
important machine learning is architecture engineering is a new feature engineering and if you want to
win if you have a start-up if you if you have an idea if you want to win you at
this point you have to implement some sort of AI because if you don't someone else will right so I want to make
machine learning you know democratize and make it accessible and understandable as possible to as many people as possible so I'm just going to
keep going down that path and do whatever it takes to make that and that's going to be lots and lots of videos in the future you've got a new
project that you're working on is that something that you can talk about that's going to be public as well that's going to be public yeah it's not going to be
on my channel it's going to be on theirs but a big ml I've done partner drive now you know I've signed a deal with big ml
so I'm going to be making a video series for them about their product and it's going to be it's called cloud machine
learning and it's using big ml to do a bunch of pragmatic real world
applications so the first one is going to be about climate change and how we
can use machine learning to prevent climate change okay so I'm super excited about that one and then so like because
video content takes up so much of my time I don't really have time to do things like client acquisition and yeah
you know all this all this stuff so that the clients that I do have other people who have come to me and right now have like seven or eight and they're kind of
in a queue and yeah I'm just taking on as much as I can handle at a time and as
I grow I'm going to start start looking at more you know ideally you know my
goal is to one day partner with deepmind I want to make videos for deepmind but
they're like I consider them like the Navy SEALs machines I've got to get I've got to get to that level you know the
Apollo program for intelligence uh-huh you know if we solve intelligence we can
apply to anything like just think of it as an objective function or X any problem you can ever think of if you
have the right learning algorithm and you say solve for X it could solve it
scientific research problems or even existential problems the questions that have plagued us since day one who are we
why are we here what's the point of the universe we might not be capable of figuring this stuff out ourselves but a
highly intelligent AI could we might not like the answers we might we might not
like the answers we might not like answers but we're so where do you fall on the whole singularity thing that's
what Rick wakes that's why I wake up in the morning I want to make it a benevolent singularity happen
as soon as possible uh-huh as soon as possible what do you think about the the open AI research stuff that they put out
a few weeks ago on safe machine learning have you been following that stuff also
so specifically like ways to they publish this framework for like four or
five different areas of research that need to be kind of dug into so that we can ensure the safety and you know
benevolence as you put it of they I like you know if we've got a AI powered robot
you know how do we how do we ensure that you know it doesn't learn how to gain
the system and and you know for example if it's being programmed to clean right
how does how do we know that how do we program it so that it doesn't sweep stuff onto the carpet right yeah I think
yeah and then Google had like the Killswitch paper which I thought was super cool I like that opening is
thinking about this I love opening I in general deep though the concept behind it I think yeah it's like preventing AI
from doing bad things is going to be
really important I mean technology has always been a double-edged sword you know with the engineering with the National iron and I think that you know
with security I think that's going to be one of the first where we're going to see we're going to see the power of AI
and when it comes to protecting humans if you have an AI and you train it to
get really good at breaking into systems the only thing that's going to be able
to stop that is an AI that's good at detecting an AI that can break into systems so I think it's a great thing
what they're doing I think it's really important I think it's really important and and what Miri is doing as well and
you know the ethics committee that deepmind has at Google to prevent you
know malevolent types of AI all this stuff is super super important marries the machine intelligence Research Institute yes yeah and Berkeley
yeah and you know there's always a question why can we stop it you know who knows but it's good to try
and honestly if malevolent AI doesn't
kill us then something else likely will so this is something that just something
that's really important so we'll see or maybe taking a step back like for folks
that are trying to do you have a quick like if someone you know Frank comes
with fuses okay I really you know I really want to learn this stuff now like what's your curriculum what's your you
know one two three list of stuff to do is it do you think are you trying to
build your videos so that someone could just follow those and get everything that they need or are there some set of
resources that you think are kind of canonical yeah so I think that my videos
are good if you know some basic Python if you know Python then my group my
videos are a great starting point but I think that my videos alone are not enough you know it's one of the things
of like combining different data sources so I think my videos in addition to some long-form content I think so for me big
ml has some great long-form content there's so you know I actually don't
think I think that a there's a deep
learning course on Udacity by a Google engineer who works at Google brain I forgot what it's called but if you if
you google just like Udacity deep learning that that course is really good that to me is even the tensorflow course
or not the tensor for course that's that's a great one but there's one specifically on deep
learning in general there's just so much
I think it's one of those things where it's like okay so if you're saying like I want to learn machine learning I would
say like okay first one Python by reading the book learn Python the
hard way and then once you once you feel like you're comfortable with Python just
start building things just start building things and and my videos are good because it's application-specific and it make it really easy for you to
you know just when you get compiled and you see your model train train and then you can apply to other things that is
like super useful for for your confidence as a machine learner and also
just as a developer so and also just go to github and search for machine
learning projects like search for like machine learning demo or machine learning simple and just look at those
read Me's download them compile them open it in a text editor and just like go through them one by one and like
really try to understand what's happening you know and and I would say start off at a high level because you
know some people would say it the other way I start off at a low level like learn exactly how to angle move these models from scratch no no I would say
startup at a high level and once you get it at a high level then you can start like trying to rebuild you know you know
you know neural net from scratch yeah like custom and implement from ground up
or implement some research or something like yeah yeah taraj torch lots of great libraries these days nice what a Cora is
well sorry Quora is awesome I've learned so much from Cora just like you know cuz
you because I'll find one question on Quora on deep learning and on the sidebar it's like oh my god all these
questions are amazing and then you have people like yawn Laocoon answering them and like Monica Anderson and like all
these like really famous for search scientists yeah so I've learned a lot other people that you is it primarily
like search based that where you find stuff or are you there following particular people and just kind of
keeping up with them there it's search based search base yeah it's search base how about on Twitter are there you
mention Jana there other folks that you yeah final good signal de noise machine
learning folks on for sure I think for me I think Chris
Dixon is a partner in injuries in Horowitz he he's good for like knowing you know what's up and coming in machine
learning I think is a good eye for that one person in general that I really respect about technology is Balaji
Sreenivasan who's also a partner in Greece in Horowitz that guy knows he
lives in the future and yeah bored yon Laocoon is also like a great twitter handle I don't think
I've come across that one yet but it sounds funny I really like it yeah yeah nice and yeah and then oh and following
these big companies like Amazon and Google is really important because you can see like oh they just released you
know DSST any their new machine learning library which needed to be renamed but yeah
destiny it's great I don't think you're not a fan I'm not a fan I mean in
general I think machine running needs better marketing like a lot ah you know
not I'm not going to do anybody so nice
so for folks that aren't familiar with your your videos are there you know two
or three that like man these were my favorite or these were my best or yeah I
think so the the one that ended up being was popular was AI composure that was the
second video I made for machine learning for hackers so AI composure so the top three would be like AI composure the one
I'm most proud of is generative adversarial networks because it was the hardest and the one that I thought was
the dopest was a build an AI artist because I just thought that application
was really cool like applying some style to some novel you know picture this like
the thing that prism is doing now that prism is doing yeah nice exact nice and so what is the what's composure
composure is generating machine like machine
generated music so you feed it some music like a data set of like you know 500 songs it will learn the style of
that song and then it can generate new music in that same style okay and I trained him in the video over British
folk music but you could apply anything to it one idea I thought would be really cool that should do is take hans zimmer
music and generate music in the style of hans zimmer so prism for music printer
that's kind of what the magenta magenta is current well I'm admit they're not trying to do specifically that but did
you use magenta any of their code in your in this project I didn't no no no this was a four that
it was um not that yeah I was uh I found
it I found it on github oh can I modified it okay yeah nice interesting
we're going to ask you oh you've done a you've done a couple videos on chat bots
and chat bots platforms I was a good one what do you think about that space and
like what would you learn and you know over a few attempts at at playing around with that stuff yeah I think you know I
with the marketing effort I expected I expected wit AI like Facebook's
acquisition that that chat bar chat bot building technology to be way better
than it was but what ended up happening is I found that API AI had a much better it was much easier for me to build a
chat bot with API oh yeah yeah I think that chat bots in general are going to get really popular and we're going to replace all of our apps with chat BOTS
this is already happening in Asia so like with WeChat like most lot of people don't even use apps anymore yeah you
know in China and stuff because it's so easy to say like you know you can even
combine different apps together like book me an uber in 30 minutes at this location and take me to my favorite
restaurant and that's querying like Yelp your Google or whatever you know your
preferences locally and the uber app or if it was in China Quoddy so there's a
lot of potential for chat BOTS and if you are like right now thinking about you know building a
startup I like if it was me if it was me I would be doing some kind of chatbot
cap off Forex where there is no chat bot because this is just going to get more more popular like I already use chat
bots and messenger for like you know like detecting like scores and stuff
like that it scores uh like like Ward sports in fluff yeah okay no I mean uh not that I watch sports but
like I just play around with them yeah yeah what does what are the other useful chat BOTS I haven't found anything
that's particularly useful like the thing that I haven't played around with a bunch of them but you know they're all
kind of it was it doesn't feel like we're there yet yeah we're not there yet we're not there
yet but we will be in like a year that's how fast this faces movies yeah yeah
it's just going to it's yeah right now there's a lot of people who are like really needy pin this stuff in their
building but we're going to see a lot of releases and like you know because the bigger players haven't caught on yet you
know that's one of the reasons but depth I promise you Eber has a team dedicated to this Airbnb has it's not getting late
right absolutely and then Facebook's releasing em which they're training right now full-time looks like humans and machines and just
getting better and better people internally at Facebook are using this and I talked to it you know some of these people and they really like it and
I was like please give me an invite to em like once you like I haven't invited oh my oh my god if anyone at face Facebook is listening
we both want invites to us yes please please nice nice
well that it's been great it's been great chatting with you anything that you'd want to leave folks with or point
them to or you know have them to check out yeah yeah I would say definitely
subscribe to my channel because I'm just getting started and that's where I'm putting all of my effort into right now
and what else I would say if you're a
unicorn video producer or univ machine mercy yeah or if you're a video editor who happens to know how to program as
well definitely you know I see me on Twitter because I'm looking for you because I need you and yeah just
don't don't give up if you know machine learning is you know it's kind of hard
but it's a worthwhile endeavor and you can make a lot of money for it from it
and you can learn a lot and it's going to and if you if you get good at
learning about machine learning which is one of the it can be one of the hardest things on the planet to learn like
solving intelligence like the human brain like how do we work is equivalent to asking like what is the universe if
you can get good at that it's just going to train your brain to be good at so many different things so yeah don't give
up awesome awesome well thanks yeah thanks alright
everyone that's it for today's interview before we go a reminder that this week in machine learning and AI and O'Reilly
have partnered to offer one lucky listener a free pass to the inaugural O'Reilly AI conference which will be
held at the end of September in New York City you can enter via Twitter or the twill Malaya comm website by doing one
of the following three things the preferred way of entering is via Twitter just follow at swim la I Twi ml AI and
retweet the contest tweet that I'll pin to the account and post in the show notes do those two things and you'll be
entered if you're not on Twitter you can sign up for my newsletter at 2 a Malay Icom slash newsletter and add a note
please enter me in the additional comments field finally if you're not on
Twitter and you aren't interested in the newsletter no problem just go to the contact form
on to Malaya comm and send me a message with that form using AI contest as the
subject the drawing will be open to entries through September 1st and I'll announce the winner on the September 2nd
show good luck and hope to see you in New York thanks again for listening


----------

-----
--01--

-----
Date: 2017.03.01
Link: [Engineering Practical Machine Learning Systems with Xavier Amatriain - #3](https://www.youtube.com/watch?v=PjI24wKYfcw)
Transcription:

My guest this time is Xavier Amatriain. Xavier is a former researcher who went on to lead the machine learning recommendations team at Netflix, and is now the vice president of engineering at Quora, the Q&A site. We spend quite a bit of time digging into each of these experiences in the interview. Here are just a few of the things we cover in our discussion: Why Netflix invested $1 million in the Netflix Prize, but didn’t use the winning solution; What goes into engineering practical machine learning systems; The problem Xavier has with the deep learning hype; And, what the heck is a multi-arm bandit and how can it help us.


Intro
[Music]
hello everyone and welcome to twill talk the podcast where I interview interesting people doing interesting
things and machine learning and artificial intelligence I am very excited to share this interview with you
for the show my guest is cha VA imani a chav EA is a former researcher who went
on to lead the machine learning recommendations team at netflix and is now the vice president of engineering at
Quora the Q&A site cha VA and I spend quite a bit of time digging into each of
these experiences in the interview here are just a few of the things you'll learn from our discussion why Netflix
invested 1 million dollars in a Netflix prize it didn't use the winning solution
what goes into engineering practical machine learning systems anyway the
problem that cha VA has with the deep learning hype and what the heck is a multi-armed bandit and how can it help
us of course I'll be linking to the resources we mentioned in the show notes which you'll be able to find at twillie
Icom slash talk slash three its twi mla i comm slash ta lk slash the number
three a quick note before the interview you've got just a few days left to enter
into my drawing to win a free ticket to the O'Reilly AI conference I'll talk
about how to enter after the interview and in the show notes and now onto the show [Music]
hey everyone I'm here with javi I'm at 3 on and javi hey why don't we get started
Xaviers background
by your at your core now why don't we have you talk a little bit about what you do there sure so I'm at kora and VP
of engineering so I leave the whole engineering organization right now my
background though is more in machine learning previously - Korra I was at Netflix and I was leaving the machine
learning recommendation steam at Netflix and even before that I was doing
research and I was in academia and my background again is on recommendations
machine learning and so on and I've published papers on that space for some years so it's kind of interesting that
somebody with this kind of background is now the VP of engineering of a growing
company like Korra where I I need to deal with a lot of different controllers not only machine learning right but it
also tells you a little bit of story of what is important for Korra as a company
as a product and that also aligned with with some of the trends that we're seeing in industry right that more and
more the machine learning AI people that used to be like closed in a room by a
corner and they were like the weirdos in the lab now they're having a lot more influence on decisions that are being
made on how to design products and how to run companies and in my case I'm
that's probably like one of the reasons that I'm in this position now leading
the whole engineering organization because for us machine learning it's like like a big part of our success and
how we're growing all right so there's there's a ton in there and and we'd really like to get to know you a little
How did you learn machine learning
bit better so let's let's rewind a bit you mentioned that you spend some time in academia yeah how did you learn
machine learning where did you go to school and where did you where were you working in academia yeah that's a good
question so I'm I'm actually kind of old for what you see right now and I have a
long history behind me and I'm saying that because so I when I did my PhD which by
the way I did it back in Spain I'm originally from Barcelona Spain so when I did my PhD I was mostly interested in
signal processing and particularly in signal processing and systems design
related to audio and music actually that's what my PhD was based on and at
that point in time it was that age when
multimedia and signal processing was kind of like the hot thing and machine learning was not so much so I did use
some machine learning here and there for different aspects of my research and particularly for some of the initial
recommendation system that I work on that were related to music but it wasn't my core area so I was more into signal
processing and systems doing my PhD so I would say that I I got into machine learning more on the chops and after I
left my you know my I did my PhD I went
did some more multimedia related research in the University of California
Santa Barbara UCSB so I was there I was working on virtual reality and mersive
environments and that was also very cool it's kind of coming back again now but I
was really interested in that space combining signal processing and multimedia and this kind of immersive
and virtual reality environment but after that I became more and more
interested on the data side I was like how do we use the data and how do we infer information from the data and
particularly very interested in how do we understand users from the data right so that's what kind of led me to cook
forget a little bit more about the signals so that we're a little bit like you know more there's they're also data
but they're like cold data that come from systems and focus more on the human
generated data and try to build intelligent systems that understand so I
I did then I switched my research and went into working for a few years in
recommendations and using machine learning and different kind of approaches know in machine learning but
also human-computer interaction approaches to build this intelligent
sort of like assistance that tell you what you like and what you don't like so that's what actually led me eventually
internet person to leading the recommendations team there okay now you you dangled a big shiny object
Signals processing and machine learning
in front of my eyes and that is signals processing that was an area that I studied in grad school as well and I'm
curious well hey I'm curious if you could explain wavelets to me because
that was one thing that he's getting be a hard time but actually no we're not going to talk about that I'm wondering
if you see any parallels I'm wondering if there are any interesting things
happening at the intersection of signals processing and machine learning just out
of curiosity do you have you seen anything there's actually a ton of those
intersections there's there's more of like the principles and how they intersect that I would say probably more
interesting now there is the intersection and the application side of things right so if you think about it a
lot of the systems that are now being being that are being built using mmm
machine learning approaches particularly deep learning to understand things like
speech recognition or image recognition those were considered in the past like
signal processing applications and and for example although I didn't
professionally focus too much in speech recognition I did study quite a lot of that ran you know at that time where we
were using hidden Markov models and these other techniques that for us
in the signal processing world it wasn't you know they were just tools and means to an end so it wasn't like the most important
part of the system although you know it would really like the core of it but now that's moved towards some the
learning and RN ends and so on so there's always been an intersection right between machine learning and
signal processing and there's always a lot to say about how to interpret
signals wherever they come from and those signals again could be audio could
be meat speech music video images and you need to build system that actually
either understand those things or even able to generate them in some way and
there's always a well not always but at some point it's clear that that's evolved more into having a layer of
intelligence in the middle that it's going to be learned and that comes from a machine learning system that it's sort
of like at the heart of any of those systems mm-hmm right great so you you
Netflix
made your way from academia and ended up at Netflix immediately prior to where
you are now Quora and your focus there was on recommendation systems yeah I
started with a very specific focus on recommendation systems we which you
could consider there's a continuation and natural continuation of the Netflix
prize the famous 1 million dollar Netflix prize which by the way that's what got me connected to Netflix as I
was dabbling with it and also part of using that data set port for some of my
research ok so so yeah so I started with wood you could consider like the
continuation of that Netflix price but already working for Netflix and we eventually grew the team to be more of a
core machine learning algorithms team that was building not only recommendations but algorithms for
search and for different things where they two images and it was it grew to
sort of like being a core machine learning / algorithms team that was serving different purposes beyond
recommendations but recommendations is something that is very important for an
epoch right so that was really like probably the core of the team at any given time okay
Netflix Prize
so in terms of the you mentioned the next the Netflix prize am I correct that
the the winning prize entry was never
really implemented at Netflix I'm glad you asked this because I get this
question all the time and I I react to
it by saying it is correct the final entry that doesn't mean that it was
useless right so there's I'm saying that because people immediately when I say
that and we wrote it in a blog post at net when I was in Netflix at some point and even though it was very clearly
explained people still took away like Oh Netflix wasted a million dollars and they didn't use the outcome that's not
true actually net we've got way more than 1 million dollar back in research and in interesting stuff that is being
used and was used in different parts of different systems so so if there's a
difference between was the final entry used and the answer is no it was not used there were over a hundred and
thirty different machine learning models combined in an example most of the
different models that were there were adding just a tiny increase in accuracy and a lot of complexity and they were
not worth it so the reality is that two of the models on their own gave like
enough accuracy that the other hundred and thirty-some were not needed or they
were not worthy are I that's it doesn't mean that they were not useful to understand what they were adding and how
they were adding it so again the story is the final Prize winning entry with
the complex combination of all those methods in an example was not used as it was but the learnings were worth much
more than what was invested in the prize and part of the final winning entry the
most important method were actually used directly in production okay yeah this is I came across this recently
Economics of Machine Learning
in an interesting blog post by Josh bloom over at wise and he talked about
the economics of machine learning basically all of the various trade offs
that get you know that come up when real business is trying to figure out how to
put machine learning into production and that was one of the examples he used about how I forget how many pages or
something the the final algorithm was but a hundred Garrity models that's a huge that's a huge model yeah we're
friends okay and he knows a lot about so we've talked about this in person and
you know the thing is that story is so juicy that you can spin it in many
different ways I actually recently got this is pretty crazy but I did get in my
facebook feed and advertisement from MathWorks trying to sell me MATLAB that was using
that story and saying something like Netflix did not use their final winning entry we can help you with MATLAB and
all that was so I don't even I don't get
where they're going at all with that well I don't know but you know that's the point is that yeah the real story is
yes you do need to be concerned and I'm always say the same I mean you know you
need to be concern about system complexity and about making sure that whatever you do in research it's
actually be playable and it's and it's good too or easy to build engineering
around it but that's very different from saying that the Netflix prize was a waste of time or money
sure so can you maybe spend some time walking walking through some of the
System Complexity
various factors right so you mentioned engineering time and there's you know so
there's obviously like an implementable 'ti from a complexity perspective you know they're going to be data aspects
there's computational obviously you know when you think about
you know practical machine learning and the the issues of you know urine you're an engineering VP of engineering now not
a VP of machine learning research or some when you think about you know engineering these systems at large-scale
what are the things that you need to think about oh there's like a long list
of things and you mentioned a few of them system complexity is one which
actually spans into different sub areas
and different concerns are related to the system to the complex of the system one of them which is often overlooked is
simply cost right it's like if you can do something in a single machine which I
have this kind of infamous slide that I when I show people some people don't
like very much is I tell people that they can do probably almost everything they need to do in machine learning in a
single machine and I have reasons to say that but the point is that if you add
unnecessary system complexity first of all you're gonna have a lot more cost so you're gonna have this now huge number
of machine of machines that you and I have to maintain in a cluster or paid Amazon for hey double you guys cost
rahab so that's one and it's probably obvious and it's probably not the most important the most important one is
system complexity reduces your speed of
innovation and if you have a system that it's really complex from the get-go in the waiting on it becomes like a huge
pain right because then I'm trying to tweak something and it turns out that that's something it's just one of the
10,000 knobs that are in the system and it's hard to know what it did it's hard to understand whether it improve things
and if you keep your system as simple as possible as long as possible your
innovation is going to improve and your innovation speed because you're going to be in a much better position to then
change things dramatically improve them understand what you're doing and what is improving and at some point you need to
add complexity there's no way around it it's like complexity at enough improvement and either in
accuracy or basically whatever metric you care about that it's worth adding but the problem is you don't want
arbitrary complexity from the start because that mid term and long term is
gonna impact you're gonna be end up in a local optima so to sort of speak and
you're never going to reach that global one that you would be getting if you keep your options simple as much as
possible interesting to me the thing that it brought up was the the notion of
Algorithmic Debt
technical debt that's typically applied to code write code debt isn't has anyone have you come across anyone that's
thought this through in terms of algorithmic debt oh yeah there's this
interesting paper that was published actually originally was published in the works of that ICO organizing nips and
it's called a high interest credit car of machine learning depth and it's it's
a very good read it's by a couple of authors from Google by the way so they
know what they're talking about in terms of machine learning death and so it's
something that it's been discussed again even in papers right so all right so it's it's it's an something that any
organization will face at some point and it's something that it's really important and it's really important and
many levels not only at the level of the system itself but also and I would go
further that that's part of like the the core of the machine learning algorithm
algorithmic design right it's like it's a camp razor principle of you know if
you have a possibility of choosing between two things always to the simplest one and part of the reason is
because you want to minimize your depth as long as possible and only make things more complicated when they really need
to be and they're adding up enough so that goes back to the lesson learned
from the Netflix prizes like you know yeah sure you can have you can go for the more complex solution but it's the
Delta an improvement that is adding more the huge increase in complexity and many times the answer is gonna be no that's
Know Deep Learning
an interesting segue to one of the topics that I wanted to chat with you about you recently tweeted about a
natural language processing course and the hashtag you use was know deep
learning across a number of your public appearances you've maybe developed a
little reputation for mr. hashtag know deep learning and of course I'm being I'm being artificially you know
controversial here yeah I understand it this is you know it's a it's a tool in the toolbox but some of our earlier
discussion about system complexity I think is one of the issues that you have with deep learning maybe walk us through you know what your position how you
think of your position on deep learning and you know why you bring it up interesting when I talk about deep
learning I always start by having a few slides in my presentation that explain how deep learning works right so I want
to get that out of the way and say hey I know the deep learning works and it's great for a few things actually
particularly for natural language processing I think that it's getting to a point where it's the default tool for
many things and it's great so the reason I was using the hashtag is just to warn
people that if they were looking for deep learning it wasn't available in that course so I think it is it's very
important for people to understand what
is the right tool for the right task and for example we use deep learning at
Quora for several things but we have a lot of text and going back to the NLP example there's many things now in text
processing that RNN are you know they're actually the simplest solution there is
because you can you can find some of this ready available open source to to
kids that have already been trained and you can even use the model as it is you don't even need to have your own data
set or then you can retrain it but that basically becomes simple enough that
that could be your for approach to a an NLP task that you
have in hand but that's very different from saying that that's equally true for
all machine learning applications and you need to understand like what is the
complexity you're paying for defaulting to machine learning for everything you have and I've seen a couple of examples
recently where I think we're you know in a dangerous situation where a lot of
people especially like more junior researchers or engineers that they're
you know they've come into industry right at the cusp of the deep learning
bubble or wave or whatever we want to call it and their their mind goes straight into deep learning as the
default solution for anything and I've seen cases where I've had engineers in
some companies tell me hey I'm using this central flow architecture on a
problem where I have ten thousand examples and thirty features and I want
to ask you a question and my answer like why are you doing this to yourself I mean if you have ten thousand example
than thirty features do you really think you need deep learning model with a
bunch of layers and most of the time the answer is no and even if the classifier
you're building with that deep learning
architecture is let's say in the best case one percent better than the one you
could be building with a simple logistic regression you're still going to be better off going for the logistic
regression because what I'm going back to what I was saying before your ability to innovate on that initial model is
gonna be much bigger than your ability to innovate on a very complex the neural
net that you don't really understand what's going on in inside so I guess my the point that I'm trying to make when I
talk about quote-unquote know deep learning is that deep learning should be
another of the tools we have in our toolkit and there's a lot of other very interesting machine learning tools and
even research that is going on that it's we should still pay attention to there's
a problem also in the research world right now with deep learning is that because it's so new and there are so
many so much low-hanging fruit it feels like you know that it's the easiest way
to get a paper except that is to an incremental improvement or not so incremental an improvement on some deep
learning approach and that's why we're seeing all the conferences now dominated with deep learning things right even
when you go to a comfort like KDE or the
ACM recommender systems conference that I'm going to be attending in September
you start seeing like a bunch of deep learning papers because it's new it's easily innovating using deep learning
but we run the risk of like saying oh yeah this is the one thing that works for everything and we're going to try to
find all the nails that apply to this hammer and we'll think that they're all
they all look the same and and I think that's there is a danger in that so
Cora
you've touched a little bit on some of the things you're doing it Cora maybe tell us a little bit about you know tell
us a bit about your experiences there and you know what are some of the interesting problems that you face there
yeah sure so I that's a great question
one of the things that I love about Cora and one of the reasons as I said before
that we have a VP of engineering with this kind of background in machine learning and algorithms is that
everywhere I look on our product and our day shoes that we're dealing with I see
problems that are solvable and should be solved through machine learning right so
now if I sorry for interrupting but it's likely that most of the people listening
What is Cora
know what Cora is but maybe you can start with just an explanation of the
site and the mission sure that's yeah that's that's a very good point and it's a very
good point because also even people that know us and users frequently they have a misconception about what core is so core
is on the surface is a question and answer site and application mmm but our
mission goes beyond that so the mission of Cora is to grow and share the world's
knowledge and we think that the question/answer paradigm is really well-suited for actually growing and
sharing knowledge just to give a different example of the only other
quote/unquote company that has a similar mission which would be Wikipedia Wikipedia also believes in the spreading
or growing the knowledge but they believe in the encyclopedic format and that leads to a bunch of different
product decisions of course so we feel like question entering and a broader
notion of what knowledge is so Wikipedia is about factual knowledge we think that
for example an expert opinion is also knowledge and should be included in any knowledge base so all of that defines
our decisions and using question answer for now it's working really well and we
think it's the ideal vehicle but we are not close to trying different things and actually we do have even different
things as of today in our product that enable that knowledge growing and knowledge sharing so so another way to
look at and to understand Cora is the different sort of like networks that
overlay in the product so we do have obviously a knowledge network and even
another one that it's a topical network so we have entities of knowledge that are connected to each other topics that
are related to each other and then on top of that we add the social aspect
right so then we have people and we have people that are connected to other people and we have people are connected
to topics and to knowledge entities and this sort of like different overlays of
different graphs at different levels and the different connections between them is what makes the whole data problem
very exciting because we have a lot of applications that cross the different networks in different directions and we
have for example algorithms that are purely on the content space and they tell us how good is the quality of a
given piece of content we have other algorithms that tell us how likely is a person to answer a question on a given
topic we have different kinds of machine learning algorithms that their purpose
is sort of like trying to understand and predict different aspects of this dynamic system and the relations between
all these different entities so again examples of things that we do we do a
lot of recommendations you have initially in your home page you'll see a
feed of different stories that include questions and answers that we're
optimizing for you to be interested on and that's kind of similar to the
Facebook feed but has other implications and a different objective function so
recommendations like that recommendations that you get through email we optimize the notifications that
you get through different devices also using machine learning that's all on the personalization side of things then we
have content approaches to infer the quality of a content to do things like
ranking answers according to how good they are we have things related to a lot of the
text side of things automatic topic labeling hunting for a topic out of a given text how to find
similarities and questions and answers how to find duplicates and then also we
have the whole abuse side of things which also uses machine learning we need
to one of the things that cor is known about for is you know keeping high quality content and that's the quality
piece but also keeping a very healthy positive community and we do that with
very good Norm's and also algorithms that detect any form of spam harassment bad actors
and so on so forth and each one of them is a different machine learning algorithm so it's really exciting in
that sense because we have covering sort of like a huge space of applications and data tabs that go into this applications
interesting can you talk a little bit about the extent to which you use hybrid
Hybrid ML
machine learning plus human yeah obviously there's a big component of the site that you could argue as hybrid as
users are ranking different answers but are there ways that you're using hybrid
approaches behind the scenes yes we are so so one way to think about it is
initially all everything all of this was manual right then the first initial better version of korah
there were no algorithms in place and all of it needed to be manual so we do
have a team of moderators and people that look at content and there's always
a point where algorithms are not gonna be sufficient and you need somebody to look at the nuances of like is this
answer about this politician really violating our norms yet or no and it's
like really nuanced and we need to have person look at it so the way we think about it is there's if you think about
any content moderation issue there's always gonna be a high portion of the
stuff that you have on your side that is gonna be good and it's gonna be good with no doubt so you can have algorithms
that say hey above this threshold I'm totally positive this is good stuff we don't need to worry about it there's
always gonna be a huge another huge but at some part of your content is going to be really bad and there's no doubt about
it so there's another threshold that tells you below this threshold I'm just gonna remove this stuff because it's basically
crap and you don't want it that's how you keep the quality of your content in
the side right now there's this gray area between those two thresholds and the tricky part right so you have to do
two things one is there's you know you have to have people then look at this
gray area and decide yeah this is not really that bad it should where it should be okay with it and at the same
time you need to improve your algorithms to get those two thresholds as close to each other as possible and that's very
interesting right because it represents sort of like a research challenge for us to improve our machine learning algorithms say hey we won the gray area
of the things are uncertain too over time become as small as possible and
we're doing that and at the same time the gray area is still there and when when we have things in the gray area we
need to use some humans in the loop to understand what's going on uh-huh so if
Challenges
if Cora were to do a corner prize analogous to the next Netflix plot
surprise what would it be about what are some of the biggest challenges that you
face well there's in each of those dimensions
that I mentioned before there's there's challenges that are still not resolved
but I guess thinking of the Netflix Brydon is something that would be kind
of similar and I think it's very interesting and probably that an obvious
direction we would go is that for something like knowledge there's also a
problem which is similar to a network price of how do you get the right piece of content to the right person and
content is expressed in two ways right one is a content that you can consume so
that's an answer that you can read and you can enjoy you can learn from it and the other one is a question that you can
answer so both of those things how to route them to the right person and how
to optimize algorithm for those two things are at the core of what we're doing and they're very important for us
so I think we could think of like again drawing the analogy of the Netflix prize
of like question and answer recommendation being like a very interesting
topic that for us it's like a super interesting challenge it also connects
like many different dimensions on the different overlays that I was talking
about because it's not only about personalization but you also have to care about content quality right and you
have to care about those different aspects and how they feed into what the
users are going to be doing and reacting to short term but more importantly what
they're going to be reacting to long term I've talked about that in the past in some of my presentations like this serve
like tension between short term metrics and long term metrics and that's
something that a lot of companies have done the wrong thing and they've gone downhill because of that and it's really
important to understand for example in the context of content how to avoid
clickbait right and if you're optimizing for some things you're gonna get clicks sure but those clicks are gonna turn
into people not visiting your site ever again after a number of weeks so all
those things sort of like fit into this picture of like content recommendation
or knowledge recommendation how do you address the short term long term
Tradeoff
trade-off now maybe even in the context of a clickbait type of application so so
there's different things that go into it I would say that that that's one of the
most interesting research areas that I don't think it's been really solved even
in research literature because there's it's very hard to get enough good quality data sets to even do something
about it if you're if you're a research in academia and in the industry I mean
as far as I know from the people that I talk there's obviously different things that we're all doing but a holistic
approach to it is it's hard the one important thing is you do need to make
sure that you're running your a B test with right sort of metrics right because at
the end of the day you can be optimizing whatever you want in the lab and say oh it's a ranking problem I'm going to be
optimizing NBC G but the reality of that metric that you're optimizing in the lab
with your algorithm might not really correlate perfectly to what you want to
get and the product in that long term metric so first you need to make sure that you whatever you tune in you're in
the lab you run a b test long enough term with the right metric to understand
like what is the met what what are the effects that whatever you're doing have
on the users and then you kind of work backwards from that right once you have
the right metric on your a B test you know oh if I do this my users end up not coming back after two weeks what did I
do then you back you kind of work backwards from that and try to understand like what are the metrics in the lab that you
could have used to sort of like predict that kind of behavior in the kind of effect right so building regression
models from sort of like your easy to compute metrics which they're all gonna
be related to some kind of error or some kind of information retrieval precision
and recall whatever you will into the real world of usage I think that's
that's very important and then there's a there's a ton of other things that you can do once you understand those
dynamics in trying to define your training set in a way that actually mmm
defines the problem in the right way and and sometimes I have talked about this
also in the past people have this mistake of I need to use all the data
that I have and I need to use the raw data that I have and sometimes that's
not really the answer you might need to use some data and not others because some of the data that you might be
feeding into the into your model might be teaching them all the wrong thing or you might need to wake your data in a
way that some is more important than because it leads to longer term effects that you're interested on while other
might lead to a click but nothing else so there's there's a lot of sort of like
different details going into the recipe but again I don't think there is a very
holistic approach to it or not that I'm aware of okay one thing that that came to mind
Deep Learning
for me was and this is maybe going back to our discussion around deep learning
there is some research happening around our n ends and you know when the the
reinforcement or the score you know comes later and how the RNA and can optimize for you know this delayed
gratification so to speak and so you know maybe this is where you know if this gets sophisticated enough this is
where you get some benefit from the introducing the complexity of our n ends
where an otherwise simple model might come into play yeah that's definitely
true so multiples or approaches that have any sense of sequencing or time or
evolution over time to have some depth
some benefits and and and you can use them it's not only about a RN and another thing that comes to mind it's
some reinforcement learning approaches I mean the typical one of the typical ways
to deal with this is to use and some form of multi-armed bandit approach to
deal with the exploration exploitation trade-off it's it's more of like yeah you know I know that you're picking on
this but let me try to explore more things let me try to come up over time have you know my model converged to
something that is a global optimal rather than getting stuck on that local one where I am right now so yes you're
right I mean and some of the sequential RN ends with some form of memory and and
ability to sort of like remember different stages and sort of like end up
converging over time into a better optimal they're super interesting
before we before we get too far can you explain simply multi-armed
Multiarmed bandit
bandit yeah so the idea is pretty simple
I mean multi-armed bandit comes from this notion of you have the typical
image that people use is the slot machines in a casino you imagine that
you go into a casino and you have ten slot machines in front of you and you don't know which arm you should pull
that where the multi-armed bandit come from and you start trying one you say oh this one is giving me some interesting
prices but should I try another one because maybe the one that I have next to me it's actually better than this one
and how to deal with this dilemma of out of multiple arms that you could be pulling there's some that you have more
information about and you know with a degree of certainty how well they're
doing and there are others that you don't really know anything about them should you risk yourself and go into the
ones you don't know anything about them or should you just stick to the one that kind of works but maybe it's not the
optimal one so I think that's the whole point of the multi-armed bandit
approaches it's like they try to find a way in which you can have an optimal
policy to deciding whether you should continue pulling from the same arm or you should go to a different one and
there's there's a lot of literature on
on this in and you can read about it and
I usually joke about it there's a lot of literature about multi-armed bandit but
there's only one that actually works in practice but I don't know if I want to
give that away I mean it's it's it's pretty it's pretty well known in an
industry that comes from sampling is the easiest and sort of like more practical
approach to multi-armed bandit so I think that and I'm not giving too much away by saying that
right so what uh what what are you finding most exciting about machine
Most exciting thing about machine learning
learning right now obviously there is a ton of things going on there's deep
learning stuff there's the work that's happening around BOTS there's applying deep learning to NLP like you know given
everything that's going on like what what's the most exciting and and do you
get to apply that in your work and what's the most exciting thing that you're actually working on so I think
the most exciting thing for me it's almost a non-technical thing it's more
of a this thing coming from society as a whole that it's accepted as a given that
machine learning and AI is inevitably part of making a better future and I
think you know there are still so people that were argue about dangers and and about robots taking over and so on but I
think generally speaking society is convinced and it's pretty much you know
all bought in you know self-driving cars a couple years ago people thought we
were crazy about self-driving cars and now they're already being tested with people writing in them so so I think
this sort of like change in society and in mindset and people realizing that oh
machine learning is not really evil it can be it's a tool it can be used in my
benefit and it's something that I expect things to have to have so not very long
ago seeing something that was an algorithm or machine learning was like
whoa what's going on I'm losing control this is not something I like and now it's shifting to the opposites like you
expect applications you expect gadgets to have intelligence and to have machine
learning otherwise you're disappointed like oh my gosh I need to tell this phone everything I want the phone should
know what I want right so I think that's that's a very very interesting shift and
and it kind of connects a lot with some things were doing at Quora right in kora
we are very user focused and we want to we want to keep this warm feeling of
you're in the community you're sharing knowledge this is very important for you it's very important for the people but
you're gonna be surrounded by all this different algorithms that make your life much better and they protect you from
bad people and they protect you from horrible content that you don't want to read and they help you get your content
to the right people that want to read about it and they're going to be helped by it so this combination of silver the
warmth of community social aspects and knowledge but also surrounded by all
this different algorithms in a seamless way I think that's super exciting and it's something that you need to strike
the right balance but it's something that just a few years ago we wouldn't thought about because you know again
algorithms were the this cold evil thing that you kind of like wanted to stay
away from so I think that's that's a very interesting trend and something that I'm excited about mm-hmm we're
Favorite conferences
coming to the end of our time but I've got a couple more quick questions for you the first is you go to a lot of
conferences what are your favorite conferences in the space I would say I
go to a lot of conferences unfortunately especially now since my time as a VP of
engineering is pretty precious and I don't get that much time there's some
confidence that I have ties for a very long time and I keep going to them because I am very interested in the
content but also I'm interested in the community one of them is it's a small conference actually the it's the ACM
recommender systems conference that's a conference that is purely focus on personalization and recommendations and
I help start the whole thing I was a the general chair for that in 2010 back in
Barcelona and I kept kind of keep in touch it's in one of the interesting things about this community which I
think it's a little bit similar to for example kdd is that it's a very diverse
kind of audience and you don't get machine learning nips audience everyone
focus on the algorithm and you know squeezing 1% more or less pharmacy or
maa out of their algorithm there's a combination of algorithms but also
application and then user oriented research which I think connects to the
vision that I was saying right this connection between user orientation and algorithms it's very interesting so yeah
the ACM recommender systems conference which by the way is happening in Boston if anyone is listening from Boston or
wants to travel there this year is in the US and it's gonna be super interesting and when is it it's coming
up right yeah it's in September 15 so yeah in a few weeks we're gonna be there
and just to give an example I'm giving a tutorial with together with Deepak our
well from LinkedIn on all the latest research and all the evolution of
recommendation systems in industry and we're going to be giving a holistic perspective of me coming from Netflix
and now Cora and him having been at Yahoo and now reading machine learning at LinkedIn so it's gonna be sort of
like an overview of all this kind of machine learning techniques for
recommendations so that's that's an example of a small focus conference but
also with a very broad audience which I kind of enjoyed kdd which just happened
to be in San Francisco recently I like the community a lot and I think I can
find all of very interesting approaches in applications I usually yeah I'm very
application driven in my approach to machine learning so although I will I will read all the papers or not not all
sorry some papers from nips and ICML I I
tend to go to more sort of like application driven conferences and and there's also a lot
of small conferences that are organized now there are kind of local and focused
on the industry side of machine learning ml comp is one that comes to mind that I
attend regularly because I find the audience to be very interesting and very
engaging and it's a lot of practitioners from industry mixed together with a
bunch of researchers and that intersection I think it's it's really interesting mm-hmm great great and then
one more question that you're in a particularly good place to answer for us
and that is who are the people to follow the machine learning folks to follow on Quora oh that's a great question but we
have a lot of them so we've been doing actually a very strong push for this
product feature that we have with these sessions which is similar to an MA AMA and we brought in I would say like all
the top machine learning researchers to do some session in the past we've had
people like I mean most of the deep learning folks like young laocoön and
joshua banjo and we've had Andrew Inc we've had Peter Norbeck we've had a lot
of different researchers and I would say most of the authors of the famous
machine learning books like Kevin Murphy from Google and so on or we we had Ian
Goodfellow the main author of the deep learning book also recently so there's
like a good you I would say 50 people that you would follow we've also had
people that leave machine learning in different companies like Amazon we have
my friend Ralph furbish from Amazon or Joaquin from Facebook
so there's like a huge machine learning community in kora that it's very active and very strong so it's one of our
strongest areas right now so I would recommend people who are interested in
machine learning there's like a ton of knowledge there and growing so yeah
great great well chubby thank you so much for spending the time with us I
Outro
learned a ton and I'm sure the folks that listen well as well anything you'd
like to leave us with no I mean thanks for having me and it was great to share
a little bit of that knowledge in this different format which it's also a way
of spreading knowledge and I look forward to interacting with people especially on Quora I myself write a lot
of different answers on different topics in doing machine learning oh that's a good point before we go where can folks
find you how can folks engage with you I'm pretty public on Twitter as you
mentioned you you had seen a bunch of my tweets so I'm they can find me on Twitter on
Chama at X am eighty or on Korra I'm also very active so you can follow me on
Korra and message me there I usually keep a very active public profile so
it's not hard to find me and I have a pretty weird name and last name so it's like it's really to go into the wrong
direction if you if you google my name yeah alright great thanks so much have you yeah thank you Sam
alright everyone that's it for today's interview before we go a reminder that
this week in machine learning and AI and O'Reilly have partnered to offer one lucky listener a free pass to the
inaugural O'Reilly AI conference which will be held at the end of September in New York City you can enter via Twitter
or the twill Malaya comm website by doing one of the following three things the preferred way of entering is via
Twitter just follow at twimble AI Twi la i and retweet the contest tweet that
I'll pin to the account and post in the shownotes do those two things and you'll be entered if you're not on Twitter you can
sign up for my newsletter at twill Malaya com / newsletter and add a note please enter me in the additional
comments field finally if you're not on Twitter and you aren't interested in the newsletter no problem just go to the
contact form on - Malaya comm and send me a message with that form using AI
contest as the subject the drawing will be open to entries through September 1st
and I'll announce the winner on the September 2nd show good luck and hope to see you in New York thanks again for
listening

**Introduction**

- Podcast: TwiML Talk, discussing machine learning and AI.
- Guest: Xavier Amatriain, VP of Engineering at Quora, former leader of Netflix's machine learning recommendations team.

**Xavier's Background**

- Initially focused on signal processing and multimedia research.
- Transitioned to focus on machine learning, especially in understanding user preferences through data.
- Shifted from academia to Netflix, leading their machine learning recommendations team, and later to Quora as VP of Engineering.

**Key Discussions**

1. **Netflix Prize**
    
    - Netflix invested $1 million but didn't use the final winning solution directly.
    - The prize led to valuable research and learnings.
    - Final entry's complexity outweighed its benefits, though some components were used.
2. **Engineering Practical Machine Learning Systems**
    
    - Importance of balancing system complexity with the ability to innovate.
    - Avoiding unnecessary complexity helps in maintaining a fast pace of innovation.
    - The concept of 'algorithmic debt' – the cost of maintaining complex ML systems.
3. **Deep Learning Hype**
    
    - Xavier acknowledges deep learning's effectiveness in certain domains like NLP.
    - Warns against using deep learning as a default solution for all ML problems.
    - Emphasizes understanding the right tool for the task to avoid unnecessary complexity.
4. **Quora's Engineering Challenges**
    
    - Quora's focus on knowledge sharing and the importance of machine learning in their product.
    - Various ML applications in Quora, from content quality assessment to user behavior prediction.
    - The use of hybrid machine learning-human approaches, especially in content moderation.
5. **Challenges and Future Directions**
    
    - The balance between short-term metrics and long-term goals.
    - The challenge of optimizing content delivery to users.
    - The importance of using the right metrics and data in training models to align with long-term objectives.
6. **Exciting Aspects of Machine Learning**
    
    - Societal acceptance and expectation of ML/AI integration in products and services.
    - The blend of community warmth and algorithmic intelligence in products like Quora.
7. **Conferences and Community**
    
    - Favorite conferences include ACM Recommender Systems Conference, KDD, and ML Conf.
    - The value of conferences lies in their blend of algorithmic, application-driven content and community engagement.
8. **Influencers on Quora**
    
    - Quora hosts many ML experts and conducts AMAs with prominent figures in the field.
    - Xavier recommends following these experts for insights and discussions on ML topics.

**Conclusion**

- Xavier shares insights on the intersection of engineering, machine learning, and community-driven products.
- Emphasizes the importance of understanding the tools and methodologies in ML to apply them effectively in real-world applications.
- Highlights the growing integration of ML in everyday products and the societal shift in perception towards AI and ML.


----------
