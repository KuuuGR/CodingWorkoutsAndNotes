I discovered that the Lex Fridman Podcast covers some of the most interesting topics, and the people he interviews are incredible. Therefore, I've decided to use his channel as the initial subject for testing language data.
[Lex Clips](https://www.youtube.com/@LexClips/videos) -> Mostly Short
[Lex Fridman](https://www.youtube.com/@lexfridman/videos)

-----
--99--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--98--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--97--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--96--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--95--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--94--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--93--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--92--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--91--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--90--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--89--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--88--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--87--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--86--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--85--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--84--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--83--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--82--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--81--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--80--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--79--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--78--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--77--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--76--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--75--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--74--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--73--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--72--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--71--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--70--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--69--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--68--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--67--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--66--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--65--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--64--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--63--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--62--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--61--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--60--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--59--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--58--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--57--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--56--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--55--  https://www.youtube.com/@lexfridman/videos

-----
Date:
Link:
Transcription:

paste here

----------

-----
--54--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--53--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--52--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--51--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--50--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--49--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--48--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--47--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--46--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--45--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--44--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--43--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--42--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--41--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--40--

-----
Date:
Link:
Transcription:

paste here

----------

-----
--39--

-----
Date: 2018.04.19
Link:   [# Max Tegmark: Life 3.0 | Lex Fridman Podcast #1](https://www.youtube.com/watch?v=Gi8LUnhP5yU)
Transcription:


Introduction
as part of MIT course six as $0.99 artificial general intelligence I've gotten the chance to sit down with max
tegmark he is a professor here at MIT is a physicist spent a large part of his
career studying the mysteries of our cosmological universe but he's also
studied and delved into the beneficial possibilities and the existential risks
of artificial intelligence amongst many other things he's the co-founder of the
future of life Institute author of two books both of which I highly recommend first our Mathematica universe
second is life 3.0 he's truly an out-of-the-box thinker and fun
personality so I really enjoyed talking to him if you would like to see more of these videos in the future please subscribe and also click the little bell
icon to make sure you don't miss any videos also Twitter linked in AGI that
MIT that I do if you want to watch other lectures or conversations like this one
better yet go read Max's book life 3.0 chapter 7 on goals is my favorite it's
really where philosophy and engineer and come together and it opens with a quote by dusty s key the mystery of human
existence lies not and just stayin alive but in finding something to live for lastly I believe that every failure
rewards us with an opportunity to learn in that sense I've been very fortunate
to fail in so many new and exciting ways and this conversation was no different
I've learned about something called radio frequency interference or RFI look
it up apparently music and conversations from local radio stations can bleed into the audio that you're recording in such a
way that almost completely ruins that audio it's an exceptionally difficult sound source to remove so I've gotten
the opportunity to learn how to avoid RFI in the future during recording
sessions of also gotten the opportunity to learn how to use Adobe Audition and isotope rx6 to do some noise some
audio repair of course this is exceptionally difficult noise to remove I am an
engineer I'm not an audio engineer neither is anybody else in our group but we did our best
nevertheless I thank you for your patience and I hope you're still able to
enjoy this conversation do you think there's intelligent life out there in the universe let's open up with an easy question I
Are there intelligent life out there
have a lien minority of you here actually when I give public lectures Alfred asked for show of hands who
thinks there's intelligent life out there somewhere else and almost everyone put their hands up and when I ask why
they'll be like oh there's so many galaxies out there there's gonna be but
I'm a numbers nerd right so when you look more carefully at it it's not so
clear at all if we when we talk about our universe first of all we don't mean all of space did we actually mean I
We dont mean all of space
don't you can throw me in the universe if she wants behind you there it's we simply mean the spherical region of
space from which light has a time to reach us so far during the fourteen
point eight billion year 13.8 billion years since our Big Bang there's more space here but this is what we call a
universe because that's all we have access to mm-hmm so is there intelligent life here that's gotten to the point of
building telescopes and computers my guess is no actually no the probability
of it happening on any given planet is some number we don't know what it is and
what we do know is that the number can't be super-high because there's over a
billion earth-like planets in the Milky Way galaxy alone many of which are
billions of years older than Earth and aside from some UFO believers in other
reason is much evidence that any super 20 civilization has come here at all and so that's the famous Fermi paradox right
and then if you if you work the numbers what you find is that if you have no
clue what the probability is of getting life on a given planet could be 10 to the minus 10 and the
minus 20 or 10 minus to any power tensor equally likely if you want to be really
open-minded that translates into it being equally likely that our nearest neighbor is 10 to the 16 meters away 10
to the 17 meters away 10 to the 18 now by the time he gets much less than than
10 to the 16 already we pretty much know there is nothing else that's close and
when you get the other would have discovered us they yeah they would have been discovered as long or if they're really close we would have probably know
that some engineering projects that they're doing and if it's beyond 10 to the 26 meters that's already outside of
here so my guess is actually that there are we are the only life in here they've
gotten the point of building advanced tech which i think is very um puts a lot
of responsibility on our shoulders not screw up you know I think people who take for granted that it's okay for us
to screw up have an accident in a nuclear war or go extinct somehow because there's a star trek-like
situation out there with some other life forms are gonna come and bail us out and doesn't matters what I think they're
lulling us into a false sense of security I think it's much more prudent to say you know let's be really grateful
for this amazing opportunity we've had and make the best of it just in case it
Intelligent life
is down to us so from a physics perspective do you think intelligent
life so it's unique from a sort of statistical view of the size of the universe but from the basic matter of
the universe how difficult is it for intelligent life to come about the kind of advanced tech building life I in is
implied in your statement that is really difficult to create something like a human species well I think I think what
we know is that going from no life to having life that can do ARCA a level of
tech there's some sort of - going beyond that than actually settling our whole
universe with life there is some road major roadblock there which is
great filter as that's sometimes called which which tough to get through it's
either that roadblock is either beef behind us or in front of us I'm hoping
very much that it's behind us I'm super excited every time we get a new report
from NASA saying they failed to find any life on Mars like just awesome because that suggests
that the hard part maybe what maybe he was getting the first ribosome or or some some very low-level kind of
stepping stone so they were home free cuz if that's true then the future is
really only limited by our own imagination it'd be much luckier if it turns out that this level of life is
kind of a dime a dozen but maybe there is some other problem like as soon as a civilization gets advanced technology
within a hundred years they get into some stupid fight with themselves and poof yeah no that would be a bummer yeah so you've explored the mysteries of
Space and intelligence
the universe the cosmological universe the one that's sitting between us today
I think you've also begun to explore the other universe which is sort of the
mystery the mysterious universe of the mind of intelligence of intelligent life
so is there a common thread between your interest or in the way you think about space and intelligence oh yeah when I
was a teenager yeah I was already very fascinated by the biggest questions and
I felt that the two biggest quite mysteries of all in science where our universe out there and our universe in
here yeah so it's quite natural after having spent a quarter of a century on
my career thinking a lot about this one I'm now indulging in the luxury of doing research on this one it's just so cool I
feel the time is right now for you Trane's greatly deepening our
understanding of this just start exploring this one yeah because I think I think a lot of people view intelligence as something mysterious
that can only exist and biological organisms like us and therefore dismiss
all talk about artificial general intelligence is science fiction but from my perspective as a physicist
you know I am a blob of quarks and electrons moving around in a certain
pattern and processing information in certain ways and this is also a blob of quarks and electrons
I am NOT smarter than the water bottle because I'm made of different kind of works I'm made of up quarks and down
quarks exact same kind as this it's a there's no secret sauce I think in me
it's all about the pattern of the information processing and this means
that there's no law of physics saying the way that we can't create technology which can have helped us by being
incredibly intelligent and helped us crack mysteries that we couldn't in other words I think we really only seen the tip of the intelligence iceberg so
far yeah so the perceptron ium yeah so
you can't go in this amazing term it's a hypothetical state of matter sort of thinking from a physics perspective what
is the kind of matter that can help as you're saying a subjective experience emerged consciousness emerge so how do
you think about consciousness from this physics perspective very good question
so again I think many people have
underestimated our ability to make progress on this and by convincing
themselves it's hopeless because somehow we're missing some ingredient that we
need or some new consciousness particle or whatever I happen to think that we're
not missing anything and and that it's or not the interesting thing about consciousness that gives us this amazing
subjective experience of colors and sounds and emotions and so on is rather
something at the higher level about the patterns of information processing that's why that's why I am like to think
about this idea of perceptron Neum what does it mean for an arbitrary physical system to be conscious in terms of what
its particles are doing or its information is doing I don't think I don't hate carbon chauvinism you know
this attitude you have to be made of carbon atoms to be smart or conscious something about the information
processing yes kind of matter performs yeah and you know yeah I have my favorite equations here describing
various fundamental aspects of the world I feel that I think one day maybe someone who's watching this will come up
with the equations that information processing has to satisfy to be conscious I'm quite convinced there is
big discovery to be made there yeah because let's face it sumit we know that
some information processing is conscious because we are yeah conscious but we
also know that a lot of information processing is not conscious like most of the information processing happening in your brain right now is not conscious
there is like 10 megabytes per second coming in and even just through your visual system you are not conscious
about your heartbeat regulation or or most things by even even like if I just
ask you to like read what it says here you look at it and then oh now you know what it said but you don't aware of how
the computation actually happened you're like the your consciousness is like the CEO that got an email at the end we
leave with a final answer so what is it that makes the difference I think that's
a both of us great science mystery we're actually starting it a little bit in my lab here at MIT but I also I think it's
just a really urgent question the answer for started I mean if you're an emergency room doctor and you have an
unresponsive patient coming in and wouldn't it be great if in addition to having a CT scan
you had a consciousness scanner mm-hmm that could figure out whether this person is actually having locked-in
syndrome or is actually comatose and in the future imagine if we build the
robots or the machine that we can have really good conversations which I think
it's mostly very likely to happen right wouldn't you want to know like if your home helped a robot is actually
experiencing anything or just like a zombie I mean would you prefer what
would you prefer would you prefer that it's actually unconscious so that you don't have to feel guilty about switching it off or giving me boring
chores or would you prefer well the certainly would we would prefer
I would prefer the appearance of consciousness but the question is whether the appearance of consciousness is different
than cost consciousness itself and sort of ask that as a question yeah do you
think we need to you know understand what consciousness is solve the hard problem of consciousness in order to
build something like an a GI system no I don't think that and I think we we will
probably be able to build things even if we don't answer that question but if we want to make sure that what happens is a good thing we better solve it first so
it's a wonderful controversy you're raising there there where you have basically three points of view about the
heart problem sir there are two different points of view that both conclude that the hard problem of
consciousness is BS you're on one hand you have some people like Daniel Dennett who say this is our consciousness is
just BS because consciousness is the same thing as intelligence there's no difference so anything which acts
conscious is conscious just like like we are and then there are also a lot of
people including many top AI researchers I know you say all conscience is just because of course machines
should never be conscious tonight they're always gonna is gonna be zombies never have to feel guilty about how you
treat them and then there's a third group of people including Giulio Tononi
for example and and another just of chakana brothers
I would put myself Falls on this middle camp who say that actually some information processing is conscious and
in some is not so let's find the equation which can be used to determine which it is and I think we've just been
a little bit lazy kind of running away from this problem for a long time it's
been almost taboo would even mention the c-word a lot of circles because look but
we should stop making excuses this is a science question and we can the rock
there are ways we can even test test any theory that makes predictions for this and coming back to this helper robot I
mean so you said you'd want to help a robot to certainly act conscious and treat you like to have conversations
with us I think so wouldn't you would you feel would you feel a little bit creeped out if you realize that it was just glossed up the tape recorder they
know there was just Sambi and there's some faking emotion would you prefer that it actually had an experience or
Does consciousness have an experience
will you prefer that it's actually not experiencing anything so you feel you don't have to feel guilty about what you
do to it it's such a difficult question because you know it's like when you're in a relationship and you say well I
love you and the other person I love you back it's like asking well do they really love you back or are they just
saying they love you back do you don't you really want them to actually love you I it's hard to it's
hard to really know the difference between everything seeming like there's
consciousness present there's intelligence present there's affection passion love and and actually being
there I'm not sure do you have a question let's just like to make it a bit more pointed so Mass General
Hospital is right across the river right yes suppose suppose you're going in for a medical procedure and they're like you
know furnish the agent what we're gonna do is we're gonna give you a muscle relaxant so you won't be able to move and you're
gonna feel excruciating pain during the whole surgery but you won't be able to do anything about it but then we're gonna give you this drug that erases
your memory of it would you be cool about that no what
difference that you're conscious about it or not if there's no behavioral change right right that's a really
that's a really clear way to put it that's yeah it feels like in that sense experiencing it is a valuable quality so
actually being able to have subjective experiences at least in that cases is
valuable and I think we humans have a little bit of a bad track record also of
making these self-serving arguments that other entities aren't conscious you know
people often say oh these animals can't feel pain right it's okay to boil lobsters because we asked them if it
hurt and they didn't say anything and now there was just the paper out saying lobsters did do feel pain when you boil them and they're banning it in
Switzerland it and and we did this with slaves too often and say oh they don't mind they don't maybe or aren't
conscious or women don't have souls or whatever I'm a little bit nervous when I hear people just take as an axiom that
machines can't have experience ever I think this is just this really fascinating science question is what it
is let's research it and try to figure out what it is it makes the difference between unconscious intelligent behavior and
conscious intelligent behavior so in terms of so if you think of a Boston
Dynamics human robot being sort of with a broom being pushed around the its
starts it starts pushing on his consciousness question so let me ask do you think an AGI system like a few
neuroscientists believe needs to have a physical embodiment needs to have a body
or something like a body no I don't think so you mean to have to have a
conscious experience to have consciousness I do think it helps a lot
to have a physical embodiment learn the kind of things about the world that they're important to us humans for sure
but I don't think bah diamond is necessary after you've learned it just have the experience
think about when you're dreaming right your eyes are closed you're not getting
any sensory input you're not behaving or moving in any way but there's still an experience there right and so there's
clearly the experience that you have when you see something tool in your dreams isn't coming from your eyes it's just the information processing itself
Selfpreservation instinct
in your brain which is that experience right but if I put another way I'll say
because it comes from neuroscience is the reason you want to have a body and a physical something like a physical like
a you know a physical system is because you want to be able to preserve something in order to have a self you
could argue would you you need to have some kind of embodiment of self to want
to preserve well now we're getting a little bit on Drop amorphic that's inter
and super more fising things miss Mamie tossing like self-preservation instincts I mean we are evolved organisms right
right so Darwinian evolution endowed us and other involve all organism with the
self-preservation instinct as those that didn't have those self-preservation genes are clean out of the gene pool
right right but if you build an artificial general intelligence the mind
space that you can design is much much larger than just a specific subset of minds that can evolve that happen so
they CERN a GI mind doesn't necessarily have to have any self-preservation instinct it also doesn't necessarily
Fear of death
have to be so individualistic as I'd like imagine if you could just first of all it or we're also very afraid of
death you know I suppose you could back yourself up every five minutes and then your airplane is about to crash you like
shucks I'm just counted I'm gonna lose the last five minutes of experiences it's my last cloud backup you're dying
you know it's not this big a deal or if we could just copy experiences between
our minds easily like me which we could easily do if we were silicon based right
then maybe we would feel a little bit more like a hive mind actually but maybe is
he so so there's a so I don't think we should take for granted at all that AG I will have to have any of those sort of
competitive as alpha male instincts right on the other hand you know this is
really interesting because I think some people go too far and say oh of course we don't have to have any concerns
either that advanced AI will have those instincts because we can build anything
you want that there's there's a very nice set of arguments going back to Steve Omohundro and Nick Bostrom and
others just pointing out that when we build machines we normally build them with some kind of goal you know win this
chess game drive this car safely or whatever and as soon as you put in a goal into machine especially if it's
kind of open-ended goal and the machine is very intelligent it'll break that down into a bunch of sub goals and one
of those gold will almost always be self-preservation because if it breaks or dies in the process it's not gonna
accomplish the goal right like suppose you just build a little you have a little robot and you tell it to go down the Starmark get here and and and get
you some food make your cookin Italian dinner you know and then someone mugs it and tries to break it down the way that
robot has an incentive to not destroy it and defend itself or run away because
otherwise it's gonna fail and cooking you dinner it's not afraid of death but it really wants to complete the dinner
cooking gold so it will have a self-preservation instinct continue being a functional Asian yeah and and
similarly if you give any kind of warm and they she's go to an AGI
it's very likely they want to acquire more resources so it can do that better and it's exactly from those sort of sub
goals that we might not have intended that but some of the concerns about AGI safety come you give it some goal which
seems completely harmless and then before you realize it it's also trying
to do these other things which you didn't want it to do and it's moment be smarter than us so so lastly and let me
Intelligence and consciousness
pause just because I in a very kind of human centric way see
fear of death is a valuable motivator haha so you don't think do you think
that's an artifact of evolution so that's the kind of mind space evolution created they were sort of almost
obsessed about self-preservation kind of genetic well you don't think that's necessary to be afraid of death so not
just a kind of sub goal of self-preservation just so you can keep doing the thing but more fundamentally
sort of have the finite thing like this ends for you at some point the
interesting do I think it's necessary before what precisely for intelligence
but also for consciousness so for those for both do you think really like a
finite death and the fear of it is important so before I can answer well
before we can agree on whether it's necessary for intelligence or for consciousness we should be clear or how we define those two words because share
a lot of really smart people to find them in very different ways I was in this on this panel and with AI experts
and they couldn't they couldn't agree on how to define intelligence even so I define intelligence simply as the
ability to accomplish complex goals I like your broad definition because again I don't want to be a carbon
chauvinist right and in that case no it
certainly certainly doesn't require fear of death I would say alpha go alpha zero is quite intelligent
I don't think alpha zero has any fear of being turned off because it doesn't understand the concept of that even and
and similarly consciousness I mean you could certainly imagine very simple kind
of experience if you know if certain plants have any kind of experience I don't think they were afraid of dying if
there's nothing they can do about it anyway much so there wasn't that much value and but more seriously I think if
you ask not just about being conscious but maybe having what you would
we might call an exciting life for you feel passion and I didn't really appreciate the little things maybe there
but somehow maybe there perhaps it does help having having my backdrop today it's finite you know let's let's make
the most of this this live to the fullest so if you if you knew you were gonna slip forever if you think you
would change your yeah in some perspective it would be an incredibly
boring life living forever so in the sort of loose subjective terms that you
said of something exciting and something in this that other humans would understand I think is yeah it seems that
the the finiteness of it is important well the good news I have for you then is based on what we understand about
cosmology everything is in our universe is Pro ultimately probably finite alone
although pay crunch or bit or big what's to expand anything yeah we couldn't have a Big Chill or a Big Crunch or a big rip
or that's the big snap or death bubbles all over more than a billion years away
so we should we certainly have vastly more time than our ancestors thought but
there is still it's still pretty hard to squeeze in an infinite number of compute
cycles even though there are some loophole let's just might be possible but I think I you know some people like
to say that you should live as if you're about you're gonna die in five years or something that sort of optimal maybe
it's a good it subs we should build our civilization as if it's all finite to be
on the safe side right exactly so you mentioned in defining intelligence as
the ability solve complex goals where would you draw a line how would you try
to define human level intelligence and superhuman level intelligence where this
consciousness part of that definition no consciousness does not come into this definition so so I think your
AI
intelligence is it's a spectrum but there are very many different kinds of goals you can have you can have a goal to be a good chess player a good goal
player a good car driver a good investor good poet etc so
intelligence that bind by its very nature isn't something you can measure but it's one number overall goodness no
no there are some people who are more better at this some people are better than that um right now we have machines
that are much better than us at some very narrow tasks like multiplying large
numbers fast memorizing large databases playing chess playing go and soon
driving cars but there's still no machine that can match a human child in
general intelligence but but artificial general intelligence AGI in the name of your course of course that is by its
very definition the the quests the build a mission in seen that can do everything as well as we can up to the old holy
grail of AI from from back to its inception and then 60s if that ever
happens of course I think it's gonna be the biggest transition in the history of life on earth but it but it doesn't
necessarily have to wait the big impact about until machines are better than us at knitting the really big change
doesn't come exactly the moment they're better than us at everything the really big change comes first there
big changes when they start becoming better at us at doing most of the jobs that we do because that's it can takes
away much of the demand for human labor and then the really whopping change
comes when they become better than us at AI research right right because right
now the timescale of AI researcher is limited by the human research and
development cycle of years typically at all along the tape from one release of some software or iPhone or whatever to
the next but once once we have once Google can replace 40,000 engineers by
40,000 equivalent pieces of software or whatever right then that doesn't there's
no reason that has to be years it can be in principle much faster and the timescale of future progress in AI and
also all of science and technology will will be driven by machines not so it's this point simple point which
lives right this incredibly fun controversy about whether it can be an
intelligence explosion so-called singularities Vernor Vinge called it that the idea is articulated by IJ good
obviously way back fifties but you can see Alan Turing and others thought about
it even earlier not did you ask me what
exactly what I define England's yeah so this the the glib answer is it to say
something which is better than us at all cognitive tasks will look better than any human and all cognitive tasks but
the really interesting bar I think goes a little bit lower than that actually it's when they can when they're better
than us it AI programming and can a general learning so that they can can if
they want to get better than I said anything by just studying so they're better is a keyword and better as
towards this kind of spectrum of the complexity of goals it's able to accomplish yeah so another way to so no
and that's certainly a very clear definition of human law so there's it's
almost like a sea that's rising you could do more and more and more things as a graphic that you show it's really
nice way to put it so there's some Peaks that and there's an ocean level elevating and you saw more and more
problems but you know just kind of to take a pause and we took a bunch of questions and a lot of social networks
and a bunch of people asked a sort of a slightly different direction on creativity and and things like that
perhaps aren't a peak the it's you know
human beings are flawed and perhaps better means having being a having contradiction being fought in some way
so let me sort of yeah start and start easy first of all so you
have a lot of cool equations let me ask what's your favorite equation first of all I know they're all like your
Quantum Mechanics
children but like which one is that it's the master key of
want the mechanics of the microworld this equation to protect you like everything to do with atoms molecules
and all that we have yeah so okay it's a
quantum mechanics is certainly a beautiful mysterious formulation of our world so I'd like to sort of ask you
just as an example it perhaps doesn't have the same beauty as physics does but
in mathematics abstract the Andrew Wiles who proved the firm as last theta so he
just saw this recently and it kind of caught my eye a little bit this is three hundred fifty eight years after it was
conjectured so this very simple formulation everybody tried to prove it everybody failed and say here's this guy
comes along and eventually it proves it and then fails to prove it and proves it
again in 94 and he said like the moment when everything connected into place the
in an interview said it was so indescribably beautiful that moment when you finally realized the connecting
piece of two conjectures he said it was so indescribably beautiful it was so
simple and so elegant I couldn't understand how I'd missed it and I just stared at it in disbelief for twenty
minutes then then during the day I walked around the department and at Keamy keep coming back to my desk
looking to see if it was still there it was still there I couldn't contain myself I was so excited it was the most
important moment on my working life nothing I ever do again will mean as much so that particular moment and it
kind of made me think of what would it take and I think we have all been there
at small levels maybe let me ask have you had a moment like that in your life
where you just had an ideas like wow yes I wouldn't
self and the same breath as Andrew wilds but I've certainly had a number of um
aha moments mo when I realized something very cool about physics just as
The Future
completely made my head explode in fact some of my favorite discoveries I made later I later realize if they had been
discovered earlier someone who sometimes got quite famous for it so I find this too late for me to even publish it but
that doesn't diminish in any way an emotional experience you have when you realize it like yeah Wow yeah
so what would it take and at that moment that wow that was yours in a moment so
what do you think it takes for an intelligent system and a GI system an AI
system to have a moment like that that's a tricky question because there are actually two parts to it right one of
them is cannot accomplish that proof it cannot prove that you can never write a
to the N plus B to the N equals 3/2 that equals e to the N for all integers well
etc etc when when n is bigger than 2 the
simply in any question about intelligence can you build machines that are that intelligent and I think by the
time we get a machine that can independently come up with that level of proofs probably quite close to AGI the
second question is a question about consciousness when will we will willins
how likely is it that such a machine will actually have any experience at all as opposed to just being like a zombie
and would we expect it to have some sort of emotional response to this or
anything at all I can to human emotion work no but when it accomplishes its
machine goal it did the views it to somehow it's something very positive and right and and sublime and and and and
deeply meaningful I would certainly hope that if in the future we do create
machines that are our peers or even our dis
since yeah I would certainly hope that they do have this sort of sublime sublime appreciation of life in a way my
absolutely worst nightmare would be that
in at some point in the future the distant future maybe I cost much as
teeming with all this post biological life doing all the seemingly cool stuff and maybe the fun last humans or the
time era our species eventually fizzles out we'll be like well that's ok because
we're so proud of our descendants here and look what I like my most nightmare
is that we haven't solved the consciousness problem and we haven't realized that these are all the zombies
they're not aware of anything anymore than the tape recorders it has an any kind of experience so the whole thing
has just become a play for empty benches that would be like the ultimate zombie apocalypse me III would much rather in
that case that we have these beings which just really appreciate how how
amazing it is and in that picture what would be the role of creativity we had a few people
Creativity
ask about creativity do you think when you think about intelligence I mean
certainly the the story told the beginning of your book involved you know creating movies and so on yeah sort of
making making money you know you can make a lot of money in our modern world with music and movies so if you are
intelligent system you may want to get good at that yeah but that's not necessarily what I mean by creativity is
it important on that complex goals where the sea is rising for there to be
something creative creative or am I being very human centric and thinking
creativity somehow special relative to intelligence my hunch is
that we should think your creativity simply as an aspect of intelligence and
[Music] we we have to be very careful with with human vanity we had we have this
tendency to very often one and say as soon as machines can do something we try to diminish it that's a long but that's
not like real intelligence you know you're the night trader or there were or this or that or the other thing maybe if
we ask ourselves to write down a definition of what we actually mean by being creative what we mean by Andrew
Wiles what he did there for example don't we often mean that someone takes you very unexpected leap mm-hmm it's not
like taking feet 573 and multiplying in my 224 by justice step of
straightforward cookbook like rules right if this you may be making you even
make a connection between two things that people have never thought was connect very surprising or something like that I think I think this is an
aspect of intelligence and this is some actually one of the most important aspect of it maybe the reason we humans
are tend to be better at it than traditional computers is because it's something that comes more naturally if
you're a neural network then if you're a traditional logic gate based computer
machine you know we physically have all these connections and you activate here
activator here activate here ping you know I my hunch is that if we ever build
a machine where you could just give it the task hey hey you say hey you know I
just realized that I have I want to travel around the world instead this
months can you teach my eight a GI course for me and it's like ok I'll do it and it does everything that you would
have done and they provides us and so yeah that that would in my mind involve a lot of creativity yeah so I had such a
beautiful way to put it I think we do try to grab grasp at the you know the
definition of intelligence is everything we don't understand how how to build so
like so we as humans try to find things well that we have on machines don't happen maybe creativity is just one of
the things one of the words we use to describe that that's really interesting where to put it out think we need to be that defensive I
don't think anything good comes out of saying oh we're somehow special you know I it's contrariwise there are many
examples in history of we're trying to pretend that were somehow superior to
all other intelligent beings has led the pretty bad results right
Nazi Germany they said that they were somehow superior to other people today we still do a lot of cruelty to animals
by saying that we're social superiors and how and the other they can't feel pain slavery was justified by the same kind
of really weak weak arguments and and I don't think if we actually go ahead and
build artificial general intelligence it can do things better than us I don't
think we should try to found our self-worth on some sort of bogus claims
of superiority in in terms of our intelligence I think it's we shouldn't
stand Joe find our calling and then the
meaning of life from from experiences that we have right you know I can have I can have very meaningful experiences
even if there are other people who are smarter than me you know when I go to
faculty meeting here and I was talking about something that I certainly realize oh boy he has a Nobel Prize he has a
Nobel Prize he has no pride I don't have what does that make me enjoy life any less or would enjoy talking those people
less of course not see my and the contrariwise I I feel very honored and
privileged to get to interact with with other very intelligent beings that are
better than me a lot of stuff so I don't think there's any reason why we can't have the same approach with with
intelligent machines that's a really interesting so people don't often think about that they think about when there's
Intelligent Machines
going if there's machines that are more intelligent you naturally think that that's not going to be a beneficial type
of intelligence you don't realise it could be you know like peers of Nobel Prizes that
that would be just fun to talk with and they might be clever about certain topics and you can have fun having a few
drinks with them so well another example is we can all relate to it why it
doesn't have to be a terrible thing to be impressed the friends of people are even smarter than us all around is when
when you and I were both two years old I mean our parents were much more intelligent than us right here worked
out okay yeah because their goals were aligned with our goals yeah and that I think is really the
number one T issue we have to solve its value align the value alignment problem
exactly because people who see too many Hollywood movies with lousy science
fiction plot lines they worry about the wrong thing right they worry about some machines only turning evil it's not
malice they wish that the issue probably concerned its competence by definition
intelligent makes you makes you very competent if you have a more intelligent
goal playing mr. computer playing as the less intelligent one and when we define
intelligence is the ability to accomplish go winning right it's gonna be the more intelligent one that wins my
and if you have a human and then you have an AGI and that's more intelligent
in all ways and they have different goals guess who's gonna get their way right so I was just reading about I was
just reading about this particular rhinoceros species that was driven
extinct just a few years ago bummer is looking at this cute picture mommy run oestrus with it's it's child you know
and why did we humans private extinction wasn't because we were evil Rhino haters
right as a whole it was just because we our goals weren't aligned with those of the rhinoceros and it didn't work out so
well for the rhinoceros because we were more intelligent right so I think it's just so important that if we ever do
build AGI before we we have to make sure that it it learns
to understand our goals that it adopts our goals and it retains those goals so
Human Values
the cool interesting problem there is being able us as human beings trying to
formulate our values so you know you could think of the United States Constitution as a as a way that people
sat down at the time a bunch of white men but which is a good example I should
we should say they formulated the goals for this country and a lot of people agree that those goals actually hold up
pretty well that's an interesting formulation of values and failed miserably in other ways so for the value
alignment problem and a solution to it we have to be able to put on paper or in
in in a program human values how difficult do you think that is very but
it's so important we really have to give it our best and it's difficult for two separate reasons there's the technical
value alignment problem of figuring out just how to make machines understand our
goals adopt them and retain them and then there's a separate part of it the philosophical part whose values anyway
and since we it's not like we have any great consensus on this planet on values how what mechanism should we create them
to aggregate and decide okay what's a good compromise right at that second discussion can't this be left the tech
nerds like myself right that's right and if we refuse to talk about it and then
AGI gets built who's gonna be actually making the decision about whose values it's gonna be a bunch of dudes and some
tech company yeah yeah and are they necessarily - it's it's so
representative of all humankind that we want to just entrusted to them or they even is uniquely qualified to speak the
future human happiness just because they're good at programming any I I'd much rather have this be a really inclusive conversation but do you think
Is it possible
it's possible sort of so you create a beautiful vision that includes so the
diversity cultural diversity and various specs on discussing rights freedoms human dignity but how hard is it to come
to that consensus do you think it's certainly a really important thing that we should all try to do but do you think
it's feasible I I think there's no better way to guarantee failure than to
try to refuse to talk about it or or refuse to try and I also think it's a really bad strategy to say okay let's
first have a discussion for a long time and then once we reach complete consensus then we'll try to load it into
the Machine know it we shouldn't let perfect be the enemy of good instead we
should start with the kindergarten ethics - pretty much everybody agrees on and put that into our machines now we're
not doing that even look at the you know anyone who builds this passenger aircraft wants it to never under any
circumstances fly into a building or mountain right yet the September 11 hijackers were able to do that and even
more embarrassingly you know and that he has Lubitz this depressed Germanwings pilot when he flew his
passenger jet into the Alps killing over a hundred people he just told the autopilot to do it he told the freaking
computer to change the altitude 200 meters and even though it had the GPS maps everything the computer was like
okay no so which we should take those very basic values though where the
problem is not that we don't agree that maybe the problem is just we've been too lazy to try to put it into our machines
and make sure but from now on air airplanes will just which all have computers in them but we'll just never
just refuse to do something like that go into safe mode maybe lock the cockpit door or than here at the airport and and there's so much
other technology in our world as well now where it's really quite becoming quite timely to put in some sort of very
basic values like this even in cars we were have enough vehicle terrorism
attacks by now of you love different trucks and bands into pedestrians that it's not at all a crazy idea to just
have that hardwired into the car just yeah there are a lot of there's always gonna be people who for some reason want
to harm others most of those people don't have the technical expertise to figure out how to work around something like that so if
the car just won't do it it helps it let's start there so there's a lot of that's a great point so not not
Cellular automata
chasing perfect there's a lot of things that a lot that most of the world agrees on yeah and this look there let's start
there and and then once we start there we'll also get into the habit of having these kind of conversations about okay
what else should we put in here and I have these discussions this should be a gradual process then great so but that
also means describing these things and describing it to a machine so one thing we had a few conversation
was Stephen Wolfram I'm not sure if you're familiar with Stephen but yeah I know quite well so he is you know he
played you know works with a bunch of things but you know cellular automata are these simple computable things these
computation systems and he kind of mentioned that you know we probably have already within these systems already
something that's AGI meaning like we
just don't know it because we can't talk to it so if you give me this chance to
try to try to release form a question out of this is I think it's an
interesting idea to think that we can have intelligent systems but we don't know how to describe something to them
and they can't communicate with us I know you're doing a little bit work an explainable AI trying to get AI to
explain itself so what are your thoughts of natural language processing or some
kind of other communication how how does the AI explain something to us how do we explain something to it to machines or
you think of it differently so there are two separate parts to your question
there are them one of them has to do with communication which is super interesting you don't get that insect
the other is whether we already have AGI but we just haven't noticed it yeah right there I beg to differ right and
don't think there's anything in any cellular automaton or anything or the internet itself or whatever that has
artificial it didn't really do exactly everything we humans can do better I think today if
the day that happens when that happens we will very soon notice we will
probably notice even before andif because in a very very big way but for the second part though sorry so the
because you you have this beautiful way to formulating consciousness as as a you
Information processing
know as information processing you can think of intelligence and information processing and this you can think of the entire universe there's these particles
and these systems roaming around that have this information processing power you don't you don't think there is
something with the power to process information in the way that we human
beings do that's out there that that needs to be sort of connected to it
seems a little bit philosophical perhaps but there's something compelling to the idea that the power is already there
would you know yes the focus should be more on these I'm being able to communicate with it mhm well I agree
that that and some in a certain sense the hardware processing power is already
out there because our universe itself can think of it as being a computer
already right it's constantly computing what water waves have evolved the water waves and the river Charles and how to
move the air molecules around that s Lloyd has pointed out my colleague here that you can even in a very rigorous way
think of our entire universe as being a quantum computer it's pretty clear that our universe supports this amazing
processing power because you can even the within this physics computer that we
live in right we can even build actually laptops and stuff so clearly the power is there it's just that most of the
compute power that nature has it's in my opinion kind of wasting on boring stuff like simulating yet another ocean wave
somewhere where no one is even looking right so in a sense of what life does what we are doing when we build
computers is where we channeling all this compute that nature is doing anyway
into doing things that are more interesting than just yet another ocean wave you know and let's do something
cool here so the raw hardware power and sherbet and then and even just like
computing what's gonna happen for the next five seconds in this water ball you know it takes in a ridiculous amount of
compute if you do it on a human computer in yeah this water ball was did it but that does
not mean this water bottle has AGI and because AGI means it should also be able
to like have written my book during his interview yes and I don't think it's just communication problems as far as
you know don't think it can do it and other Buddhists say when they watch the
Communication
water and that there is some beauty that there's some depth and being sure that
they can communicate with communication that's also very important here because I mean look part of my job is being a
teacher and I know some very intelligent professors even who just have a better
hard time communicating they come up with all these brilliant ideas but but
to communicate with somebody else you have to also be able to simulate their own mind yes and pettite build well
enough and understand model of their mind that you can say things that they will understand and that's quite
difficult and that's why today it's so frustrating if you have a computer that makes some cancer diagnosis and you ask
it well why are you saying I should have a surgery if it and if they don't know can only reply or I was trained on five
terabytes of data and this is my diagnosis boop boop beep beep yeah I
didn't doesn't really instill a lot of confidence right right so I think we have a lot of work do one on
communication there so what kind of what kind of I think you're doing a little
bit work and explainable eh uh yeah what do you think are the most promising avenues is it mostly about sort of the
Alexa problem of natural language processing of being able to actually use human interpretable methods of
communication so being able to talk to a system and talk back to you or is there some more fundamental problems to be
solved I think it's all of above human the natural language processing is obviously important but they're also
more nerdy fundamental problems like if you if you take you play chess
mmm I have to give this Paris key when
did you learn Russian nobody watching papyrus key I talk after the back more people can you teach yourself Russian to
Tao what amalgam of bills of sim through dinner Wow but I would see languages do you
know wow that's really impressive I've had some contact base but my point was if
you play chess but you have you looked at the alpha zero games there are the actual games now just checking out some
of them are just mind-blowing really beautiful and if you ask how did it do
that you got that talk to them is hassabis I know others from beef mine all they will
ultimately be able to give you is big tables of numbers matrices that define the neural networking and you can stare
at these know people's numbers till your face turned blue and it's you know I can understand much about why it made that
move and even if you have a natural language processing that can tell you in
human language about all five seven points to eight still not gonna really help so I think think there's a whole
spectrum of a fun challenge they're involved in and taking a computation that does intelligent things and
transforming me into something equally good equally intelligent but it's more
understandable and I think that's really valuable because I think as we put
machines in charge of evermore infrastructure in our world the power grid the trading on the stock market
weapons systems and so on it's absolutely crucial that we can trust these a is a do or I want and
trust really comes from understanding all right in a very fundamental way and
that's why I'm that's why I'm working on this because I think the more if we're gonna have some hope of ensuring that
machines have adopted our goals and that they're gonna retain them that kind of trust and
thank you needs to be based on things you can actually understand preferably even make it perfectly to improve
theorems on even with a self-driving car right if someone just tells you it's been trained on tons of data and I never
crashed it's it's less reassuring than if someone actually has a proof maybe it's a computer verified proof but still
it says that under no circumstances is this car just gonna swerve into oncoming traffic and that kind of information
helps will build trust and build the alignment the alignment of goals the at
least awareness that your goals your values are aligned and I think even a very short term if you look at her you
know that today right this absolutely pathetic state of cybersecurity that we have when it's or is it three billion
yahoo accounts which are packed almost every American's credit card and so on
you know it's why is this happening it's ultimately happening because we have
software took nobody fully understood how it worked that's why the bugs hadn't
been found right now and I think AI can be used very effectively for offense for
hacking but it can also be used for defense know hopefully automating
verifiability and creating is systems that are built in different
so you can actually prove things about them right and it's it's important so speaking of software that nobody
understands how it works of course a bunch of people asked by your paper about your thoughts of why does deep and
cheap learning work so well that's the paper but what what are your thoughts on deep learning these kind of simplified
models of our own brains have been able to do some successful perception work
pattern recognition work and now with alpha zero and so on do some some clever things what are your thoughts about the
promise limitations of this piece great I think there are a number of very
important insights very important lessons we can already draw from these kind of successes one of them is when
you look at the human brain you see it's very complicated a tenth of eleven neurons and there are all these different kinds of neurons and
yadda-yadda and there's been a long debate about whether the fact that we have dozens of different kinds is actually necessary for intelligence
which a now I think quite convincingly answer that question no it's enough to
have just one kind if you look under the hood of alpha zero there's only one kind of neuron and it's ridiculously simple a
simple mathematical thing so it's it's not the it's just like in physics it's not the D if you have a gas with waves
in it it's not the detailed nature of the molecule the matter it's the collective behavior or somehow it
similarly it's it's it's this higher-level structure of the network
that matters not that you have twenty guys I think whom our brain is such a complicated mess because it wasn't
devolved just to be intelligent it was evolved to also be self assembling right
and self repairing right and evolutionarily attainable matches and so
on yeah so I think it's pretty my my hunch is that we're gonna understand how to build a GI before we fully understand
how our brains work just like we we understood how to build flying machines long before we were able to build a
mechanical work bird yes are you going names you're given that the example
exactly of mechanical birds and airplanes yeah my plans do a pretty good job of flying without really mimicking bird
flight and even now after 100 is 100 years later did you see the TED talk with a mr. mechanical bird you mentioned
it's amazing but even after that right we still don't fly in mechanical birds because it turned out the way we came up
with but simpler and it's better for our purposes and I think it might be the same there that's one lesson and another
lesson it is one what did when our paper was about well first we wife is a physicist
thought it was fascinating how there is a very closed mathematical relationship actually between our artificial neural
networks and a lot of things that we've studied for in physics go by nerdy names
like the renormalization group equation and napoleons and yada yada yada and
when you look a little more closely at this you have you at first there was a
well there's something crazy here that doesn't make sense because we know that
if you even want to build a super simple neural network with hell that part cat
pictures and dog pictures right that you can do that very very well now but if
you think about it a little bit you convince yourself it must be impossible because if I have one megapixel even if
each pixel is just black or white there's 2 to the power 1 million possible images which is way more than
there are atoms in our universe right so in order to and then for each one of
those I have to assign a number which is the probability that it's a dog right so an arbitrary function of images is a
list of more numbers than there are atoms in our universe so clearly I can't
store that under the hood of my my GPU or maybe my computer yet somehow works so what does that mean well it means
that the out of all of the problems that you could all try to solve with a neural
network almost all of them are impossible to solve with a reasonably sized one but
then what we should show it in our paper was was that they the fraks the kind of
problems the fraction of all the problems that you could possibly pose that there that we actually care
about given the laws of physics is also an infinitesimally tiny little part and amazingly they're basically the same
part yeah it's almost such that our world was created for I mean they kind of come together yeah but you could say
maybe where the world created the world that the world was created for us but I have a more modest interpretation which
is that instead evolution in downest but neural networks precisely for that reason because this particular
architecture as opposed to the one in your laptop is very very well adapted
solving the kind of problems of nature kept presenting it our ancestors will read so it makes sense that why do we
have a brain in the first place it's to be able to make predictions about the future mm-hm and so on so if we had a
sucky system which could never solve it wouldn't have a logic so but it's so
this is this is a I think you're very beautiful fact yeah we also we also
realize that there's there that we they've been it's been earlier work on on why deeper networks are good but we
were able to show an additional cool fact there which is that them even incredibly simple problems like suppose
I gave you it I found the numbers and asked you to multiply them together in re you can write it's the few lines of
code boom done trivial if you just try to do that with a neural network that has only one single hidden layer in it
you can do it but you're gonna need two to the power a thousand neurons and to
multiply a thousand numbers which is again more neurons than their atoms in our universe okay that's nothing but if
you're allowed if you love yourself make it a deep network with many layers you only need four thousand neurons it's
perfectly feasible so that's really interesting there yeah yeah so on
another architecture type I mean you mentioned Schrodinger's equation and what what are your thoughts about
quantum computing and the role of this kind of computational unit in creating
an intelligent system in some Hollywood movies that are a lot mentioned my name
you don't want to spoil them the the NAD is building a quantum computer list yes because the word quantum sounds
cool and so it's right mines first of all I think we don't need quantum
computers they build a GI I suspect your brain is not quantum computer and then
they found sense so you don't even wrote a paper about that what many years ago would excite Chocula the decoherence
so-called B coherence time that how long it takes until the quantum computer nosov what your neurons are doing gets
erased mm-hmm why just random noise from the environment and then it's about 10
to the minus 21 seconds so as cool as it would be to have a quantum computer in
my head I don't think that fast yeah on the other hand there are very
cool things you could do with quantum computers though I think we'll be able to do soon when we get big what bigger
ones that might actually help machine learning do even better than the brain mm-hmm though for example one this is
moonshot but hey you know learning is
very much same thing is search mm-hmm if you have if you try to train a neural
network to get really learn to do something really well you'd have some lost function you have some you have a
bunch of knobs you can turn represented by a bunch of numbers and you're trying to tweak them so that it become as good
as possible at this thing so if you think of a landscape with some Valley
where each dimension of the landscape corresponds to some number you can change you're trying to find the minimum
and it's well-known that if you have a very high dimensional landscape complicated things super hard to find
the minimum later quantum mechanics is amazingly good at this right if I want
to know what's the lowest energy state this water can possibly have incredibly
hard to compute but we can but nature will happily figure this out for you if you just cool it down and make you very
very cold if you put a ball somewhere it'll roll down to its minimum and this
happens metaphorically and the energy landscape too and quantum mechanics even used as a mode some
clever tricks which today is machine learning systems don't like if you're trying to find the minimum and you get
stuck in a little local minima here in quantum mechanics you can actually tunnel through the barrier and get
unstuck in Yemen and that's really interesting yeah so it may be for example it will one day use quantum
computers that help train neural networks better that's really
interesting okay so as a component of kind of the learning process for example yeah
let me ask sort of wrapping up here a little bit let me let me return to the
questions of our human nature and and love as I mentioned so do you think you
mentioned sort of a helper robot you can think of also personal robots do you think the way we human beings fall in
love and get connected to each other it's possible to achieve in an AI system
and human-level AI intelligence system do you think we would ever see that kind of connection or you know in all this
discussion about solving complex goals yeah as this kind of human social connection do you think that's one of
the goals and the peaks and valleys that with the raising sea levels that we'll be able to achieve or do you think
that's something that's ultimately or at least in the short term relative to other goals is not achievable I think
it's all possible and I mean in in recent there's that there's a very wide
range of guesses as you know among AI researchers when we're gonna get a GI
some people you know like your friend Rodney Brooks says it's gonna be hundred hundreds of years least and then there
are many others I think it's gonna happen relative much sooner and recent polls and be half or so or AI
researchers think it's we're gonna get AGI within decades so if that happens of
course then I think these things are all possible but in terms of whether it will happen I don't I think we shouldn't
spend so much time asking what do we think will happen in the future as if we are just some sort of pathetic your
passive bystanders you know waiting for the future happen to us hey we're the ones creating
this future right so we should be proactive about it and ask us of what
sort of future we would like to have happen that's right trying to make it like that well what I prefer it to some sort of incredibly
boring zombie like future where there's all these mechanical things happen it is no fashion no emotion no experience
maybe even no I would of course much rather prefer it if all the things that
we find that we value the most about humanity our subjective experience
passion inspiration you love you know if we can create a future where those are
those things do exist no I think ultimately it's not our universe giving
meaning to us just us giving me the universe and if we build more advanced
intelligence let's let's make sure we're building in such a way that meaning
these but it's part of it I want a lot of people that seriously study this problem and think of it from different
angles have trouble and the majority of cases if they think through that happen
you know are the ones that are not beneficial to humanity right and so yeah
so what what are your thoughts was an engine what's what should people you
know I really don't like people to be terrified you should what's a way for people to
think about it in a way that instead you know we can solve it okay to make it better yeah no I don't think panicking
is gonna help in any way it's not increase chances of things going well either even if you are in a situation
where there is a real threat does it help if everybody just freaks out right no of course of course not I
think yeah there are of course ways in which things can go horribly wrong first
of all it's important when we think about this thing this about the problems and risks that also remember how huge
the upsides can be if we get it right I had everything everything we love about society and civilization of the product
of intelligence so if we can amplify our intelligence or machine intelligence and not anymore lose our loved one to what
we're told as an uncurable disease and things like this of we should aspire to that so that can be
a motivator I think reminding ourselves that the reason we try to solve problems is not just because we're trying to
avoid gloom but because we're trying to do something great but then in terms of the risks I think um the entry the
important question is to ask what can we do today they will actually help yes how come good may in it and dismissing the
risk is not one of them you know it I find it quite funny often when I'm in on discussion panels about these things how
the people who work for come for companies lobbies they're always like oh
nothing to worry about nothing to worry about nothing to worry about and the it's always all it's only academics sometimes it's expressed
concerns that's not surprising at all if you think about it Upton Sinclair
quipped right that it's hard to make your man believe in something when you think some the fans are not believing in
it and and frankly we know a lot of these people in companies and that they're just as concerned as anyone else
but if you're the CEO of a company that's not something you want to go on record saying when you have silly journalists so we're gonna put a picture
of a Terminator robot when they quote you so so the issues are real and the
way I am the way I think about what the issue is it is basically you know but
the real choice we have is first of all are we gonna stir this dismiss this the
risks and say well you know let's just go ahead and build machines that can do everything we can do better and cheaper
you know let's just make yourselves obsolete as fast as possible or what could possibly go wrong right that's one
attitude the opposite attitude that I think is to say is incredible potential
you know let's think about what kind of future we're really really excited about
what are the shared goals that we can really aspire towards and then let's
think really hard on how about how we can actually get there as it's a start with it no don't start thinking about the risk start thinking about the goals
goals yeah and then when you do that then you can think about the obstacles you want to avoid well they often get
students coming in right here into my office for career advising always ask them this very question where you
want to be in the future man if all she can say is oh maybe I'll have cancer maybe I'll run over by a tortoise and
obstacles instead of the bill he's just gonna end up a hypochondriac paranoid yeah whereas if she comes in and fire in
her eyes and it's like I want to be there and then we can talk about the obstacles and see how we can circumvent
them that's I think a much much healthier attitude and um that's really
well plan and I I feel it's it's very challenging to come up with a vision for
the future which we wish we are unequivocally excited about I'm not just talking now in the vague terms like yeah
let's cure cancer fine I'm talking about what kind of society do we want to create what do we want it to mean you
know to be human in the Age of AI in the age of AGI so if we can have this
conversation broad inclusive conversation and gradually start
converging towards some some future that with some direction at least that we
want to steer towards right then then no we'll be much more motivated to constructively take on the obstacles and
I think if I had if I had the I think if you make if I try to wrap this up in a
more sixteenth way I think I think we can all agree already now that we should
aspire to build AGI but doesn't
overpower us but that empowers us and think of the many various ways that can
do that whether that's from my side of the world of autonomous vehicles I I'm
personally actually from the camp that believes there's human level intelligence is required to to achieve something like vehicles that would
actually be something we would enjoy using and being part of so that's one example and certainly there's a lot of
other types of robots in medicine and so on so focusing on those and then and
then coming up with the obstacles coming up with the ways that that can go wrong and solving those one at a time and just
because you can build an autonomous vehicle even if you could build one that would drive this finalize you know maybe
there are some things in life that we would actually want to do ourselves that's right my like for example if you think of our
society as a whole there's something that we find very meaningful to do and that doesn't mean we have to stop doing
them just because machines can do them better you know I'm not gonna stop playing tennis just the base of my build
a tennis robot yeah beat me people are still playing chess and even go yeah and I in this in the very near term even
some people are advocating basic income replace jobs but if you if the
government is gonna be willing to just hand out cash to people for doing nothing then one should also seriously
consider whether the government should also just hire a lot more teachers and nurses and the kind of jobs which people
often find great fulfillment in doing right I get very tired of hearing politicians saying oh we can't afford
hiring more teachers but we're going to maybe have basic income if we can have more serious research and thought into
what gives meaning to our lives and the jobs give so much more than income right mm-hm and then think about in the future
well what are the role of the yeah what are the roles that we want to have
people feeling empowered by machines and I think sort of I come from the Russia
from the Soviet Union and I think for a lot of people in the 20th century going to the moon going to space was an
inspiring thing I feel like the the the universe of the mind so AI understanding
creating intelligence is that for the 21st century so it's really surprising and I've heard you mention this it's
really surprising to me both on the research funding side that it's not funded as greatly as it could be but
most importantly on the politicians side that it's not part of the public discourse except in the kilobots
Terminator kind of view that people are not yet I think perhaps excited by the
possible positive future that we can build together certainly should be because politicians usually just focus
on the next election cycle right the single most important thing I feel we humans have learned and the entire
history of science is there were the Masters of underestimation we underestimated
the science of our cosmos again and again realizing of everything
we thought existed was just a small part of something grander right planet solar system the galaxy clusters of guises
universe so and we now know that we but
the future has just so much more potential than our ancestors could ever have dreamt of this cosmos well imagine
if all of Earth was completely devoid of life except for Cambridge Massachusetts
that would wouldn't it be kind of lame if all we ever aspired to it to stay in
Cambridge Massachusetts forever and then go extinct in one week even though Earth was gonna continue on
for longer that that sort of attitude I think we have now on the cosmic scale we
can fluid life can flourish on earth not foreign for four years but for billions of years yes I can even tell you about
how to move it out of harm's way when its own Sun gets too hot and and then we
have so much more resources out here which today yeah maybe there are a lot
of other planets with bacteria or a cow like life on them but I most of this all
this opportunity seems as far as we can fail to be largely dead like the Sahara
Desert and yet we have the opportunity but to help life flourish promise there are billions
of year and so like let's quit squabbling about when some little border
should be drawn one-fifth one mile to the left to right and realize hey you
know we can do such incredible things yeah and that's I think why it's really exciting that yeah you and others are
connected with some of the working la mosque is doing because he's literally going out into that space we're
exploring our universe and it's wonderful that is exactly why Elon Musk is so it misunderstood right misconstrue
him is some kind of pessimistic dooms there the reason he cares so much about the I safety is because he more than
almost anyone else appreciates these amazing opportunities they will squander if we wipe out out here on earth
and we're not just gonna wipe out the next generation but all generations and this incredible opportunity that's out
there that would be really a waste and AI for people who think that we better
to do without technology well let me just mention that if we don't improve
our technology the question isn't whether humanity is gonna go extinct question is just whether we're gonna get taken out by the next big asteroid or
the next supervolcano or something else dumb that we could easily prevent with more tech right and if we want life to
flourish throughout the cosmos AI is the key to it as I mentioned a lot of detail
in my book right there even many of the most inspired sci-fi writers I feel have
totally underestimated the opportunities for space travel especially to other
galaxies because they weren't thinking about the possibility of AGI which just
makes it so much easier right yeah so that goes to your view of AGI that
enables our progress that enables a better life so that's a beautiful that's a beautiful way to put it and then
something to strive for so max thank you so much thank you for your time today it's been awesome thank you so much
thanks Super Bowls Rory yes
you

----------

-----
--38--

-----
Date: 2018.04.17
Link: [# MIT AGI: Autonomous Weapons Systems Policy (Richard Moyes)](https://www.youtube.com/watch?v=U6lJI-NSfBY)
Transcription:

welcome back to 6s $0.99 artificial general intelligence today we have
Richard Moyes he's the founder managing director of article 36 a uk-based
not-for-profit organization working to prevent the unintended unnecessary and
unacceptable harm caused by certain weapons including autonomous weapons and nuclear weapons he will talk with us
today about autonomous weapon systems in the context of AI safety this is an
extremely important topic for engineers humanitarians legal minds policy makers
and everybody involved in paving the path for safe positive future for AI in
our society which I hope is what this course is about Richard flew all the way
from the UK to visit us today in snow in Massachusetts so please give him a warm welcome
thanks very much Lex and thank you all for coming out as Lex said I work for a
not-for-profit organization based in the UK we specialize in thinking about policy and legal frameworks around weapon
technologies particularly and generally about how to establish more constraining policy and legal frameworks around
around weapons and I guess I'm mainly going to talk today about these issues
of to what extent we should enable machines to kill people to make
decisions to kill people it's I think your conceptually very interesting topic quite challenging in lots of ways
there's lots of unstable terminology and lots of sort of blurry boundaries my own
background as we said we work on weapons policy issues that I've worked on the development of two legal international
legal treaties prohibiting certain types of weapons I worked on the development of a 2008 Convention on Cluster
Munitions which prohibits cluster bombs and worked on our organization pioneered
the idea of a treaty prohibition on nuclear weapons which was agreed last year in the UN and we're part of the
steering group of ICANN the international campaign to abolish nuclear weapons which won the Nobel Peace Prize last year so that was a good
year for for us the issue of autonomous weapons killer robots which I'm going to
talk about the day we're also part of an NGO non-governmental organization coalition on this issue called the
campaign to stop killer robots it's a good name however I think when we get
into some of the details of the issue will find that perhaps the the snappiness of the name in a way masks
some of the complexity that lies on underneath this but this is a live issue
in international policy and legal discussions at the United Nations for the last several years three or four
years now there have been groups of governments coming together to discuss autonomous weapons and whether or not
there should be some new legal instrument that that tackles this this issue so it's a it's a live political
issue that is being debated in in policy legal circles and really my my comments that
they are going to be speaking to that context I guess I'm going to try and give us a bit of a briefing about what
the issues are in this international debate how different actors are orientating to these issues some of the
conceptual models that we we use in that so I'm not really going to give you a particular sales pitch as to what you
should think about this issue though my own biases are probably going to be fairly evident during the process but
really to try and lay out a bit of a sense of how these these questions debated in the international political
scene and maybe in a way that's useful for reflecting on sort of wider
questions of how AI technologies might be orientated to an approach by by
policymakers and and and the legal framework so in terms of the structure
of my comments I'm going to talk a bit about some of the pros and cons that have put forward around autonomous
weapons or movements towards greater autonomy in weapon systems I'm going to talk a bit more about the political
legal framework within within which these discussions are taking place and then I'm going to try to sort of lay out
some of the models the conceptual models that we as an organization have developed and sort of using in relation
to these issues and perhaps to reflect a bit on where I see the political conversation the legal conversation on
this going at an international level and maybe just finally to try to reflect on
or just draw out some more general thoughts that I think occur to me about
what some of this says about thinking about AI functions in in different social social roles but before getting
into that sort of pros and cons type type stuff I just wanted to start by
suggesting a bit of a sort of conceptual timeline because one of the things this
could be the present one of the things
you find when we start when you say to somebody when we work on this issue of autonomous weapons they tend to
orientate to it in into fairly distant ways some people will say oh you mean
armed drones and you know we know what armed drones are they being used in the
world today and that's kind of an issue here and in the present right drones but
other people most of the media and certainly pretty much every media photo
editor thinks you're talking about the Terminator over here yeah and maybe a
bit of Skynet thrown in so this is a sort of advanced futuristic sci-fi
orientation to the to the issues am i thinking about this I come from a background of working on the impact of
weapons in the present unless I'm less concerned about this area of my thinking
of my anxieties or my concerns around this issue don't come from from this area I do this light a bit Wiggly here
because I also don't want to suggest there's any kind of you know teleological certainty going on here
this is just an imaginary timeline but I think it's important just in terms of
situating where I'm coming from in the debate that I'm definitely not starting at that end and yet in the political
discussion amongst governments and States well you have people come in and at all sorts of different positions
along here imagining their autonomous weapons may exist at you know somewhere
along this this sort of spectrum so I'm going to think more about stuff that's going on around here and how some of our
conceptual models really build around some of this thinking not so much actually armed drones but some other
some other systems but my background before I started working on policy and
law around weapons was was setting up and managing land my clearance
operations overseas and well they've been around for they've been around for
quite a long time land mines and
I think it's interesting just to start with just to reflect on the basic anti-personnel landmine it's simple but
it gives us I think some sort of useful entry points into thinking about what an autonomous some weapon system might be
in its most simple form if we think about a landmine well essentially we
have a person and there's an input into the landmine and there's a function that
goes on here pressure is greater than X person they turn on the landmine there's
a basic mechanical algorithm goes on and you get an output explosion that goes
back against the person who trod on the landmine so it's a fairly simple system
of a signal a sensor taking a signal from the outside world the landmine is
viewing the outside world through its sensor it's a basic pressure plate and according to a certain calculus here you
get an output and it's directed back at this person and it's a loop and that's one of the things that I think is
fundamental essentially to understanding the idea of autonomous weapons and in a way this is where the autonomy comes in
but there's no other person intervening in this process at any point there's just a sort of straightforward
relationship from the person or object that has initiated the the system back
into the back into the effects that are being that are being applied so in some
ways we'll come back to this later and think about how some of the basic building blocks of this may be there in
our thinking about other weapon systems and weapon technologies as they're they're developing and maybe thinking
about landmines and thinking about these the processes of technological change we see a number of different dynamics at
play in this sort of imaginary timeline anti-personnel landmines of course a static they just sit in the ground where
you have left them but we get more and more mobility pups as we go through this system certainly armed drones and other
systems that I'll talk about you start to see more mobility in the in the weapon system
perhaps greater sophistication of sensors I mean a basic pressure plate
just gauging weight that's a very simple sensor structure for interrogating the
world we have much more sophisticated sensor systems in weapons now so we have
weapon systems now that are looking at radar signatures they're looking at the heat shapes of objects and we'll come
back and talk about that but more sophistication of sensors and more sophistication of the of the computer
algorithms that are basically interrogating those sensor those sensor inputs perhaps a little bit as well of a
movement in this sort of trajectory from physically very unified objects always
sort of wrestle slightly whether this is the word I want but it's a sort of self-contained entity the the landmine
whereas as we move in this direction maybe we see more dispersal of functions through different through different
systems and I think that's another dynamic that when we think about the development of autonomy and weapon
systems it might not all live in one place physically moving around in one place it can be an array of different
systems functioning in different different places perhaps for people with a sort of AI
type of mindset maybe there's some sort of movement from more specific types of
AI functioning here use of different specific AI functions here to something more general going in this direction I'm
wary of necessarily buying straight forward into that but maybe you could see some movement in that sort of direction so I just want to sort of put
this to one side for now but we'll come back to it and think about some systems that are existing here that I think sort
of raise issues for us and around which we could expand some some models but I just wanted to have this in mind when
thinking about this that we're not necessarily we're definitely not for me thinking about humanoid robots walking
around fighting like a soldier rather we're thinking about developments
and trajectories we can see coming out of established military systems now so I
was going to now a bit about the political and the legal context obviously there's a lot of
complexity in the worlds of politics and legal structures and I don't want to get too bogged down in it but I think in
terms of understanding the basics of this debate on the international landscape have to have a bit of
background in that area essentially there's I think three main types of
international law that we're concerned with here and again concerned with international law rather than domestic
legislation which any individual state can put in place whatever domestic legislation they want we're looking at
the international legal landscape basically you have international human rights law which applies in pretty much
all circumstances and it involves the right to life and the right to dignity and a various other legal protections
people and then particularly prominent in this debate if you have what's called
international humanitarian law which is the rules that govern behavior during
armed conflict and provide obligations on militaries engage in armed conflict for how they have to conduct themselves
this isn't the legal framework that decides whether it's okay to have a war or not this is a legal framework that
once you have in the war this is the obligations that you've got you've got to follow and it basically includes
rules that say you know you're not allowed to directly kill civilians you've got a aim your military efforts
at the forces of the of the enemy at enemy combatants you're not allowed to
kill civilians directly or deliberately but you are allowed to kill some civilians as long as you don't kill too
many of them for the military advantage that you're trying to achieve so there's a sort of balancing acts like this this
is called proportionality nobody ever really knows where the balance lies but
it's a it's a sort of principle of the law that once you can't kill civilians you mustn't kill an excessive number of
civilians these are general rules these apply pretty much to all states in in
armed conflict situations and then you have treaties on specific weapon specific weapon types and this is really
where you have weapons that are considered to be particularly problematic in some way and it's decided
a group of states decide to develop and put in place agree a treaty that that applies specifically to
those to those weapons I think it's important to recognize that these legal
treaties are all developed and agreed by States they're agreed by international
governments talking together negotiating what they think the law should say and they generally only bind on States if
they choose to adopt that legal instrument so I guess what I'm
emphasizing there is a sense that these are sort of social products in a way their political products it isn't a sort
of magical law that's come down from on high perfectly written to match the needs of humanity
it's a negotiated outcome developed by a complicated set of actors who may or may
not agree with each other on all sorts of things and what that means is there's quite a lot of wiggle room in these
legal frameworks and quite a lot of uncertainty within them it lawyers of international military and law will tell
you that's not true but that's because they're particularly keen on that legal framework but in reality there's a lot
of a lot of fuzziness to what some of the legal provisions what some of the legal provisions to say and it also
means that the extent to which this law binds on people and bears on people is also requires some social enactment
there's not a sort of world police who who can follow up on all of these these
legal frameworks it requires a sort of social function from States and from other actors to keep articulating their
sense of the importance of these legal rules and keep trying to put pressure on other actors to to accord with them so
the issue of autonomous weapons is in discussion at the United Nations under a
framework called the UN Convention on conventional weapons and this is a body that has the capacity to agree new
protocols new treaties essentially on specific weapon systems and that means that diplomats from lots of countries
diplomats from the US from the UK from Russia and Brazil and China and other
countries of the world will be sitting around in a conference room putting forward their perspectives on this issue
and trying to find common ground or trying not to find common depending on what sort of outcome
they're they're working towards so you know the UN isn't a completely separate entity of its own it's just the
community of states in the world sitting together talking about talking about things so main focus of concern in those
discussions when it comes to autonomy is not some sort of generalized autonomy or
not autonomy of all of its forms that may be pertinent in the military space it's it's rather much more these these
questions of how the targets of an attack are selected identified decided
upon and how is the decision to apply force to those targets made and it's really these are sort of the critical
functions of weapon systems where the movement towards greater autonomy is considered a source of anxiety
essentially that we may see machines making decisions on what is a target for
an attack and choosing when and where forces apply to that specific to that
specific target so obviously in this context not
everybody's like minded on this there are potential advantages to increasing autonomy in weapon systems and there's
potential disadvantages and problems associated with it and within the you
know within this international discussion we see different perspectives laid out and some states of course will
be able to see some some advantages and some disadvantages it's not a black and white sort of discussion in terms of the
possible advantages for autonomy I mean one of the key ones ultimately is framed
in terms of military advantage that we want to have more autonomy and weapon systems because it will maintain or give
us military advantage over possible adversaries because in the end military stuff is about winning wars right so you
want to maintain military advantage and military advantage number of factors
really within that speed is one of the speed of decision making can computerize
or timer systems make decisions about where to apply force faster than a human would be capable of doing and and
therefore this is this is advantageous for us also speed allows for
coordination of numbers so if you want to have swarms of systems you know
swarms of small drones or somesuch you need quite a lot of probably autonomy
and decision-making and communication between those systems because again the level of complexity and the speed
involved is is greater than a human would be able to sort of manually engineer so speed both in terms of
responding to external effects but also practical donating your own forces reach
potential for autonomous systems to be able to operate in communication denied
environments where if you're relying on an electronic communications link to say a current armed drone maybe in a future
battle space where the enemy is denying communications in some way you could use
an autonomous system to still fulfill a mission without without needing to rely on that communications infrastructure
general force multiplication there's a bit of a sense that there's going to be more and more teaming of machines with with humans so machines
operating alongside humans in the battlespace and then there's importantly
as it's presented at least a sense that these are systems which could allow you to reduce the risk to your own forces
that maybe if we can put some sort of autonomous robotic system at work in a specific environment then we don't need
to put one of our own soldiers in that position and as a result we're less likely to have casualties coming home
which of course politically is problematic for maintaining any sort of conflict posture so against all that
stuff there's a sense that I think most fundamentally there's perhaps a moral
hazard that we come across at some point that there's some sort of boundary we're
seeing or conceptualizing a situation where machines are deciding who to kill
in a certain context is just somehow wrong and well that's not a very easy
argument to just start you know articulate in a sort of rationalized sense but there's some sort of moral
revulsion that perhaps comes about at this sense that machines are now deciding who should be killed in a
particular in a particular environment there's a set of legal concerns can
these systems be used in accordance with the existing legal obligations and I'm
going to come on a little bit later to our orientation and the legal side which is which is also about how they may
stretch the fabric of the law and the structure of the law as we see it there's some concerns in this sort of
legal arguments for me that we sometimes slip into a language of talking about
machines making legal decisions will a machine be able to apply the rule of
proportionality properly there's dangers in that I know what it means but at the
same time the law is addressed to humans the law isn't addressed to machine so it's humans who have the obligation to
enact the legal obligation a machine may do a function that is sort of analogous to that legal decision but ultimately to
my mind is still a human in us be making the legal determination based on some predict prediction of what that
machine will do and I think this is a very dangerous slippage because even you know senior legal academics can slip
into this mindset which is which is a little bit like handing over the legal framework to machines before you've even
got on to arguing about what we should or shouldn't have so we need to be careful in that area and it's a little
bit to do with for me continuing to treat these technologies as machines rather than into treating them as agents
in some way of a sort of equal or equivalent or similar moral standing to
two humans and then we have a whole set of wider concerns that are raised so
we've got moral anxieties legal concerns and then a set of other concerns around risks that could be unpredictable risks
there's a sort of normal accidents Theory maybe you come across that stuff there's a bit of that sort of language in the debate about complicated systems
and not being able to you know not be able to avoid accidents in in some respects some anxieties about maybe this
will reduce the barriers to engaging in military action maybe being able to use
autonomous weapons will make it easier to go to war and some anxieties about sort of international security and
balance of power and arms races and the like these are all significant concerns
I don't tend to think much in this area partly because there's they involved put a lot of speculation about what may or
may not be in the future and they're quite difficult to populate with with sort of more grounded arguments I find
but that doesn't mean that they aren't significant in themselves but I find them less straightforward as an entry
point so in all of these different issues there's lots of unstable
terminology lots of arguments come in in different directions and our job as an NGO in a way is we're trying to find
ways of building a constructive conversation in this environment which can move towards States adopting a more
constraining orientation to this movement towards autonomy and the main
tool we've used to work towards that so far has been - perhaps stop focusing on the
technology per se and the idea of what is autonomy and how much autonomy is a
problem and to bring the focus back a bit onto what is the human element that we want to preserve in all of this
because it seems like most of the anxieties that come from a sense of a
problem with autonomous weapons are about some sort of absence of a human element that we want to preserve but
unless we can in some way define what this human element is that we want to preserve I'm not sure we can expect to
define its absence very straightforwardly so I kind of feel like we want to pull the discussion on to a focus on the on the human on the human
element and the tool we've used for this so far has been basically a terminology
about the need for meaningful human control and this is just a form of words that we've sort of introduced into the
debate and we've promoted it in discussions with diplomats and with different actors and we've built up the
idea of this terminology as as being a sort of tool it's a bit like a meme right you you create the the terms and
then you use that to sort of structure the discussion in a in a productive in a productive way one of the reasons I like
it is it works partly because the word meaningful doesn't mean anything particular or at least it means whatever
you might want it to mean and I find that an enjoyable sort of tension in in
that but the term meaningful human control has been quite well picked up in in the literature on this issue and in
the diplomatic discourse and it's helping to structure us towards a what
we think of the key questions basic arguments for the idea of meaningful
human control from my perspective are quite simple and intended to use basically a sort of absurdist sort of
logic such a thing first of all really to recognize that no
governments are in favor of an autonomous weapon system that has no human control whatsoever right nobody is
arguing that it would be a good idea for us to have some sort of autonomous
weapon that just flies around the world deciding to kill people we don't know who it's going to kill or why it doesn't
have to report back to us but you know we're in favor nobody's in favor of this right this is obviously ridiculous so there needs to
be some form of human control because we can rule out that sort of you know
ridiculous extension of the argument and on the other hand if you just have a
person in a dark room with a red light that comes on every now and again and
they don't know anything else about what's going on but they're the human who's controlling this autonomous weapon and when the red light comes on they
push the fire button to launch a rocket or something we know that that isn't
sufficient human control either right there's a person doing something there's a person engaged in the process but
clearly it's just some sort of mechanistic pro-forma human engagement
so between these two kind of ridiculous extremes I think we get the idea that
there's the some sort of fuzzy fuzzy line that that must exist in there in
there somewhere and that everybody can in some way agree to the idea that such a line should exist so the question then
for us is how to move the conversation in the international community towards a productive sort of discussion of where
the parameters of this line might be conceptualized so that's brought us on
to thinking about a more substantive set of questions about what are the key elements of meaningful human control and
we've laid out some basic elements so I might get rid of that fuzzy line because it's a bit useless anyway isn't it and
then I can put my key elements on here well one of them is predictable reliable
transparent technology this is kind of
before you get into exactly what the system is going to do we want the technology itself to be sort of well
made and it's you know it's going to basically do what it says it's going to do whatever that is and we want to be
able to understand it to some extent obviously this becomes a bit of a challenge in some of the AI TAC
functions where you start to have machine learning issues unlike these issues of transparency perhaps they
start to come up a little bit a little bit there but these are kind of issues in the design and the development of
systems another thing we want to have is and I think this is a key one is
accurate information and it's accurate
information on the intent of the commander the world the outcome what's
the outcome we're trying to achieve how does the technology work and what's the
context third one there's only four so
it won't take long third one is timely intervention human
intervention it should be it would be good if we could turn it off at some
point if it's going to be a very long acting system good if we turn it off maybe and then the fourth one is just a
sort of framework of accountability
so we're thinking that basic elements of human control can be broken down into
these areas some of them are about the technology itself how it's designed and
made how do you verify and validate that it's going to do what the manufacturers have said it's going to do can you
understand it this one I think is the key one in terms of thinking about the
issue and this is what I'm going to talk about a bit more no but accurate information on what's the commander's
intent what do you want to achieve in the use of this system what effects
is it going to have I mean this makes a big difference how it works these factors here involve what are the target
profiles that it's going to use where is our land mine on the land mine of course it was just pressure right pressure is
being taken as a pressure on the ground is being taken as a proxy for a military target for a human who would you know
assume as a military target but in these systems we're going to have different target profiles different heat
shapes different different patterns of data that the system is going to operate on the basis of what sort of actual
weapon is it going to use to apply force it makes a difference if it's going to just fire a bullet from a gun or if it's going to drop a 2,000 pound bomb I mean
that has a different effect and the way in which you envisage and sort of control for those effects is going to be
different in those different cases and finally very importantly these issues of
context information on the context in which the system will will operate
context of includes are there going to be civilians present in the area can you assess are
there going to be other objects in the area that may present a similar pattern to the proxy data you know if you're
using a heat shape of a vehicle engine now it might be aimed at a tank but if there's an ambulance in the same
location is an ambulances vehicle engine heat shape sufficiently similar to the
tank to cause some confusion between the two sets of information like that in
context of course varies in different you know obviously varies in different environments but I think we can see different domains in this area as well
which is significant that operating in the water or in the ocean you've probably got a less cluttered
environment a less complex environment than if you're operating in an urban in an urban area so that's another factor
that needs to be taken into into accounting in this so I just wanted to
talk a little bit about some existing systems perhaps and think about them in
the context of this these sort of set of issues here one system that you may may
be aware of is sir it's on a boat okay
so something like the Phalanx anti-missile system it's on a boat but
there's various anti-missile systems and it doesn't the details don't matter in this context these are systems that a
human turns it on so a human is choosing when to turn it on and a human turns it
off again but when its operating it's the radar is basically scanning an area
of an area of sky up here and it's
looking for fast-moving incoming objects because basically it's designed to automatically shoot down incoming
missiles or rockets right so thinking about these characteristics you know
what the outcome you want is you want your boat not to get blown up by an incoming missile and you want to shoot down any incoming
missiles you know how the technology works because you know that it's basically using radar to see incoming fast-moving
signatures and you have a pretty good idea of the context because the skies
are fairly uncluttered comparatively and you'd like to think that any fast-moving
incoming objects toward you here are probably going to be incoming missiles not guaranteed to be the case one of
these systems shot Donna and Iranian passenger airliner but uh by accident which is obviously a significant
accident but basically you have a sense of you know so the fact that the data
that you're using tracked pretty well to the target objects if not absolutely precisely you've got a relatively
controllable environment in terms of the sky and you've got a human being this
system isn't really mobile I mean it's kind of mobile insofar as the boat can move around but the person who's
operating it is you know they're mobile in the same place so so it's relatively
static so I think looking at that you could suggest that there's still a
reasonable amount of human control over this system because when we look at it in terms of a number of the functions
here we can understand how that how that system is being managed in a human
controlled way and although there's still a degree of autonomy or at least it's sort of highly automated in the way
that it actually identifies the targets and moves the gun and shoots down the incoming object the basic framework is
one in which I feel like and I mean it's not for me to say but I feel like still a reasonable amount of human control as
being being applied okay I know that sort of system know I've got to draw
some tanks or something now see okay well I'm just going to draw them like
that because it otherwise it's actually long okay these are tanks armored
fighting vehicles ignore the graphic design skills there are sensor fuse
weapon systems so a commander at a significant distance right can't necessarily see the location of
the of the tanks but they know that there's some enemy tanks in this area over here right and maybe they have some
sense of what this area is they're not in the middle of a town they're out in the open so they have an understanding
of the context but maybe not a detailed understanding of the context so the weapon system is going to fire multiple
warheads into this target area the commander is decided upon the target of
the attack this group of tanks here but as the warheads approach the target area
the warheads are going to communicate amongst themselves and they're going to allocate themselves to the specific to
the specific objects and they're going to detect the heat shape of the vehicles
engines they're going to match that with some profile that says this is a enemy armored fighting vehicle as far as we're
concerned and then they're going to apply force downwards from the air using
a bit of explosive engineering shaped-charge which focuses a blasts a blast of explosive basically a jet of
explosives downwards onto the specific targets and okay
so in this situation well have the has the weapon system chosen the target well
it's a bit ambiguous because as long as we conceptualize the group of tanks as that as the target then a human has
chosen the target and the weapon system is essentially just been efficient in its distribution of force to the target
objects but if we see in the individual vehicles as individual targets maybe the
weapon system has chosen the targets potentially some advantages of automated
of autonomy in this situation from my perspective this kind of ability to
focus a jet of explosive force directly on the object that you're looking to strike so long as you've got the right
object this is much better than setting off lots of artillery shells in this area which would have a much greater
explosive force effect on the surrounding area probably put a wider population at risk
so there's a sort of set of considerations here that I think significant so we have these systems you
know these systems exist exist today you can ask questions about whether those Heat shape profiles of those
objects sufficiently tightly tied to enemy fighting vehicles or whatever but I think it can be conceptualized
reasonably straightforwardly in those terms but the area where I start have a
problem with this stuff is in the potential for this circle or this
pattern just to get bigger and bigger essentially because it's all reasonably
straightforward when you put the tanks reasonably close together and you can envisage having one sort of one sort of
information about this area which allows you to make the legal determinations that you need to make but once these
tanks get spread out over a much larger area and you have a weapon system that using basically the same sorts of
technological approach is able to cover a substantially wider area of enemy
terrain over a longer period of time then it suddenly gets much more difficult for the for the military
commander to have any really detailed information about the context in which force will actually be applied and for
me this is I think the main point of anxiety or point of concern that I have in the way in which autonomy and weapon
systems is is likely to develop over the immediate future because under the legal
framework a military commander has an obligation to apply certain rules in an attack and an attack is it's not
precisely defined but it needs to have some I think some spatial and conceptual
boundaries to it that allow a sufficient granularity of legal application because
if you if you treat this as an attack I think that's fine as you expand it out so you've got vehicles across a whole
wide area of a country say across the country as a whole using the same sort of extension logic as is in some
previous arguments once you've got vehicles across the whole country and you're saying in this attack I'm going
to just target the vehicles of the enemy and you send out your warheads across the whole location now I don't think
that's going to happen in immediate term but I'm just using that as a sort of conceptual
challenge you start to have applications of actual physical force in all sorts of locations where a commander really can't
assess in any realistic way what the actual effects of that are going to be and I think at that point you can't you
can no longer say that there is sufficient human control being being applied so this capacity of AI enabled
systems or AI driven systems to expand attacks across a much wider geographical
area and potentially over a longer period of time I think is a significant challenge to how the legal framework is
is understood at present not one that relies upon determinations about whether
this weapon system will apply the rules properly or not but rather one which
involves the the frequency and the proximity of human decision-making to be
sort of diluted progressively over progressively over time so that's a
significant area of concern for me final sort of sort of concerns in these areas is around these issues about
encoding of targets I think we could say pretty clearly that weight is a very
meager basis for evaluating whether something is a valid military target or
or not right the significant problems we're suggesting that we could just take
the weight of something as being sufficient for us to decide is this a target or not in any of these processes
we have to decide that certain patterns of data represent military objects of
some type and of course in a way I think what we sort of scenes are proponents of
greater and greater autonomy and weapon systems is a sense that well as we expand the scope of this attack we just
need to have a more sophisticated system that's undertaking the attack that can take on more of the evaluation and more
of this process of basically mapping the coding of the world into a set of
decisions about the application of force but overall yeah I'm skeptical about the
way in which our social systems are likely to go about mapping people
indicators of identities into some sort of fixed sense of military objects or
military targets as a society over you know the last hundred years there's been
plenty of times where we've applied certain labels to certain types of people certain groups of people based on
various indicators which apparently seemed reasonable to some significant
section of society at the time but that ultimately I think we've subsequently thought were highly problematic and so I
think we need to be very wary of any sort of ideas of thinking that we can encode in terms of humans particularly
very concrete indicators that certain groups of people should be considered valid targets or or not just going to
say a couple of final things about future discussions in the CCW the chair
of the group of governmental experts that's the body that's going to discuss autonomous weapons has asked States for
the next meeting which would take place in April to come prepared with ideas about the touch points of human machine
interaction this is a sort of code for what are the ways in which we can
control technology so I suppose from our context as an organization we'll be
looking to get States to start to try and lay out this kind of framework as being the basis for their perception of
the ways in which the entry points to control of technology could be thought about again it's really a question of
structuring the debate we won't get into detail across all of this but I think it's plausible that this year and next
we'll start to see the debate falling into some adoption of this kind of framework which i think will give us
some tools to work with I think at least if we start to get some agreement from a significant body of states that these
are the sort of entry points we should be thinking about in terms of control of technology it will give us a bit of
leverage for a start towards suggesting an overarching obligation that there should be some sort of meaningful or
sufficient human control but also in a way of thinking about that and interrogating that as new technologies
develop in the future that we can leverage in some in some I feel reasonably confident about that
but it's a difficult political environment and you know it's quite possible that I don't see any rush among
states to move towards any legal controls in this in this area just as a few very final thoughts which may be a
bit more abstract in my thinking on this I feel like and this sort of reflecting
on maybe some dynamics of AI functioning my anxiety here about the expansion of
the concept of attacks and in the same in in conjunction without a sort of
breaking down at the granularity of the legal framework I think this is another a sort of generalizing function again
and it's a movement away from more specific legal application by humans to perhaps
pushing humans people towards a more general legal orientation and I feel
like in the context of conflict we should be pushing for a more specific and more focused and more regular
application of human judgments and moral moral agency that isn't to say that I
think humans are perfect in any way there's lots of problems with humans but at the same time I think that we should
be very wary of thinking that violence is something that can be somehow perfected and that we can encode how to
conduct violence in some machinery that will then provide an adequate social
product for society as a whole and I guess there's a very final thought a bit linked to that is there's some questions
in my mind about how this all relates to bureaucracy in a way and in a sense that some of the functions that we're seeing here and some of the AI functions that
we see here in many ways related I think to bureaucracy to the encoding and
categorization of data in certain ways and just a very fast management of that
bureaucracy which is really an extension of the bureaucracies that we already that we already have and I think
extending that too far into the world of violence and the application of force to people will will precipitate painful
effects for us as a society and as it brings to the fore I think some of the underpinning and the paintings are
rationales of that of that beer framework so there we go it's a bit of a
broad-brush sketch so this questions got
Q&A
a little bit multifaceted but as humans evolve and adapt to increasingly autonomous weapons the complexity and
sophistication could increase with expansion of targets and types and target area is do you think there's like
a limit to which we can prepare against such an epic evolution and do you think
that bureaucracy can keep up with how fast these the autonomy of these weapons
couldn't develop over time yeah I'm not sure I caught all the first business
question but there's definitely it's definitely a challenge that the types of legal discussions at the UN Convention
on conventional weapons they they're not famous for going too quickly in fact
they're incredibly slow and in that framework every state essentially has a
veto over everything so even over the agenda of the next meeting if you know
if the US wants to block the agenda they can block the agenda let alone block the outcome that might come if you could
agree an agenda so so every state has an ability to keep things moving very
slowly there and that's definitely a challenge in the context where pace of technological development moves pretty
quickly the only thing I would say which I forgot to mention before in terms of thinking about the dynamics in this debate is that it's not
straightforwardly a situation where militaries really want loads more autonomous weapons and other people
don't I mean military commanders also like control and they they like troops on the
ground like control and they like trust and confidence in the systems that they're operating around you don't wanna
get blown up by their own equipment and military commanders like control and like to know what's happening so there
are some constraints within the military structures as well to to the overall
sort of development here I guess from our side in terms of this sort of how to constrain against this expansion of
attacks in the expansion of sort of so objects that may be attacked for
autonomous systems in a way that's why I feel like developing the idea that there's a principle of human control that needs to be applied even if it's a
bit fuzzy in its boundaries we can use that and interrogate it as a social process to try and keep constraint going
back towards the specific because in the end like I said earlier these legal structures are sort of social processes
as well and it's not very easy it's not something where you can just straightforwardly draw a line and then no new technologies will come along that
challenge your expectations right rather we need to find the sort of camp on the international legal political landscape
we need to sketch out that parameters of that camp in legal terms and then we
need people to turn up at those meetings and continuously complain about things and put pressure on things because
that's the only way over time where you maintain that sort of interrogation of future technologies as they come out of
the pipeline or or whatever so it's a sort of social function I think the
balance between like how fast science would be like dancing in this field versus like how fast we are so you can
move to keep up yeah I don't know you can just be resolved I think it's an ongoing it's got to be an ongoing social political process in a way given that
this course is on AGI and we'll likely see a wide variety of different kinds of
autonomous systems in the future can you give us perhaps some sort of extrapolation from this domain to a
broader set of potentially risky behaviors that more autonomous and more
intelligent systems would do and ways that you know the creators of such systems such as essentially the folks
sitting in this room can change what they're doing to make those safer yeah I
mean I think useful to think about in some ways these
ideas of from the present from where we are now how can people involved in
developing different technologies new technological capacities just be thinking of the potential outcomes in
this sort of weaponization area and building in some orientation to their work that the thinks about that and
thinks about what the potential consequences of work can be I mean I think in some ways the risky outcomes
type thinking I mean again it gets you into hypothetical arguments but the the
idea of two sides both with substantial autonomous weapon system capabilities is
probably the sort of area where these ideas of accidental escalations come to
the fore that if you've got to you know adversary orientated States with
substantial autonomous systems then there's a potential for interactions to
occur between those systems that rapidly escalate a violent situation in a way
that greater capacity for human engagement would allow you to to curtail
it and to stall it and I think I mean I know in other areas of you know
algorithm functioning in society we've seen aspects of that right in a sort of
probably in the financial sector and another such location so so I think yeah
those area those ideas have sort of rapidly escalating cascading risks is is
a is a concern in that area again based on hypothetical thinking about you know stuff last question all right what do
you think of this criteria so we have this tank example on the right our
simulations our ability to simulate things is getting better and better what if we showed a simulation of what would
happen to a person that has the ability to hit the Go button on it and if the simulation does not have enough fidelity
we consider that a no-go we we cannot do that or if the simulation shows it does
have a fun fidelity and it shows a bad outcome then maybe that would be a criterion in which
to to judge this circumstance on the right and that could also let us as that
circle gets bigger and bigger it can let us kind of put a it could let us cap
that by saying hey if we don't if we do not have enough information to make this
simulation to even show the person then it's a no-go yeah yeah I think in a way
this is an issue of modeling right based on contextual information that you that you have so maybe with technological
developments you have a better capacity for modeling specific situations I
suppose the challenge is how do you in a sort of timely manner especially in a conflict environment where tempo is
significant can you can you put the data that you have into some sort of modeling
system adequately but I don't see any problem with the idea of using AI to
model the outcomes of specific attacks and you know give you readouts on what
the likely effects are going to be I guess the challenge is that what counts as of adequate effect and where the
boundary lines of sufficient information and insufficient information fall they're kind of open questions as well
right and you know militaries tend to like to leave some openness on those
those points as well but but I think there can be definitely a role for modeling in better understanding what
what effects it can be great let's give her a chair big hand thank you

----------

-----
--37--

-----
Date: 2018.04.09
Link: [# MIT-AVT: Data Collection Device (for Large-Scale Semi-Autonomous Driving)](https://www.youtube.com/watch?v=HVx9bwiMWGQ)
Transcription:

the MIT autonomous vehicle technology
study is all about collecting large
amounts of naturalistic driving data
behind that data collection is this box
right here that Dan is term writer a Dan
is behind a lot of the hardware work we
do embedded systems and Michael is
behind a lot of the software the data
pipeline as well as just offloading the
data from the device would like to tell
you some of the details behind rider and
behind the sensors now we have three
cameras in the car and the wires are
running back into the trunk and that's
where the rider is sitting there's a lot
of design specifications to make this
system work month after month reliably
across multiple vehicles across multiple
weather conditions and so on at the end
of the day with multiple sensor streams
we have the three cameras coming in we
have IMU GPS and all of the raw canned
messages coming from the vehicle itself
and all of that has to be collected
reliably synchronized and post processed
once we offload the data first we have a
single board computer here running a
custom version of Linux that we wrote
specifically for this application this
single board computer integrates all of
the cameras all the sensors GPS can IMU
and offloads it all on to the
solid-state hard drive that we have on
board there are some extra components
here for cellular communication as well
as power management throughout the
device here we have our single board
computer as well as sensor integration
and our power system this is our
solid-state drive that connects directly
to our single board computer on our
single board computer we have a sensory
integration board on top here you'll be
able to see our real-time clock as well
as its battery backup and can
transceiver on the reverse side of this
board we have our GPS receiver an IMU
this is our can't control power board
which monitors can throughout the car
and determines whether or not the system
should be on or off when the system is
on this sends power through a buck
converter to reduce the 12 volts from
the vehicle down to 5 volts to operate
the single board computer we also have a
4G wireless connection on board to
monitor the health of rider and
determine things like free capacity left
on our dry
as well as temperature and power usage
information the cameras connect to Ryder
through this USB hub right here so we
needed the box to do at least three
things one was record from at least
three cameras record can vehicle
telemetry data and then lastly be able
to store all this data onboard for a
long period of time such that people
could drive around for months without
having us to offload the data from their
vehicles and so when we're talking about
hundreds of thousands of miles of worth
the data so for about every hundred
thousand miles uncompressed that's about
a hundred petabytes of video data so one
of the key other requirements was how to
store all this data both on the device
and how to be able to then offload us
successfully onto thousands of machines
to be then processed with the computer
vision with a deep learning algorithms
that we're using and one of the
essential elements for that was to do
compression onboard so these are
Logitech c920 webcam
they can do up to 1080p at 30 frames a
second the major reason why we went with
these is because they do onboard h.264
compression of the video so that allows
us to offload all the processing from
our single board computer onto these
individual cameras allowing us to use a
very slim pared-down
lightweight single board computer to run
all of these sensors this is the
original Logitech c920 that you would
buy at a store these are the two same
Logitech c920 s although they were put
into a custom-made camera case just for
this application what this allows us to
do is at our own C s type lenses to
enable us to have a zoom lens as well as
a fisheye lens from within the car
allowing us a greater range of field of
views inside the vehicle so this is the
fisheye lens this is the zoom lens and
the CS type there's also C type those
are types of standard lenses that are
connect to these types of cameras often
to the industrial cameras that are often
used for our Thomas vehicle applications
we tested these cameras to see what
would happen to them if placed inside of
a
a hot car and it's um on a summer day we
wanted to see what these cameras still
be able to hold up to this to the heat
in the summer and still function as
needed we put these cameras in a toaster
a scientific toaster what was the
temperature that went up to we cycled
these cameras between 58 and 75 degrees
Celsius which is about the maximum of a
hundred and fifty degree Fahrenheit max
temperature that a car would get in in
the summer we also cranked it up to 127
degrees Celsius just to see what would
happen to these cameras after prolonged
long-term high heat in fact these
cameras continued to work perfectly fine
after that creating a system that would
intelligently and autonomously turn off
and on to start and end recording was
also a key aspect to this device since
people were just going to be driving
their normal cars we couldn't rely on
them necessarily to start and end
recording so this device rider
intelligently figures out when the car
is running and when it's off to start
and stop recording automatically so how
does writers specifically know when to
turn on so we use can to determine when
the system should turn off and on when
can is active the car is running and we
should turn the system on when can is
inactive we should turn the system off
and end recording this also gives us the
ability to trigger on certain can
messages so for instance if we want to
start recording as soon as they approach
the car and unlock the door we can do
that or if they turn the car on or they
put it into drive or so on
the cost of the car that the system
resides in is about a thousand times
more than the system itself so these a
hundred thousand plus dollar cars so
we'll have to make sure that we design
the system we'll run the wires in such a
way that doesn't do any damage to the
vehicles what kind of things fail when
they fail the biggest issue we've had
with the system our camera cables
becoming unplugged so when a camera
cable becomes unplugged the system will
try to restart that subsystem multiple
times and if it's unable to it
completely shuts off recording and as
long as that cable is still unplugged
writer will not start up the next
so one issue that we've seen is that
cables becoming a plugged causes us to
lose the potential to record some data
and that was one of the requirements of
the system from the very beginning is
that all the video streams are always
recorded perfectly and synchronized now
if any of the systems are failing to be
recording from the sensors that we try
again restart the system restart the
system and if it's still not working it
should shut down so the video in order
to understand what drivers are doing
these systems the video is essential so
if one of the cameras is not working
that means the system that's not working
as a whole the other crucial component
of having a data collection system
that's taking the multiple streams is
that those streams have to be
synchronized perfectly synchronization
was the highest priority from the very
beginning of writers design we have a
real-time clock
onboard writer that allows us down to
two parts per million of accuracy and
time stamping this means over the course
of a one and a half hour drive our time
stamps issue to each of the different
subsystems may drift up to seven or so
milliseconds relatively this is
extremely small compared to most clocks
on computers today and once the data is
offloaded the very first thing we do is
make sure that the time stamping that
the data was time stamp correctly so
that we can synchronize it and the very
first thing is part of the data pipeline
would do is synchronize the data that
means using the time stamp that came
from the real-time clock that was
assigned to every single piece of sensor
data using that time stamp to align the
data together now for video that means
30 frames a second perfectly aligned
with other GPS signals and so on there
are some other sensors like I am you and
the can messages coming from the car
that come much more frequently than 30
Hertz 30 frames a second so we have a
different synchronization scheme there
but overall synchronization from the
very beginning of the design of the
hardware to the very end of the design
of the software pipeline is crucial
because we want to be able to analyze
what people are doing in these semi
autonomous vehicles how they're
interacting with the technology and that
means using data that comes from the
face camera the body camera the forward
view synchronized together with a GPS
that I'm you and all the messages coming
from the vehicle telemetry from camp the
video stream compression which is a very
much CPU or GPU intensive operations
performed onboard the camera there are
other CPU intensive operation performed
on Ryder like the sense of fusion for
IMU but for the most part there's
sufficient CPU cycles left for the
actual data collection to not have any
skips or drifts in the census stream
collection one of the questions we get
is how do we get the data from this box
to our computers then to the cluster
that's doing the compute so when we
receive a hard drive from one of these
Ryder boxes that we're swapping we
connect the hard drive locally to our
computers and then we do a remote copy
to a server that contains all of our
data we then check the data for
consistency and perform any fixes and
the raw data in preparation for a
synchronization operation so we're not
doing any remote offloading of data so
the data lives on Ryder until the
subjects the drivers the owners of the
car come back to us and offload the data
so we take the hard drive swap it out
and aweful the data from the hard drive
can you tell me this the journey that a
pixel takes on its way from the camera
to our cluster well first the camera
records the raw image data based on
these settings that we've configured
from the Ryder box and that raw image
data is compressed on the camera itself
into an h.264 come format and then
transmitted over the USB wire to the
single board computer on the Ryder box
then it's recorded on to the solid-state
drive in a video file where it will stay
until we do an offload in the course of
about six months for rnds subjects and
in one month for 50 subjects after that
it is connected to a local computer
synchronized within a remote server
and is then processed with initial
cleaning algorithms in order to remove
any corrupt data or to fix any subject
data in the configuration files for that
particular trip after the initial
cleaning is taken care of it is
synchronized at 30 frames per second and
can then be used for different detection
algorithms or manual annotation so the
important hard work behind the magic
that deep learning computer vision
unlocks is the synchronization the
cleaning of the messy data making sure
we get anything that's at all weird in
any way in the in the data out so that
at the end of the pipeline we have a
clean data set of multiple sensor
streams perfectly synchronized that we
can then use for both analysis and for
annotation so that we can improve the
neural network models used for the
various detection tasks so writers done
an amazing job over 30 vehicles of
collecting hundreds of thousands of
miles worth of data billions of video
frames so we're talking about an
incredible amount of data all compressed
with h.264 that's close to 300 terabytes
worth of data but of course you can
always improve so so what our next steps
one huge improvement for writer would be
transitioning to another single board
computer in particular a Jetson tx2
there's a lot more capability for added
sensors as well as much more compute
power and even the possibility for
developing some real-time systems with a
Jetson one of the critical things when
you're collecting huge amounts of data
and driving is you realize that most of
driving is quite boring nothing
interesting in terms of understanding
driver behavior or training computer
vision models for edge cases and so on
nothing interesting happens so one of
the future steps we're taking is based
on the thing we found in the data so far
we know which parts are interesting
which are not and so when a design on
board algorithms that are processing in
real time that video data
Herman is this the kind of data I want
to keep it this time and if not throw it
out that means we can collect more
efficiently just the bits that are
interesting for edge case neural network
model training or for understanding
human behavior now this is a totally
unknown open area because really we
don't understand what people do and send
me a time with vehicles when the car is
driving itself and the human is driving
itself so the initial stages of the
study were to keep all the data so we
can do the analysis to analyze the body
pose glance allocation activity
smartphone usage all the various Sun
decelerations autopilot usage where it's
used how it's used geographic weather
night so on but as we start to
understand where the fundamental
insights come from we can start to be
more and more selective about which
epochs of data we want to be collecting
now that requires real time processing
of the data and as Dan said that's where
the justin tx2 the power that the justin
takes to brings is becomes more and more
useful now all of this work is part of
the MIT autonomous vehicle technology
study we've collected over three hundred
twenty thousand miles so far and
collecting five hundred to a thousand
miles every day so we're always growing
adding new vehicles we're working at
adding a Tesla Model 3 a Cadillac ct-6
super cruise system and others one of
the driving principles behind our work
is that the kind of data collection we
need to design safe semi autonomous and
autonomous vehicles is that we need to
record not just the forward roadway or
any kind of sensor collection on the
external environment we need to have
rich sensor information about the
internal environment what the driver is
doing everything about their face the
glance all the cognitive load and body
pose everything about their activity we
truly believe that autonomy autonomous
vehicles require an understanding of how
human supervisors of those systems
behave how we can keep them attentive
keep their glance on the road keep them
as effective efficient supervisors of
those systems
you

----------

-----
--36--

-----
Date: 2018.03.20
Link: [# MIT AGI: Cognitive Architecture (Nate Derbinsky)](https://www.youtube.com/watch?v=bfO4EkoGh40)
Transcription:

Intro
so today we have nadir bin ski he's a professor at Northeastern University working on various aspects of
computational agents that exhibit human level intelligence please give Nate a warm welcome thanks a
lot and thanks for having me here so the title that was on the page was cognitive
modeling I'll kind of get there but I wanted to put it in context so the the bigger theme here is I want to talk
about what's called cognitive architecture and if you've never heard about that before that's great and I
wanted to contextualize that as how are we what how is that one approach to get us to AGI
Outline
and I say what my view of AGI is and put up a whole bunch of TV and movie
characters that I grew up with that inspire me that will lead us into what is this thing called cognitive
architecture it's a whole research field that crosses neuroscience psychology cognitive science and all the way into
AI so I'll try to give you kind of the historical big-picture view of it what some of the actual systems are out there
that might be of interest to you and then we'll kind of zoom in on one of them that I've done a good amount of work with called soar and what I'll try
to do is tell a story a research story of how we started with kind of a core
research question we look to how humans operate understood that phenomenon and
then took it and so really interesting results from it and so at the end if this field is of interest there's a few
pointers for you to go read more and and go experience more of cognitive architecture so just rough definition of
AGI given this in AGI class depending the direction that you're coming from it
might be kind of understanding intelligence or maybe developing intelligent systems they're operating at
the level of human level intelligence the the typical differences between this
and other sorts of maybe AI machine learning systems we want systems that are going to persist for a long period
of time we want them robust to different conditions we want them learning over time and here's the crux of it working
on different tasks and in a lot of cases tasks they didn't know we're coming ahead of time I got into this because I
clearly watched too much TV and too many movies and then I looked back at this
and I realized I think I'm covering 70's 80's 90's nots I guess it is and today
and so this is what I wanted out of AI and this is what I wanted to work with
and then there's the the reality that we have today
Expectations Meet (Current) Reality
so instead of so who's watched Knight Rider for instance I I don't think that
exists yet but but maybe we're getting there and in particular for fun during the
Amazon sale day I got myself an Alexa and I could just see myself at some point saying Alexa please might write me
an R sync script you know to sync my class and if you have an Alexa you
probably know the following phrase this this just always hurts me inside which is sorry I don't know that one which is
okay right that's a lot of people have no idea what I'm asking let alone how to do that so what I want Alexa to respond
with after that is do you have time to teach me and to provide some sort of
interface by which back and forth we can kind of talk through this that we aren't there yet to say the least but I'll talk
later about some work on a system called Rosie that's working in that direction
we're starting to see see some ideas about being able to teach systems out of work
Common Motivations
so folks who are in this field I think generally fall into these three
categories they're just curious they want to learn new things generate knowledge work on hard problems great I
think there are folks who are in kind of that middle cognitive modeling realm and
so I'll use this term a lot it's really understanding how humans think how
humans operate human intelligence at multiple levels and if you can do that one there's just knowledge in and of
itself of how we operate but there's a lot of really important applications that you can think of if we were able to
not only understand but predict how humans would respond react in various
tasks medicine is is an easy one there's some work in HCI or HR I I'll get to
later where if you can predict how humans would respond to a test you can iterate tightly and develop better
interfaces it's already being used in the realm of simulation and in defense
industries I happen to fall into the latter group which or the bottom group
which is systems development which is to say just the desire to build systems for various tasks that are working on tasks
that kind of current AI machine learning can't operate on and I think when you're
working at this level or on any system that nobody's really achieved before
what do you do you you kind of look to the examples that you have which in this case that we know of
it's just humans right irrespective of your motivation when you have kind of an
Motivations/Questions Dictate Approach
intent that you want to achieve in your research you kind of let that drive your approach and so I often show my AI
students this the touring test you might have heard of or variants of it that have come before
these were folks who are trying to create system that acted in a certain way that acted intelligently and the
kind of line that they drew the benchmark that they used was to say let's make systems that operate like
humans do cognitive modelers will fit up into this top point here to say it's not enough to
act that way but by some definition of thinking we want the system to do what
humans do or at least be able to make predictions about it so that might be things like what errors would the human
make on this task or how long would it take them to perform this task or what emotion would be produced in this task
there are folks who are still thinking about how the computer is operating but
it's trying to apply kind of rational rules to it so a logician for instance
would say if you have a and you have B the a gives you B B gives you see a
should definitely give you C that's just what's rational and so there folks operate in that direction and then if
you go to intro AI class anywhere in the country particularly Berkeley because
they have graphics designers that I get to steal from the benchmark would be what the system produces in terms of
action and the benchmark is some sort of optimal rational bound irrespective of
where you work in the space there's kind of a common output that arrives when you
research these areas which is you can learn individual bits and pieces and it
can be hard to bring them together to build a system that either predicts or acts on different tasks so this is part
of the transfer learning problem but it's also part of having distinct theories that are hard to combine
together so I'm going to give an example that come comes out of cognitive modeling or perhaps three examples so if
you were in a HCI class or some interest psychology classes one of the first things you'll learn about is Fitz law
which provides you the ability to predict the difficulty level of
basically a human pointing from where they start to a particular place and it turns out that you can
learn some parameters and model this based upon just the distance from where you are to the targets and the size of
the target so both moving along distance will take a while but also if you're aiming for a very small point that can
take longer then if there's a large area that you just kind of have to get yourself to and so this is held true for
many humans so let's say we've learned this and then we move on to the next task
and we learn about what's called the power law of practice which has been
shown true in a number of different tasks what I'm showing here is one of them where you're going to draw a line
through sequential set of circles here starting at 1 going to 2 and so forth not making a mistake or at least not
trying to and try to do this as fast as possible and so for a particular person
we would fit the a B and C parameters and we'd see a power law so as you perform this task more you're going to
see a decrease in the amount of reaction time required to complete the task great
we've learned two things about humans let's add some more in so for those who might have done some reinforcement
learning TV learning is one of those approaches temporal difference learning that's had some evidence of similar
sorts of processes in the dopamine centers of the brain and it basically says in a sequential learning tasks you
perform the task you get some sort of reward how are you going to kind of update your representation of what to do
in the future such as to maximize expectation of future reward and there are various models of how that changes
over time and you can build up functions that allow you to form better and better and better a given trial and error great
so we've learned three interesting models here that hold true over multiple
people multiple tasks and so my question is if we take these together and add
them together how do we start to understand a task as quote/unquote
simple as chess which is to say we could ask questions how how long would it take
for a person to play what mistakes would they make they played a few games how would they
adapt themselves or if we want to develop system that ended up being good
at chess or at least learning to become better at chess my question is if you could there
doesn't seem to be a clear way to take these very very individual theories and kind of smash them together and get a
reasonable answer of how to play chess or how do humans play chess and so
Unified Theories of Cognition
gentlemen in this slide is Alan Newell one of the founders of AI did incredible
work in psychology and other fields he gave a series of lectures at Harvard in 1987 and they were published in 1990
called the unified theories of cognition and his argument to the psychology community at that point was the argument
on the prior slide they had many individual studies many individual results and so the question was how do
you bring them together to gain this overall theory how do you make forward progress and so his proposal was unified
theories of cognition which became known as cognitive architecture which is to
say to bring together your core assumptions your core beliefs of what
are the fixed mechanisms and processes that intelligent agents would use across
tasks so the representations the learning mechanisms the memory systems
bring them together implement them in a theory and use that across tasks and the
core idea is that when you actually have to implement this and see how it's going to work across different tasks the
interconnections between these different processes and representations would add
constraint and over time the constraints would start limiting the design space of
what is necessary and what is possible in terms of building intelligent systems and so the overall goal from there was
to understand and exhibit human level intelligence using these cognitive architectures a'nature
Making (Scientific) Progress Lakatos 1970
question asked is okay so we've gone from a methodology of science that we
understand how to operate in we make a hypothesis we construct a study we
gather our data we evaluate that data and we falsify we do not falsify the original hypothesis and we can do that
over and over again and we know that we're making for progress scientifically if I've now taken that model and changed
it into I have a piece of software and it's representing my theories and to
some extent I can configure that software in different ways to work on different tasks how do I know that I'm making progress and so there's a form of
science called lactose ium and it's kind of shown pictorially here where you
start with your core of what your your beliefs are about where your head what
is necessary for achieving the goal that you have and around that you'll have kind of ephemeral hypotheses and
assumptions that over time may grow and shrink and so you're trying out different things trying out different things and if an assumption is around
there long enough it becomes part of that core and so as you work on more tasks can learn more
either by your work or by data coming in from with someone else the core is growing larger and larger
you've got more constraints and you've made more progress and so what I wanted to look at we're in this community what
are some of the core assumptions that are driving forward scientific progress so one of them actually came out of
Time Scales of Human Action Newell 1990
those lectures they're referred to as Newell's time scales of human action and so off on the left the left two columns
are both time units just expect somewhat differently second from the left being maybe more useful to a lot of us in
understanding daily life one step over from there would be kind of at what level processes are occurring so the
lowest three are down at kind of the substrate the neuronal level we're building up to deliberate tasks that
occur in the brain and tasks that are operating on the order of ten seconds some of these might occur in the
psychology laboratory but probably a step up to and ours and then above that really
becomes interactions between agents over time and so if we start with that the things to take away is that regular the
hypothesis is that regularities will occur at these different time scales and that they're useful and so those who
operate at that lowest time scale might be considering neuroscience cognitive neuroscience when you shift up to the
next couple levels what we would think about in terms of the areas of science that deal with that would be psychology
and cognitive science and then we shift up a level and we're talking about sociology and economics and the
interplay between agents over time and so what we'll find with cognitive
architecture is that most of them will tend to sit at the deliberate act we're trying to take knowledge of a situation
and make a single decision and then sequences of decisions over time will
build to tasks and tasks over time will build to more interesting phenomenon I'm actually going to show that that isn't
strictly true that there are folks working in this field that actually do operate one level below some other
Bounded Rationality Simon 1967
assumptions so this is herb Simon receiving the Nobel Prize in Economics
and part of what he received that award for was an idea of bounded rationality
so in various fields we tend to model humans as rational and his argument was
let's consider that human beings are operating under various kinds of
constraints and so to model the rational with respect to and bounded by how
complex the problem is that they're working on how big is that search space that they have to conquer cognitive
limitations so speed of operations amount of memory short-term as well as
long-term as well as other aspects of our computing infrastructure that are going to keep us from being able to
arbitrarily solve complex problems as well as how much time is available to
make that decision and so this is actually a phrase that came out of his speech when he received the Nobel Prize
decision-makers can satisfice either by finding optimum solutions for a simplified world just to
say take your big problem simplify in some way and then solve that or by
finding satisfactory solutions for a more realistic world take the world and all its complexity take the problem in
all its complexity and try to find something that works neither approach in general dominates the other and both
have continued to co-exist and so what you're actually going to see throughout the cognitive architecture community is
this understanding that some problems you're not going to be able to get an
optimal solution to if you consider for instance bounded amount of computation
bounded time the need to be reactive to a changing environment these sorts of issues and so in some sense we can
decompose problems that come up over and over again into simpler problems solve those newer optimally or optimally fix
those in optimize those but more general problems we might have to satisfy some
there's also the idea of the simple system hypothesis so this is Alan Newell
Physical Symbol System Hypothesis Newal & Simon 1976
and herb Simon they're considering how a computer could play the game of chess so
the physical system physical symbol system talks about the idea of taking something some signal abstractly
referred to as symbol combining them in some ways to form expressions and then
having operations that produce new expressions a weak interpretation of the
idea that symbol systems are necessary and sufficient for intelligent systems a
very weak way of talking about it is the claim that there's nothing unique about
the neuronal infrastructure that we have but if we got the software right we
could implement it in the bits bytes Ram and processor that make up modern computers that's kind of the weakest way
to look at this that we can do it with silicon and not carbon stronger way that
this used to be looked at was more of a logical standpoint which is to say if we
can encode rules of logic these tend to line up if we think intuitively of
planning and problem solving and if we can just get that right and get enough fat's in there and enough
in there that somehow intelligence well that's what we need for intelligence and eventually we can get to the point of
intelligence and that's what you need for intelligence and that was a starting
point that lasted for a while I think by now most folks in this field would agree
that that's necessary to be able to operate logically but that there are going to be representations and
processes that will benefit from non symbolic representation so particularly perceptual processing visual auditory
and processing things in a more kind of standard machine learning sort of way as
well as kind of statistic taking advantage of statistical representations
so we're getting closer to actually looking at cognitive architectures I did
Active Architectures by Focus
want to go back to the idea that different researchers are coming with different research foci foci and we'll
start off with kind of the lowest level and understanding biological modeling so Leiber and spawn both try to model
different degrees of low-level details parameters firing rates connectivities
between different kind of levels of neuronal representations they build that
up and then they tried to build tasks above that layer but always being very cautious about being true to human
biological processes at a layer above there would be psychological modeling
which is to say trying to build systems that are true in some sense to areas of
the brain interactions in the brain and being able to predict errors that we made timing that we produced by the
human mind and so there I'll talk a little bit about Akhtar this final level
down here these are systems that are focused mainly on producing functional
systems that exhibit really cool artifacts and and solve really cool
problems and so I'll spend most of the time talking about soar but I want to point out a relative newcomer in the
game called Sigma so to talk about spawn a little bit we'll see if the sound works in here I'm
going to let the Creator take this one
or not see how the AV system likes this
Semantic Pointer Architecture Unified Network
[Music]
but of course if I wouldn't be pleased with a pad of the microseconds and all celebrated since we're engineering is
critical goal engineering allows you to break down equation intense very precise descriptions which we can test like
building actual models one probably do recently is called the song model this Moscow has the two and a half million
individual mountains isolated and evens the model is a knife and the up the
canal is Bernard so essentially you can see images of numbers and that is something like a
progressive in the case of the discarded into that seat but magic tide reproduces style that
yeah so for instance it's easy to get the on environment and actually forgive separately and silently on
medical side we all know that we have cognitive town concession we get over and we can try that address accountant
spicy alienating process with these nice models another potential area in fact it's on artificial intelligence a lot of
working out visual Imperfects thanks to donations that are exceeded it at one pass pretty since plane test what's
special is fine is that it's like that many different paths and this X we have not found it might appear californee the
flow of information through different parts of the model something I haven't seen very well so provide a pointer at
the end he's got a really cool book called how to build a brain and if you google and you can google spun you can
find a toolkit where you can kind of construct circuits that will approximate
functions that you're interested in connect them together set certain properties that you would want at a low
level and build them up and actually work on tasks at the level of vision and
robotic actuation so that's a really cool system as we move into
Prototypical Architecture
architectures that are sitting above that biological level I wanted to give you kind of an overall sense of what
they're going to look like what a prototypical architecture is going to look like so they're gonna have some ability to have perception the
modalities typically are more digital symbolic but they will depending on the
architecture be able to handle vision audition and various sensory inputs
these will get represented in some sort of short-term memory whatever the state's representation for the
particular system is there it's typical to have a representation of the
knowledge of what tasks can be performed when they should be performed how they should be controlled and so these are
typically both actions that take place internally that manage the internal
state of the system and perform internal computations but also about external actuation and external might be
a digital system a game AI but it might also be some sort of robotic actuation in real world there's typically some
sort of mechanism by which to select from the available actions in a particular situation there's typically
some way to augment this procedural information which is to say learn about
new actions possibly modify existing ones there's typically some semblance of what's called declarative memory so
whereas procedural at least in humans if I asked you to describe how to ride a
bike you might be able to say get on the seats and pedal but in terms of keeping
your balance there you'd have a pretty hard time describing it declaratively so
that's kind of the procedural side the implicit representation of knowledge whereas declarative would include facts
geography math but it also include experiences that the agent has had a more episodic
representation of declarative memory and they'll typically have some way of learning this information on mending it
over time and then finally some way of taking actions in the world and they'll
all have some sort of cycle which is perception comes in knowledge that the
agent has is brought to bear on that an action is selected knowledge that knows
to condition on that action will act accordingly both with internal processes as well as eventually to take action and
then rinse and repeat so when we talk about in an AI system an agent in this
context that would be the fixed representation which is whatever architecture we're talking about plus
set of knowledge that is typically specific to the task but might be more
general so oftentimes these systems could incorporate a more general knowledge base of facts of linguistic
facts of Geographic facts let's take Wikipedia and let's just stick it in the brain of the system
there'll be more tasks in general but then also whatever it is that you're doing right now how should you proceed
in that and then it's typical to see this processing cycle and going back to
the prior assumption the idea is that these primitive cycles allow for the
agent to be reactive to its environment so if new things come in that has react to if the Lions sitting over there I
better run and maybe not do my calculus homework right so as long as this cycle
is going I'm reactive but at the same time if multiple actions are taken over time I'm able to get complex behavior
over the long term so this is the act our cognitive architecture it has many
of the kind of core pieces that I talked about before let's see if the mouse yes
mouse is useful up there so we have the procedural model here a short-term
memory is going to be these buffers that are on the outside the procedural memory is encoded as what I call production
rules or if-then rules if is this is the state of my short-term memory this is
what I think should happen as a result you have a selection of the appropriate
rule to fire and an execution you're seeing associated parts of the brain
being represented here cool thing that has been done over time in the act our community is to make predictions about
brain areas and then perform MRIs and gather that data and correlate that data
so when you use the system you will get predictions about things like timing of
operations errors that will occur probabilities that something is learned but you'll also get predictions about to
degree that they can kind of brain areas that are going to line light up and if
you want to that's actively being developed at Carnegie Mellon to the left
is John Anderson who developed this cognitive architecture ooh 30 ish years
ago and until the last about five years he was the primary researcher developer
behind it with Christian and then recently he's decided to spend more time on cognitive tutoring systems and so
Christian has become the primary developer there is an annual akhtar
workshop there's a summer school where if you're thinking about modeling a
particular task you can kind of bring your task to them bring your data they teach you how to use the system and try
to get that study going right there on the spot to give you a sense of what
ACT-R Notes
kinds of tasks this could be applied to so this is a representative of a certain
class of tasks certainly not the only one let's try this again
I think powerpoints going to want a restart every time okay so we're getting
predictions about basically where the eye is going to move what you're not seeing is it's actually processing
things like text and colors and making predictions about what to do and how to represent the information and how to
process the graph as a whole I had alluded to this earlier there's
work by Bonnie John very similar so making predictions about how humans
would use computer interfaces and at the time she got hired away by IBM and so
they wanted the ability to have software that you can put in front of software designers and when they think they have
a good interface press a button this model of human cognition would try to perform the tasks that have been told to
do and make predictions about how long it would take and so you can have this tight feedback loop from designers
saying here's how good your particular interfaces so act are as a whole it's
very prevalent in this community I went to their web page and counted up just the papers that they knew about it was
over 1,100 papers over time if you're interested in it the main distribution
is in Lisp but many people have used this and wanted to apply it to systems that need a little more processing power
so there's the NRL has a Java port of it that they use in robotics the Air Force
Research Lab and Dayton has implemented it in Erlang for a parallel processing
of large declare knowledge bases they're trying to do service-oriented architectures with it CUDA because they
want what it has to say they don't want to wait around for it to have to figure that stuff out
so that's the two minutes about Akhtar Sigma is a relative newcomer and it's
developed out at the University of Southern California by man named Paul rosenbloom mmm mentioned a couple
minutes because he was one of the prime developers of soar at Carnegie Mellon so he knows a lot about how store works and
he's worked on it over the years and I think originally I'm gonna speak for him and he'll probably say I was wrong I
think originally it was kind of a mental exercise of can i reproduce or using a
uniform substrate I'll talk about so in a little bit it's thirty years of research code if anybody is dealt with
research code it's thirty years of C and C++ with dozens of graduate students
over time it's not pretty at all and and theoretically it's got these boxes
sitting out here and so he reimplemented the the core functionality of soar all
using factor graphs and message passing algorithms under the hood he got to that
point and then said there's nothing stopping me from going further and so now it can do all sorts of modern
machine learning vision optimization sort of things that would take some time in any other architecture to be able to
integrate well so it's been an interesting experience it's now going to
be the basis for the virtual human project out at the Institute for Creative Technology it's Institute
associated with University of Southern California for him until recently could
get your hands on it but in the last couple years he's done some tutorials on it he's got a public release with
documentation so that's something interesting to keep an eye on but I'm
gonna spend all the remaining time on the Soraa cognitive architecture and so you see it looks quite a bit like the
prototypical architecture and I'll give a sense again about how this all operates give a sense of the people
involved we already talked about Alan Newell so both John Laird who is my advisor and Paul Rosenbloom were
students of Alan Newell John's thesis project was related to the chunking
mechanism and soar which learns new rules based upon sub-goal reasoning so
he finished that I believe the year I was born and so he's one of the few
researchers you'll find who's still actively working on their thesis project
beyond that's about I think about ten years ago he founded soar technology
which is company up in Ann Arbor Michigan while it's called solar technology it doesn't do exclusively soar but that's a part of the portfolio
general intelligence system stuff a lot of Defense Association so some notes of
Soar Notes
what's going to make soar different from the other other architectures that fall into this kind of functional
architecture category a big thing is a focus on efficiency so john wants to be
able to run soar on just about anything we just got on the soar mailing list a
desire to run it on a real-time processor and our answer while we had
never done it before was probably it'll work every release there's timing tests
and we always what we what we look at is in a bunch of different domains for a bunch of different reasons that relate
to human processing there's this magic number that comes out which is 50 milliseconds which is to say in terms of
responding to tasks if you're above that time humans will sense a delay and you
don't want that to happen now if we're working in a robotics task 50 milliseconds if you're dramatically
above that you just fell off the curb or worse or you just hit somebody in a car right so we're trying to keep that as
low as possible and for most agents it it doesn't even register it's below 1
millisecond fractions of millisecond but I'll come back to this because a lot of the work that I was doing was computer
science AI and a lot of efficient algorithms and data structures and 50 milliseconds was that very high upper
bound it's also one of the projects that has a public distribution you can get in all sorts of operating systems we use
something called swig that allows you to interface with it in a bunch of different languages we kind of describe the meta description and you are able to
basically generate bindings and different platforms Korres C++ there was
a team at sore tech that said we don't like C++ it gets messy so they actually did a port over to pure Java in case
that appeals to you there's an annual soar workshop that takes place in Ann
Arbor typically it's free you can go there get a sort tutorial and talk to
folks who are working on soar and it's fun I've been there every year but one in the last decade it's just fun to see
the people around the world that are using the system and all sorts of interesting ways to give you a sense of
the diversity of the applications one of the first was our one store which was back in the days when it was an actual
challenge to build a computer which is to say that your choice of certain components would have radical
implications for other parts of the computer so it wasn't just the Dell website where you just I want this much
RAM I want this much CPU there was a lot of thinking that went behind it and then physical labor that went to construct
your computer and so it was making that process a lot better there are folks that apply to natural language
processing I saw r7 was the core of the virtual humans project for a long time
HCI tasks terrasaur was one of the largest rule-based systems tens of thousands of rules over 48 hours
it was a very large-scale simulation a defense simulation lots of games it's
been applied to for various reasons and then in the last few years porting it on to mobile robotics
platforms this is Edwin Olsen's splinter bot an early version of it that went on
to win the magic competition then I went
on to put soar on the web and if after this talk you're really interested in a dice game that I'm going to talk about
you can actually go to the iOS App Store and download it's called Michigan liar's
dice it's free you don't have to pay for it but you can actually play a liar's dice with soar and it's even set the
difficulty level it's pretty good it beats me on a regular basis I wanted to
give you a couple other just kind of really weird feeling sort of applications and really cool applications the first one
Luminal ADAM Lab @ GATech
is out of Georgia Tech go PowerPoint is
dom-based interactive art installation in which she participants can engage and
collaborate the movement improvisation with each other and virtual advance permits this thing her actually creates
a hyperspace English virtual and quicker real bodies meet the line between human
and non-human is learned through images to examine a relationship with technology the night installation
ultimately examines how humans and machine can co-create experiences and it
ducks out in a playful environment the don't creates a social space that encourages human human interaction and
collective dance experiences allowing the depends to create an explorer movement while having fun the
development of lumini has been a hundred exploration in our forms of theatre and dance as well as research and artificial
intelligence and cognitive science lumahai draws inspiration from the
ancient art form of shot here the original two-dimensional version of the
installation led the conceptualization of the dome in the liminal space which
even silhouettes and virtual character is being danced together on the projection surface rather than relying
on a predominant library of movement responses the virtual dancer learns in
this part measurements and utilizes new points movement theory to systematically reason about them and working
improvisational shoes under the moon response the points theory is based in
dance and theater and analyzes the performance along the dimensions of tempo duration repetition kinesthetic
response shape spatial relationships gesture architecture and
Photography the virtual dancer is able to use several different strategies to
respond to human movements these include mimicry of a movement transformation of the movement along
viewpoints and mentions we're calling a similar or complementary movement from memory in terms of you fight revolutions
and define actually sponsor patterns of the agent has learned while dancing with its human partner the reason we did this
is this is part of a larger effort in our lab for understanding the relationship between compeition cognition and creativity
where a large amount of our efforts go into understanding human creativity and
how we make things together out were created together as a way that almost understand how we can build co-created
AI that serves the same purpose where to be a colleague and collaborate with us
and create things with us so Brian was a
graduate student in John leritz lab as well before I start this I lude
Rosie
into this earlier where we're getting closer to rosie saying can you teach me so let me give you some introduction to
this in the lower left you're seeing the view of a Kinect camera onto a flat
surface there's a robotic arm mainly 3d printed parts few servos above that
you're seeing an interpretation of the scene we're giving it kind of associations of the four areas with
semantic titles like one is the table one is the garbage just just semantic
terms for areas but other than that the agent doesn't actually know all that much and it's going to operate in two
modalities one is we'll call it natural language natural ich language restricted
subset of English as well as some quote unquote pointing so you're gonna see
some Mouse pointers in the upper left saying I'll talk about this and this is just a way to indicate location and so
starting off we're gonna say things like you know pick up the blue block and it's gonna be like I don't know what is what is blue we say oh well that's a
color okay you know so go get the green thing
what's green oh it's a color okay move the blue thing to a particular location where's that point it okay what is
moving like really it has to start from the beginning and it's described and it said okay now you've finished and once
we got to that point now I can say move the green thing over here and it's got everything that it needs to be able to
then reproduce the task given new parameters and it's learned that ability so let me give it a little bit of time
so you can look a little bit at top left in terms of the pointers you're going to see some text commands being entered so
what kind of attribute is blue we're gonna say it's a color and so that can map it then to a particular sensory
modality this is green so the pointing what kind of thing is green okay color
so now it knows how to understand blue and green as colors with respect to the visual scene move rectangle to the table
what is rectangle okay now I can map that on to or understanding parts of the
world is this the blue rectangle so the arm is actually pointing itself to get confirmation from the instructor and
then we're trying to understand in general when you say move something what is the goal of this operation and so
then it also has a declared representation of the idea of this task not only that it completed it then it
can look back on having completed the task and understand what were the steps that led to achieving a particular goal
so in order move it you're gonna have to pick it up it knows which one the blue thing is
great now
in the table so that's a particular location and at this point we can say
you're done you have accomplished the moved blue rectangle to the table and so
I can understand what that very simple kind of process is like and associate
that with the verb to move and now we can say move the green object or not do
the garbage and without any further interaction based upon everything that
learned up till that point it can successfully complete that task so this is work of chavala Mohan and others at
the shore group at the University of Michigan on the bruisy project and they're extending this to playing games
and learning the rules of games through text-based descriptions and multimodal experience so in order to build up to
here's a story and so I wanted to give you a sense of how research occurs in the group and so there's these back and
forth that occur over time between there's this piece of software called soar we want to make this thing better
and give it new capabilities and so all our agents are going to become better and we always have to keep in mind and
you'll see this as I go further that it has to be useful to a wide variety of agents it has to be task independent and
it has to be efficient for us to do anything in the architecture all of those have to hold true so we do
something cool in the architecture and then we say okay let's solve a cool problem so it's build some agents to do
this and so this ends up testing what are the limitations what are the issues that arise in a particular mechanism as
well as integration with others and we get to solve interesting problems we usually find there was something missing and then we can go back to the
architecture and rinse and repeat just to give you an idea again how sore works
Soar 9 [Laird 2012] Memory Integration
so the working memory is actually a directed connected graph the perception
is just a subset of that graph and so there's going to be symbolic representations of most of the world
there is a visual subsystem in which you can provide a scene graph just not showing it here actions are also a
subset of that graph and so the procedural knowledge which is also production rules can modify can
sections of the input modify sections of the output as well as arbitrary parts of the graph to take actions so the
decision procedure says of all the things that I know to do and I've kind of ranked them according to various preferences what single things should I
do semantic memory for facts there's episodic memory the agent is always
actually storing every experience it's ever had over time in episodic memory and it has the ability to get back to
that and so the similar cycle we saw before we get input in this perception called the input link rules are going to
fire all in parallel and say here's everything I know about the situation here's all the things I could do decision procedure says here's what
we're going to do based upon the selected operator all sorts of things
could happen with respect to memories providing input rules firing to perform
computations and as well as potentially output in the world and remember agent
reactivity is required we want the system to be able to react to things in
the world at a very quick pace so anything that happens in this cycle at max the overall cycle has to be under 50
milliseconds and so that's gonna be constraint we hold ourselves to and so the story I'll be telling will say how
we got to a point where we started actually forgetting things and we're an architecture that doesn't want to be
like humans we want to create cool systems but what we realized was something that we do there's probably
some benefit to it and we actually put it into our system in the lead to good outputs so here's the research path I'm
One Research Path
going to walk down we had just a simple problem which was we have these memory
systems and sometimes they're going to get a cue that could relate to multiple memories and the question is if you have
a fixed mechanism what should you return in a task independent way which one of
these many memories should you return that was our question and we looked to some human data on this something called
the rational analysis of memory done by John Anderson and realized that in human
language there are recency and frequency effects that maybe it would be useful
and so we actually did an analysis found that not only does this occur but it's useful in what
are called word sense disambiguation tasks I'll get to that what that means in a second develop some algorithms to
scale this really well and it turned out to worked out well not only in the original task when we learn look to two
other completely different ones the same underlying mechanism ended up producing some really interesting
outputs so let me talk about word sense disambiguation real quick this is a core problem in natural language processing
Problem ala Word-Sense Disambiguation
if you haven't heard of it before let's say we have an agent and for some reason it needs to understand the verb to run
looks to its memory and finds that it could you know run in the park it could
be running a fever could run an election it could run a program and the question is what should an task independent
memory mechanism return if all you've been given is the verb to run and so the
rational analysis of memory looks through multiple text corpora and what they found was if a particular word had
been used recently it's very likely to be reused again and if it hadn't been
used recently there's going to be this effect where the expression here the T is time since
the most recent use it's going to sum those with a exponential decay and so
what it looks like if time is going to the right activation hire as better as
you get these individual usages you get these little drops and then eventually drop down and so if we had just one
usage of a word the read would be what the decay would look like and so the
core problem here is if we're at a particular point and we want to select between kind of the blue thing or the red thing blue would have a higher
activation and so maybe that's useful this is how things are modeled with
human memory but is it useful in general for tasks and so we looked at common
WSD Evaluation Historical Memory Retrieval Blas
corpora used in word sense disambiguation and just said well if we just look at this corporate twice and we
just use answers prior answers you know I ask the question what is the sense of
this word I took a guess I got the right answer and I used that recency and frequency information in my task
independent memory would that be useful and somewhat of a surprise but somewhat maybe not of a
it actually performed really well across multiple corpora so we said okay this
seems like a reasonable mechanism let's look at implementing this efficiently in
Efficiency
the architecture and the problem was this term right here said for every
memory for every time step you're having to pay everything that doesn't sound
like a recipe for efficiency if you're talking about lots and lots of knowledge over long periods of time so we made use
of a nice approximation that petrol that come up with to approximate tale effect
so accesses that happen long long ago we could basically approximate their effect on the overall
sum so now we had a fixed set of values and what we basically said is since
these are always decreasing and all we care about is relative order let's just only recompute when someone gets a new
value so it's a guess it's a heuristic and approximation but we looked at how
this worked on the same set of corpora and in terms of query time if we made
these approximations well under our 50 millisecond the effect on task
performance was negligible in fact hunt a couple of these it got ever so slightly better terms of accuracy and
actually if we looked at the individual decisions that were being made making these sorts of approximations were
leading to up to 90 sorry at least 90 percent of the decisions being made were
identical to having done the true full calculation so I said this is great
and we implemented this and worked really well and then we started working on what seemed like completely unrelated
Related Problem: Memory Size
problems one was in mobile robotics we had a mobile robot I'll show picture of in a
little while roaming around the halls performing all sorts of tasks and what we're finding was if you have a system
that's remembering everything in your short-term memory and your short-term memory gets really really big I don't
know about you my short-term memory feels really really small I would love it to be big but if you make your memory
really big and you try to remember something you're not having to pull lots and lots and lots of information into your
short-term memory so the system was actually getting slower simply because it had a lot of short-term memory
representation of the overall map it was looking up so large working memory a
problem Liars dices game you play with dice we were doing in our L base system on this reinforcement learning and it
turned out it's a really really big value function we're having to store lots of data and we didn't know which
stuff we had to keep around to keep the performance up so we had a hypothesis
that forgetting was actually going to be a beneficial thing that maybe maybe the
problem we have with our memories that we really really dislike this forgetting thing maybe it's actually useful and so
we experimented with the following policy we said let's forget a memory if one we haven't really it's not predicted
to be useful by this base level activation we haven't used it recently we haven't used it frequently maybe it's not worth it
that and we felt confident that we could approximately reconstruct it if we
absolutely had to and if those two things held we could forget something so
it's this bait same basic algorithm but instead of the ranking them it's if we
set a threshold for base level activation finding when it is that a
memory is going to pass that threshold and try to forget based upon that in a way that's efficient that isn't going to
scale really really poorly so we were able to come up with an efficient way to
Efficient Implementation
implement this using an approximation
Approximation Quality
that ended up for most memories to be
exactly correct to the original I'm happy to go over details of this if anybody's interested later but end up
being a fairly close approximation one that as compared to an accurate
Prediction Complexity
completely accurate search for the value ended up being somewhere between 15 to
Prediction Computation
20 times faster and so when we looked at our mobile robot here oh sorry let me
Task #1: Mobile Robotics
get this back because our little robots actually going around it's the third floor of the computer science building at the University of
Michigan it's going around he's building a map and again the idea was this map is getting too big so here was the basic
idea as the robots going around it's going to need this map information about rooms the color there is describing kind
of the strength of the memory and as it gets farther and farther away and it hasn't used part of the map for planning
or other purposes basically make it 2 K away so that by the time it gets to the bottom it's forgotten about the top but
we had the belief that we could reconstruct portion that map if
necessary and so the hypothesis was this would take care of our speed problems and so what we looked at was here's our
Results: Decision Time
50 millisecond thresholds if we do no forgetting whatsoever bad things were happening over time so
just 3,600 seconds this isn't a very long time we're passing that threshold
this is dangerous for the robot if we implement a task specific basically cleanup rules which is really hard to
get right that basically solved the problem when we looked at our general forgetting mechanism that we're using in other
places at an appropriate level of decay we were actually doing better than hand-tuned rules so this was kind of a
surprise win for us the other task seems totally unrelated it's a dice game you
Task #2: Liar's Dice Michigan Liar's Dice
cover your dice you make bids about what are under other people's cups this is played in Pirates of the Caribbean when
they're on the boat in the second movie and bidding for lives of service honestly this is a game we love to play
in the University of Michigan lab and so we're like hmm could soar play this and so we built a system that could learn to
play this game rather well with reinforcement learning and so the basic idea was in a particular state of the
Reasoning -- Action Knowledge
game soar would have options of actions to perform it could construct estimates
of their associated value it would choose one of those and depending on the outcome something good happened you
might update that value and the big problem was that the size of the state space the number of possible states and
actions just is enormous and so memory was blowing up and so what we said
similar sort of hypothesis if we decay away these estimates that we could
Forgetting Action Knowledge
probably reconstructs and we haven't used it in a while our things going to get better and so if we don't forget it
all 40,000 games isn't a whole lot when it comes to reinforcement learning we were up at two gigs we wanted to put this on
an iPhone that wasn't going to work so well there had been prior work that had
used a similar approach they were down at four or five hundred Meg's the iPhones are not going to be happy
but it'll work so that gave us some hope and we implemented our system okay
we're somewhere in the middle we can fit on the iPhone a very good iPhone maybe an iPad the question was though one
efficiency yeah we we fit under our 50 milliseconds but - how does the system actually perform when you start
forgetting stuff can it learn to play well and so y-axis here you're seeing
Results: Competence
competency you play a thousand games how many do you win so the bottom here 500 that's you know flipping a coin whether
or not you're going to win if we do know forgetting whatsoever this is a pretty
good system the prior work while keeping the memory low is also suffering with
respect to how well it was playing the game and kind of cool was the system
that was basically more than having the memory requirement was still performing at the level of no forgetting whatsoever
so just to bring back why I went through this story was we had a problem we
looked to our example of human level AI which is humans themselves we took an idea it turned out to be
beneficial we found in efficient implementations and then found it was useful in other parts of the architecture and other tasks that didn't
seem to relate whatsoever but if you download soar right now you would gain access to all these mechanisms for
whatever task you want it to perform just to give some sense in the field of
Some CogArch Open Issues
cognitive architecture what some of the open issues are I think this is true in a lot of fields in AI but integration of
systems over time the goal was they wouldn't have all these theories and so
you could just kind of build over time particularly when folks are working on different architectures that becomes hard but also when you have very
different initial starting points that can still be an issue transfer learning is an issue we're building into the
space of multimodal representations which is to say not only abstract symbolic but also visual wouldn't it be
nice if we had auditory and other senses but building that into memories and processing is still an open question
there's folks working on metacognition which is to say the agent self assessing
its own State its own processing some work has been done in here but still a lot and I think the last one is a really
important question for anybody taking this kind of class which is what would happen if we did succeed if we did make
human-level AI and if you don't know that picture right there it's from a show that I recommend that
you watch that's by the BBC it's called humans and it's basically what if we were able to develop what are called
synths in the show think the robot that can clean up after your laundry and cook and all that good stuff interact with
you it looks and interacts as a human but is completely our servants and then
hilarity and complex issues ensue so I highly recommend if you haven't seen
that to go watch that I think these days there's a lot of attention play pay to
machine learning and particular deep learning methods as well it should they're doing absolutely amazing things and often the question is well you're
doing this and there's deep learning over there you know how do they compare
and I honestly don't feel that that's always a fruitful question because most
of the time they tend to be working on different problems if I'm trying to find
objects in the scene I'm gonna pull out tensorflow I'm really not going to pull outs or it doesn't make sense it's not
the right tool for the job they haven't been said there are times when they tend to work together really really well so
the Rosi system that you saw there there was some I believe neural networks being
used in the object recognition mechanisms for the vision system there's TD learning going
in terms of the dice game where we can pick and choose and use this stuff absolutely great because there are problems that are best solved by these
methods so why avoid it and then on the other side if you're trying to develop a
system where you you know in different situations know exactly what you want the system to do soar or other rule
based systems end up being the right tool for the right job so absolutely why not make it a piece of the overall system some recommended
readings and some venues I'd mentioned unified theories of cognition this is Harvard Press I believe the short
cognitive architecture was MIT press came out in 2012 I'll say I'm co-author and theoretically
would get proceeds but I've donated them all to the University of Michigan so I can just make this recommendation free
of ethical concerns personally it's an interesting book it brings together lots of history and lots of the new features
it's if you're really interested in soar it's an easy sell
I'd mentioned crystallize Smith's how to build a brain really cool read download the software go through toriel's it's
it's really great how can the human mind occur in the physical universe is one of
the court akhtar books so it talks through a lot of the psychological underpinnings and how the architecture
works it's a fascinating read one of the papers trying to remember what year 2008
this goes through a lot of different architectures in the field it's ten years old but it gives you a good kind
of broad sweep if you want something a little more recent this is last month's
issue of AI magazine completely dedicated to cognitive systems so it's a
good place to look for the sort of stuff in terms of academic venues triple AI often has cognitive systems track
there's a conference called aiccm international conference on cognitive modeling where you'll see kind of a span
from biologic all the way up to AI cognitive science or cogs AI they have a conference as well as a journal ACS has
a conference as well as an online journal advances in cognitive systems cognitive systems research is a journal
that has a lot of this good stuff there's AGI the conference Vica is biologically inspired cognitive
architectures and I had mentioned both there's a soar workshop and an act our workshop that go on annually so leave it
at this there's some contact information there and a lot of what I do these days actually involves kind of explainable
machine learning integrating that with cognitive systems as well as optimization and robotics that scales
really well and also integrates with cognitive systems so thank you if you
have a question please line up to one of these two microphones so what what are
the main heuristics that you're using in soar there can be heuristics at the task
level in the agent level or there's the heuristics that are built into the architecture to operate efficiently so
I'll give you a core example that comes into the architecture and it's a fun trick that if you're a programmer you
could use all the time which is only process changes which is to say one of
the cool things about soar is you can load it up with literally billions of rules and I say literally because we've
done it and we know that it can turnover still in under a millisecond and this happens because instead of most systems
which process all the rules we just say well anytime anything changes in the world that's what we're going to react
to and of course if you look at the biological world similar sorts of tricks are being used so that's one of the core
ones that actually permeates multiple of the mechanisms when it comes to individual tasks it really is task
specific what that is so for instance with the liar's dice game if you were to
go and download it when you're setting the level of difficulty of it what you're basically selecting is the subset
of heuristics that are being applied and it starts very simply with things like if I see lots of sixes then I'm likely
to believe a high number of sixes exist but if I don't they're probably not there at all so it's a start
but any Bayesian wouldn't really buy that argument so then you start tacking
on a little bit of probabilistic calculation and then it tacks on some history of prior actions of the agents
so it really just builds now the Rosi system one of the cool things they're
doing is game learning and specifically having the agent be able to accept by a
text like natural text heuristics about how to play the game even when it's not
sure what to do so you at one point you mentioned about like generating new rules yeah so I'm wondering like how do
you do that's so true and I'm the first thing that comes to my mind are local search methods okay so one thing is you
can actually implement heuristic search in rules in the system and that's actually how the robot navigates itself
so it does heuristic search but at the level of rules generating new rules the
chunking mechanism says the following if it's the case that in order to solve a
problem you had to kind of sub goal and do some other work and you figure out how to solve all that work and you've
got a result then and I'm greatly oversimplifying but if you ever were in the same situation again why don't I
just memorize the solution for that same situation so it basically learns over
all the sub processing that was done and encodes the situation I was in as conditions and the results that were
produced as action and that's the new rule all right thank you yeah hi so deep
learning and neural networks you know it looks as though there's a bit of an impedance mismatch between your system
and those types of system because you've got a fixed kind of memory architecture and they've got the memory and the rules
all kind of mixed together into one system but could you interface your system or a saw like system with deep
learning by playing in deep learning agents has rules in your system so you'd have to have some local memory but is
that is there some reason you can't plug in deep learning as a kind of a rule like module so I'm going to answer this
you work on it is that's the been any work on that oh it's yeah so I'll answer at multiple levels
one is you are writing a system and you want to use both of these things how do
you make them talk and there is an API that you can interface with any environment and any set of tools and if
deep learning is one of them great and if so or is the other one cool you have no problem and you can do that today and we have done this numerous times in
terms of integration into the architecture all we have to do is think
of a sub-problem in which all over simplify this but
basically function approximation is useful I'm seeing basically kind of the fixed structure of input I'm getting
feedback as to the output and I want to learn the mapping to that over time if you can make that case then you
integrate it as a part of the module great and we have learning mechanisms
that do some of that deep learning just hasn't been used to my knowledge to
solve any of those subproblems there's nothing keeping it from being one of those particularly when it comes down to
the low-level visual part of things a problem that arises so I'll say what
would actually make some of this difficult and it's a general problem called simple grounding so at the level
of what most have what happens mostly in store it is symbols being manipulated in the highly discrete way and so how do
you get yourself from pixels and low-level non symbolic representations to something that's stable and discrete
and can be manipulated and that is absolutely an open question in that
community and and that will make things hard so spawn actually has an
interesting answer to that and it has a distributive representation and it operates over distributed representations in what might feel like
a symbolic way so they're kind of ahead of us on that but they're they're starting from a lower point and so they
dealt with some of these issues and they have a pretty good answer to that and that's how they're moving up and that's also why I showed Sigma which is at its
low level it's message passing algorithms it's implementing things like slam and Sat solving and
other sorts of really really it can implement those on very low level primitives but higher up it can also be
doing what soar is doing so there's an answer there as well okay thank you so another way of doing it would be to layer the system so have one system
pre-processing the the the sensory input or post-processing their draft but the
other one that would be another way of combining two system and that's actually what's going on in the rosey system so the detection of objects in the scene is
a just just software that somebody wrote I don't believe it's a deep learning specifically but like the color
detection out of it I think is an SVM if I'm correct so easily could be deep
learning thanks you mentioned like the importance of forgetting in order for memory issues but you said
you could only forget because you could reconstruct and then curse how do you when you said we can start you need to know that it happened before so do you
just compress the data like do you really forget it order okay so and I put
quotes up and I said you think you can reconstruct it so we came up with
approximations of this and so let me try to answer this very grounded when it
comes to the mobile robot and you had rooms that you had been to before the
entire map in its entirety was being constructed in the robots semantic
memory so here's fats this room is connected this room which is connected this room which connected this room so we had those sorts of representations
that existed up in at semantic memory the rules can only operates down on anything that's in short-term memory so
basically we were removing things from the short-term memory and as necessary be able to reconstruct it from the
long-term you could end up in some situations in which you had made a change locally in short-term memory
didn't get a chance to get it up and it actually happened to be forgotten away so you weren't guaranteed but it was
good enough that the connectivity survived the agent was able to perform the exact same task and we gained some
benefit for the RL system the rule we came up with was the initial estimates
in the valley you system which is here's how good I think that is that's based on the heuristics I described earlier some
simple probabilistic calculations of counting some stuff that's where that number came from we computed before we could compute it again
the only time we can't reconstruct it completely is if it had seen a certain
number of updates over time it's such a large state space there are so many
actions so many states that most of the states were never being seen so most of
those could be exactly reproduced by the agent just thinking about it a little bit and there were only a tiny tiny I'm
gonna say under 1% of the estimate the value system that ever got updates and
that's actually not inconsistent with a lot of these kinds of problems that have really really large state spaces so I
think the statement was something like if we had ever updated it don't forget
it and you saw that was already reducing more than half of the memory load we
could have something higher to say 10 times something like that and that would say we could reconstruct almost all of
it the prior work that I referenced was strictly saying if it falls below
threshold no matter how many times in an update no matter how much information was there and so what we're adding was probably can reconstruct and that was
getting us the the balance between the efficiency and the ability to forget so
just under 7 you say we can probably we can show it means that you keep trying that you used to know it and so if you need to be constructed you will but it's
just you're gonna run it again in some times on the fly if I get back into that situation and I happen to forget it the
the system knew how to compute it the first time it goes and looks at all the hand and it just pretends it's in that
situation for the very very first time reconstructs that value estimate again
you're on that work question okay so the actual mechanism of forgetting is
fascinating so l STM's rnns have mechanisms for learning what to
forget and what not to forget have you has there been any exploration of
learning the forgetting process just doing something complicated or interesting with which parts to forget
or not the closest I will say was kind
of a metacognition project that's 10 or 15 years old at this point which was
what happens when soar gets into a place where it actually knows that it learned something that's harmful to it that's
that's leading to poor decisions and in that case it was still a very rule-based
process but it wasn't learning to forget he was actually learning to override its
prior knowledge which might be closer to some of what we do when we know we have a bad habit we don't have a way of
forgetting that habit but instead we can try to learn something on top of that that leads to better operation in the
future to my knowledge that's the only work at least in soar that's been done just sorry I find the topic really
fascinating what lessons do you think we can draw from the fact that forgetting
it's ultimately your the action of forgetting is driven by the fact you
want to improve performance but do you think forgetting is essential for AGI
the act of forgetting for building systems that operate in this world how
important is forgetting I can think of easy answers to that so one might be if
we take the cognitive modeling approach we know humans do forget and we know regularities of how humans forget and so
whether or not the system itself forgets it's at least has to model the fact that the humans that's interacting with are
going to forget and so at least it has to have that ability to model in order to interact effectively because if it
assumes we always remember everything and it can't operate well in that environment I think we're going to have
a problem is true forgetting going to be
necessary that's interesting our our AGI system is going to hold a grudge for all
eternity we might want them to forget this early age when we were forcing them to work in our laboratory I
think I know what you're trying to yeah exactly yeah exactly and how do we build such a
system yeah anyways go ahead so I have
two quick two quick questions and one is would you be able to speculate on how
you can connect function approximator such as deep networks you know to symbols and the second question
completely different this is regarding your action selection I know we didn't
speak much about that when you have different theories in your knowledge representation and you have an action
selection which has to make construct a plan by reasoning about the different
theories and the different pieces of knowledge that are now held within your
memory or anything like all your rules what kind of algorithms do you use in
the action selection to come up with the plan you know is there any concept of differentiation of the symbols or you
know or grammars or admissible grammars and things like that that you use in action selection I'm actually gonna
answer the second question first and then you're gonna have to probably remind me of what the first one was when
I get to the end so the action selection mechanism one of these core tenants I said is it's got to get through this
cycle fast so everything that's really really built in has to be really really simple and so the decision procedure is
actually really really simple it says the rules are gonna fire the rules are going the production rules are gonna
fire and there's gonna be a subset of them that will say something like here's an operator that you could select -
these are carlos acceptable operator preferences they're ones that going to say well based upon the fact that you said that that was acceptable I think
it's the best thing or the worst thing or I think 50/50 chance I'm going to get reward out of this there's actually a
fixed language of preferences that are being asserted and actually a nice fixed procedure by which if I have a set of
preferences to make a very quick and clean decision so what's basically
happened is you've pushed the hard questions of how to make complex decisions about actions up to a higher
level the low level architecture is always given a set of Jen's going to be able to make a
relatively quick decision and it gets pushed into the knowledge of the agent
to construct a sequence of decisions that over time is going to get to the
more interesting questions you're talking about but how can you reason that that sequence will take you to the goal that you desire so people is there
any guarantee on that is that in general across tasks no but people have for
instance implemented a star I was mentioning as wouls right yeah so I know given certain
properties about the search tack that task that's being searched based upon these rules given a finite search space
eventually it will get there and if I have a good heuristic in there I know certain properties about the optimality
so I can reason at that level in general I think this comes back to the assumption I made earlier about bounded
rationality to say parts of the architecture of solving subproblems optimally the general problems that it's
going to work on it's going to try its best based upon the knowledge that it has and that's about the end of
guarantees that you can typically make in the architecture okay I think your first question was speculate on
connecting symbol approach I mean function approximate is you know you know you know multiple layer function
approximate is like deep learning networks to two symbols that you can
reason about at a higher level yeah I think that's a great open space if I had
time this would be somebody I'll be working on right now which is somewhere before it basically said taking in a
scene and then detecting objects out of that scene and using those as symbols and reasoning about those over time I
think the spawn work is quite interesting so the symbols that they're
operating on are actually a distributed
representation of the input space and the closest I can get to this is if
you've seen a word Tyvek where you're taking a language corpus and what you're getting out of there is a vector number
that has certain properties but it's also a vector you can operate on as a unit so it has nice properties you can
operate with it on other vectors you know that if I got the same word in the
same context I would get back to that exact same vector so those are that's
the kind of representation that seems like it's going to be able to bridge that chasm where we can get from sensory
information to something that can be operated on and reasoned about in this sort of symbolic architecture and get us
from there from actual sensory information I had a question what do you
think are the biggest strengths of the cognitive architecture approach compared
to other approaches in artificial intelligence and the flip side of that what do you think are the biggest
shortcomings of cognitive architecture with respect to us with respect to you
being humans yeah a human level like like what needs to be like how come
cognitive architecture has not solved AGI because we want job security that's
the answer we've totally solved it already so strengths I think
conceptually is keeping an eye on the ball which is if what you're looking at
is trying to make human-level AI I it's
hard it's challenging it's ambitious to say that's the goal because for decades
we haven't done it it's extraordinarily hard it it is less difficult in some
ways to constrain it yourself down to a single problem that having been said I'm
not very good at making a car drive itself in some ways that's a simpler problem it's great at challenging it of
itself and it'll have great impact on humanity it's a great problem to work on human level AI is huge it's not even
well-defined as a problem and so
what's the strength here bravery stupidity in the face of failure
resilience over time keeping alive this idea of trying to reproduce a level of
human intelligence that's more general I don't know if that's a very satisfactory answer for you
downside home runs are fairly rare and
by home run I mean a system that finds its way to the the general populace to
the marketplace I'd mentioned Bonnie Johns specifically because you know this
is twenty thirty years of research and then she found a way that actually makes a whole lot of sense under direct
application so it was a lot of a lot of years of basic research a lot of researchers and then there was there was
the big win there what was this one oh this was a bunny John was a researcher
this was using akhtar models of I gaze and reaction and so forth to be able to
make predictions about how humans would use user interfaces so those sorts of
outcomes are rare it it if you work in AI one of the first things you learn
about is blocks world it's kind of in the classic AI textbook I will tell you
I've worked on that problem at about three different variants I've gone to many conferences where presentations
have been made about blocks world which is to say we're good progress is being made but the way you end up thinking
about is it really really small constrained problems ironically you you have this big vision but in order to
make progress that ends up being on moving blocks on a table and so it's
it's a big challenge I just think it'll take a lot of time the I'll say the
other thing they haven't we haven't really gotten to although I brought up spawn and I brought up Sigma
an idea of how to scale this thing something I like about deep learning is
just some extent with lots of asterisks and 10,000 foot view it's kind of like well we've gotten this far all right
let's just provided different inputs different outputs and we'll have some tricks on the middle and suddenly you have you know end to end deep learning
of a bigger problem and a bigger problem there's a way to see how this expands given enough data given enough computing
and incremental advances when it comes to soar it takes not only a big idea but
it takes a lot of software engineering to integrate it there's a lot of constraints built into it it slows it
down so something like Sigma is oh well I can change a little bit of the
configuration of the graph I can use variants on the algorithm boom it's integrated I can experiment fairly
quickly so starting with that sort of infrastructure does not give you the
constraint you kind of want with your big picture vision of going towards human level AI but in terms of being
able to be agile in your research it's it's kind of incredible Izzie thank you you'd mention that ideas such
as base level decay at these techniques they were based their original inspirations were based off of human
cognition and and because humans can't remember everything so were there any instances of the other way around where
some discovery in cognitive modeling fueled it another discovery in cognitive
science so what one thing I'm gonna
point out and your question was based on the decay with respect to human cognition the study actually was let's
look at text and properties of text and use that to then make predictions about
what must be true about human cognition so John Anderson and the other
researchers looked at believe it was New York Times articles
his Oh John Anderson's emails and I'm trying to remember what the third I
think it was parents utterances with their kids or something like this it was
actually looking at text corpora and the words that were occurring in at varying
frequencies that that analysis that rational analysis actually led to models
that got integrated within the act arc architecture that then became validated
through multiple trials that then became validated with respect to MRI scans and is now being used to both do study back
with humans but also develop systems that interact well with humans so I
think that in and of itself ends up being an example it's a cheat but the
UAV the soar UAV system I believe is a single robot that has multi multiple
agents running on it so where is this I
got it off your website ok but either way your systems allow for multi agents
ok so my question is how are you preventing them from converging with new
data and are you changing what they're forgetting selectively as one of those
ways so I'll say yes you can have multi agent source systems on a single system
on multiple systems there's not any real strong theory that relates to
multi-agent systems so there's no real constraint there that you can come up with a protocol for them interacting
each one is going to have its own set of memories set of knowledge there really
is no constraint on you being able to communicate like you would if it were any other system interacting with soar
so I don't really think I have a great answer for it so that is to say if you
had goo Theory's good algorithms about how multi-agent systems work and how they
can bring knowledge together form a fusion sort of way it might be something
that you could bring to a multi agent source system but there's nothing really there to help you there's no mechanisms
there really to help you do that any better than you would otherwise and you would have to kind of constraints of
your representations the process as to what it has fixed in terms of its sort of memory and its sort of processing cycle thank you


----------

-----
--35--

-----
Date: 2018.03.14
Link: [# Sterling Anderson, Co-Founder, Aurora - MIT Self-Driving Cars](https://www.youtube.com/watch?v=HKBhP9JISF0)
Transcription:

today we have sterling Anderson he's the co-founder of Aurora an exciting new
self-driving car company previously he was the head of the Tesla auto pilot team that brought both the first the
second generation auto pilot to life before that he did his PhD at MIT
working on shared human machine control of ground vehicles the very thing I've
been harping on over and over in this class and now he's back at MIT to talk
with us please give him a warm welcome [Applause]
thank you it's good to be here I was telling Lex just before I think it's been a little while since I've been back
after the Institute and it's great to be here I want to apologize in advance I've just landed this afternoon from Korea
via Germany where I've been spending the last week and so I may speak a little
slower than normal please bear with me if I become incoherent or slurred my speech
somebody flag at 2:00 and Lola will try to make corrections so tonight I thought I'd chat with you a little bit about my
journey over the last decade it's been just over ten years since I was at MIT a lot has changed a lot has changed for
the better in the self-driving community and I've been privileged to be a part of
many of those changes and so I wanted to talk with you a little bit about some of the things that I've learned some of the things that I've experienced and then
maybe end by talking about sort of where we go from here and and what the next
steps are both for you know the industry at large but also for the company that we're building that as Lex mention is
called Aurora to start out with and there are a few sort of key phases or
transitions in my journey over the last 10 years as Lex mentioned when I started
MIT I worked with Carly on Yemma Amelio Fazoli's John Leonard a few others on
some of these sort of shared adaptive automation approaches I'll talk a little
bit about those from there I spent some time at Tesla where I first led the Model X program as
we both finish the development and ultimately launched I took over the autopilot program where we introduced a
number of new both active safety but also sort of you know enhanced
convenience features from auto steer to adaptive cruise control that were able refine in a few unique ways and we'll
talk a little bit about that and then from there in December of last year of 2016 I guess now we started a new
company called Aurora and I'll tell you a little bit about that so to start out with when I KN OIT was 2007 the DARPA
urban challenge is were well underway at that stage and one of the things that we wanted to do is find a way to address
some of these safety issues in human driving earlier than potentially full
self-driving Qadeer and so we developed what became known as the intelligent co-pilot what you see here is a
simulation of that operating I'll tell you a little bit more about that in just a second but to explain a little bit
about the the methodology the innovation the key approach that we took that was
slightly different from what in traditional planning control theory we were doing was instead of designing in
path space for the robot we instead found a way to identify plan optimize
and design a controller subject to a set of constraints rather than paths and so
what we were doing is looking for Hama top Eastern environment so imagine for a moment an environment that's pockmarked
by objects by their vehicles by pedestrians etc if you were to create
the Voronoi diagram through that environment you would have a set of each unique set of paths or Hama top is
continuously deformable paths that will take you from one one location to another through it
if you then turn that into its dual which is the de'longhi triangulation of set environment presuming that you've
got convex obstacles you can then tile those together rather trivially to create a set of homotopy sand
transitions across which those paths can can stake out sort of a given set of
options for the human eye turns out humans tend to this tends to be a more intuitive way of imposing certain
constraints on human operation rather than enforcing that the ego vehicle
stick to some arbitrary position within you know some distance of a safe path you instead look to enforce only that
the that the state of the vehicle remain within a constraint bounded and dimensional tube in state space those
constraints being spatial imagine for a moment edges of the roadway or you know circumventing various objects in the
roadway imagine them also being dynamic right so limits of tire tire friction
imposed limits on side slip angles and so using that what we did is found a way
to create those Hammurabi's forwards simulate the trajectory of the vehicle given its current state and some optimal
set of controls inputs that would optimize its stability through that we use model creative control in that work
and then taking that forward simulated trajectory computing some metric of
threat for instance if the objective function for that minimize the or
maximize stability or minimize some some of these parameters like wheel side slip then wheel side slip is a fairly good
indication of how threatening that optimal maneuver is becoming and so what
we did is then use that in a modulation of control between the human and the car such that should the car ever find
itself in a state where that forward simulated optimal trajectory is very near the limits of what the vehicle and
it can actually handle we will have transition control fully to the to the vehicle to the automated system so that
it can avoid an accident and then it transitions back in some manner and we played with a number of different methods of transitioning this
control to ensure that that we didn't throw off the human mental model which
was which was one of the key concerns we also wanted to make sure that we were able to arrest accidents before they
happen what you see here is a simulation that was fairly faithful to the behavior
we saw in test drivers up at Dearborn in Dearborn Michigan Ford provided was provided us with a Jaguar s-type to test
this on and what we did so what you see here is there's a blue vehicle in the gray vehicle both in both cases we have
a poorly tuned driver model in this case if your pursuit controller with a fairly short look ahead shorter than would be
appropriate given this scenario in these dynamics the grey vehicle is without the
intelligent copilot in the loop you'll notice that obviously the driver becomes unstable loses control and
leaves the safe roadway the co-pilot remember is in is interested not in
following any given path it doesn't care where the vehicle lands on this road why
provided it remains inside the road in the blue vehicles case it's the exact
same human driver model now with the copilot in the loop you'll notice that as as this scenario continues what you
see here on the left is the green is in this green bar is the portion of available control authorities being
taken by the automated system you'll notice that it never exceeds half of the available control which is to say that
the steering inputs received by the vehicle end up being a blend of what the human and what the automation are
providing and what what results is a path for the blue vehicle that actually
better tracks the humans intended trajectory then even the copilot
understood right again the copilot is keeping the vehicle stable it's keeping it on the road the human is healing to
the centerline of that roadway so there was some very interesting things that came out of this there were a lot of we
did a lot of work in understanding what kind of feedback was most natural to provide to
a human our biggest concern was if you throw off a human's mental model by causing the vehicles at behaviors to
deviate from what they expect it to do in response to British control inputs that could be a problem so we tried
various things from you know adjusting for instance one of the one of the key questions that we had early on was if we
couple the computer control and the human control via planetary gear and
allow the human to feel a actually a backwards torque to what the vehicle is
doing so the car starts to turn right human will feel the wheel turn left they'll see it start to turn left
is that more confusing or less confusing they're human and it turns out it depends on how experienced a human is
some some drivers will modulate their input space on the torque feedback that they feel through the wheel and it for
instance a very experienced driver expects to feel the wheel pull left when they're turning right however less
experienced drivers in response to seeing the wheel turning opposite to what the what the car supposed to be
doing this for a rather confusing experience so there were a lot of really interesting human interface challenges
that we were dealing with here we ended up working through a lot of that
developing a number of sort of micro
applications for it one of those at the time Gill Pratt was leading a DARPA program focused on what they call the
time maximal mobility manipulation we decided to see what this system could do
in application to unmanned ground vehicles so in this case what you see is a human driver sitting at a remote
console as one would when operating an unmanned vehicle for instance in the
military what you see on the left top left is the top-down view of what the
vehicle sees I should have played this in repeat mode with bounding boxes
bounding various cones and what we did is we set up about 20 drivers 2020 test
subjects looking at this this troll screen and operating the vehicle
through this track and we set this up as a race with prizes for the winners as
one would expect and penalize them for every barrel they hit if they knocked
over the barrel I think they got a five-second penalty if they brushed a barrel they got a one-second penalty and they were to cross they work across the
field as fast as possible they couldn't they had no line-of-sight connection the vehicle and we played with some things on their interface we did you know we
caused it to drop out occasionally we delayed it as one would realistically expect in the field and then we either
engaged or didn't engage the copilot to try to understand what effect that had
on their performance and their experience and what we found was not surprisingly the incidence of collisions
declined it climbed by about 72% when the copilot was engaged versus when it was not we also found that you know even
with that seventy-two percent decline in collisions the speed increased by I'm blanking on the the amount but it was
you know 20 to 30 percentage finally in perhaps the most interesting to me after
every run I would ask the driver and again these were blind tests they didn't know if the copilot was active or not
and I would ask them how much control did you feel like you had over the vehicle and I found that there was a
statistically significant increase of about 12% when the copilot was engaged in that is to say drivers reported
feeling more control of the vehicle 12% more of the time when the copilot was engaged and when it wasn't and then
noticed the statistics it turns out they actually at the average level of control the the copilot was taking was 43% so
they were reporting that they felt more in control when in fact there were 43 percent less in control which was which
was interesting and I think a bears a little bit on sort of the human psyche
in terms of you know they were reporting the vehicle was doing what I wanted to do maybe not what I told it to do which
was which was kind of fun observation and and fun too I think I think the most
enjoyable part of this was getting together with the with the whole group at the end of the study and presenting
some of this and seeing some of the reaction so from there you know we looked at a
few other areas my Carl um and I looked
at a few different opportunities to commercialize this again this was years ago and the industry was in a very
different place than it is today we started a company first called gimlet then another called ride this is the
logo it may look familiar to you we turned that into we at the time it
intended to roll this out across various
automakers in their operations at the time very few saw self-driving as a
technology was really gonna impact their business going forward they were in fact
even even ride-sharing at the time was a fairly new concept that was I think to a
large degree viewed as unproven so as I mentioned December of last year i
co-founded aurora with a couple of folks who have been making significant
progress in this space for many years at Chris Urmson who formerly led Google's self-driving car group at drew back now
as a professor at Carnegie Mellon University exceptional machine learning in apply machine learning was one of the
founding members of Ober self-driving car team and led autonomy and perception there we felt like we had a unique
opportunity at the convergence of a few things one the automotive world has
really come into the full-on realization that self-driving and particularly self-driving and ride-sharing and
vehicle electrification are three vectors that will change the industry that was something that didn't exist ten
years ago two significant advances have been made in you know some of these
machine learning techniques in particular deep learning and other neural network network approaches in the
computers that run them and the availability of you know low-power GPU
and TPU options to really do that well in sensing technologies
in high-resolution radar and a lot of the light our development so it's really a unique time in the self-driving world
a lot of these things are really coming together now and we felt like by bringing together an experienced team we
had an interesting opportunity to build from a clean sheet a new platform a new
self-driving architecture that leverage the latest advances in most Reichman fly machine learning together with our
together with our experience of where some of the pitfalls tend to be down the road as you develop these systems
because you don't tend to see them early on they tend to express themselves as you get into the long tail of corner cases that you end up needing to resolve
so we've built that team we have offices in Palo Alto California and Pittsburgh
Pennsylvania we've got fleets of vehicles operating in both pallet on Pennsylvania a couple
of weeks ago we announced that Volkswagen Group one of the largest automakers in the world
Ondine Motor Company also one of the largest automakers in the world have both partnered with Aurora we will be
developing and are developing with them a set of platforms and ultimately will will scale that our technology on their
vehicles across the world and one of the important the important elements of building Lexus is Lex before coming out
here what this group would be most interested in hearing one of the things that he mentioned was what does it take to build a self-driving you know build a
new company in a space like this one of the things that we found very important was a business model that was
non-threatening to others we recognized that our strengths and our experience
over the last in my case a decade in Chris's case almost two really lies in
the development of the self-driving systems not in building vehicles that I
have had some experience there but but in developing the self-driving so our our feeling was if our mission is to get
a technology to market as quickly as broadly as safely as possible that mission is best served by playing our
position and working well with others who can play theirs which is why you see
the model that we've adopted and is now you'll start to see some of the fruits of that it through the partnerships with some of these
automakers so the end of the day our aspiration in our hope is that this
technology that that is so important the world in increasing safety in improving
access to transportation in improving efficiency in the utilization of our roadways in our cities I mean I this is
maybe the first stock I've ever given where I didn't start by rattling off statistics about safety and all the these other things if you haven't heard
them yet you should look them up there they're stark right the fact that most vehicles
in the United States today have an average on average three parking space
as space is allocated to them the amount of land that's taken up across the world
in housing vehicles that are used less than 5% of the time the number of people
I think in the United States the estimate has spent somewhere between 6 and 15 million people don't have access
to the transportation they need either the because they're elderly or disabled or you know one of many other factors
and so this technology is potentially one of the most impactful for our
society in the coming years it's a tremendously exciting technological challenge and you know the confluence of
those two things I think is a really unique opportunity for engineers and others who are not engineers who really
want to get involved to play a role in changing our changing our world going forward so with that maybe I'll maybe
I'll stop with this and we can go to go to questions
I am Wayne - hello thanks for coming um
I'm a question a lot of self-driving car companies are making extensive use of
lidar but you don't see a lot of that with Tesla wanted to know if you had any thoughts about that yeah I don't want to
talk about Tesla too much in terms of our specific any anything that wasn't public information I'm not going to get
into you I will say that for Aurora we believe that the right approach is
getting the market quickly and you get to market and doing so safely and you get to market most quickly and safely if
you leverage multiple modalities including layer these are the just to clarify what's running the
background these are all just aurora videos of our cars driving on various
test routes yeah hi I'm Luke ramzan from the stone school a lot of so a lot of customers have visceral type connections
to their automobile I was wondering how you see that market the car enthusiast market being affected by AVS and then
vice versa how the how the AVS will be designed around those type of oh yeah customers yeah it's a good question
thanks for asking but I am one of those enthusiasts I very much appreciate being
able to drive a car in certain settings I very much don't appreciate driving in
others right I remember distinctly several evenings I almost literally
pounding my steering wheel sitting in Quogue in in Boston traffic you know on
my way to somewhere I do the same in San Francisco I think the opportunity really
is to turn that it turned sort of personal vehicle ownership and driving into more of a sport and something you
do for leisure I see it a gentleman
some time ago asked me to talk hey don't you think this is a problem for the
country I think you meant the world if people don't learn how to drive that's just something a human should know how
to do my perspective is it's as much of a problem as people not intrinsically
knowing how to ride a horse today if you want to know how to ride a horse go ride a horse if you want to you want to race
a car go to a racetrack or go out to you know a mountain road that's been allocated for it ultimately I think I
think there is an important place for that because I certainly agree with you I'm very much a vehicle enthusiast
myself but I think there is so much opportunity here in alleviating some of
these other problems particularly in places where it's not fun to drive that I think there's a place for both yeah
yeah yeah congratulations on the partnership that was announced recently I think so I have
a two-part question the first one is so we heard last week from I think there
was a gentleman from talking about how long they have been working on this autonomous car technology and you simply
have rammed up extremely fast so is there a licensing model that you have
taken that I mean how are you able to commercialize the technology in one year
so just to be clear we're not actually commercializing we're just to
distinguish we are partnering and developing vehicles and Walter may be running pilots as we announced you know
we could to ago with the Moya shuttles we are however I will distinguish that
from broad commercialization of the technology and I don't want to get too much into you know the nuances of that
business model I will say that it is is one that's done in very close partnership with our automotive partners
because you know they at the end of the day they understand their cars they understand their customers they have
distribution networks they are you know our automotive partners are fairly well
positioned it provided they have the right support in developing a self-driving technology the fairly
fairly well positioned to you know roll it out of the scale so the second part
of my question is again looking at this you know pace of adoption and the maturity of technology do you see like
an open source model for autonomous you know cars as they become more and more
unclear I am not convinced that an open source model is what gets to market most
quickly in the long run it's not clear
to me what will happen I think there will be a handful of successful self-driving stacks that will make it
nowhere near the number of self-driving companies today but a handful I think
two questions one is in invariably a new product development there's typically
two types of bottlenecks there's a technological bottleneck and an economic bottleneck right so technological
bottleneck might be a you know the sensors aren't good enough or the machine learning algorithms aren't good
enough and so on I'd be interested to hear and it'll shift obviously over time so I'd be interested to know what you
would say is the current thing that if hey yeah if this part of the of the architecture was ten times better we
would and that on the economic side I'd be interested to know you know gee if if sensors were 100 times cheaper then so
it'd be interested to hear your perspective on that's a great question let me start with the economic side of
it and just to get that at the wake is a little bit quicker answer the economics
of operating a self-driving vehicle and a shared network today would close that
that business case closes even with high costs of sensors that is not that is not what's stopping us and that's part of
why the the gentleman earlier who asked you know should use lighter or not if
your target is to initially deploy these in fleets you would be wise to start at
the top end of the market develop and deploy a system that's as capable as possible as quickly as possible and then
costs it down over time and you can do that as computer vision and precision recall increase today they're not good
enough right and so so economically depending on your model of going to
market and we believe that the right model is through
mobility services you can cost out your
cost down the center inevitably you know there's no unobtainium in light our units today there's no reason fundamentally that he
should conserve a light our unit will lead you to a seventy thousand dollar price point right however if you build
anything in low enough volumes is going to be expensive many of these things will work their way into the standard automotive process
they'll work their way into Tier one suppliers and when they do the automotive community has shown
themselves to be exceptional at driving those costs down and so I expect them to come way down to your other question
technological bottlenecks and challenges one of the key challenges of self-driving rima is and remains that of
forecasting the intent and B and future behaviors of other actors both in
response to one another but also in response to your own decisions in motion that's a perception problem but it's
something more than a perception problem it's also a you know prediction and you
know there there are a number of different things that come together to have that have to come together to solve
this we're excited about some of the tools that we're using and interleaving various of modern machine learning
techniques throughout the system to do things like project our own behaviors
that were learned for the ego vehicle on others and assume that they'll behave as we would had we been in that situation
like an expert system kind of approach yeah yeah you you assume nominal
behavior and you guard against off nominal right but it's it's very much it's not a solved problem I wouldn't say
it's it's very much as you get into that really long tail of development when
you're no longer you know putting out demonstration videos but you're instead just putting your head down and eking
out those you know fine on lines that's the kind of problem you tend to deal with again so this question isn't
necessarily about the development of self-driving cars but more like an ethics question when you're putting
human lives into like the hands of software isn't there always the possibility for like outside agents with malicious
intent to use it for their own gain and how do you guys if you do have a plan
how do you intend to protect against yeah so security is a very real
aspect so we saw it's a constant game of cat
and mouse and so I think it just requires a very good you know team and a concerted effort
over time I I don't think I don't think you solve at once and I certainly
wouldn't pretend to have a plan that solves it and is done with it we're we
we try to leverage best practices where we can in the fundamental architecture of the system to make it less exposed
and in particular key parts of the system was exposed to nefarious actions of others but at the end of the day it's
just a constant is a constant development effort thank you for being
here so I had a question about what opportunities self-driving cars open up since driving has kind of been designed
around like a human being at the center since the beginning if you put a computer at the center what you know
society-wide differences and maybe even like within individual car differences that open up like you know could cars go
150 miles an hour on the highway and get places much faster what cars be like like look differently when a human
doesn't need to be paying attention and stuff like that yeah I think the answer is yes the and that's that something is
very exciting right so one of the I think one of the unique opportunities that automakers in particular have when
self-driving technology gets incorporated into their vehicles is they can do things like play like differentiate the user experience they
can provide services you know augmented reality services or
you know location services many other sort of it opens a new window into an
entirely new market that automakers haven't historically played in and it
allows them to change the the very vehicles themselves as you've mentioned
the interior can change as we validate
some of these self-driving systems and confirm that they do in fact reduce the collision the the rate of collisions is
we hope they will you can start to pull out a lot of the extra you know mass and
other things that we've added to vehicles to make them more passively safe right roll cages crumple zones airbags you
know a lot of these things you know presumably in a world where we don't
crash there is there is much less need for passive safety systems so yes I have
a question about the go no-go tests that you conduct for certain features like you mentioned the throttle control where
you slow down the throttle assuming that the driver has pressed the wrong wrong pedal when you test when you decide to
launch that feature how do you know it's definitely going to work in all scenarios because your data set might not be oh it's a it's a it's a
statistical evaluation every case right you're right there you will this is this
is part of the art of self-driving vehicle development is you will never have comprehensively captured every case
every scenario that is as my some of you
may want to correct me on this I think that's an unbounded set it may in fact be bounded at some point but I think
it's on and so you'll never you know there actually have characterized everything what you will have done
hopefully if you do it right is you will have established with a reasonable
degree of confidence that you can perform at a level of safety that's better than the average human driver and once you've reached that threshold and
you're confident that you've reached that threshold I think it the opportunity to launch is is real and you
should seriously consider it so thank you for your talk today first and my
question is self-driving seems to be able to ultimately take over the world to some extent but just like other
technologists today they open up new opportunities but also bring in adverse effects so how do you respond to fear
and nected effects that may come in one day and especially what do you see as the positive and active implications of
future day self-driving positive and negative implications
so the positive ones like kind of listed and you'll find your favorite press
article and they'll list them as well the negative ones in the near term I do
I do worry a little bit about the displacement of jobs not a little bit
this will happen it happens with every technology like this I think it's
incumbent on us to find a good way of transitioning those who are employed in
some of the transportation sectors that will be affected into better work right
there are a few opportunities that are interesting in that regard but I think
it's an important thing to start discussing now because it's gonna take you know a few years and you know by the
time we got these self-driving systems on the roads really starting to place that labor I'd really like to have a new
home for it now I I'm kasha from the Sloan School my question was more about
your business model again with partnering with both VW and he and a and
you're just perspective and how you were able to effectively do that did not one of them want to go sort of
exclusive with you and what was your sort of thought process about that yeah so our our mission as I mentioned used
to get the technology to market broadly and quickly and safely we are you know
have been and remain convinced that the right way to do that is by providing it to as much the industry as possible
to every automaker who shares our vision in our approach and we were pleased to see that both Volkswagen Group and I'm
assuming you all know the scope of Volkswagen right this is a massive automaker Hyundai Motor also very large
across Hyundai Kia and Genesis they both shared our vision of how we should do
this which was important to us they both shared you know a a keen interest in
making a difference at scale through their platforms Volkswagen has you know
I can give very admirable set of initiatives around electric and vehicle electrification a few other things Honda is doing similar things and so you know
for us it was important that we enable everyone and that was kind of what Aurora was started to do hi I had a
question now that I see a lot of companies are coming up with self-driving cars right so most of the
cars are pretty much all the technology is bound only to the car so would we see
something like an open network where car communicate with each other regardless of which company they come from and
would this in any way you know increase the safety or the performance of vehicles and stuff like that yeah I
think I think you're getting it vehicle to vehicle vehicle infrastructure type communication there there efforts
ongoing in that and it certainly it's it's only positive right the having that
information available to you can only make things better the challenge has historically been with
vehicle the vehicle and back to particular vehicle to infrastructure or vice versa it doesn't scale well one and two it's
been slow it's been much slower and coming than our development and so when we develop these systems we develop them
without the expectation that those that those communication protocol are available to us will certainly protect
for them and it will certainly be you know a benefit once or once they're here but until then many of the hard problems
that I would have welcomed 10 years ago to have a beacon on every traffic light that just told me at state rather than
having to perceive it I would have certainly used those ten years ago now they're less significant because
we've kind of worked our way through a lot of the problems that would have solved thank you for your talk my question is what's your opinion about
cooperation of self-driving vehicles so maybe I think if you can control a group
of self-driving vehicles at the same time you can achieve a lot of benefits to the traffic yes that is where one of
the that is where a lot of the benefits come from and infrastructure utilization or and is in ride-sharing with
autonomous vehicles and and specifically you know the better we understand demand
patterns people movement goods movement the better we can sort of optimally
allocate these vehicles and at locations where they're needed so yes that's that
certainly that that coordination this is where as I mentioned these three vectors of vehicle electrification ride-sharing
autonomy or transfer mobility as a service and autonomy really come together with a unique value
proposition yeah okay thank you yeah thank you so much for a great talking

----------

-----
--34--

-----
Date: 2018.03.09
Link: [# Emilio Frazzoli, CTO, nuTonomy - MIT Self-Driving Cars](https://www.youtube.com/watch?v=dWSbItd0HEA)
Transcription:


Introduction
today we have ameliafe Rizzoli he's the CTO of new Tata me one of the most
successful autonomous vehicle companies in the world he's the inventor of the RR T star algorithm
formerly a professor at MIT directing research group that put the first
autonomous vehicles on road in Singapore and now he returns to MIT to talk with
us give him a warm welcome oh thank you Lex
it's a great opportunity is a great pleasure to be back here I spent 15 years of my life here at MIT first as a
graduate student and then as a faculty as a faculty member and this is where
autonomy the company essentially was born and we did a lot of the research that led us to you know to start this
company and eventually you know develop all this technology what I will talk about today is a little bit about you
know our vision on autonomous vehicles why we want to have autonomous vehicles you know some of the guidelines you know
on the technology development why we are doing things in a certain way let's get started but and you know I really would
Why selfdriving cars
like to tell you you know a number of stories about why I started doing this
and why I think this is an important technology why we ended up starting this company so you know I've been a faculty
member here for 10 years I mean I was happily working with my UAVs and I was in Aero Astra at some point around 2005
mm yeah something you know there was these DARPA Grand Challenges that sounded cool right so I started working
on on cars as well but they are that were that I was doing was mostly you
know I was working on airplanes and cars to make them fly and drive by themselves
because it was cool you know just look you know no hands you know it drives and as it controls guys roboticist that's
all I needed right but then in 2009 there was this
new project that was starting in the team that was you know getting together to write a proposal for a project on
future urban mobility in Singapore okay now telling you the whole story but
essentially you know I got interested in that project just because I wanted to go to Singapore okay and then I you know
then I called the person who was putting together the team and okay yeah thank
you for your interest but you know what do you think that you bring to the table and you know we had just done the dark
urban child and so well you know I know how to make autonomous cars so what this is a project on future
urban mobility so what do cars have to do with with urban mobility autonomous
cars you know what would they had to do with mobility and you know there was the phone call the five minute phone call
that changed my life okay because she asked me this question that actually was Cindy Bernard who is now a chancellor
right and then I had to come up with an excuse right so why well imagine they
have a smart phone and then a smart phone app and then you use this app to call a car the car comes to you you get
on the car drive wherever you go want to go step off the car and the car you know goes to pick up somebody else it goes to
park or something right so this was two thousand in nine uber twas Travis kalanick and a couple of guys and black
cars in San Francisco right so and essentially she bought it so I joined
the team and and we started this activity but you know the important
thing is that I started thinking about you know there was something an excuse that they made up in those five minutes
okay but you know what kind of sounds like a good idea and I started thinking more
Why selfdriving vehicles
about this and I started thinking more about why do we want to have self-driving vehicles okay so the number
one reason that you typically hear is we want to have self-driving vehicles so that we make roads safer
okay a very large number of people die on on the road road accidents every year
what these people do not realize is that most of those people are actually you
know fairly young like in their 20s and 30s okay ompletely they you know what
people usually say is that you know Sebastian Thrun and you know back in the day he gave all these TED talks up
talking about his best friend from where he was young who died in a road accident right and then he made a mission for his
life to reduce the road accidents right but and I mean so any idea is that you
most of the road accidents are due to human errors you remove the human you remove the error
right and then you save lives okay so this is this is typically the number one reason that people mention when they
talk about why you want to have some tiny vehicles second reason is convenience
essentially if the car is driving by itself you can do other things you can
sleep you can read you you can text legally to your heart's content you can
check your emails or so and so forth right this is also great third thing is
you know improved access to mobility you know people who cannot drive me because
and I have some physical you know impairment or maybe they are too young they're too old already too intoxicated
to drive right so then you know if computer can take them home
another thing is increase efficiency throughput in in a city as cars can
communicate beyond you know visual range for example another one is reduce
environmental impact okay now these are all fantastic reasons you know why we
may want to have some driving vehicles the problem with me is that if you think
about this these are all you know good reasons but these are all ways that you take the status quo
you know how cars are used today and you make it a little bit better maybe a lot better but you do not make it different
okay and really that is what I am mostly what I was mostly interested in can we
you know use this technology leverage this technology to change the way that we think of mobility okay so how do you
Cost of selfdriving vehicles
compare all these different things okay so this is you know quick back of the envelope kind of calculation that you
can do in on your own you can question the numbers but I think that the orders
of magnitude are right okay so you know the first thing is okay so fine we heard
that a big reason for self-driving cars is to increase safety you know save lives great now how much is your life
worth well to yourself to your loved ones your friends your family is
probably you know priceless - the government is what about nine million dollars okay so this is what is
called the this is what is called the cost of a statistical life there was a
report that was released a few years ago probably you know there is an update now but I haven't seen it
the economic cost in road accidents the United States is evaluated to be about you know 300 billion dollars a year the
societal harm you know of road accidents is another you know all the pain and
suffering is evaluated to be another six hundred billion dollars a year so what we are getting to is about almost 1
trillion dollars okay it's a big number okay but let's look at where the other
effects are okay what is the cost of congestion is an estimate hundred
billion dollars a year the health cost of congestion of the extra pollution so
another fifty billion dollars a year so you see that these are a little it just a small change right the next effect is
actually important right so what is the value of the time that we as everybody
in society will get back from not having to drive okay simple calculation what I
did is I multiplied one half the median wage of workers in the United States
which is an embarrassingly low number multiplied by the number of hours that
Americans spend behind the wheel okay and what you get is about you know what
was it about 1.2 trillion dollars a year so something that you may notice is that
the value to society of getting the time back from having to drive is actually
more than the value to society or increased safety okay of course it's a
little bit cynical okay so take it with a grain of salt and a grain of salt but you start seeing you know how these
things compare and what you may notice from this pie chart is that you know there is still half of that is missing
what is the other half the other half is actually the value that you provide to society to you know
all individuals okay by essentially making car sharing finally something
that is convenient to use affordable reliable okay so for me car sharing or
you know vehicle share in general is a concept that everybody loves but nobody uses okay or not as many people as we
would like to you know use this kind of services examples when I was you know
here at MIT I really like using hub way you know the bicycle you know sharing
but you have to be very careful you know if you wait too long in the afternoon sorry there are no more bikes on campus
right or maybe very often you cannot find a bike or maybe you cannot find a
parking spot for your bike so then you had to buy somewhere else and then work so that defeats the purpose of of using
that bike same thing with with cars right so typically with you know car
sharing systems what you have is either you have a like a two-way which is essentially hourly rental right or you
have a one-way but in one way system then the distribution of cars tend to get skewed right and unless the company
you know rip repositions cars in some you know clever way then the year you're
not guaranteed that you will get a car where you need it and you're not guaranteed that you will get a spot a
parking spot when you don't need the car anymore okay if you think of that these are both like a friction points you know
for using vehicle sharing and these are both pre friction points that are actually addressed by if the car can
drive itself okay so if you bring in all the economic you know benefits of a a
car sharing system that actually works that's something that we estimate it to be you know it's about two thousand
dollars a year so you see that this actually it has a like a big chunk in this in this pie chart okay and that is
using an estimate of what we call the sheriff factor of four meaning that one of the shared vehicles can essentially
substitute for for in privately owned vehicles okay there are some studies that you know get
to this sharing factor up to ten and of course the benefits are even more now
every time I see inter write a round number like that I get suspicious right you know ten is a little bit too
convenient to be true right but any so that's something that you can find in
the literature so so this is really
What is a selfdriving car
where I think that the major impact of of autonomous driving or certain cars
will come from now if you I think also there is a lot of confusion in the community in the world about what a
self-driving car means now what I'm doing here I just listed this you know
five levels socially six levels of automation you know these are the Society of Automotive Engineers levels
okay so level zero is not a mission that's your you know great-grandfather's
car right driver assistance level one there is for example cruise control or
you know some simple single channel automation partial automation you have
you know something like for example lane-keeping and cruise control but you
still require the driver to pay attention and intervene conditioner
automation level three a driver is a necessity it's not required to pay attention all the time
but needs to be able to intervene given some notice okay and you know that some
losses I think is like ill-defined concept and then you have level four
level five that are like a higher donation essentially no driver needed in some condition that is level four and in
all conditions that level five okay now my first reaction when I started seeing
these levels and you know there is also similar version by Nitza
is that listening to me you know a horrible idea and the horrible idea in the sense in because they are given
numeric levels so you have level zero one two three five whenever you have a
sequence of numbers you are led to believe that these are actually sequential right that you do level zero
then you do level one thing you do level two three four five I think this isn't
like an enormously bad idea because I think that level 2 and level 3 that is anything where you require the human to
pay attention and supervise the automation and be ready to intervene with no notice or with some ambiguously
defined you know like a sufficiently notice they just go behind you know go
against human nature and you know this is especially painful for me as a former
aeronautics and astronautics professor where we saw in the airline industry that as soon as Auto Palace were being
introduced and everybody thought that accidents would go down actually there were more accidents
because now you have new failure modes induced by auto pilots okay you have
multiple fusion Pylos flu situation awareness pylos lose the ability to react in case of an emergency okay so
the idler and Industry had to essentially educate itself on how to deal with automation in a good way and
think of pile you know pilots are highly trained professionals which is not the
same that you can say about your everyday driver right so how do you train people who probably you know you
know the last time they said with a with an instructor in a car was you know when they were 16 right how do you train
people to use the automation technology in and do it safely right so I think
that you know distantly that front very scary on the other hand I think that you know the full automation when the car is
essentially able to drive itself does not rely on a human to take over isn't
it that in a sense is easier and you know this is what we are doing and but you know the point is that not
Value of selfdriving cars
all it is easier but I think that is essential to capture the value of the technology now if you think of it so how
do you realize the value of these self-driving vehicles okay so the first thing that people say is safety I think
it is true that eventually asymptotically self-driving cars will be
safer than their human driven counterparts however at what point can
we be confident that that is the case are we there yet not sure okay
so so how do you demonstrate the
reliability of these self-driving cars so we know you know they've driven that
cars for three million miles right so with a readily small number of accidents
if I remember correctly only one was their fault right but um actually humans
drive for many you know many times that without accidents or so how do you
really make sure that even though the number sounds impressive it really
doesn't have that much of a statistical significance right and then every time you make an update to a change to your
system to your software you really have to validate again right so I think that making the case for safety is actually
is a very challenging issue and we may not be positive that these self-driving
cars are actually safer than the human counterparts you know until you know a really long time from now
okay so safety for me remains kind of an questioned open question at this point
how do you get back the time value of driving if you had you know at least I'm
speaking for myself if I have to constantly pay attention to what the car is doing excuse me but I rather drive
myself okay because you know if the car is driving and you know this is the paradox
right so the better the car drives the harder it is for me to keep paying attention right and this is where the
whole problem is right so there would be very hard for me not to fall asleep or you know not to get distracted so if I
want to get that time back really you know the car must be able to drive itself without requiring me to pay
attention captioning again you know is a you know in order to make car sharing
really convenient and reliable and sounds fourth you need the car to come to you with nobody inside and Ford it
for that you need level four or level five okay anything else just doesn't cut
it you know everything else for me is just a nice gadget that you have on your
car that you show off to your friends or to your girlfriend okay so that's about it right is it's not that useful so my
point is that level four or five automation is really essential to capture the value of this technology and
in fact the one game-changing feature of these cars is the fact that these cars
now can move around with nobody inside that's really the game-changing feature okay good and you know this is you know
really what we like to do now there are many paths that you can go after this
target okay I usually show this this fear okay so on this figure what I show
on the horizontal axis is the scale or the scope of the kind of driving that
you can do okay so on the left is like a small you know pilot maybe a closed
course on the right is driving everywhere okay on the you know like
complex environments right mass deployment and so forth on the left
there is on the vertical axis is the level of automation okay now really what
we would like to do is get to the top right corner right so we have millions of cars driving all over the world
are completely you know completely out in a completely automated way okay
what I see is there are two different paths that the industry is taking okay what I show here is what I call this is
the OEM path okay so this is the the automaker's right so
they're used to thinking of production production of cars in the orders of many
many millions okay and essentially what they do is they make a lot of cars and they are adding
features to discuss you know advanced driver assistance systems and so on so forth right and essentially they're
following these levels 0 1 2 3 4 5 ok and you know today you can buy cars
which even though they claim fully autonomous you know package for $5,000
plus another $40,000 or something in the fine print this is level 2 rights or
level 2 or level 3 so you know Tesla said is I think the bau-t who the new
Audi a8 is a 8 they're coming out with this we just kind of feature Cadillac I think as a similar thing okay
the problem with that I seen that you know you had to cross this this red band
okay this red band where you're actually requiring human supervision you know of
your automation system another path where people are following is this other
okay so this is what we are doing what way more you know where these are where
all the indications or that Weimer is doing of course they're not telling me exactly what they do similar thing for
uber right so essentially what they're doing is they're working on cars would be fully automated from the beginning
and they start with a small you know maybe geofence application and then
scale that update operations outright but always remaining at the full you
know High full automation level okay another thing that is important that you
Consumer vs service
know people make a lot of confusion and don't seem to realize the big difference is the following when people ask me when
do you think that we will see autonomous vehicles everywhere on the city aware you know autonomous vehicle would be and
would be common I guess I'm okay but you know what do you mean exactly by that
right because if you ask me when you see that you will be able to walk into a car dealership and get out with the keys to
a car that you know you just push a button it takes you home that's not happening for another 20 years or at
least okay on the other hand if you ask me when you will be able to go to some
new city and some on one of these vehicles that piece you up and takes you to your destination the other thing is
happening within a couple of years okay what is the difference there is a big
difference between autonomous vehicles self-driving cars is a consumer product versus a service that you provide you
know to two passengers okay so for example what is the scope you know where
do these cars need to be able to drive okay if it's a product and I pay you
know ten thousand dollars for it then I want this thing to work everywhere right
so take me home you know pick two to be into this little alley you know drive me through the countryside on the other
hand if I'm a service provider and I'm offering the service I can decide you know I'm offering this service in this
particular location and by the way I'm offering this service under these weather conditions and maybe under these
traffic conditions okay so just the problem becomes much more much easier
what are the financials right so if I have to sell you in autonomy a car with
an autonomy package how much can i cost you know what would what are my
cross-country constraints on that autonomy package if I sell it to you you
know first of all the cost of the autonomy package must be comparable to the cost of the vehicle okay
you know you will not buy a $20,000 car with a half a million dollar autonomy
package right also you can do so another
back-of-the-envelope calculation that it is okay so let's say that what is the value to you as the buyer of this
autonomy package let's say that the value to you is the fact that now instead of dragging you know for the
rest of you know for the next 10 years you can have the computer grinding for you what is the value of your time as
you are not driving right so do a quick calculations again you know total number
of hours that Americans spend behind the wheel median wage or in a value of time
what you get is you know what I get is that you know the net present value of
the drivers time over the next 10 years is about twenty to twenty thousand
dollars okay so then you know a rational buyer will not pay more than that you
know to buy this autonomy package right so now you're constrained by twenty thousand dollars okay
or actually if you want to make a profit out of it you know your constraint your autonomy package cannot cost more than a
few thousand dollars okay on the other hand if you're thinking of this as a service then what you are comparing to
is the cost of providing the same service using a carbon-based life form
like a human behind the wheel okay so now you want to provide 24/7 service you
need to hire at least say three drivers per car okay then the cost is comparable
of the order of hundred K a year okay so now I'm comparing the cost of my
automation package to something that is going to cost me $100,000 a year over
the life of the car okay so now the cost of the Atlanta computer or that fancy
radar or something doesn't matter that much okay so I have much more freedom in
buying the sensor that I need infrastructure for example people talk
about maps HD maps right now again if I want to sell it as a product I need to
enter have to sell it I want to sell it on a globe scale well global could mean older the United States for example or all of
Europe then I need to have maps HD maps of the whole of Europe or the continent
or any other stays or whatever I want to sell the you know the cars if I'm providing a service then I only need to
map the area where I want to provide the service and by the way how do how does
the complexity of the maps scale with the customer base that you're serving if
you think of a uniform people density okay so then actually they land you
think that the complexity and the cost of generating Maps scales with the length of the road network then the cost
of the maps scales with the square root of my customer base meaning that will
become negligible as I serve more people okay so HD maps yes it's a pain in the
neck to collect them and to maintain them but it's much less of a pain in the neck that actually open it in the
logistics of a fleet serving the population of a city okay and servicing
and maintenance you know how would you calibrate your cameras and your sensors
you know that's not something that you would do as a normal consumer right oh we are not used to that when I was
little I was used to my father you know he was tinkering with the car all the time you know checking the you know the
timing belt or changing the oil or you don't do any of that nowadays right so
you just sit in the car switch it on if the yellow light you know Check Engine comes up into the dealership right
that's all you do now imagine that you know now you have if you want to use
your autonomy package you had to calibrate the sensors every every time you go out or you know you have to
upload you know like a new version of the drivers and these are that so you don't want to do that on the other hand
in the service model I had the maintenance crew that can take care of it in a professional way okay
so big difference between the two models so there are a couple of important takeaways right so one thing is that the
cost of the autonomy package is not really an issue really the cheaper I can make it the
better it is right but that is not really the main driver in particular if you need a lighter sensor for example to
detect a big truck that is crossing your path by the ladder sensor okay so that
is not making the difference and maybe you can save some lives okay any reference to other things is intentional
HD Maps
the other thing is HD Maps the people worry about you know 12 you know very
much today from my point of view HD Maps my expectation is that HD maps within a
few years will be a dime a dozen okay what is complicated what is expensive now in generating all these HD
maps the mapping companies need to put these sensors on a car on you know and
send these cars around now imagine that I have a fleet of 1,000 cars with these
sensors on board and these cars are just driving around the city all the time the
generating gigantic amount of data that I can just use to make and maintain my
HD maps so I think that you know especially from the point of view of the operators the providers of these
mobility services very easy to collect data to essentially make you know make
and maintain their own Maps okay so if you need HD maps that's fine because as
soon as you start offering this service you will be able to collect all the data you need to generate this a generate and
maintain these maps oh by the way this is showing an animation showing you know like a simulation of a fleet of I think
it's a couple of hundred vehicles in Zurich in Switzerland right so that's
where I was based until a few days ago and as you see in essentially you have
vehicles that going through go through most of the city you know every few hours okay I think that for
example the uber fleet goes through 95 percent of Manhattan every two hours or so
Advantages
cos advantages you know of course you know the you know most of the cost of
you know taxi services nowadays is is the driver you know it's about half of
course you remove the driver from the picture you don't have to pay them of course the automation costs you a little
bit more servicing cost you a little bit more but you see that you know you still have you know you you know you can get
like a really significant increase in the margin right meaning that you can
pass some of those you know savings to customers right but also you can make a very strong business case however this
is also misleading now if you think of it okay so typically what the reaction
that you get is the following oh my goodness now you make this thing
and then all taxi drivers all truck drivers would be out of a job okay
and in fact one day I was summoned by the Singapore Ministry of Manpower okay
and I was terrified oh my goodness they're gonna shut me down because they're afraid that that will put all of their taxi drivers on a
State on a street in the sense of being unemployed turns out it was the opposite
Mobility paradox
what most people do not realize is that actually mobility services worldwide are
actually meant power-limited okay in Singapore they would like to buy more
buses but they don't have enough people who are able and willing to ride the
buses okay this is true pretty much the same to for tracking same for Tarsus now
this is another back-of-the-envelope calculation that you can do on your own now imagine so as we know you know Ebers
be widely successful you know very high valuation a lot of this valuation is
predicated on the fact that everybody in the world will eventually use uber right or something similar now something that
people don't think about is the following now if everybody in the world you uber for their mobility means how many
people in the world need to be drivers for uber do the calculation what you see
is that one person out of seven must drive for uber if uber is surveying the whole world do you see that happening no
way right so people still need to be you know teachers doctors you know policemen
firemen you know or you know some people need to be kids you know so that is something this cannot happen how are we
facing these paradox in a sense right so you know today what you have is people who drive around but what is happening
today is that we are all doubling up as drivers for ourselves and in fact we do
spend about one-seventh one-eighth of our productive day behind the wheel
very often ok so you know for me you know did the big the big change is will
be more on the supply of mobility rather than on job loss I mean of course if you
increase supply of mobility you know the the cost of mobility will have to you know we will you know probably go down
wages for drivers will go down right so that is that is a that is a that is an
issue but you know maybe other you know baby balance by like a added value and
service or other things that you can imagine another thing about truck
drivers you know something that they recently learned 25 percent of all job
related deaths in the u.s. are actually by trucks drivers ok is the most the
single most dangerous industry that you can be in so maybe if you can take some
of those people out of those trucks and maybe supervise remotely control a truck sitting in their office instead of
sitting in the truck you know that that may be actually benefit to them back to the question of when we lot on most
When will selfdriving cars arrive
vehicles arrive and you know in a sense this is what you know what our prediction our vision is right so what
we will see is that what we think is that you have a fairly rapid adoption of
self-driving vehicles in these mobility as a service model okay as a fleet of
shared autonomous vehicles that people can use you know to go from point to point right rather than all of course
eventually you know people will be able to buy these cars and maybe own them if they really want but you know that is
something that is much later in time for for a number of reasons some which I
discussed okay so this is you know what we expect in terms of the timeline for
this now what is the state of the art
State of the art for autonomous technology today
for autonomous technology today you do see a lot of demos for from a number of
companies you know doing a number of things right but but a lot of the things
that you see are not too much different from this video I don't know if any of
you recognizes this video but you know look at the cars this was actually done
by LSD commands in the late 90s in Germany okay no fancy GPUs no it was
just a cameras and some you know basic computer vision algorithms but
essentially he was able to drive for hundreds of miles on the German highways okay if you're not showing
something that goes beyond that you have not made any progress you know over then over the past 20 years okay yeah you're
using fancy deep learning and GPUs and things nowadays but you're doing what people were doing 20 years ago you know
okay so you see arena clearly there's a lot of hype in these things but you know
if you see something like that I don't think it's very impressive okay people people you know knew how to
do that for for a very long time something that I find a little bit I may be biased clearly right but this is
Driving in traffic
something that I find a little bit more exciting this is actually footage from you know our daily drives in Singapore
okay this is four times in real time we don't drive that fast okay but essentially what we're doing in
Singapore we are driving you know in you know public roads normal traffic what you will see is not
so but you know do we have you know construction zones intersections traffic you know you know of both sides we will
get to a pretty interesting intersection has a red light will turn to green in a
second human mind in Singapore they drive on the left right so making the
right turn is what is hard because you had to cross traffic right and here you have in a lot of traffic and you know
the car is making the right decision in all of these without any human intervention right so I think that in
this day and age if you're not showing the capability of driving in traffic in
an urban situation like that you're not really showing any advance over what people were able to do 20 years ago okay
and you know I mean as you can see if I saw the intercessions other cars pedestrians you know all kind of like a
crazy interactions you know you know the cars park in the middle of the street that you had to avoid go to the other
lane you know things like that okay so this this is what you had to do every day and you know this is what we are
doing every day in Singapore we are doing every day here in the c4 if you're aware of botany we are driving
you know cars we are allowed by the city of Boston to drive our cars autonomously in the Seaport area so what are the
Technical challenges
technical challenges okay so actually this is a slide that I did I'm fairly
reusing from a talk that I'm not chakra the founder and CEO of mobile I gave
here at MIT a few months ago okay so this is what he said okay so it's not
what I say what he says is that the big challenges are sensing you know
perception it's mapping and then is what he called driving policy right that I
will call more like a decision-making okay now what he said is that sensing
perception is a challenge but is a challenge we are aware of and then we
are making rapid progress on getting better and better sensing perception algorithms okay second it's HD maps what
he said is that it was a huge logistical nightmare so he didn't want to deal with that you know like mobile I tries to
avoid that from my point of view as I said you know for me it's the maps it is
a replay in the neck to get those maps but in a few years
maps will be a dime a dozen okay so we'll get all the mapping data that we want and we need so the big problem is
during policy okay the remaining problem is drawn in policies so how do you do it not and you
Traffic light example
know this is a typical example of things that we encounter in you know in any color urban driving situation so you
will see a video so this is a case where we are at the traffic light we are
stopping the traffic you know the light turns green we are making the turn this is a pedestrian crossing the street wait
for the press tree and go through it and then we see that there is a truck that is part in the middle of our lane so we
need to go to the other lane which is in the opposite direction there is a model excuse me a motorcycle coming so we had
to handle all that kind of situation right so how do you write your software
in such a way that your car is able to deal with this kind of complicated situation by itself okay and
Rules of the road
my point is that you know this is not really about negotiation is not about
policy why do you have rules of the road my claim I have not proved it
mathematically yet but my claim is the following the touching the rules of the road were introduced exactly to avoid the need for
negotiation when you drive okay when you're walking as a person you just walking down the
hallway you know walking down the infinite corridor and there is a person come in the other direction there's always that awkward moment right
away you're trying to linger I go left I'll go right right with cause you you don't do that right so in cars the side everybody go
right or in other places everybody go left period and you don't negotiate that okay you get to an intersection the the
the light is red you stop you know saying I'm putting I'm really in a rush you know do you mind if I go no you
don't do that right so it's red and you stop okay so the rules of the road have been invented by humans in order to
minimize the amount of negotiation and you know and you know in particular okay
so this is a slightly I mean this is actually very old video but I kind of like it so now our car is a little bit
more aggressive but you know what you see here is this case you know this is how the car behaved in that particular
situation so you see it's raining red light turns green there's a pedestrian
crossing our path so we heel to the pedestrian you see that there is a you will see that there
is a truck that is parked on the on the left lane in the middle of the lane so we had to go around it but this is a
motorcycle that is approaching so we had to be careful in going to the other lane
okay so we squeezed through the through the motorcycle you know we try to go very slowly next to squishy targets
right but then as soon as we pass the truck the truck driver decides to get
moving okay so then what we do is we wait for the truck to get you know to get going and
then go back to our lane now imagine writing a script you know or you know if then else if there is a
track but the truck is moving and then do this and this the network so you know what to do that right so how do you
handle this kind of situations okay so the industry standard you know this approach to this was - and
The industry standard
by the way this is what we did at the time of the dark urban challenge okay so we had a lot of if-then-else statements
or you know finesse test machines or some logic that was encoded by you know some furnaces machine kind of kind of
things the problem with that is it's
very hard to come up with this logic and is essentially impossible to debug it
and verify it right so I spent many miserable months sitting in the naval
airbase in Weymouth right so here in a rental car just plain interference with
our autonomous car trying to adjust all these logic and parameters and things so
I vowed that I would never do it again I was just miserable experience I'm
happy to say that actually we did come up with a much better way of doing it and you know by the way this is a video
Caltechs mistake
from the Caltech team at the dark urban challenge as you can see they're trying to go to an intersection they decide to
go then for some reason they decide not to back up out of the intersection so
the director of DARPA you know Tony Taylor at the time he was there he went
like that so they were out of the race okay so as soon as CCO saw that what
happened here there was essentially a bug in the logic Caltech you know very a
team of very smart people very capable dedicated people work on these for months they didn't catch this Bank this
Bank they were out of the race right so it's very easy to make mistakes and it's very hard to find those bugs okay so as
Too many rules of the road
a reaction to that you know there is this new
is it possible to cut the sound thank
you so now what people what you hear people saying is well there are too many rules
of the road it's impossible to code all of them correctly so let's not do that
just feed the data you know feed the car a lot of data and let the car learn by
itself how to behave okay okay and this is what you see you know
you know there are a number of circuits and other efforts that are trying to use all these you know deep learning or
learning approaches to to get to the fore end to end driving of of cars okay
so you see a video from Nvidia okay
understand this is a course on deep learning for cars right but so so I
don't want to sound too negative on the other hand I will try to be honest in
what I think ok so you know there are a number of problems right so that's what
Learning the wrong thing
is happened to us right so one of our developers you know you know super bright lady from you know you know
Caltech and you know the first version of the code for dealing with traffic
lights essentially the reaction that you know that that they had for for the yellow light was if you see a yellow
light speed up what the heck oh this is what my brother does okay so there is
always the danger that you learn the wrong thing okay did the wrong behavior
in a sense of course there are some situations in which accelerating when
you see a yellow light is actually the right response but it is not always the case right so there are some other
features of the situation that you need to examine right also the other thing is
as a cartoon right so you know you want to be able to explain why the car did
something and I would say that more than explaining because now you also see articles in which people say Oh
a fun way of explaining why they do not not for him to carve decided to do something right I want to show you is
some okay so these are the noodles that were activated just saying that you know
what if I do an F in a fast MRI of the brain and they see what neurons what
areas of the brain are activated when I watch a movie then I know how the brain works no I have no idea okay the point
is that yes you want to trace the reason the cause for why they can't behave in a certain way but you also want to be able
to revert the cost right so you want that information would be actionable in some sense right so you want you want to
know that okay this happened because of this reason and this is how I fix it okay and the other thing that you know
society that is hard to do with purely based learning algorithms on the other
hand you can let me actually skip that in the interest of time okay the reality
The reality
is the following that it is simply not
true that there are too many rules of the road in fact any 16 year old in the
states can go to the DMV get the booklet study the booklet do a written test and
be given a learner's permit okay and actually this is what we require of
every single licensed driver in the United States okay we don't say just
drive with your dad or mom for a few thousand miles and that will give you the license no we ask them you know show
me that you study the rules and you understand the rules okay so how many
are the rules of the road actually went to an exercise of counting okay and what
they did I can do like a cluster them so essentially you have rules on who can
drive when and where what can be driven whenever you know at what speed in what
direction who yields to whom right how you use your signals active signalling how do
you interpret the signals that you see on the road right and where you can park
away you can stop that's essentially it you know this is this these are all the roads okay so not that many it's
collected twelve categories what is true is that the number of possible
combinations of rules and the instance instantiation of the rules given the
context of you know the scenario where other actors are pedestrians are and
where other cars are that is a humongous number okay
so you don't want to code you don't want to be to essentially any generative
model that gives you what is the right response to all possible combinations of
rules and instantiations of actors that is something that is just coming up
totally you know intractable you just cannot do that but the point is that not
only it is hard to code the good behavior what to do in every one of these situations I claim that is also
hard to learn the good behavior because now you have you need to have enough training data for every possible
combination of rules and instantiations good luck with that
okay on the other hand it is very easy
to assess what is a good behavior and that's why I was showing this slice on
np-hardness right so what is the problem that is np-hard the problem is np-hard
where if you have a non deterministic system that is generating a a candidate
solution then it is very easy to check whether or not that candidate is
actually a solution of your problem and that's something that you do in polynomial time okay so in a sense what
what I claim is that if you have an engine that is able to generate a very large
number of candidates and all you do is checking and then you know what you do
is checking whether or not each one of those candidates is good with respect to
the rules then that's all you need and turns out that you know the algorithms
that I worked on during my you know academic career where exactly generating that very large number in our TRC star these are
algorithms that work by generating a very large graph exploring all potential
trajectories reasonable trajectories that a robot a system that can take and then what you do is you check them for
you know whether they satisfy the rules or not you see that is very different
from giving the rules generates something that satisfies everything
rather than given a candidate check whether or not this candidate satisfies
the rules the generating the rules the generating candidates given all the all
the constraints is a combinatorial problem checking a single candidate for
compliance with a number of rules is a linear operation in the number of rules so that's something that you can do very
easily okay and then essentially what we have in our cars today we are using
Formal methods
these formal methods okay so essentially we write down all the rules in a in a
formal language you know so you know very precise you know like your syntax and then what you can do is you can
verify whether your trajectories satisfy all these rules written in this language
that is automatically that can be automatically translated into something look like a finite state machine by
computer okay but there's not something that you do by hand it's something that is done automatically and then what
Automatic trajectories
happens is that what we have is we generate trajectories these trajectories are you know you can think of these as
trajectories that now are not all the trajectories in the physical space and time but are also trajectories evolving
in this logical space telling me whether or not and to what extent I am satisfying the rules okay and that's all
there is okay so this is um you know for example
Automatic parking
regular little example so you know initially what we are doing is work so
this was very early days on Deuteronomy where we're still working on a research project with industry with customers so
our customer in this case wanted us to do an automated parking application and then what you see on the left is our
planner eager planet that is just trying to to park the car right avoiding hitting other cars but you see is kind
of ignoring the fact that you have lanes and direction of travels right so you're putting the rules and what you see is
what is on the on the right where now what the car is doing is not only finding the trajectory to go park but it
does so obeying all the rules that are imposed on that particular parking structure okay something that is very
Hierarchy of rules
important and you know this is something that we as humans do every day is to deal with infeasibility okay so very
often you're doing your planning you're trying to plan your trajectory you have
a number of constraints and well sorry but turns out that there is no trajectory there's no possible behavior
that you can do that will satisfy all the rules so what do you do the computer
time sorry does not compute unfeasible still driving this car I need to do
something right so you do need a way of dealing with infeasibility the way that we approach this problem is
being having this idea of hierarchy of rules okay and my claim is that all
bodies of rules generated by humans are actually organized hierarchically typical example is the Three Laws of
Robotics by asana right so the first law of robotics is a robot will not harm a human right or cause a human to come to
our second law is a robot will obey a human orders by a human a human unless
they violate the first law and the third law is a robot will try to preserve its own life or preserve itself unless it
violates the first two laws right same thing in in when you drive right so
there are some rules that are more important than others right so for example do not hit people do not hit other cars and then lower priority level
is to be driving your lane the lower priority level is maybe maintaining the speeding or something like that okay and
then what we do is come up with now we have this product graph of trajectories
Total order
in the physical and logical space on top of that we can give them a cost right what we need is a essentially a total
order what we use a lesser graphic or drink okay when we have violating an
important rule even by a tiny amount is much worse than violating a less important rule by a large amount okay so
that gives a total order structure for the cars and then essentially what we do is we solve a shortest path problem on
this graph okay which is exactly what you do the robot is one on one when you
try to do you know do any kind of motion planning okay and well you know this is
Other lane
in a collection of a few interesting things so here we need to go to the other Lane but you see that there is the
other vehicle coming so technically we could not go to the other Lane but you see that you know as long as it is safe
to do so the car will go into the other Lane okay you know and again you have
like a lot of you know like a difficult situations that the car was able to handle by itself without any scripting
or without any like a special instruction for that particular case okay so what is
The problem
here the problem here is that okay so you can do all of this right and but
then you know assuming that everybody is running this minimum violation planning you know everything will be okay the
problem is that humans introduce a lot of uncertainty in the whole thing okay
The fundamental norm
now you can think of disease asking the question so when I was young in a if that is two years ago I thought that I
take all the rules of the road and you convert them to this formal language you put them in your software and you're
done and then and then you go and look at these rules of the road and then you see that they are a mess
okay these rules are just not the sound theory in the sense that not complete do not cover every possible case and are
not consistent you know they're kind of like tell you to do different things in different cases my my prefer my favorite
rule is this one is actually called the fundamental norm in these with roots of the road look at that all road users
must behave in such a way not to post an obstacle of danger to other road users that behave according to the rules do
you see a problem there okay that doesn't mean that if I see somebody who is violating the rule I can just hit
them right so you can imagine that you have a fleet of vigilantes you know autonomous cars that just go around and
if you run the red light I'm gonna kill you right I mean technically they you
know the autonomous cars will be you know will be right right so the other the other guy would be you know they
want to blame right well you know do we really want that probably not right in
the fence of the Swiss they actually have you know that rule continuous a special care must be exerted in case do
you have evidence that other people are not following the rules but still doesn't tell you what you're supposed to
do when somebody else is violating the rule okay and you have totally problems
right so probably you have heard you know you hear about all these trolley problems to no end right and most of
these are fine you know I mean truly stupid you know in the sense is like a big waste of time in
the sense that yeah sorry I think it's extremely unlikely that you will be
given the choice of killing either Mother Teresa Hitler right so I mean for sure that
will never happen right but you know anything remotely similar will never happen to you on the other hand there
are versions of the trolley problem which are actually meaningful okay so this is one that you know my collaborator and their agency came up
with okay look at this case so you're driving down the road and you see a pedestrian that is jaywalking in front
of you okay if we stay the our current
course we will kill the pedestrian before reading one okay but it's not our
fault okay it's his fault panting Oh her his or her fault that they stepped in the road when they shouldn't have on the
other hand what we could do is we can try to swerve right but then with some probability P we may kill another person
who had nothing to do with this thing you know they were just walking around you know peacefully right so the reason
why I like this is because this problem actually has clear solutions in there to
extreme cases right so if P is one okay in the sense that if this word will kill
somebody else then we clearly kill the guy who was jaywalking right if P is
zero that is I'm sure that I'm not killing anybody if I swerve then clearly I will is worth what is the boundary so
I know that the solution exists for P is equal zero and all the solution exists with P equal one by some continuity
argument if you know I must have some value of P at which the solution changes
what is that value nobody knows how do
you evaluate that P nobody knows but you know these are the kind of question that
we actually need to answer somehow so it's a more you know a little bit more
sophisticated case now what we and you know this is what happens every day in our cars right so when the our computer
vision system is telling me that there is a pedestrian in front of us it's not telling me that there is a pedestrian for sure right so it's telling me that I
think that there is a pedestrian in front of us and you know I'm you know eighty percent confident
you know some probability Q okay now a wall combination of probability on the
pedestrian actually been there and my probability of killing somebody else would as well right so because if I is
worth and killed somebody because just a ghost you know like a false positive he'll be in serious
trouble right so how do we explain that well I thought there was someone in front of me was nobody there right so again you know you do have
solutions for some extreme cases but then you have this whole two-dimensional domain now which you had to you know
there would be a boundary where do you put the boundary okay and this is some something that somebody will need to
answer okay I I don't think it should be me you know of course I can't come up with an answer
when I write my code but I actually think it should be you right in the sense this should be the a community
effort in which the community agrees on how the car should behave or you know in
these kind of situations so let me conclude by saying you know when people ask me what do you think is the biggest
challenge in autonomous vehicles and something that I've come to realize only recently is that I think that the
biggest challenge in the development of autonomous vehicle technology is that we do not understand in a very precise way
rigorous way how we want vehicles in general including human driven vehicles
to behave okay a lot of these rules of the road are just like a giant pile of I
wouldn't say garbage but almost you know it's a it's very uncertain language very you know no rigorous laws rather rules
for example a lot of the rules are predicated on a concept of right away
you know I looked everywhere there is not a single definition of what right
away means in mathematical terms I know that he has something to do with distance as something to do the relative
speed maybe with absolute speed but know what are the values I don't know
what are the numbers if I had to write a function so if you see this car approaching and this car is farther away
than this distance and the relative speed is more than this then stop otherwise go there's nobody who is
telling me what that relationship should be anything again what we need is we
need to develop a sound theory for these rules of the road ok that cover precisely any kind of situation and
tells me you know any kind situation what is the right behavior what is the wrong behavior or little bit more maybe
what is if behavior hey if you have two behaviors which one is better okay I
need to be able to better do the comparison now we can use formal methods
at window there is a lot of room here for statistical or learning based methods you know like look at look at
what people actually do when what you know at what point will people funkateers right rather than in the
field at your cutting them off versus you know they feel that they a little done okay so we need to develop this
sound theory we need to be assessed the behaviors on realized space and time
trajectories what you thought that you had seen that doesn't matter okay oh you
know because if you say well if I if I didn't see the pedestrian that is not my fault that I hidden well then people
will start removing sensors right so if you don't see anything you can it hit anything you want you're not to blame
right but I really think that you know
the compliance of the rules once we have this precise rigorous rules will actually derive a lot of requirements
for the sensing perception system for the planning control system okay so from
my point of view the main message today is what I think is the biggest challenge
is that we don't know how precisely how we want human driven vehicles to to
behave okay once we answer that question I think that also designing automated
vehicles will be much much easier okay so let me stop here okay so I'm just
giving you know a few references so some of our you know published work you know on these topics and you know let me just
conclude you know okay so this is you know the company what we are trying to do allow me you know you're also hiding
so anybody's interested you know feel free to you know send me an email you
know contact us we want to double our size in the next couple of years so we're hanging having a couple of hundred
people okay thank you for your thank you

----------

-----

--33--

-----
Date: 2018.03.02
Link: [# Stephen Wolfram: Computational Universe | MIT 6.S099: Artificial General Intelligence (AGI)](https://www.youtube.com/watch?v=P7kX7BuHSFI)
Transcription:

welcome back to success $0.99 artificial general intelligence today we have
Stephen Wolfram Wow
that's the first I didn't even get started you're already clapping in his
book a new kind of science he has explored and revealed the power beauty and complexity of cellular automata as
simple computational systems for which incredible complexity can emerge it's
actually one of the books that really inspired me to get into artificial intelligence he's created the Wolfram Alpha competition knowledge engine
created Mathematica that has now expanded to become Wolfram language both he and his son were involved in helping
analyze create the alien language from the movie arrival of which they use the
Wolfram language please again gives Steven a warm welcome boy so I gather
the brief here is to talk about how artificial general intelligence is going to be achieved is that they set the
basic picture so I maybe I'm reminded of kind of a storage I don't think I've ever told in public but that something
that happened just a few buildings over from here so this was 2009 and Wolfram Alpha was was about to arrive on the
scene I assume most of you have used wolf now for a scene wolf alpha yes the
how many of you've used wolf alpha ok that's good so I had long been a friend
of Marvin Minsky's and Marvin was a sort of pioneer of the AI world and I kind of
seen for years you know question answering systems that tried to do sort
of general intelligence question answering and so at Marvin and so I was going to show Marvin you know Wolfram
Alpha he looks at it and he's like okay that's fine whatever said no Marvin this
time it actually works you can try real questions this is actually something useful this is not
just a toy and it was kind of interesting to see it took took about five minutes for Marvin to realize that
this was finally a to an answering system that could actually answer questions that were useful to people and so one question is
how did we how do we achieve that so you know you go to Wolf's malphur and you can ask it I mean it's I don't know what
we can ask it I don't know what's the some random question what is the
population of Cambridge actually here's a question / let's try that what's the
population of Cambridge is probably going to figure out that we mean Cambridge Massachusetts it's going to give us some number it's gonna give us
some plot actually what I want to know is number of students at MIT divided by
population of Cambridge see if it can figure that out and okay it's kind of
interesting right oh no that's / ah that's interesting a guest that we were talking about Cambridge University as
the as the denominator there so it says the number of students at MIT divided by the number of students at Cambridge
University that's interesting I'm actually surprised let's see what happens if I say Cambridge MA there now
as it probably fail horribly no that's that's good okay so no that's
interesting that's a plot as a function of time of the fraction of the of okay
so anyway so I'm glad it works the so
one one question is how did we manage to get so that many things have to work in order to get stuff like this to work you
have to be able to understand the natural language you have to have that data sources you have to be able to compute things from the data and so on
one of the things that was a surprise to me was in terms of natural language understanding was the critical thing
turned out to be just knowing a lot of stuff the actual pausing of the natural language is kind of I think it's kind of
clever and we use a bunch of ideas that came from my new kind of science project and so on but I think the most important
thing is just knowing a lot of stuff about the world is is really important to actually being able to to understand
natural language in a useful situation I think the other thing is having
actually having access to lots of data let me show you a typical example here of what is needed so I asked about the
ISS and hopefully it'll wake up and tell us something here come on what's going on here there we go okay so it figured
out that we probably are talking about a spacecraft not a file format and now it's going to give us a plot that shows
us where the ISS is right now so to make this work we obviously have to have some
feed of you know radar tracking data about satellites and so on which we have
for every satellite that's that's out there but then that's not good enough to just have that feed then you also have
to be able to do celestial mechanics to work out well where is the ISS actually right now based on the orbital elements
that have been deduced from radar and then if we want to know things like okay when is it going to it's not currently
visible from Boston Massachusetts it will next rise at 7:30 6:00 p.m. on
Monday on today so you know this requires a mixture of data about what's
going on in the world together with models about how the world is supposed to work being able to predict things and
so on and I think another thing that kind of realized about about AI and so
on from the wolfman alpha effort has been that you know one of the earlier
ideas for how one would achieve AI was let's make it work kind of like brains do and let's make it figure stuff out
and so if it has to do physics let's have it do physics by pure reasoning like you know people at least used to do
physics but in the last 300 years we've had a different way to do physics that wasn't sort of based on natural
philosophy it was instead based on things like mathematics and so one of the things that we were doing in in
Wolfman alpha was to kind of cheat relative to what had been done in previous AI systems which was instead of
using kind of reasoning type methods we're just saying okay we want to compute where the ISS is going to be well we've got a bunch of equations of
motion that corresponds to differential equations we're just going to solve the equations of motion and get an answer that's kind of leveraging the last 300
years or so of of exact science that have been done rather than trying to make use of kind
of human reasoning ideas and I might might say that in terms of the the
history of the wolf malphur project when I was a kid a disgustingly a long time
ago I was interested in AI kinds of things and I in fact I was kind of upset
recently to find a bunch of stuff I did when I was 12 years old kind of trying to assemble a pre version of Wolfram
Alpha way back before it was technologically possible but it's also a reminder that one just does the same
thing once whole life so to speak at some level um but what happened was when
when I am I started off working mainly in physics and then I got involved in
building computer systems to do things like mathematical computation and so on and I then sort of got interested in
okay so can we generalize this stuff and can we can we really make systems that
can answer sort of arbitrary questions about the world and for example sort of the the the the promise would be if
there's something that is systematically known in our civilization make it automatic to answer questions on the
basis of that systematic knowledge and back in the in around late 1970s early
1980s my conclusion was if you want to do something like that the only realistic path to being able to do it
was to build something much like a brain and so I got interested in neural nets and I tried to do things with neural
nets back in 1980 and nothing very interesting happened well I couldn't get him to do anything very interesting and
that um so I kind of had the idea that that the only way to get the kind of
thing that now exists in alpha for example was to build a brain like thing and then many years later for reasons I
can explain I kind of came back to this and realized actually it wasn't true that you had to build a brain like
things sort of mere computation was sufficient and that was kind of what got me started actually trying to build
Wolfram Alpha when we started building wolf malphur one of these I did was go to a sort of a field trip to a big
reference library and you know you see all these shelves of books and so on and the question is can we take all of this
knowledge that exists in all of these books and actually automate being able to answer questions on the base
Javad and I think we've pretty much done that for that at least the books you find in a typical reference library so
that was it looked kind of daunting at the beginning because it's this there's a lot of knowledge and information out
there but actually it turns out there are a few thousand domains and we've steadily gone through and worked on
these different domains another feature of the worth mouthful project was that we didn't really you know I've been
involved a lot in doing basic science and in trying to have sort of grand theories of the world one of my principles in building Wolfram Alpha was
not to start from a grand theory of the world that is not to kind of start from some global ontology of the world and
then try and build down into all these different domains but instead to work up from having you know hundreds then
thousands of domains that actually work whether they're you know information about cars or information about sports
or information about movies or whatever else how each of these domains sort of
building up from the bottom in each of these domains and then finding that there were common themes in these domains that we could then build into
frameworks and then sort of construct the whole system on the basis of that and that's kind of that's kind of how
its worked and I can talk about some of the actual frameworks that we end up using and so on but maybe I should
explain a little bit more so so one question is how does how does Wolf's
mouth actually sort of work inside and the answer is it's a big program it's
about it's the core system is about 15 million lines of Wolfram language code and it's some number of terabytes of raw
data and so the the way the thing that
sort of made building wolf now for possible was this language wolf and language which started with Mathematica
which came out in 1988 and has been sort of progressively growing since then so
maybe I should show you some things about both language and and you know it's easy you can you know use this mit
has a site license for it you can use it all over the places you can find it on the web but cetera etc etc but okay
the basics work the let's let's start off with something like let's make a
random graph and let's say we have a random graph with two hundred nodes
400 vertices okay so there's a random graph a first important thing about wolfing language is it's a symbolic
language so I can just pick up this graph and I could say you know I don't do some analysis of this graph that
graph is just a symbolic thing that I can just do computations on oh I could say let's let's get a another good thing
to always do is get a current image see there we go and now I could go and say
something like let's let's do some basic thing let's say let's edgy detect that
image again this this image is just a a thing that we can manipulate we could
take the image we could make it I don't know we could take the image and
partition it little pieces do computations on that I don't know simple let's do let's just say sort each row of
the image assemble the image again whoops assemble that image again we'll
get some some mixed up picture there if I wanted to I could for example let's say let's make that the current image
and let's say make that dynamic now I can be just running that code hopefully
and little loop and there we can make that work so the you know one one
general point here is there's you know this is just an image for us is just a
piece of data like anything else if we just have a variable a thing called X it just says okay that's X I don't need to
know particular value it's just a symbolic thing the corresponds to that's
a thing called X now you know what gets interesting when you have a symbolic
language and so on is we're interested in having it represent stuff about the world as well as just abstract kinds of
things that many you know I can abstractly say you know find some funky integral I don't know what you know
that's then representing using symbolic variables to represent algebraic kinds
of things but I could also just say I don't know something like Boston and Boston is another kind of symbolic thing
that has if I say what what is it really inside that's it's the
today a City Boston Massachusetts United States actually noticed when I type that
in I was using natural language to type it in and it gave me a bunch of disambiguation here it said assuming
Boston is a city assuming Boston Massachusetts use Boston New York or okay there's let's use let's use Boston
and the Philippines which I've never heard of but but um let's try using that instead and now if I look at that it'll
say it's Boston in some province of the Philippines etc etc now I might ask it
of that I could say something like what's the population of that and it um
okay it's a fairly small place or I could say for example let me let me do this let me say a geo list plot from
that Boston let's take from that Boston - and now
let's type in Boston again and now let's have it used the default meaning of the word of Boston and then let's join those
up and now this should plot this should show me a plot there we go okay so
there's the path from the Boston that we picked in the Philippines to the Boston
here oh we could ask you don't know I could just say I could ask it the distance from one to another or
something like that so the the one of the things here one things we found
really really useful actually in language was first of all there's a way of representing stuff about the world
like cities for example or let's say I want to say let's let's do this let's say let's do something with cities let's
say capital cities in South America okay so notice this is a piece of natural language this will get interpreted into
something which is precise symbolic wolfram language code that we can then
compute with and that will give us the citizens out the capital cities in South America I could for example let's say I
say find shortest to US and I'm going to use some some oops no I don't want to do
that what I want to do first is to say show me the geo positions of all those
cities on line 21 there so now it will find the geo positions and now it will say compute the shortest tour
so that's saying there's a 10,000 mile traveling salesman tour around those
cities so I could take those cities were on line 21 and I could say order the cities according to this and then I
could make another geo list plot of that join it up and this should now show us a
traveling salesman tour of the of the capital cities in South America um so
you know it's it's sort of interesting to see what's involved in making stuff like this work the one of you know my my
goal has been to sort of automate as much as possible about things that have
to be computed and that means knowing as many algorithms as possible and also
knowing as much data about the world as possible and I kind of view this as sort of a knowledge-based programming
approach where you have you know a typical kind of idea in programming languages is you know you have some
small programming languages has a few primitives that are pretty much tied into what a machine can intrinsically do
and then maybe you'll have libraries that add on to that and so on my kind of crazy idea of many many years
ago has been to build an integrated system where all of the stuff about
different domains of knowledge and so on are all just built into the system and and designed in a coherent way I mean
this has been kind of the story of my life for the last thirty years is trying to keep the design of the system coherent even as one adds all sorts of
different areas of of capability so as
some I mean we can go and dive into all sorts of different kinds of things here but maybe as an example well let's do
what could we do here we could take come let's try how about this is that a bone
I think so that's a bone so let's try that as a mesh region see if that works
so this will now use a completely different domain of human endeavor okay
oops there's two of those bones let's try let's just try them let's try
humorous let's try the that the mesh region for that and now we should have a
bone here okay there's a there's a representation of a bone let's take that bone and we could for example say let's
take the surface area of that as in some some units or I could let's do some much
more outrageous thing let's say we take region distance so we're going to take
the distance from some from that bone to a point let's say 0 0 Z and let's make a
plot of that distance with Z going from let's say I don't have no idea where the
where the spawn is but let's try something like this so that was really boring um let's try them so what this is
doing again a whole bunch of stuff has to work in order for this to operate this has to be this is a this is some
region in 3d space that's represented by some mesh you have to compute you know do the computational geometry to figure
out where it is if I want it to let's try anatomy anatomy plot 3d and let's
say something like left hand for example and now it's going to show us probably
the complete data that it has about the geometry of the left hand there we go ok
so there's there's the results and we could take that apart and start computing things from it and so on so
what um so this this is some so there's
a there's a lot of kind of computational knowledge that's built in here one let's
talk a little bit about kind of the modern machine learning story so for instance if I say let's get a picture
here let's say um let's let's just say picture of symbol got a favorite kind of
animal what's Panda okay so let's try ok
giant panda okay okay there's a panda let's see what now let's try saying um let's try for
this panda let's try saying image identify and now here we'll be embarrassed probably but let's just see
let's see what happens if I say image identify that and now it'll hopefully
wake up wake up wake up this only takes a few hundred milliseconds okay very good giant panda let's let's
see what it's we'll see what the runners-up were to the giant panda let's
say we want to say the ten runners-up in all categories for that thing okay so a
giant panda a prop here Ned which I've never heard of are pandas carniverous
ate bamboo shoots okay so that was so lucky I didn't get that one it's really
sure it's a mammal and it's absolutely certain it's a vertebrate okay so you
might ask how did it figure this out and so then you can kind of look under the hood and say so we have a whole
framework for representing neural nets symbolically and so this is the actual model that it's using to do this so this
is a so there's a neural net and it's got we can drill down and we can see there's there's a piece of the neuron
that we can drill down even further to one of these and we can probably see what that's a batch normalization layer
somewhere deep deep inside the entrails of the not panda but of this thing okay
so now let's take that object which is just a symbolic object and let's feed it the picture of the Panda and we can see
and there oops I was not giving it the
right thing what did I just do wrong here okay let's let's take our isolated okay let's take this thing and feed it
the picture of the Panda and it says a giant panda okay how about we do something more outrageous let's take
that neuron that and let's only use the first let's say 10 layers of the neuron that so let's just take out 10 layers of
the neuron that's and feed it the Panda and now what we'll get is something from the insides of the neuron that and I
could say for example let's just make those into images okay so that's what that's what the neuron that had figured out about the Panda after 10
layers of going through the neuron that and maybe actually be interesting to see let's do a feature space plots and now
we're going to of those intermediate things in the sort of in the brain of
the neuron that sort of speak this is now taking so what this is just doing is to do dimension reduction on this space
of images and so it's not very exciting it's probably mostly distinguishing these by total gray level but that's
kind of showing us the space of of different ton of different sort of features of the insides of the Shinra on
that so it's also what's interesting to see here is things like the symbolic representation of the neuron that's and
if you if you're wondering how does that hatch will work inside it's underneath it's using a max net which we happen to have contributed to a lot and there's
sort of a bunch of symbolic layers on top of that that feed into that and maybe I can show you here let me show
you how you would train one of these neural nets that's also kind of fun so we have a data repository that has all
sorts of useful data one piece of data it has is a bunch of neuron that training sets so this is a standard emne
straining set of handwritten digits okay so there's m missed and you notice
that these things here that's just an image which i could copy out and i could do you know let's say I could do color
negate on that image because it's just an image and there's there's the results and so on and now I could say let's take
let's take a neuron that like let's take a simple neuron that like Linette for example okay so let's take Linette and
then let's take the untrained initial evaluation Network so this is now a
version of Linette simple standard neural nets that didn't get trained so for example if I if I take that that
symbolic representation of Lynette and I could say net initialize then it will
take that and it'll just put random weights into Lynette okay so if I take those random weights and I feed it a
zero here I feed it that image of a zero it will presumably produce something completely random in this particular
case - right so now now what I would like to do is to take this so that was
just randomly initializing the weights so now what I'd like to do is to take the emne straining set and I'd like to
actually train Lynette using MMS training set so let's take let's take
this and let's take a random sample of let's say I don't know a thousand pieces
of Lynette come on why is it having to load it again there we go okay so
there's a there's a random sample there was on line 21 and now let me go down here and say where was it well look we
can just take this this thing here so this is the uninitialized version of
Lynette and we can say take that and then let's say net train of that with
the thing on line 21 which was that thousand instances so now what it's doing is its running training on and
that's you see the loss going down and so on it's running training for for
those thousand instances of Lynette and it will we can stop it if we want to
actually this is a new display this is very nice this is this is a new version of both languages is coming out next week which
I'm showing you but it's quite similar to what exists today but because that's
one of the features of running a software company is that you always run the the very latest version of things for better or worse and that's and this
is also a good way to debug it because supposed to come out next week if I find some horrifying bug maybe it will get
delayed but let's try them let's sum let's try this okay now it says it's
zero okay and so so this is now a trained version of Lynette trained with that with that
training data um one of the things so you know we can talk about all kinds of
details of your mats and so on but maybe I should zoom out to talk a little bit about bigger picture as I see it so one
question is sort of a question of what is in principle possible to do with
computation so you know we have as we're you know we're building all kinds of things we're making image identifies
we're figuring out those kinds of things about where the International Space Station is and so on question is what is
what is in principle possible to compute and so the you know
one of the places one can ask that question is when one looks at for example models of the natural world one
can say you know how do we make models of the natural world kind of a a traditional approach has been let's use
mathematical equations to make models of the natural world a question is if we want to kind of generalize that and say
well what are all possible ways to make models of things what can we say about that question so I spent many years of
my life trying to address that question and basically what what I've thought about a lot is that if you want to make
a model of a thing you have to have definite rules by which the thing operates what's the most general way to represent possible rules well in today's
world we think of that as a program so the next question is well what does the space of all possible programs look like
and most of the time you know we're writing programs like Wolfen language is 50 million lines of code and it's a big
complicated program that was for built for a fairly specific purpose but the question is if we just look at sort of
the space of possible programs more or less at random what's out there in the space of possible program so I got an
interest in many years ago in cellular automata which are a really good example of a very simple kind of program so let
me show you an example of one of these so this is these are the rules for a typical cellular automaton and this just
says you have a row of black and white squares and this just says you look at a black a look at a square say what color
is that square what color left or it's left and right neighbors decide what color the square will be on the next
step based on that rule okay so really simple rule so now let's let's take a look at what what actually happens if we
use that rule a bunch of times so we can take that rule the 254 is just the binary digits that correspond to those
positions in this rule so now I can say this I could say let's do 50 steps let
me do this sum and now if I run according to the rule I just defined it
turns out to be pretty trivial it's just saying if any if any square is if we
start off with a black square if any square is if any neighboring square is black make a black square so we've we've
used a very simple program we've got a very simple results out okay let's try a different program we can try
changing this we'll get some that's a program with one bit different now we
get that kind of pattern so the question is well what happens you might say okay
if you've got such a trivial program it's not surprising you're just going to get Trevor a results out so but you can
do an experiment to test that hypothesis you can just say let's take all possible programs there are 256 possible programs
that are based on these eight bits here let's just take well let's just whoops
let's just take come let's say the first 64 of those programs and let's just make
a echo let's just make a table of the
results that we get by running those first 64 programs here so here we get
the result and what you see is well most of them are pretty trivially the lake they start off with one black cell in
the middle and it just tools after one side occasionally we get something more exciting happening like here's a nice
nested pattern that we get if we were to continue it longer it would it would make you know more detailed nesting but
then my all-time favorite science discovery if you go on and just look at
these after a while you find this one here which is rule 30 in this in this
numbering scheme and that's doing something a bit more complicated you say well what's going on here you know we
just started off with this very simple rule let's see what happens maybe after a while you know if we run rule 30 long
enough it will resolve into something simpler so let's try running it let's say 500 steps and that's the whoops
that's the result we get I'd say let's
just make it fullscreen okay it's aliasing a bit on the projector
there but but you get the basic idea this is a so this just started off from one black cell at the top and this is
what it made and that's pretty weird because all this is you know this is sort of not the way it's supposed things
are supposed to work because what we have here is just that little program down there and it makes this big
complicated pattern here and you know we can see there's a certain amount of regularity on one side but for example
the center column this pattern is for all practical purposes completely random in fact it was reused as a random number generator
in Mathematica and Wolfram language for many years it was recently retired after after excellent service because we found
a somewhat more efficient one um the but the so you know what do we learn
from this what we learn from this is out in the computational universe of possible programs it's possible to get
even with very simple programs very rich complicated behavior well that's
important if you're interested in modeling the natural world because you might think that there are programs that
represent systems in nature that might work this way and so on it's also important for technology because it says
ok let's say you're trying to find a let's say you're trying to find a
program that's a good random number generator how are you going to do that well you could start thinking very hard and you could try makeup you know you
could try and write down all kinds of flowcharts about how this random number generator is going to work or you can
say forget that I'm just going to search the computational universe for possible programs and just look for one that
serves as a good random number generator in this particular case after you've searched 30 programs you'll find one
that makes a good random number generator why does it work that's a complicated story it's not a story that
I think necessarily we can really tell very well but what's important is that this is this idea that out in the
computational universe there's a lot of rich sophisticated stuff that can be
essentially mind for our technological purposes that's the important thing whether we understand how this works is
a different matter I mean it's like when we look at the natural world the physical world were used to kind of
mining things you know we started using magnets to do magnetic stuff long before we understand understood the theory of
ferromagnetism and so on and so similarly here we can sort of go out into the computational universe and find
stuff that's useful for our purposes now in fact the world of sort of deep
learning and neural nets and so on is a little bit like this it uses the trick that there's a certain degree of differentiability there so you can kind
of home in on let's try and find something that's incremental II better and for certain kinds of problems that
works pretty well I think the thing that we've done a lot I've done a lot it's just sort of exhaustive search in the computational
universe of possible programs just search of trillion programs and try and find one that does something interesting and useful for you um there's a lot of
things to say about what well actually in in these search of trillion programs and find one that's useful let me show
you another example of that um see so I was interested a while ago in the I have
to look something up here sorry um in C
in boolean algebra and in I was
interested in in the space of all possible mathematic says um and let me
just see here I I'm not finding what I
wanted to find sorry I was a good example I should have memorized this but
I haven't so um there we go there it is um so I was interested in if you just
look at so we talked about sort of looking at the space of all possible the
space of all possible programs another thing you can do is say if you're going to invent mathematics from nothing what
possible axiom systems could be used in mathematics so I was curious where do
and that again might seem like a completely crazy thing to do to just say let's just start enumerate axiom systems
at random and see if we find one that's interesting and useful but it turns out once you have this idea that out in the
computational universe or possible programs there's actually a lot of low-hanging fruit to be found it turns
out you can apply that in lots of places I mean the thing to understand is why why do we not see a lot of engineering
structures that look like this the reason is because our traditional model of engineering has been we engineer
things in a way where we where we can foresee what the outcome of our engineering steps are going to be and
when it comes to something like this we can find it out in the computational universe what we can't readily foresee
what's going to happen we can't do sort of a step by step design of this particular thing and so in
engineering and human engineering as it's been practiced so far most of it has consisted of building things where
we can foresee step by step what the outcome of our engineering going to be and we see that in programs we see that
in other kinds of engineering structures and so there's sort of a different kind of engineering which is about mining the
computational universe of possible programs and it's worth realizing there's a lot more that can be done a
lot more efficiently by mining the computational universe of possible programs than by just constructing
things step by step as a human so for example if you look for optimal algorithms for things like I don't know
even something like sorting networks the optimal sorting networks look very complicated they're not things that you
would construct by sort of step-by-step thinking about things with in a kind of
in a kind of typical human way and so this this idea you know if you're really
going to have computation work efficiently you are going to end up with these programs that are sort of just
mined from the computational universe and one of the issues with mining things so they're there this makes use of
computation much more efficiently than a typical thing that we might construct now one feature of this is it's hard to
understand what's going on and there's actually a fundamental reason for that which is in our efforts to sort of
understand what's going on we get to use our brains our computers our mathematics or whatever and our goal is this this
particular little program did a certain amount of computation to work out this pattern the question is can we kind of
outrun that computation and say oh I can tell that actually this particular bit down here is going to be a black black
bit you don't have to go and do all that computation but it turns out that then
again this will maybe as a digression which which there's this phenomenon I call computational irreducibility which
i think is really common and it's a consequence of this thing I call principle of computational equivalence and that the principle of computational
equivalence basically says as soon as you have a system whose behavior isn't fairly easy to analyze the chances are
that the computation it's doing is essentially as sophisticated as it could be and that has consequences like it
implies that the typical thing like this will correspond to a universal computer that you can use to program
anything it also has the consequence of this computational irreducibility phenomenon that says you can't expect
our brains to be able to outrun the computations that are going on inside the system if there was computational
reducibility then we can expect that this thing went to a lot of trouble and did a million steps of evolution but
actually just by using our brains we can jump ahead and see what the answer will be computational irreducibility suggests
that isn't the case if we're going to make the most efficient use of computational resources we will
inevitably run into computational irreducibility all over the place it has the consequence that we get the
situation where we can't readily sort of foresee and understand what's going to happen so back to mathematics for a
second so this is just an axiom system that so I looked for all possible look
through sort of all possible axiom systems starting off with very really tiny ones and I asked the question what's the first axiom system that
corresponds to boolean algebra so it turns out this this thing here this tiny little thing here generates all theorems
of boolean algebra it is that it is the simplest axiom for boolean algebra now something I have to show you this
because it's a new feature you see they um if I say find equation or proof let's
say I want to prove commutativity of the NAND operation I'm going to show you something here this is going to try to
generate let's see if this works this is going to try to generate an automated
proof based on that axiom system of that result so it had 102 steps in the proof
and let's try and say let's look at for example the proof network here actually
let's look at the proof data set um now that's not what I wanted I should learn
how to use this shouldn't I um let's see
what I want is the you know proof data set there we go very good ok so this is
actually let's let's say first of all let's say the proof graph ok so this is
going to show me the how that proof was done so they're a bunch of lemmas that got
proved and from those lemmas those lemmas were combined and eventually it proved the result so let's let's take a
look at the let's take a look at what some of those llamas were okay so here's
the results so after so it goes through and these are various lemmas it's using and eventually after many pages of
nonsense it will get to the result okay each one of these some of these llamas are kind of complicated there that's
that's that llama it's a pretty complicated lemma etc etcetera etcetera so you might ask what on earth is going
on here and the answer is so I first generated a version of this proof 20 years ago and I tried to understand what
was going on and I completely failed and it's sort of embarrassing because this is supposed to be a proof it's supposed
to be you know demonstrating some results and what we realize is that you know what does it mean to have a proof
of something what does it mean to explain how a thing is done you know what is the purpose of a proof purpose
of a proof is basically to let humans understand why something is true and so for example if you go to let's say we go
to wolf now fur and we do you know some random thing where we say let's do you
know an integral of something or another it will be able to very quickly in fact it will take it only milliseconds
internally to work out the answer to that integral okay but then somebody whose wants to hand in a piece of
homework or something like that needs to explain why is this true okay well we
have this handy step-by-step solution thing here which
explains why it's true now the thing I should admit about the step-by-step solution is it's completely fake that is
the steps that are described in the step by step solution have absolutely nothing to do with the way that internally that
integral was computed these are steps created purely for the purpose of telling a story to humans about why this
integral came out the way it did and now what we're seeing and so that's a so that's one thing is knowing the answer
the other thing is being able to tell a story about why the answer worked that way well what we see here is this is a proof
but it was an automatically generated proof and it's a really lousy story for us humans I mean if it turned out that
one of these theorems here was one that had been proved by Gauss or something and appeared in all the textbooks we
would be much happier because then we would start to have a kind of human representable story about what was going
on instead we just get a bunch of machine generated lemmas that we can't understand that we can't kind of wrap our brains around and it's sort of the
same thing that's going on in when we look at when these neural nets we're
seeing you know when we were looking wherever it was at the innards of that neuron that and we say well how is it
figuring out that that's a picture of a panda well the answer is it decided that you know if we humans were saying how
would you figure out if it's a picture of panda we might say well look and see if it has eyes that's a clue for whether
it's an animal look and see if it's looks like it's kind of round and furry and things that's a version of whether
it's a panda and Len cetera etcetera etcetera but what it's doing is it learnt a bunch of criteria for you know
is it a panda or is it one of 10,000 other possible things that it could have recognized and it learnt those criteria
in a way that was somehow optimal based on the training that it got and so on but it learnt things which were
distinctions which are different from the distinctions that we humans make in the language that we as humans use and
so in some sense you know when we start talking about will describe a picture we
have a certain human language for describing that picture we have you know in our human in typical human languages
we have maybe thirty to fifty thousand words that we use to describe things those words are words that have sort of
evolved as being useful for describing the world that we live in um when it comes to there's known that
it could be using it could say well that the words that it is effectively learnt
which allow it to make distinctions about what's going on in the in the analysis that it's doing it has
effectively invented words that describe distinctions but those words have nothing to do with our historically
invented words that exist in our languages so it's kind of an interesting situation that that it is its way of
thinking so to speak if you say well what's it thinking about how do we describe what it's thinking that's a tough thing to answer because just like
with the with the automated theorem we're we're sort of stuck having to say
well we can't really tell a human story because the things that it invented are things for which we don't even have
words in our languages and so on okay so one thing to realize is in this kind of
space of sort of all possible computations there's a lot of stuff out there that can be done there's this kind
of ocean of sophisticated computation and then the question that we have to
ask for us humans is okay how do we make use of all of that stuff so what we've
got kind of on the one hand is we've got the things we know how to think about human language is our way of describing
things our way of talking about stuff that's the one one set of things the other set of things we have is this very
powerful kind of seething ocean of computation on the other side where lots of things can happen so the question is
how do we make use of this sort of ocean of computation in the best possible way for our human purposes and building
technology and so on and so the the way I see you know my kind of part of what
I've spent a very long time doing is kind of building a language that allows us to take human thinking on the one
hand and describe and sort of provide a sort of computational communication
language that allows us to get the benefit of what's possible over in the sort of ocean of computation in a way
that's rooted in what we humans actually want to do and so I kind of view both
from language as being sort of an attempt to make a bridge between so you on the one hand there's all possible
computations on the other hand there's things we think we want to do and I view
or from language as being my best attempt right now to make a way to take
our sort of human computational thinking and be able to actually implement it so
in a sense it's a language which works in two on two sides it's both a language where you as a as a the machine can
understand okay it's it's looking at this and that's what it's going to compute but on the other hand it's also a language for us humans to think about
things in computational terms so you know if I go and I don't know one of these one of these things that I'm doing
here whatever it is that this wasn't that exciting but but you know fine shortest tour of the Geo position of the
capital cities in South America that is a language that's a representation in a precise language of something and the
idea is that that's a language which we humans can find useful in thinking about
things in computational terms it also happens to be a language that the machine can immediately understand and
execute and so I think this is sort of a general you know when I think about AI in general the you know what is the sort
of what's the overall problem well part of the overall problem is so how do we tell the AI is what to do so to speak
there's this very powerful you know this sort of ocean of computation is what we get to mine for purposes of building AI
kinds of things but then the question is how do we tell the AI is what to do and the what I see what I've tried to do
with Wolfram language is to provide a a way of kind of accessing that
computation and sort of making use of the knowledge that our civilization has
accumulated and because that's the you know there's the general computation on
on this side and there's the specific things that we humans have thought about and the question is to make use of the
things that we've thought about to do do things that we care about doing actually if you're interested in these kinds of
things I happen to just write a blog post where last couple of days ago it's
kind the funny blog posts it's about some but you can see the title there it came because a friend of miners has this
crazy project to put little little sort of discs or something that should
represent kind of the best achievements of human civilization so to speak to send out it's it's hitchhiking on
various spacecraft that are going out into the solar system in the next little
while and the question is what to put on this little disc that kind of represents you know the achievements of
civilization it's kind of it's kind of depressing when you go back and you look at what some what people have tried to
do on this before and realizing how hard it is to tell even whether something is an artifact or not but this is this was
sort of a yeah that's a good one that's from 11,000 years ago can you the question is can you figure out what on earth it is and what it means and and
this is but but so what what's relevant about this is the this this whole
question of there are things that are out there in the computational universe and you know when we think about
extraterrestrial intelligence I find it kind of interesting that artificial intelligence is our first example of an
alien intelligence we don't happen to have found what we view as extraterrestrial intelligence right now
but we are in the process of building pretty decent version of an alien intelligence here and the question is if
you ask questions like well you know what is it thinking is it does it have a
purpose and what it's doing and so on and you're confronted with things like this it's very we you can kind of do a
test run of you know what's what's its purpose what is it trying to do in a way
that is very similar to the kinds of questions you would ask about about extraterrestrial intelligence but in
case the the that the main point is that I see this sort of ocean of computation
there's the let's describe what we actually want to do with that ocean of computation and that's where you know
that's one of the primary problems we have now people talk about you know AI and what is AI going to allow us to automate and my basic answer that would
be we'll be able to automate everything that we can describe the problem is
it's not clear what we can describe or put another way you know you imagine various jobs and people are doing things
they're repeated judgment jobs things like this there where we can readily automate those things but the thing that
we can't really automate is saying well what are we trying to do that is what are our goals because in a sense when
when we see one of these systems you know let's say let's say it's a cellular tartan here okay the question is what is
this cellular automaton trying to do maybe I can maybe I'll give you another cellular automaton that is a little bit
more exciting here let's do this one so that the the question is what is this
cellular automaton trying to do you know it's got this whole big structure here and things are happening with it we can
go we can run it for a couple thousand steps we can ask it's a nice example of kind of undecidability in action what's
going to happen here this is kind of the halting problem is this going to halt what's it going to do there's computational irreducibility so
we actually can't tell this is the case where we know this is a universal computer in fact eventually well I don't
even spoil it for you if I went on long enough it would it would go into some kind of cycle but um we can ask what is
this thing trying to do what is it you know is it what's it thinking about what's its um you know what's its goal what's its
purpose and you know we get very quickly in a big mess thinking about those kinds of things I've one of the things that
comes out of this principle of computational equivalence is thinking about what kinds of things have are
capable of sophisticated computation so so I mentioned a while back here sort of
my personal history with Wolff malphur of having thought about doing something like wolf now for when I was a kid and then believing that you sort of had to
build a brain to make that possible and so on and one of the things that I then thought was that there was some kind of
bright line between what is intelligent and what is merely computational so to
speak in other words that there was something which is like oh we've got this great thing that we humans have that you know as intelligence and all
these things in nature and so on and all the stuff that's going on it's just computation or it's just you know things
operating according to rules that's different there's some bright line distinction doing these things well I think the
thing that came about after I'd looked at all these cellular automata and all kinds of other things like that
is I sort of came up with this principle of computational equivalence idea which
we've now got quite a lot of evidence for which I talked about people are interested in but that basically there
isn't a that once you reach a certain level of of computational sophistication
everything is equivalent and that means that that implies that there really isn't a bright line distinction between
for example the computations going on in our brains and the computations going on in these simple cellular automata and so
on and that essentially philosophical point is what actually got me to start trying to build both malphur because I
realized that gosh you know I've been looking for this sort of the the magic bullet of intelligence and I just
decided probably there isn't one and actually it's all just computation and so that means we can actually
impractical intelligent like thing and so that's
what I think is the case is that there really isn't sort of a bright line distinction and that has that has more
extreme consequences like people will say things like you know the weather has a mind of its own okay
sounds kind of silly sounds kind of animistic primitive and so on but in fact the you know fluid dynamics of the
weather is as computationally sophisticated as the stuff that goes on in our brains but we can start asking
but then you say but the weather doesn't have a purpose you know what's the purpose of the weather well you know maybe the weather is trying to equalize
the temperature between the you know the the North Pole and the tropics or something and then we have to say well
but that's not a purpose in the way that we think about purposes that's just you know and we get very confused and in the
end what we realize is when we're talking about things like purposes we have to have this kind of chain of
provenance that goes back to humans and human history and all that kind of thing and I think it's the same type of thing
when we talk about computation and AI and so on the thing that we this question of sort of purpose goals things
like this that's a thing which is intrinsically human and not something that we can ever sort of automatically
generate it makes no sense to talk about automatically generating it because these computational systems they do all kinds of stuff you know we can
say they've got a purpose we can attribute purposes to them etcetera etcetera etcetera but you know ultimately it's sort of the
human thread of purpose that we have to have to deal with so that means for example when we talk about AIS and we
were interested in things like so how do we tell you know like like we'd like to be able to tell we talk about AI ethics
for example we'd like to be able to make a statement to the AIS like you know please be nice to us humans um and
that's a you know that's something so one of the issues there is so talking
about that kind of thing one of the issues is how are we going to make a statement like be nice to us humans
what's the you know in how are we going to explain that to an AI and this is
where again you know my my efforts to build a language a computational
communication language that bridges the world of what we humans think about and
the world of what is possible in computation is important and so one of things I've been interested in is
actually building what I call a symbolic discourse language that can be a general representation for sort of the kinds of
things that we might want to put in that we might want to to say and things like
be nice to him and so sort of a little bit background to that so you know in the modern world people are keen on
smart contracts they often think of them as being deeply tied into blockchain which I don't think is really quite
right the important thing about smart contracts is it's a way of having sort
of an agreement between parties which can be executed automatically and that
agreement may be you know you may choose to sort of anchor that agreement in a
blockchain you may not but the whole point is you have to what you you know when people write legal contracts they
write them in an approximation to English they write them in legalese typically because they're trying to write them in something a little bit
more precise than regular English but the limiting case of that is to make a symbolic discourse language in which you
can write the contract and code basically and the the I've been very
interested in using wolfmann language to do that because in wolfen language we have a language which Candice bribe things about the world and we can
talk about the kinds of things that people actually talk about in contracts and so on and we're most of the way
there to being able to do that and then when you start thinking about that you
start thinking about okay so we've got this language to describe things that we that we care about in the world and so
when it comes to things like tell the AIS to be nice to the humans we can imagine using often language to sort of
build an AI Constitution that says this is how the AI supposed to work but when we talk about sort of just the the
untethered you know the untethered AI doesn't have any particular it's just going to do what it does and if we want
it to you know if we want to somehow align it with human purposes we have to have some way to sort of talk to the AI
and that's that's a you know I view my efforts to build or from language as as
a way to do that I mean I you know as I was showing at the beginning you can use you can take natural language and with
natural language you can build up a certain amount of you can say a certain number of things in natural language you
can then say well how do we make this more precise in a precise symbolic language if you want to build up more
complicated things it gets hard to do that in natural language and so you have to kind of build up more serious
programs in in in symbolic language and I've probably been numb been yakking a
while here and I'm happy to I can talk about all kinds of different things here but that
maybe I've not seen as many reactions as I might have expected to think so I I'm not sure which things people are
interested in which they're not but so maybe I should maybe I should stop here and we can have discussion questions
comments yes [Applause] if two microphones if you have questions
please come out so I have a quick question it's goes to the earlier part of your talk where you say you don't
build a top-down ontology you actually build from the bottom up but disparate domains what do you feel are the core
technologies of the knowledge representation which you use within Wolfram Alpha that allows you you know
different domains to reason about each other to come up with solutions and is there any feeling of differentiability
for examples if you were to come up with a plan to do something new within
Wolfram Alpha language you know how would you go about doing that me okay so
we've done maybe a couple of thousand domains okay the what is actually
involved in doing one of these domains it's it's a gnarly business every domain
has some crazy different thing about it I tried to make up actually a while ago we um let me show you something a kind
of a hierarchy of what it means to make um see if I can find this here kind of a
hierarchy of what it means to make a domain computable where is it that's
okay here we go so there's sort of a hierarchy of levels of what it means to make a domain computable from just you
know you've got some you know you've got some array of data that's quite structured forget you know the separate
issue about extracting things from unstructured data but let's imagine that you were given you know a a bunch of
data about landing sites of meteorites or something okay so you go through
various levels so you know things like okay the landing sites the meteorites are the are the positions just strings
or they some kind of canonical representation of geo position is the you know is the type of meteorite you
know some of them are iron meteorites some of them are stone meteorites have you made a canonical representation have
you made some kind of way to to identify what some sorry go ahead no no I mean to
do that so my question is like you know if you did have positions as a string as well as a canonical representation do
you have redundant pieces of the same redundant representations of the same information in the different no I mean I'll go you
always everything canonical that you have yeah I have a minimal representation of everything yeah our goal is to make everything canonical now
that's you know there is a lot of complexity in doing that I mean if you you know in each okay so another feature
of these domains okay so there's another another thing to say um you know it will be lovely if one
could just automate everything and cut the humans out of the loop turns out this doesn't work and in fact
whenever we do these domains it's fairly critical to have expert humans who really understand the domain or you
simply get it wrong and it's also having said that once you've done enough domains you can do a lot of cross
checking between domains and we are the number one reporters of error and of
errors and in pretty much all standardized data sources because we can do that kind of cross checking but I
think you know if you ask the question what's involved in in bringing online a
new domain it's you know those sort of hierarchy of things you know some of
those take a few hours you can get to the point of of having you know we've got good enough tools for ingesting data
figuring out oh those are names of cities in that column let's you know that's canonicalized those you know some
may be questions but many of them will be able to to nail down and to get to the full level of you've got some
complicated domain and it's fully computable is probably a year of work and and you might say well gosh why are
you wasting you their time you've got to be able to automate that so you can probably tell we're fairly sophisticated about machine learning kinds of things
and so on and we have tried you know to automate as much as we can and we have
got a pretty efficient pipeline but if you actually want to get it right and you see it is an example of what what
happens that there's a level even going between wolf now for more from language there's a level of so for example let's
say you're looking at you know lakes in Wisconsin okay so people are querying
about lakes in Wisconsin and WolframAlpha they'll name a particular lake and they want to know you know how
big is the lake okay fine in Wolfram language they'll be doing a systematic computation about
lakes in Wisconsin so if there's a lake missing you're gonna get the wrong answer and so that's a kind of higher
level of difficulty okay but the this yeah I think you're asking some more
technical questions about ontologies and I can try and answer those actually one quick question and you know that's
there's a lot of other questions yes that's right okay all right very much my cyclist as to the left here I got a
simple question who or what are your key influences oh gosh in terms of language
design for from language are in the context of machine intelligence if you like if you want to make it tighter to this audience I don't know I've been
absorbing stuff forever I think my main in terms of language design probably
list span APL were my sort of early influences but in terms of thinking
about AI hmm you know in I mean I'm kind
of quite knowledgeable I like history of science so I'm pretty knowledgeable about the the early history of kind of
mathematical logic symbolic kinds of things I would say okay maybe I can answer that in the negative okay I have
for example in building Wolfram Alpha I thought gosh let me do my homework let
me learn all about computational linguistics let me hire some computational linguistics PhDs that will be a good way to get this started
turns out we used almost nothing from the from the previous sort of history of
computational linguistics partly because what we were trying to do namely short question natural language understanding
is different from a lot of the national language processing which has been was done in the past I also have made to my
disappointment very little use of you know people like Marvin Minsky for
example I really don't think I mean I knew Marvin for years and in fact some of his early work on simple Turing
machines and things those are probably more influential to me than his work on on AI and you know probably
I my mistake of not understanding that better but really I would say that I'd been been rather uninfluenced by by sort
of the traditional AI kinds of things I mean it probably hasn't helped that I've kind of lived through a time when when
sort of AI went from you know when I was a kid a I was gonna solve everything in the world and then you know it kind of
decayed for a while and then sort of come back so I so I would say that I can describe my negative my non influences
better than my impression you gave is that you made your own head and it sounds as though that's pretty much right yeah I mean yes I I mean insofar
as those things to me I mean look things like the you know okay so for example
studying simple programs as and trying to understand the universe of simple programs actually the personal history
of that sort of interesting I mean I you know I used to do particle physics when
I was a kid basically and then I actually got interested okay so I'll
totally the history of that just as an example of how sort of interesting is a sort of history of ideas type thing so I
was interested in in how order arises in the universe so you know you start off from the hot Big Bang and then pretty
soon you end up with a bunch of humans and galaxies and things like this how does this happen so I got on just in
that question I was also interested in in things like knowing that works for
sort of AI purposes and I thought let me make a minimal model that encompasses
sort of how complex things arise from from other stuff and I ended up sort of
making simpler and simpler and simpler models and eventually wound up with cellular automata and which I didn't know were called cellular automata when
I started looking at them and then found they didn't interesting things and the two areas were cellular automata had
been singularly unuseful in analyzing things our large-scale structure in the
universe and neural networks so turned out but but that by the way the fact
that I kind of even imagined that one could just start yeah I should say you know I've been doing physics and in
physics the kind of intellectual concept is you take the world as it is and you try and drill down and find out what you
know what makes the world out of primitives and so on it's you know reduce to find things then I
built my first computer language a thing called SMP which went the other way around where I was just like I'm just
gonna make up this computer language and you know just make up what I want the primitives to be and I'm gonna build
stuff up from it I think that the fact that I kind of had the idea of doing things like making up cellular automata
as possible models for the world was a consequence of the fact that I worked on this computer language which was a thing
which worked the opposite way around from the way that one is used to doing natural science which is sort of this reductionist approach and that's I mean
so that's just an example of it you know I found I happen to have spent a bunch
of time studying as I say history of science and one of my one of my hobbies is sort of history of ideas I even wrote
this little book called idea makers which is about biographies of a bunch of people who for one reason or another I've written about and so I'm I'm always
curious about this thing about how do people actually wind up figuring out the things they figure out and you know one
of the one of the conclusions of my you know investigations of many people is there are very rarely moments of
inspiration usually it's long multi-decade kinds of things which only
later get compressed into something short and also the path is often much
you know it's it's it's quite what can I
say that the steps are quite small and you know but the path is often kind of complicated and that's what that's what
it's been for me so I simple question complex answer sorry so when I basically
see from the Wolfram languages it's a way to describe all of objective reality it's kind of formalizing just about the
entire domain of discourse use a philosophical term and you kind of hinted at this in your lecture where
that where it sort of leaves office is that when we start to talk about more esoteric philosophical concepts purpose
I guess this would lean into things like epistemology because essentially you only have science there and as amazing
as Sciences there are other things that are talked about not you know you know like idealism versus materialism etc do
you have an idea of how Wolfram might or might not be able to branch into those
discourses because I'm hearing echoes in my head at that time bostrm said that nai needs a you know
when you give an AI a purpose there's like I think he said philosophers are divided completely evenly between the
top four ways to measure how good something should be it's like utilitarianism and sure brother for most Japanese yeah so the first thing is I
mean this problem of making what okay about 300 years ago people like light
knits we're interested in the same problem that I'm interested in which is how do you formalize sort of everyday discourse and Leibniz had the original
idea you know he was originally trained as a lawyer and he had this idea if he could only reduce all law all legal
questions to matters of logic he could have a machine that would basically describe every you know answer every
legal case right he was unfortunately a few hundred years too early even though
he did have you know he tried to he tried to do all kinds of things very similar things I've tried to do like he tried to get various Dukes to assemble
big libraries of data and stuff like this but but the point so what he tried
to do was to make a formalized representation of everyday discourse for
whatever reason for the last 300 years basically people haven't tried to do that there's it's a almost completely
barren landscape there was this period of time in the 1600s when people talked
about philosophical languages Leibniz was why and a guy called John Wilkins was another and they tried to you know
break down human thought into something symbolic people haven't done that for a long time in terms of what can we do
that with you know I've been trying to figure out what the best way to do it is
I think it's actually not as hard as one might think these areas one thing you have to understand these areas like
philosophy and so on are they're on the harder end I mean things like a good example typical example you know I want
to have a piece of chocolate okay they in morphing language right now we have a pretty good description of pieces of
chocolate we know all sorts of you know we probably know 100 different kinds of chocolate we know how big the pieces are
all that kind of thing the I want part of that sentence we can't do that right now but I don't think that's that hard
and I'm you know that's now if you ask let's say we had I think the different
thing you're saying is let's say we had the omnipotent AI so to speak that was able to you know where we turn over the
control of the central bank to the AI we turn over all these other things to the AI then the question is we say to the AI
now do the right thing and then the problem with that is and this is why I
talk about you know creating AI constitutions and so on we have absolutely no idea what do the right
thing is supposed to mean and philosophers have been arguing about that you know utilitarianism is an example of that of one of the answers to
that although it's not a complete answer by any means it's not not really an answer it's just a way of posing the question and so I think that the you
know one of one of the features of so I think it's a really hard problem to you
know you think to yourself what should the AI Constitution actually say so first thing you might think is oh
there's going to be you know something like Asimov's laws of robotics there's going to be one you know golden rule for
a eyes and if we just follow that golden rule all well okay I think that that is
absolutely impossible and in fact I think you can even sort of mathematically prove that that's impossible because I think as soon as
you have a system that you know essentially what you're trying to do is you're trying to put in constraints that
okay basically as soon as you have a system that shows computational irreducibility I think it is inevitable
that you have ace of have unintended consequences of things which means that
you never get to just say put everything in this one very nice box you always
have to say let's put in a patch here let's put in a patch there and so on a version of this much more abstract version of this of girdles theorem so
girdles theorem is you know it starts up by taking the you know it's girls
theorem is trying to talk about integers it says start off with piano's axioms
turner's axioms you might say in piano thought describe the integers and nothing but the integers okay so
anything that's provable from pianos axioms will be true about integers and vice-versa okay
what girls theorem shows is that you can that will never work that there are an infinite hierarchy of patches that you have to
put on two pianos axioms if you want to describe the integers and nothing about the integers and I think the same is
true if you want to have a legal system effectively that has no bizarre unintended consequences so I don't think
it's possible to just say you know if you when you're describing something in the world that's complicated like I
don't think it's possible to just have a small set of rules that will always do
what we want so to speak I think it's inevitable that you have to have a long essentially code of laws and that's what
you know so my guess is that what will actually have to happen is you know as we try and describe what you want the
eyes to do you know I don't know the socio-political aspects of how we'll figure out whether it's 1 AI
Constitution or 1 per you know city or whatever we can talk about that that's a
separate issue but but um you know I think what will happen is it'll be much like human laws it'll be a complicated
thing that gets progressively patched and so I think it's it's some and these ideas like you know oh we'll just make
the eyes you know run the world according to you know Mills you know
John Stuart Mill's idea it's not gonna work which is not surprising this
philosophy has has has made the point that it's not as easy it's not an easy problem for the last two thousand years and they're right it's not an easy
problem thank you yeah I you're talking about computational irreducibility and
computational equivalents and also that earlier on in your intellectual adventures you're interested in particle
physics and things like that I've heard you make the comment before in other
contexts that things like molecules compute and I was just ask you exactly
you you know what you mean by that in what sense does a molecule I mean what
would you like to compute so to speak I mean in other words you what what is the case is that you know one definition of
you're computing is given a particular computation like I don't know finding square roots or something you know you
can program a you know the surprising thing is that an awful lot of stuff can be programmed to do any
computation you want that's some and you know when it comes to I mean I think for
example when you look at nanotechnology and so on the the current you know one
of the current beliefs says to make very small computers you should take what we
know about making big computers and just you know make them smaller so to speak I
don't think that's the approach you have to use I think you can take the components that exist at the level of
molecules and say how do we assemble those components to be able to do
complicated computations I mean it's like the cellular automata that the you know the underlying rule for the
cellular automaton is very simple yet when that rule is applied many times it can do a sophisticated computation so I
think that that's the that's the sense in which what can I say the raw material
that you need for computation can be you know there's a great diversity in the raw material that you can use for
computation our particular human development you know stack of of
technologies that we use for computation right now is just one particular path and we can you know so a very practical
example of this is algorithmic drugs so the question is right now drugs pretty much work by most drugs work well you
know there is a binding site and a molecule drug fits into binding site does something question is can you
imagine having something where the molecule you know is something which has computations going on in it where it
goes around and it looks at that you know that thing it's supposed to be binding to and it figures out oh there's
this knob here and that knob there it reconfigures itself it's computing something it's trying to figure out you
know is this likely to be a tumor cell or whatever based on some more complicated thing that's the type of
thing that I mean by by computations happening at an R color scale okay I guess I meant to ask if it follows from
that if in your view like the the molecules in the chalkboard and in my
face and in the table are in any sense currently during doing computer I mean the question of what computation look
one of the things to realize if you look at kind of the sort of past and future of things the the
okay so here's an observation actually I was about light nets actually and lightning says time live Nets made a
calculator type computer out of brass took him 30 years okay so in his day
there was you know at most one computer in the world as far as he was concerned right today's world there may be 10
billion computers maybe 20 billion computers I don't know the question is what's that going to look like in the future and I think the answer is that in
time probably everything we have will be made of computers in the following sense
that basically it won't be you know in today's world things are made of you know metal plastic whatever else but
actually that won't make it there won't be any point in doing that once we know how to do you know molecular scale
manufacturing and so on we might as well just make everything out of programmable stuff and I think that's a that's a
sense in which you know the the and you know the one example we have molecular computing right now is us bio in biology
you know biology does a reasonable job of specific kinds of molecular computing it's kind of embarrassing I think that
the only you know molecule we know that sort of a memory molecule is DNA that's kind of you know which is kind of the
you know the particular biological solution in time we'll know lots of others and you know I think the the sort
of the the end point is so if you're asking is you know is computation going
on in you know in this water bottle the answer is absolutely it's probably even many aspects of that computation are
pretty sophisticated if we wanted to know what would happen to particular molecules here it's going to be hard to tell there's going to be computational
irreducibility and so on can we make use of that for our human purposes can we piggyback on that to achieve something
technological that's different issue and that's the four that we have to build up this whole sort of chain of technology
to be able to connect it which is what I've kind of been been keep on talking about is how do we connect sort of what
is possible computationally in the universe to what we humans can kind of conceptualize that we want to do in
computation and that's you know that's the bridge that we have to make and that's the hard part but getting the intrinsic getting the computation done
is is you know there's computation going on all over the place there may be a
couple more questions I was hoping you could elaborate on what you're talking about earlier of like
searching the entire space of possible programs so that's very broad so maybe
like what kind of searching of that space we're good at and like what we're not and I guess what the outright so I
mean I would say that we're at an early stage in knowing how to do that okay so I've done lots of these things and they
are the thing that I've noticed is if you do an exhaustive search then you
don't miss even things that you weren't looking for if you do a non exhaustive search there is a tremendous tendency to
miss things that you weren't looking for and so you know we've done such as a
bunch of the function evaluation and Wolfram language is done by was done by searching for optimal approximations in
some big space a bunch of stuff with hashing is done that way bunch of image processing is done that way what we're
just sort of searching this you know doing exhaustive searches and maybe trillions of programs to find things now
you know there is on the other side of that story is the incremental improvements story with with deep
learning and neural networks and so on where because there is differentiable 'ti you're able to sort of incrementally
get to a better solution now in fact people are making less and less differentiability and deep learning
neural nets and so I think eventually there's going to be sort of a grand unification of these kinds of approaches
right now we're still you know I don't really know what the you know the
exhaustive search side of things which you can use for all sorts of purposes I mean there's the reason the surprising
thing that makes you is also search not crazy is that there is rich sophisticated stuff near at hand in the
computational universe if you had to go you know quadrillions you know through a quadrillion cases before you ever found
anything the exhaustive search will be hopeless but you don't in many cases and
you know I would say that we are in a fairly primitive stage of the science of how to do those searches well my guess
is that there'll be some sort of unification which needless to say I've thought a bunch of out and between kind of than known that
so you know the trade-off typically in Iran that says you can have an Iran that that is very good at that is you know
uses its computational resources well but it's really hard to train or you can have an Iran that that doesn't use its
computational resources so well but it's very easy to train but this is very you know smoothly and you know my guess is
that somewhere in the you know harder to train but makes use of things that are
closer to the complete computational universe is is where one's going to see progress but it's it's a it's a really
interesting area and you know I consider us only at the at the beginning of
figuring that out thank you for your
talk I just to give you a bit of context for my question I research how we could teach AI to kids and evolving platforms
for that how we could teach artificial intelligence and machine learning to children and I know you develop resources for that as well so I was
wondering like where do you think it's problematic that we have computation that is very efficient and can do you
know from utilitarian and problem solving perspective it's all the goals but we don't understand how we how it
works so we have to create the this fake steps and if you could think of scenarios where that could become very
problematic over time and why do we approach it such in such a deterministic way and when you mentioned that
computation and intelligence are dafair differentiated by this like very thin line how does that affect the way you
learn and how do you think that will affect the way we kids learn we learn right so I mean look my general
principle about you know future generations and what they should learn I mean first point is you know very
obvious point that you know for every field that people study you know
archeology to zoology there either is now a computational X or there will be
soon so you know every field the paradigm of computation is becoming important
perhaps the dominant paradigm in that field okay so how do you teach kids to be useful in a world where everything is
computational I think the the number one thing is to
each them how to think in computational terms what does that mean that doesn't mean writing code necessarily I mean in
other words one of the things that's happening right now as a practical matter is you know they've been these waves of enthusiasm for teaching coding
of various kinds you know we're in a we're not actually we're in the end of an uptick wave I think it's going down
again um you know it's been up and down for 40 years or so um okay why doesn't
that work well it doesn't work because while there are people like people who are students at MIT for example for whom
they really want to learn you know engineering style coding and it really makes sense to them to learn that the
vast majority of people it's just not going to be relevant because they're not going to write a low-level C program or
something and it's the same thing that's happened in math education which has been sort of a disaster there which is
the number one takeaway for most people from the math they learn in school is I don't like math and you know that's not
for all of them obviously but that's the you know if you asked no general scale you know what people and why is that
well part of the reason is because what's been taught is rather low level of mechanical it's not about
mathematical thinking particularly it's mostly about you know what teachers can teach and what assessment processes can
assess and so on okay so how should one teach computational thinking I mean I'm I'm kind of excited about what we can do
with whorfin language because I think we have a high enough level language that people can actually write you know that
for example I I reckon by age 11 or 12 and I've done many experiments on this I
have some the only problem with my experiments is most of my experiments end up being with kids who are high
achieving kids despite many efforts to reach lower achieving kids that always ends up that the kids who actually do
the things that I set up or the high achieving kids but but you know like setting that aside you know you take the typical you know
11 12 13 year-olds and so on and they can learn how to write stuff in this
language and what's interesting is they learn to start thinking here I'll show you let's be very practical I can show
you I was doing every Sunday I do a little little thing with some middle school kids and I might even be able to find my
stuff from yesterday this is this is um okay let's see programming adventures Janerio 28 okay
let's see what I did oh look at that that was that was why I thought of the South America thing here because I just done that with these kids
the and so what are we doing we were
trying to figure out this this some trying to figure out the shortest tour
thing like that I just showed you which is this is where I got what you is is what I was doing with these kids but
this this was my version of this but the kids all had various different versions of this and we had somebody suggested
you know let's just enumerate let's just look at all possible permutations of
these these cities and figure out what their distances are there's the histogram of those that's what we get
from those okay how do you get the largest distance from those etcetera etcetera and this is okay this was my
version of it but the kids had similar stuff and this is you know this is I think and it probably went off into oh
yeah there we go there's there's the one for the whole whole earth and then they wanted to know how do you do that in 3d
so I was showing them how to convert to XYZ coordinates in 3d and make the
corresponding thing in 3d so what's this maybe isn't the this is a random example
from yesterday so it's not not a highly considered example but but um what I
think is interesting is that we seem to have finally reached the point where we've automated enough of the actual
doing of the computation that the kids can be exposed mostly to thinking about
what you might want to compute and you know part of our role in language design as far as I'm concerned is to get it as
much as possible to the point where for example you can do a bunch of natural language input you can do things which
make it as easy as possible for kids to not get mixed up in the kind of what the
you know how the computation gets done but rather to just think about how you formulate the computation so for example
a typical example I've used much times in you know what does it mean to do write code versus two other
things like a typical set of test example would be I don't know you ask somebody you're gonna there's practical
problem we had in wolf's mouth you give a lat long position on the earth and you say you're gonna make a map of that like
long position what what scale of map should you make alright so if the lat/long is in the middle of the Pacific
making a ten mile you know radius map isn't very interesting if it's in the middle of
Manhattan a 10-mile radius map might be quite quite a sensible thing to do so the question is come up with an
algorithm come up with even a way of thinking about that question what do you do you know how should you figure that
out well you might say you know oh let's look at the visual complexity of the image let's look at how far it is to
another city let's fight you know there various different things but thinking about that as a kind of computational
thinking exercise that term is you know
that's the kind of thing so in terms of what one automates and whether people whether people need to understand how it
works inside okay main point is you'll
in the end it will not be possible to know how it works inside so you might as
well stop having that be a criterion I mean that is there plenty of things that one teaches people that let's say in
lots of areas of biology medicine whatever else you know maybe we'll know how it works inside one day but you can
still there's an awful lot of useful stuff you can teach without knowing how it works inside and I think also as we
get computation to be more efficient inevitably we will be dealing with things where you don't know how it works inside now you know we've seen this in
math education because I've happened to made tools that automate a bunch of things that people do in math education
and I think well to tell a silly story I'm in my my older daughter who at some
point in the past was doing you know calculus you know and learning doing integrals and things and I was saying to
her you know I didn't think humans still did that stuff anymore which was a very
unintelligent fit but in any case I mean the the you know there's a question of
whether do humans need to know how to do that stuff or not so I haven't done an integral by hand and probably thirty
five years a true more or less true then but when I
was using computers to do them the I was for a while you know I used to do
physics and so I used computers to do this stuff I was a really really good integrator except that it wasn't really
me it was me plus the computer so how did that come to be well the answer was that because I was
doing things by computer I was able to try zillions of examples and I got a much better intuition the most people
got for how these things would work roughly how what you did to make the thing go and so on whereas people who
are like I'm just working this one thing out by hand you got a different you know you don't get that intuition so I think you know
two points first of all you know this how do you think about things computationally how do you formulate the
question computationally that's really important and something that we are now in a position I think to actually teach
and it is not really something you teach by you know teaching you know traditional quotes coding because a lot
of that is okay we're gonna make a loop we're going to define variables I just as I think I probably have a copy here
yeah they I wrote this book for this is a book kind of for kids about often language except it seems to be useful to
adults as well but I wrote it for kids so it's some and one of the amusing things in this book is it doesn't
talking it talked about assigning values to variables until chapter 38 so in
other words that will be a thing that you would find in Chapter one of most you know low-level programming coding
type type things turns out it's not that relevant to know how to do that it's also kind of confusing and not necessary
and so you know in terms of the you asked where will we get in trouble when people don't know how the stuff works
inside that's I mean you know I think one just has to get used to that because
it's like you know you might say well we live in the world and it's full of natural processes where we don't know
how they work inside but somehow we manage to survive and we go to a lot of effort to do natural science to try and
figure out how stuff works inside but it turns out we can still use plenty of things even when we don't know how they
work inside we don't need to know and I think the May I think the main
point is computational irreducibility guarantees that we will be using things where we don't know and can't know how
they work inside and you know I think the the perhaps the thing that is a
little bit you know to me a little bit unfortunate as a you know as a typical
human type thing the fact that I can readily see that you know the AI stuff
we build is sort of effectively creating languages and things that are completely
outside our domain to understand and we're by that I mean you know our human
language with its fifty thousand words or whatever has been developed over the last however many you know tens of thousands of years and as a society
we've developed this way of communicating and explaining things you know the AIS are reproducing that
process very quickly but they're coming up with a and a historical you know
something you know their way of describing the world that doesn't happen to relate at all to our historical way of doing it and that's you know it's a
little bit disquieting to me as a human that that you know there are things going on inside where I know it is you
know in principle I could learn that language but it's you know not the
historical one that we've all learnt and it really wouldn't make a lot of sense to do that because you learn it for one AI and then another one gets trained and
it's going to use something different so it's some but my main I guess my main point for for education another point
about education I just make which is something I haven't figured out but but um just is you know when do we get to
make a good model for the human learner using machine learning so in other words you know part of what we're trying to do
like like I've got that automated proof I would really like to manage to figure out a way what is the best way to
present that proof so a human can understand it and basically for that we
have to have a bunch of heuristics about how humans understand things so as an example if we're doing let's say a lot
of visualization stuff in welcomed language okay we have tried to automate do automated aesthetics so what we're
doing is you know we're laying out a graph what way of laying out that graph is most likely for humans to understand
and we've done that you know by building a bunch of heuristics and so on but that's an example of you know if we
could do that for learning and we say what's the optimal path given that the person is trying to understand this
proof for example what's the optimal path to lead them through understanding that proof I suspect we will learn a lot
more in probably a fairly small number of years about that and it will be the case that you know for example if you've
got oh I don't know you can do simple things like you know you go to Wikipedia and you look at what the path of you
know how do you if you want to learn this concept what other concepts you have to learn we have much more detailed symbolic information about what is
actually necessary to know in order to understand this and so on it is I think reasonably likely that we will be able
to I mean you know if I look at I was interested recently in the history of math education so I wanted to look at
the complete sort of path of math textbooks you know for the past well
basically the like twelve hundred you know people actually produced this one
of the early math textbooks so they've been these different ways of teaching math and you know I think we've we've
gradually evolved a fairly optimized way for the typical person though it's probably the variation of the population
is not well understood for you know how to explain certain concepts and we've gone through some pretty weird ways of
doing it from the 1600s and so on where which have gone out of style and possibly you know who knows whether
that's versus because of well but anyway so so you know we've kind of learnt this path of what's the optimal way to
explain adding fractions or something for humans for the typical human but I think we'll learn a lot more about how
you know by by essentially making a model for the human a machine model for the human we'll learn more about how to
you know how to optimize how to explain stuff to humans a coming attraction but
Tim thanks by the way do you think we're close to that at all because you you said that there's a something in Wolfram
Alpha that that presents the human a nice way are we how far said attraction
yeah right so I mean in in that explaining stuff to humans thing is a
lot of human work right now being able to automate explaining stuff to humans okay so some
of these things let's see I mean so an interesting question actually just today
I was working on something that's related to this yeah it's it's it's being able to the question is given a
whole bunch of can we for example train a machine learning system from explanations that it can see roughly can
we train it to give explanations that are likely to be understandable maybe I think the okay so an example that I'd
like to do okay I'd like to do a debugging assistant where the typical thing is program runs program gives
wrong answer human says why did you get the wrong why did it give the wrong answer well the first piece of
information to the computer is that was the human thought that was the wrong answer because the computer just did
what it was told and it didn't know that was supposed to be the wrong answer so then the question is can you in fact you
know in that domain can you actually have a reasonable conversation in which the human is explaining the computer
what they thought it was supposed to do the computer is explaining what happened and why did it happen and so on same
kind of thing for math tutoring you know we have a lot of you know we've got a lot of stuff you know we're sort of very
widely used for people who want to understand the steps in math you know can we make a thing where people tell us
I think it's this okay I'll tell you one one little factoid which I which you did work out so if you do multi digit
arithmetic multi-digit addition okay okay so the basis of this is its kind of
silly silly thing but you know if you get the right answer for an addition some okay you don't get very much
information the student gives the wrong answer the question is can you tell them where they went wrong so let's say you
have a four digit addition sum and the student gives the wrong answer can you back trace and figure out what they
likely did wrong and the answer is you can you know you just make this graph of all the different things that can happen
you know when did they you know there's certain things that are more common transposing numbers and things or you
know having a 1 and a 7 mixed up those kinds of things you can with very high power let's say given a for de division
some with the wrong answer you can say this is the mistake you made which is sort of interesting and that's
you know being done in a fairly symbolic way whether one can do that in a you
know more machine learning kind of way for more complicated derivations I'm not sure but that's a that's one that works
are you sir I just had a follow-up question so do you think you know like
in the future this is it possible to simulate virtual environments which can
actually understand how the human mind works and then build you know like
finite state machines inside of this virtual environment to provide a better
learning experience and a more personalized learning experience well I mean so the question is if if you're
going to you know can you optimize if you're playing a video game or something and that video game is supposed to be
educational can you optimize the the experience based on a model of you so to
speak yeah I'm sure the answer is yes and I'm sure the you know the question of how complicated the model of you will
be is an interesting question I don't know the answer to I mean I've I've kind of wanted a similar question so I I'm a
kind of personal analytics enthusiast so I collect tons of data about myself and I mean I do it mostly because it's been
super easy to do and I've done it for like 30 years and I have you know every keystroke I've typed on a computer like
every keystroke I've typed here and I the screen of my computer every every 30 seconds or so of maybe 15 seconds I'm
not sure it there's a screen shot it's a super boring movie to watch but anyway I've been collecting all this stuff and
so a question that I've asked is is there enough data that a bot of me could
be made in other words do I have enough data about you know I've got I've
written a million emails I have all of those I've received three million emails
over that period of time I've got you know endless you know things I've typed
etc etcetera etcetera you know is there enough data to reconstruct you know me
basically I think the answer is probably yes not sure but I think the answer is
probably yes and so the question is in an environment where you are interacting with some video game trying to learn
something whatever else you know how long is it going to be before it can learn enough about you to change that
environment in a way that's useful for explaining the next thing to you I would guess I would guess that have done that
this is comparatively easy I might be wrong but that and that the I mean I
think you know it's an interesting thing because you know once dealing with you know there's a space of human personalities there's a space of human
learning styles you know I'm sort of always interested in the space of all possible XYZ and there's you know
there's that question over how do you parameterize the space of all possible human learning styles and is there a way
that we will learn you know like can we do that symbolically and say these are
ten learning styles or is it something I think that's a case where it's better to use you know sort of soft machine
learning type methods to kind of feel out that space well you know maybe very
last question I was just intuitively thinking when you spoke about an ocean I
thought of Isaac Newton when he said I mean you know the famous quote I might
not and I thought instead of Newton on the beach what if franz liszt were there
what question would he ask what would he say and I'm trying to understand you're the
alien ocean and humans through maybe Franz Liszt and music well so I mean the
the quote from Newton is is some sort of an interesting quote I think it goes something like this if you know people
are talking about how wonderful calculus and all that kind of thing are and Newton says you know to others I may
seem like I've done a lot of stuff but to me I seem like a child who who picked up a particularly elegant you know
seashell on the on the beach and I've been studying this this seashell for a while even though there's this ocean of
truth out there waiting to be discovered that's roughly the quote okay I find that quote interesting for the
following reason the what Newton did was you know calculus and things like kit if you look at the computational
universe of all possible programs there is a small corner Newton was exactly right and what he said that is he picked
off calculus which is a corner of the possible things that can happen in the computational universe that happened to
be an elegant seashell so to speak they happened to be a case where you can figure out what's going on and and so on
while there is still a sort of ocean of of other sort of computational
possibilities out there but but when it comes to you know you're asking about music I I think my computer stopped
being able to get anywhere but but um sort of interesting the cific get to the
site yeah so this is a this is a website that we made years ago and now my
computer isn't playing anything but [Music]
let's try that okay so these things are
created by basically just searching computational universe for possible programs it's sort of interesting
because everyone has kind of a story some of them are look more interesting than others let's try that one
anyway the the what's what's interesting actually what was interesting to me
about this was this is a very trivial you know what this is doing is very trivial at some level it's just it
actually happens to use cellular automata you can even have it show you I think someplace here where is it
somewhere there's a way of showing your show the evolution this is this is showing the behind the scenes what was
actually happening what it chose to use to generate that musical piece um and
what I thought was interesting about the site I thought well you know how would
computers be relevant to music etcetera etcetera etcetera well you know what would happen is a human would have an
idea and then the computer would kind of dressup that idea and then you know a bunch of years go by and I talked to
people you know who composers and things and they say oh yeah I really like that wolfram tones site okay that's nice they
say it's a very good place for me to get ideas so that's sort of the opposite of
what I would have expected namely what's happening is you know human comes here
you know listens to some 10-second fragment and they said oh that's an
interesting idea and then they kind of embellish it using kind of something that is humanly meaningful but it's like
you know you're taking a photograph and you see some interesting configuration and then kind of you're you know you're
filling that with kind of some human sort of context but but so I'm not quite
sure what what you were asking about I I
mean back to the Newton quote the thing that I think is some another way to
think about that quote is us humans you know with our sort of historical
development of you know our intellectual history have explored this very small
corner of what's possible in the computational universe and everything that we care about is contained in the
small corner and that means that you know you could say well gee you know I
want to you know what we what we end up wanting to talk about other things that
we as a society have decided we care about and what there's an interesting feedback loop I
just mentioned mr. den but but um so you might say so here's a funny thing so
let's take language for example language evolves we you say we we make up language to describe what we see in the
world okay fine that's a fine idea imagine the you know in Paleolithic times people make up
language they probably didn't have a word for table because they didn't have any tables they probably had a word for
rock but then we end up as a result of the particular you know development that
our civilization has gone through we build tables and there was sort of a
synergy between coming up with a word for table and deciding tables were a thing and we should build a bunch of
them and so there's a sort of complicated interplay between the things that we learn how to describe and how to
think about the things that we build and put in our environment and then the things that we end up wanting to talk
about because they're things that we have experience of in our environment and so that's the you know I think as we
look at sort of the progress of civilization there's you know there's various layers of first we you know we
invent a thing that we can then think about and talk about then we build an environment based on that then that
allows us to do more stuff and we build on top of that and that's why for example when we talk about computational thinking and teaching it to kids and so
on that's one reason that's kind of important because we're building a layer of things that people are then familiar
with that's different from what we've had so far and they give people a way to talk about things I give you one example that um see that I have that still up
the okay one one one example here from
this blog post of mine actually so there
okay so that that thing there is a nested pattern you know it's a bitter
sierpinski that that tile pattern was
created in 1210 ad okay and it's the first example I know of a fractal
pattern okay well the art historians wrote about
these patterns they're a bunch of this particular style of pattern they wrote about these for years they never
discussed that nested pattern these patterns also have you know pictures of lions and then you know elephants and
things like that on them they wrote about those kinds of things but they never mentioned the nested pattern until
basically about 25 years ago when fractals and so on became a thing and
then it's ah I can now talk about that it's a nested pattern it's fractal and then you know before that time the art
historians were blind to that particular part of this pattern it's just like I don't know what that is that there's no
you know I don't have a word to describe it I'm not going to I'm not going to
talk about it so that's a you know it's part of this feedback loop of things that we we learn to describe then we
build in terms of those things then we build another layer I think one of the things I mean you talk about you know
just in the sort of the thing like one thing I'm really interested in is the
evolution of purposes so you know if you look back in human history there's a you know what was thought to be worth doing
a thousand years ago it's different from what's thought to be worth doing today and part of that is is you know good
examples of things like you know walking on a treadmill or buying goods in virtual worlds both of these are hard to
explain to somebody from a thousand years ago because each one ends up being a whole sort of societal story about
we're doing this because we do that because we do that and so the question is how will these purposes evolve in the
future and I think one of the things that I view as a sort of sobering thought is that that term actually one
thing I found other disappointing and then I became less pessimistic about it is if you think about the future of the
human condition and you know we've been successful in making our AI systems and we can read out brains and we can upload
consciousnesses and things like that and we've eventually got this box with trillions of souls and and the question is what are these souls
doing and to us today it looks like they're playing video games to the rest
of eternity right and that seems like a kind of a bad outcome it's like we've gone through all of this long history
and what do we end up with we end up with a trillion souls in a box playing video games okay um and I thought this
is a very you know depressing outcome so to speak and then I realized that that actually you know if you look at the
sort of arc of human history people aren't any given time in history people have been you know they've my main
conclusion is that any time in history the things people do seem meaningful and purposeful to them at that time in
history and history moves on and you know like a thousand years ago there
were a lot of purposes that people had that you know what to do with weird
superstitions and things like that that we say why the heck are you doing that that just doesn't make any sense
right but to them at that time it made all the sense in the world and I think that you know the thing that makes me
sort of less depressed about the future so to speak is that at any given time in history you know you can still have
meaningful purposes even though they may not look meaningful from a different point in history and that there's sort
of a whole theory you can kind of build up based on kind of the the trajectories that you've followed through the space
of purposes and sort of interesting if you can't jump like you say let's get chronically frozen for a you know 300
years and then you know be be back again the the interesting cases then you know
are the purposes that you sort of you know that you find yourself in ones that
have any continuity with what we know today I should stop with that that's a beautiful way to end it

----------

-----
--32--

-----
Date: 2018.02.24
Link: [# Lisa Feldman Barrett: How the Brain Creates Emotions | MIT Artificial General Intelligence (AGI)](https://www.youtube.com/watch?v=qwsft6tmvBA)
Transcription:

Introduction
today we're going to try something different we're going to try a conversation we have Lisa Feldman
Barrett with us she is a university distinguished professor of psychology at Northeastern University director of the
interdisciplinary affective Science Laboratory author of the new amazing book how emotions are made the secret
life of the brain she studies emotion human emotion from social psychological
cognitive science and neuroscience perspectives and so I think our
conversation may help us gain intuition about how we instill emotional life into
future artificial intelligence systems as Josh Tenenbaum gave you a shout out
on Tuesday and said that if you want to understand how to create artificial general intelligence systems from an
engineering perspective we should study the human brain so with that let's have
some fun let me start with the curveball to conjure up an image of emotion have
you ever cried while watching a movie that you remember and what movie was it
Ive cried
I've cried during lots of movies
let me think the last time I cried
actually here's an interesting thing when I'm speaking about when I'm giving
an academic talk I sometimes will talk about a study that we did where we had people watch films and we have them
watch the most evocative clips of films and there's several clips that are
really powerful and a couple of them every time I talk about
them I'm gonna try not to cry now every time I talk about them I'm describing for the audience what subjects are
seeing one of the clips is from a movie called Sophie's Choice does anyone know
this film raise your hand if you know this film so we show this is a film about a woman who is forced in a
concentration camp to choose which of her children will die in the gas chambers and so I'm already you know
like if you were if I had a heart rate monitor and respiration monitor you
would see them like it's a really powerful scene Meryl Streep is Sophie
and it's very very evocative we have we
also show there's a scene from a film with Susan Sarandon
who is dying of breast cancer and she has to tell her 12 year old daughter that she's dying so there's another scene also that um
that I find very compelling and you use these scenes to listen to motion as part
of experiments we do yeah and in fact I was just giving a presentation to the
Supreme Judicial Court of Massachusetts on implicit bias and to start the you
know they want to understand the neuroscience of implicit bias and whether or not they should be crafting
jury instructions for juries to be aware of implicit bias and to open the
discussion I showed them a clip from a film that was filmed I you know almost
20 years ago actually called a time to kill a scene where Matthew McConaughey
is this kind of he's his closing statements in a case where he is
defending an african-american man who murdered two European American men who
raped his twelve-year-old daughter and so the scene is he's this completely pathetic lawyer until the end when he
masters this fantastic defense basically of this man on trial for you know
basically avenging his daughter's death and that one I hadn't seen that for 20
years that film and it brought me to tears actually it's just a really powerful powerful scene and it's it just
really punches you in the stomach you just can't help but just experience the
weight of racism in that in that room and the brilliance of the closing argument to sort of puncture through it
Common misconception
basically right okay so experience so one of the things
you talk a lot and you're working in your book is the difference in the experience of emotion the expression of
emotion so what our what's our biggest misconception about emotion for folks
who haven't considered have only considered emotions at a
surface level I would say one common misconception is that you can look at
someone's face and read their emotion the way you read words on a page that
everyone around the world when they're feeling angry they scowl when they're feeling happy they smile when they're
feeling sad that they frown the way that I'm saying it sounds preposterous and it is preposterous
but unfortunately that is actually what people a lot of people believe and tech companies around the world spend
tremendous amounts of money and you know in the investment of some of the most
creative people on this planet trying to build a motion detection systems when
really what they're building our excellent systems for reading facial movements which have no intrinsic
emotional meaning I mean sometimes facial movements communicate emotion but many times they
don't so people scowl for many reasons they scowl when they're concentrating
they scowl sometimes when they're sad they even can scowl when they're happy
people smile when they're angry they smile when they're sad they don't just
smile when they're happy they sometimes smile to indicate to send commute you know a social message that has something
to do with emotion and so the idea that there's one Universal facial expression
for example for each emotion is just there's no strong scientific evidence
How emotions are created
for that claim so how is emotion created them because we nevertheless feel like
we're observing emotional we're communicating with others so how does emotion the creation of emotion change
in the presence of others the does the audience change the display of emotion
is that essentially the message well emotions are not displayed I would say right so basically you and I are
having conversation right now and part of what you know my brain is
doing is it's guessing make making educated guesses about what your facial movements mean so right now you have a
slight smile on your face not exactly a smirk but and so you know say Russian by
the way so we're not allowed to show any emotion you know I have a friend of mine
who is Russian who told me and she when she moved to this country actually I've had several friends tell me this that
their cheeks hurt for a year for how much smiling they had to do I have also a friend from the Netherlands who moved
here who told me the same thing her face ached for how much smiling she did and in Bulgaria they haven't apparently like
a name for you know the pervasive American smile kind of like a not
flattering name for how much Americans smile but basically you when you look at
someone's face you're making a guess about how they feel and although you yourself are focused on their face to
you it feels as if that's where all the information is your brain is actually taking in the entire sensory array so
it's taking in also you know the sound of the person's voice and the person's
body posture and the dynamic temporal changes in all of those signals to make
sense of things it's not you're not really the face is not a lighthouse it's not a beacon that just displays
somebody's internal state in an obligatory way nevertheless there's some
Building blocks of emotion
signal there of course and so what are what would you say as far as I
understand there's no good answer yet from sciences what are the basic building blocks of emotion well I
wouldn't say that there's no good answer from science I would say scientists disagree I think it's very clear what
the building plugs are but you know in one way or another if
you look back all the way to ancient Greece you can see that there are two
kinds of views of emotion that have been battling it out for millennia one is the
idea that you have I will give you the modern versions of these views but they
really do have a very long history one is the idea that you are born with
circuits in your brain that are pre what you know kind of pre-wired for emotion so that everybody around the world is
born with an anger circuit a fear circuit a sadness circuit a happiness circuit some other circuits too you have
them we all have them all neurotypical brains have them and actually other animals have them too and the idea is
that when you know one of these circuits is triggered you have an emotional response which is obligatory so you have
a very stereotypic your breathing changes in a stereotypic way your you know heart rate changes
your the chemicals in your body change in a particular way you make a
particular facial expression and you are have a propensity to make a particular
action like you you know attack and anger and you run in fear or you freeze
in fear and then there's another view which says well there are some basic
kind of ingredients or building blocks that the human mind or the human brain now people talk about the brain now
there are some basic ingredients that your brain uses and it uses them to make
every kind of mental event that you experience and every action that you take the recipes are what are unique the
ingredients are common right just like you can take flour and water and salt and you can make a whole bunch of
different recipes with them some of which aren't even food like glue in a
similar way the idea is that your brain has some kind of all-purpose capacities and it can it puts these ingredients
together and it makes emotion as you need them on the spot and you don't have one anger you have a whole repertoire of
anger you don't have one sadness you have a whole repertoire of sadness and if you grow up in a culture that has no concept
for sadness you don't experience sadness and you don't perceive sadness because your brain becomes wired to make
whatever mental events that are exist in your particular culture so for
What makes us human
artificial intelligence systems the idea of emotional intelligence is really difficult and it seems like it's an
important thing to try to instill so for human beings where do you see the
importance on the priority list of what makes us human where does emotion sit I
usually think that's the wrong question to ask because no I mean I think I think
people constantly ask that question but I don't think it's the right question to ask because you know some cultures in
our on our planet don't even have a concept for emotion and people don't
while they experience what I'm going to refer to as in scientific terms we call
effect which is our simple feelings of feeling pleasant or unpleasant or feeling worked up or calm or comfortable
or discs or you know having discomfort these are feelings that come directly from the internal state of the physical
systems in your body not everybody makes emotions out of those feelings and in
fact we often don't make emotions out of this way so those feelings the way to think about it is that your brain comes
wired to regulate your body actually sort of take that back you your brain
comes wired when you're born your brain comes wired with the potential to regulate your body infants actually
don't regulate their own nervous systems very well they can't put themselves to sleep they can't calm themselves down
they can't regulate their own temperature that's what they need caregivers for and as caregivers
regulate the nervous systems of their infants that wires the infant's brain right so infant little infant brains
aren't born like little miniature adult brains they're born at waiting for a set
of wiring instructions from the world and they wire themselves to the physical and social
realities that they grow in and as they learn to do this they experience these
simple feelings that you know a feeling Pleasant or unpleasant feeling worked up or feeling calm these are kind of like
barometer readings in a way they're very simple and they lack a lot of detail because of how we're wired and we have
to make sense of them and the way that a culture makes sense of them is not always about emotion so I would say you
know in our culture when we lose something significant like a loved one we feel sad into heat intuition people
feel sick they they have an illness it's not sadness a hundred or two hundred
years ago there was an emotion called nostalgia which killed people was
thought to kill people after you know for example after serving in World War one we would now call that depression
it's not just a matter of changing the label of something it's actually the
formation of the experience is very different so if you want to build an intelligent agent
I don't think emotion is what you need to endow it with you need to endow it with these basic ingredients that it can
use to make whatever experiences or guide whatever actions make whatever
States or guide whatever actions are relevant to the situations that it's in that will be different if it's an
American or a British or a Western urbanized environment than say if you
were to go to Tanzania and steady you know the HUD's ax who are hunting
and gathering since that culture since the Pleistocene so do you have words or
Brain evolution
human interpretable labels on these basic ingredients that we can understand
so I mean I think the first thing to understand is that when we think about
building an intelligent agent we think about endowing the agent with cognition
or with emotion or with the ability to COO you to perceive things in the world because
as humans that those are these are the things that are really important to us especially in our culture thinking
feeling seeing and other cultures you know they have different ways of parsing
things but the truth is that your brain did not evolve so that you could think
and feel and see it evolved brains evolved to control bodies brains evolved
so that they could control the various systems of the body as creatures move
around in order to gain resources like food and water and a brain has to figure
out how much resources to expend on getting additional resources now that
may sound really trivial but it turns out it's actually really hard and scientists actually haven't really
figured out completely how brains do this I mean what's really interesting to
me is that if you look for example at computer vision try to figure out you know how how to make an agent see that's
a pretty much solved problem not completely it's a pretty much solved problem the basics are solved it's your
perception problems yeah the peer perception problem however if you want to create an agent that just reaches out
smoothly grabs a glass and brings it into the body to drink that problem is
not solved something is simple as movement which we think of as this really basic thing like oh it's so
trivial all animals can do it is actually the heart at one of the hardest problems to solve and the brains
basically I mean there's a whole story here about evolution which has nothing to do with having a lizard brain which
is wrapped in you know a cognitive brain or anything like that that's a reference
to the Trion brain which a lot of people believe it's a way of thinking about brain evolution instead what we what
seems to be the case is that as bodies got larger and there were more systems because the
nish of the animal the environment the animal got bigger and bigger and bigger brains also had to get bigger but they
had to get bigger to a point with some constraints like they have to be you
have to keep metabolic costs down it's really important if you don't you know
creatures get sick and die and we can talk about what that means in terms of depression or metabolic illnesses or
what-have-you but so what are the basic ingredients well one of the basic ingredients is that your brain is
controlling your body all the time whether you feel it or not whether you're thinking about it or not whether
you're asleep or awake certain parts of your brain are always very active all
the time even when you're sleeping or else you'd be dead and those parts of
the brain that are controlling your heart and your lungs and your immune system and all of those regions that are
controlling the systems of your body are also helping your brain to represent the
sensory consequences of those changes in your body which you don't feel directly
you don't feel your heart beating most of the time you don't feel your lungs
expanding most of the time and there's a really good reason why we are all wired not to feel those things and that is
you'd never pay attention to anything outside in the world ever again if you could feel every little movement that
was going on inside your body so your brain represents those as a summary these kind of simple summary feelings
you feel good you feel bad you feel great you feel like you feel really jittery you feel really calm and these
feelings are not emotions they sometimes get your brain can make them into emotions but they're with you every
waking moment of your life there are properties consciousness in the way that lightness
and darkness is a property of vision and
sometimes we make them into emotions when they're vit when we have a big change in our heart rate or a big change
in our in our breathing rate or a big change in our temperature or a big surge
of glucose we might the brain might make a motion out of those changes those very
strong changes which you will feel as really feeling unpleasant or really feeling Pleasant but your brain might
also make hunger or your brain might make an instance of a physical sensation
like nausea or your brain might even make a perception of the world like
that's a nice guy that guy is an this is a really delicious drink that's
a beautiful painting those are also moments where these simple feelings which we call effect are very strong so
effect is a basic ingredient there are others that I could talk about too it's
not the only one but one of the things I think that that sometimes people who are
studying to build AI systems don't realize is that the brain its
fundamental job is to keep your body alive and well and if you don't have
some kind of body to regulate with effective feelings that come from that
regulation or something like that it's
you're kind of gonna be out of luck I think in rendering something that looks
more something that seems more human right so maybe you can elaborate like in
Emotions are not real
the book sapiens that we as human beings are really good on mass as a thousands
millions of people together believing something even if it's not true so
while scientifically sort of from a neuroscience perspective it may be very true that emotions aren't real I didn't
think real but I said they're not there there isn't so interesting you finished
yeah yeah so what I'm trying to say is also from AI perspective is they become
these trivial ideas of mapping a smile to being happy and these kind of trivial
ideas become real to us through Hollywood through cultural spreading of
information and we start to believe this and therefore it becomes real innocent in in in as much as anything is
real about our cult our perception together so it's really important
scientifically the ideas that you're presenting but does that mean there's just because our brain doesn't feel
those explicit emotions does I mean they're not real I didn't say we don't feel explicit emotions so I want to be
really clear about this because it's an interesting this you know this sum this
inference it's an interesting inference that people often make and so but it's
it's a mistake and it's a mistake that betrays a certain kind of thinking that we do in this culture and is it's the
mistake of the following sort so when I say there is no facial expression that
is diagnostic of a single emotion that doesn't mean that people don't express
emotion they certainly do express emotion they just don't you know when when they're in a state of anger in a
state of sadness or in a state of awe their faces don't do one thing when I say well your body can do many things
when you're angry or when you're sad or but let's take anger your heart rate can
go up it can go down it can stay the same your breathing rate can go up it can go down it can stay the same the
same pattern that you see in anger you sometimes see in sadness and you sometimes see in fear you sometimes even
see it in enthusiasm and in awe so does that mean that emotions aren't real no
emotions are real but but sometimes things are real because the physical
meaning of the signal is endowed in the signal okay
so when your retina communicates to your brain that you have that you are faced
with a wavelength of that you know is 600 you know 600 nanometers the signal
is endowed in your brain that's like in the signal it's not like interpret you don't interpret that it's 600 you know
nanometers it is 600 nanometers the informations in the signal but when you
see read that information is not in the signal your brain has added information
that isn't in the signal itself in a sense your brain has imposed meaning on
a signal that the signal doesn't have on its own they'll let me back up and give
a different example to make it a little easier and then will reproach this there's a there are some things that we
impose this is true almost of all civilization right that we there are
some things that are real by virtue of the fact that we agree that they exist little pieces of paper serve as money or
now you know bitcoin or little pieces of plastic or gold or diamonds or in the
past barley salt shells rocks serve as currency have
value only because a group of people agree that they have value so we impose
meaning on objects and once we all agree
that that object actually has value we can trade it for material goods the minute that some of us disagree that we
risk we withdraw our consent right the
things lose their value that's what happened in the mortgage crisis that's what happened in the tulip crisis in the
you know the Netherlands in the 17th century or 16th century when it was
money currency this because we impose meaning on
objects in the world physical objects in the world that themselves don't have
that meaning on their own and they are very real money is very real to people I can stick somebody's head in a brain
scanner and show you that they experience value in a very real way but
that that reality is constructed by the
fact that they have learned the value in a particular culture well that's also what we do with emotion we impose
meaning on certain physical signals that they don't have by on their own and but
we have collective intentionality we all agree that scowling is sometimes anger
and so it becomes anger in a very real way just like little pieces of paper
become money so it sounds like you kind of think about the expression R emotions a kind of language as an extension of a
Emotions are a language
language that will learn in the same way that we collectively agree in a language ana lexicon and how we use that language
sure you could think of it that way I mean everything everything in our
culture is almost everything
agriculture is a function of social reality in this way we are citizens of a
country because we all agree that the country exists more or less and wow you
guys barely even laughed at that okay [Music] what's a revolution a revolution is when
some people in the country withdraw their consent they no longer agree right
a president has powers in a country because we all agree that a president has powers present only as powers by
virtue the fact that we all agree that he or she has powers if we stop agreeing
the president doesn't have those powers anymore it's very real people's lives
depend outcomes of real people depend on these social realities that we build and
nurture and we why are these social realities into the brains of our children as we socialize them and when
people move from one culture to another they have to learn the new social
reality that they are faced with and if they don't they get very sick physically
because our ability to agree on what something means actually is important
for regulating our nervous systems so can you speak to that a little bit I mean for machine learning methods for
How does an infant learn
systems that learn to behave based on a certain reward it's important to kind of
have some ground truth and and learn so how do it sounds like the expression of
emotion is learned can you talk about how we learn to fit into our culture by
expressing emotion with our phase body given the context given the rich what's
that process look like when does it happen how much sure well you know I mean yeah
I wrote a 400 page book so I'll try to do it in like a couple sentences yeah so
so how does it how does an infant learn anything so an infant's born and it
can't do anything for itself it can't regulate its own nervous system it can't keep its
body systems balanced this is a term scientific term for this is alice stasis
alice stasis is your brain's ability to predict what your body is going to need before it needs it and tries to meet
those needs before they arise so an example would be if you're gonna stand
up if your brain is gonna stand you up it has to raise your blood pressure before it stands you up if it doesn't
you'll fall that's costly from metabolic standpoint it's costly you'll hurt
yourself so an infant's brain can't do this very well an infant doesn't know
when to go to sleep and when to wake up an infant doesn't can't feed itself
can't regulate its own temperature someone else has to do it and when
someone does it the infant is learning the infant is learning it's taking in
sights and sounds and smells and the physical sensations from the body which
are comfortable and pleasant when the infant Alice stasis is maintained so
right from the get-go an infant is learning statistical learning you know
the capturing events including their consequence for the infant's body
some people think babies are born you know attached already to their caregivers but they're not actually
infants don't even know what a caregiver is it's just that the caregiver is there constantly meeting that infant's needs
that's how infants start to learn now if there are statistical regularities like
for example an infant is not born with the ability to recognize a face as a face but it learns that in like the
first couple of days of life why because human faces have some statistical regularities to them right to eyes and
nose and a mouth kind of in the same place most of the time so it learns really quickly but here's the
interesting thing around three months of age infants start to learn
what we call abstract categories they start to learn that some things which
don't look the same or sound the same or smell the same actually have the same
function and how do they learn this they learn it with words so if you do an
experiment with a three month old three months old okay and you say to that baby
very very intentionally look sweetie this is a one and you put the web down
and it makes a noise like a beep and then you say I'm like I don't have props
and then you see my wallet yeah okay and then you say give me your wallet yeah give me your wallet and then you say
look sweetie this is a and you put the down and it makes a beep if you say look
sweetie this is a that infant expects that
object to beep why is that important because in the real experiments this
might be yellow and squishy and tall and this might be red and pointy and you
know hard and this might be you know so lots of different perceptual features
but but the infant nose learns that the
word is inviting the infant to understand that the function of those
very different physical signals are actually the same similarly you can take
you know six objects that are exactly identical in their physical features how
they sound how they smell what they feel like what they look like and you can
name three of them with one word and three of them with the other word and the infant will understand that these
are actually different objects that happens a little after you know not not as early as three months but the point is that when we talk all
the time we use words all the time what do we do with infants we're constantly pointing things out and labeling them
this is a dog this is a cat you're angry this person's sad Oh mummys really happy
today oh you know mommy loves you Oh daddy's really excited about this and
so on and so forth and infants learn really really quickly words are
considered to be kind of invitations to form abstract concepts that is the basis
of almost all of the you know mental
categories that we that that we mental events that we experience we're
basically teaching children to form these abstract categories not and that's
the basis of money and it's the basis of rules that we have with each other and
what we expect from each other it's the basis of a lot of the sort of functional categories that we use in everyday life
we're impose meaning on sensory on sensory arrays that those sensory arrays
in and of themselves don't necessarily have they only have that meaning because you and I both learn that that package
of sensory array means something so when I make that you can anticipate what will happen next just because we've learned
those are they're wired into our brains you know in our culture and when we go
to a different culture we have to learn sometimes different packages yeah different mappings so you're saying that
Different mappings
there's a few sources of sensory data and a few building blocks inside us the
feelings of some kind that we learn to then from an early that we've come born with those or part of what your brain is
doing is it's trying to make sense of the sensory array around it so from your
brains purse so from your brains perspective just think about from your
brains perspective your brains perspective it spends its entire life
trapped in a dark silent box [Music]
Anna NASA makes sense of what's going on around in the world so that knows what
to do to keep itself alive right and well but it has to it has to know what
to do based on it has to know what what's happening all around it only from the effects that it receives through the
sensory systems of the body so a flash of light what's a flash of light it could be
anything what you know what's a look like a siren a siren could be you know a
firetruck or it could be somebody's car alarm went off or it could be a doorbell
or it could be right any particular sensory cue could have multiple causes so your brains trapped basically in your
skull and all it gets are the effects the sensory effects of stuff that
happens in the world but it has to figure out what those things are so that it knows what to do so how does it do
that well it has something else that it can draw on it has your past experiences your brain basically is doesn't store
experiences from the past it can reconstitute them in its wiring and
that's what it uses to guess at what those sensory cues me of those sensory
changes me so in one situation a siren
means one thing in another situation it means another a flash of light means one thing in one situation and a different
thing in another so your brain is using past experience to make guesses about what these sensory changes me and so
that it knows what to do and it has the same relationship to the sensory changes
in your body what's an ache in your stomach well it could be hunger it could be anger it
could be discussed it could be longing it could be nausea it's not that there's
one ache in your stomach for nausea and another ache in your stomach for hunger
there are many aches in your stomach any different feelings of achiness for nausea and many
different feelings of aching is for hunger and sometimes they overlap so your brain has to make the same kinds of
guesses about what's going on in your body as it does about what the sensory events mean in the world and that's
really what it's doing it's it's guessing and making sense of the sensory array so that it knows what to do next
and when it guesses wrong it takes in that the you know the the sort of
information that it didn't predict well which you know in psychology we have a
really fancy name for that we call it learning your brain takes in the
information that it didn't predict and so that it can predict better than the next time to make sense of things the
next time so you kind of answered this a
Building a robot
little bit I'd like to elaborate on it if you were to build a robot that performs maybe passes the Turing test or
performs at the low bar level of instead of myself here today it would be a robot
talking to you it'd be convincing as a human how would you what kind of how
would you build that system in a sense in paralleling the infant's what essential aspect of the infant
experience do you think is important it needs to well I mean I you know I don't
I'm not a computer scientist so but so the way that I would say it is it needs
to have a body these have something like physical systems or an analogy to physical
systems it has to do something analogous to a low stasis so whatwhat's sorry to
elaborate so what would be the goal for the system you kind of mentioned previously that the goal will be to for
the brain to just stabilize itself no it's not that the brain is stabilized though you know so people talk about
reward what is reward and machine learning is pretty easy it's it's
something but it's it's mathematical so it's there's
no philosophy to it you just oh yeah there's philosophy to everything right whether you admit it or not as a
different story right yeah so you wanted to play a game of chess playing give
them go you wanted to pick up a water bottle is uh okay but what is reward
existence dopamine is actually not
reward dopamine is effort dopamine is necessary for effort it's not necessary for reward
maybe it's commonly if you read the most caught those up to date literature that
is what you'll see that it's actually people can animals can find things
rewarding can be without without dopamine actually but they they they
need to open to move they need dopamine to encode fits and to learn information so it's really for effort that is
required to work towards getting a reward I would say and when the when a
brain an animal brain a human any kind of animal brain miss predicts what the
reward will be that's what you see a real surge of dopamine because it the
animal has to adjust its action but reward is basically bringing the body
back into a low stasis it feels good when that happens and people will and
animals will work tremendously hard to have that happen so you know what is
motivation motivation is is expending resources to get a reward so basically
if if you don't have something
like physical systems that have to be kept in balance water I mean for humans
or for actually any living creature on this planet even you know single-cell
organisms actually there's an analogy to what we're talking about here to brains but you know salt water glucose all
these systems have to be kept in balance and they have to be kept in balance in a very very efficient way and that's the
motivating so-called motivating force really that's what that's what that's
what really brains are for so yeah and that is the basis the consequence of
that regulation it are the basis of you know effective feelings which are for
many many creatures on this planet a property of consciousness ok so maybe if
Will it love you back
it's ok we'll take some questions in the audience but first let me ask the last question as so I'm building on the robot
question how would you build the same kind of robot that you would be able to
as a human being fall in love with well
you know people fall in love with their cars they fall in love with they fall in
love with their blankets they fall in love with their toys you know you don't need MIT doesn't need much to fall in
love with something the question is will it love you back I would elaborate I think I think yeah I
think you're answering that love in the way we're defining it loosely in poetry and culture as a social construct and
its relative what I mean is sort of the idea of monogamous law a long-term love
that we have deep connection with other human beings that we have you're saying you could do the same with a car like a nice 69 Mustang are you telling me that
you you telling me that you've never you know anyone who's like so in love with their car that you okay now here's the
thing so one so here's the thing we we are social animals okay what does that
mean what does it mean to be a social animal it means that we regulate each other's nervous systems so our brain my
brain isn't just regulating my nervous system right now it's regulating yours and actually it's regulating other other
people's in the audience too and vice versa and why is that you know I mean other animals do it too
we're just looking really good at it but other animals like there are some insects that are social species they
regulate each other's nervous systems they do it with chemicals they do it with smell primarily and a little bit
with touch like earwigs well you know like they can actually I have this great picture of there's like totally
disgusting looking little bug but it's like you know cuddling it's a little baby ugly little ugly baby bug - it's
adorable picture but you know what about mammals like rats well they also use
chemicals like smell but they also use touch and to some extent they also use
sound these hearing primates add vision and as primates we do all weaves all of
those senses to regulate each other and we also use something else words right
exactly and words the systems in our brains that allow us to speak and allow
us to understand words are directly connected to the parts of the brainstem
that control the body I don't mean like they're a bunch of I mean monitor I mean monosynaptic we connected so exactly the
same systems in your brain that are important for you to be able to understand language and to
speak are also directly directly affecting the systems of your body and
that is why I can say something to somebody I can speak to you and I can
have an impact on the nervous systems of people all the way at the back of this auditorium without they you know maybe
they can see me maybe they can't maybe they can't hopefully they can't smell me maybe they can hear me maybe they you
know but they can if they hear me speak words I can affect their nervous systems that's why a telephone works that's why
the telephone works to where you can feel connected to someone just merely by hearing their voice because the sound of
their voice has an effect on your nervous system it can it can make you breathe faster it can make you breathe
slower and the words also have an effect because when I say a word like hmm I
don't know when I say word like car that's a short
form I have a bunch of mental features in my mind when I say the word car and I
say that word and that invokes those similar mental features maybe not identical but similar enough that
invokes it in your mind and your mind is
made by your brain so it invokes if I just say the word car there are changes
in your motor system that would be exactly the same or very close as if you
were actually in a car right so this is something that we do and the fact we're
attachment comes from an infant to a caregiver or or to lovers or to really
close friends comes from the ability that we have to regulate each other's
nervous systems and that is why when you don't have that
kind of attachment you die sooner on average seven years sooner loneliness
kills I always tell my daughter my daughter's 19 years old I always tell her you know breaking up when you break up with someone it feels like it will
kill you but it won't loneliness however will kill you it will
kill you on average seven years earlier than it would if you didn't have an
attachment and that's because our nervous systems you know as our bodies
got really complex through evolution and our brains got bigger they could only get so big there are constraints on how
big any brain can get that have to do with you know birthing the infant but it
also has to do with the metabolic cost of a brain your brain is really expensive my brain really expensive
three pounds 20 percent of your metabolic budget that's a lot and so
what did evolution do to solve this problem well it couldn't make our brains any bigger so it just entrained other
brains to help manage our nervous systems so you bear the burden of other
people's Alice stasis and they bury your the burden of yours not always at the
same time but that's what it means to give people support when someone when you're feeling horrible and somebody
Pat's you on the back or says nice words to you or gives you a hug they are physically having an effect on your body
that they are helping your body to maintain a low stasis at a time when
your brain probably couldn't manage it on its own and so the basis of love or
attachment is basically that it's the ability to affect each other's nervous
systems in a positive way I always say to people you know the best thing for a
nervous system a human nervous system is another human and the worst thing for a
human nerve system is another human because we're
social animals well beautifully put so maybe a few
Audience questions
questions in the audience do you mind if there's a there's microphones on both sides okay sure hi thanks for talking here
you're saying cool stuff not a question
I'll take it it's all right so I was
thinking about what you're saying about reward and I'm wondering first of all is
he described as a return to a lo stasis is reward in any way linked to the and
just like the reinforcement of pathways or behavior so that the next time you get that stimulus you will you'll
respond in the same way and I'm also wondering about the link between the
desire for aloe stasis and the need for novelty and then he did like a great
it's a really great question so um the second question is so much
more interesting than the first actually so let me say this that that there is a
need for novelty the need differs right for different people I will say novelty
so first of all the we can think about
the need for novelty in a really proximal way we can think about it in a
really distal way like but basically
when I say that a brain is organized or engineered for metabolic efficiency that
doesn't mean that the goal is to to only ever have your prediction you know your
brains predicting all the time to always have its predictions completely perfect so you'll never learn nothing because
you'll be bored out of your mind right and also you know humans like to
and their niche they like to explore so it's a constant balance between what
biologists would call exploitation and exploration novelty is is um it's
exciting it there's actually an increase in norepinephrine an increase in arousal it feels really exciting but it's also
super costly novelty requires usually that you learn something new that means
that's actually really metabolically expensive thing to do and it also means usually that you're moving your body
around which is also metabolically expensive thing to do so the need for novelty is balanced by its cost and
different nervous systems can bear different amounts of cost so for example
if you take two rats that are somewhat genetically you know moderate like
they've been genetically bred one is bred when you stick it in a a novel cage
it just sits still and the other one when you put it a novel cage it like
roams all over the place and it's just you know it's going crazy kind of exploring everything well the one that
sits still scientists might say oh that's a nervous rat or that rats afraid
what is that rat doing the rat is not moving and it's not encoding anything
because encoding something is expense it's expensive this rat on the other hand is roaming
all over the place moving a lot learning a lot so it's encoding a lot spend spend spend save save save spend spend spend
there are time there are differences between people and there are also differences between times in your life
where moments where you feel like you have a little bit to spend and other moments where you feel like you really have to conserve when I talk to the
public I always talk about I don't use the word out of stasis it's just too boring a word but I sort of do is sort
of explain it like a budget you know like your brain is sort of like the financial office of a company a company
has lots of offices it has to balance the expenditures and revenues and it's
got to keep everything in balance so it might take a little money here move without office you know it's got to keep everything in balance we're just always
trying to do is spend a little bit to make a little bit more what happens when
it spends a little bit and it doesn't get a revenue back there's no reward what happens well it goes into the read a little bit so what do you do when
something goes into the read well you might do something risky you might actually spend a lot to try to really
may you know not just make back your deficit but actually make a lot that would be novelty that would be move and
spent moving and encode or you might reduce your spending you might say well
I'm gonna save a little bit now that would be I'm not gonna move too much I'm
not gonna spend too much I'm not gonna encode anything so I certainly don't mean to suggest to you that that novelty
is unimportant or that learning is unimportant and it's a really important question about what is there any
intrinsic value to novelty over and
above the rewards that it would give you an analysis at ik sense but it is really
clear to me that the extent to which you any nervous system will embrace novelty
and even seek it pretty much depends on the allostatic state of that of that
system if you don't have a lot to spend and you're already in the red you if you
at a certain point if you continue to spend when you're in the red you go bankrupt what that means in human terms
is you get depressed it means that your your brain makes you fatigued so you can't move and it makes you locks you in
so you stop paying attention to anything going on around you in the world and your experience is just what's in your
head that that's actually what depression is so that makes me think of
aloe stasis more of as a range than as a zero point it's not homeostasis it's
Alice stasis it's a range for sure but we answer some other questions and maybe I'll get back to your first question if
there's time so if I understand your argument correctly if we're going to
The need for embodied systems
make anything like a general intelligence something approaching not like like a human it
needs to be an embodied system well I want to be careful about saying that because for two reasons one because in
biology there is this concept of degeneracy just a sucky word but it's a
great concept and it means that there's more than one way to skin a cat basically you want a functional outcome
there are many ways to get to that functional outcome so genes for example you know there are a lot of
characteristics that are heritable but we don't know the genes for them and the reason why is that there isn't one set
of genes there are like multiple sets of genes that can give you actually the same outcome so what I want to say is
that you need something akin to a body it doesn't actually have to be a body I
imagine there are lots of ways that you could implement a system you could
implement an agent that has multiple systems of some sort that it has to manage but my point is that one thing
about that is very important that we continually miss when we think about building an agent with mental states we
continually miss the fact that it has a body it has about humans have bodies and
that's the brains primary task and our most fundamental feelings come from the
physical changes in our body even though we don't normally experience it that way
that actually is how it is if you just look at the wiring of the brain if you just see it so it seems to me that if
you want to build an agent that is human like it has to have something like a
body doesn't have to maybe be a body and I'm sure there are many clever ways that
you could implement something like a body without it actually being a body if you know if you understand what I mean
so Amazon Alexa could be there if we
just gave it some sort of I don't know representation of mental states or some kind of a low static yeah target sure
here's just say one other thing because I think it's really important all a brain requires is that you at some point
had a body you know right so basically being this is what phantom limb pain is
this is what chronic pain is this is what happened you know if at some point you cease to get information from your
body anymore your brain still can simulate still can reinstate the sensory
patterns that once came from the body and that's what's required right at some
point the body isn't really needed anymore yeah I think we're here yeah so
emotions in humans Lucas are they implemented by a bunch of hacks so there's a bunch of chemicals that go
into your brain those oxytocin serotonin a whole bunch of biochemical things that
diffuse around in the fluids in your brain that affect your emotional state and that seems like a Hank that we've
inherited over millions of years from premature ancestors and if you look at
the machine learning world we can do a bunch of similar things with you on that so you can increase the activation thresholds on a large scale you have
changed the map noise going into the system you can do a bunch of similar things but you don't have to rely on
fluids being kind of cleaned slowly by glial cells things don't diffuse around
in and fluid in the system necessarily seems like there's a lot more flexibility so when you come to an
implementation there's not so many constraints imposed by the evolutionary history on the whole system and it seems
like that would make it work better so when people are in negative emotional states and they can't think straight
people can think actually quite well a negative emotional state they don't tell you so but they can play with crimes
they can do very nefarious things very very effectively right so but the general point I think is true even if
that example is not so emotions kind of flood your nervous system without
emotions don't flood your nervousness all right so some of them do so now perfectly of yes sir
you thing in your bloodstream let's talk about this chemical let's talk about
this chemical there's not a single chemical in your brain or anywhere in your nervous system that is for emotion
serotonin is not for emotion don't mean it's not for emotion oxytocin is not for emotion well even opioid you know even
even Opie even opioids are not for emotion so they influence your emotions they're involved they influence every
mental event that you have not just emotion so your brain for example your
brain is a physical it's a set of physical cells that are bathed in neuro
chemical systems and the neuro chemical systems that you're referring to basically change the ease with which
information is passed back and forth between those neurons that's always true it's true regardless of whether the
event is a motion or whether it's a perception or whether it's a thought or whether it's a belief it's always true
so for example serotonin serotonin is a neurotransmitter that allows your brain
to delay gratification of a reward it allows you to expend energy now because
you anticipate a reward at some point in the future and if you have a deficit in
serotonin then you can't do that very well and it turns out for humans one of
our great superpowers is the ability to do mental time travel to remember in the
past and also to do things now because we know they're going to have an effect ten or twenty or fifty or a hundred
years from now so the the you know in when I said you have to have something
like a body I'm not saying literally you have to have a physical corporeal body I'm saying you have to have you I mean
it's just a fact it's not an argument it's a fact that your brain that brains
evolved for the purposes of regulating multiple systems and from a cybernetics
standpoint you know the best way to regulate a system is to build an internal model of it that's what your
brain is your brain is an internal model of your body in the world it's running simulations it's running this model and
if you want to have an age that is somewhat human-like that has feelings like humans then they have to
do something that they have to have to be able to do something similar and whether it's a actually physical
corporeal body or not I think is you know that's an open question right so it sounds a bit as though you're disputing
my premises before I got to my question so I started off by saying sorry I started off by saying emotions are
implemented as a bunch of hacks so would you say that was rub broadly correct or would you say that they're not hacks
they're finely tuned and adaptive and well I went there not maladaptive but would you say it's a bunch of hacks I
don't think I don't know what you mean by hacks I think luge's kind of historical
accidents I think they're wrong you're talking about emotions like they're they're talking the the premise of your
question I can't answer your question because I think it's not the correct question I mean what its emotions aren't
like blood pressure you know they don't exist in that sense they are they are
the way that they are first of all not all cultures have not all people in all
cultures have emotions everybody has effect assuming they have
a neurotypical brain of sorts but but they don't all have emotion and so to
answer your question that your answer asking me is I can't answer because I don't think it's the right I don't think
it's the right opioids and the the pain systems seem like things that influence
your emotions fairly directly so this it's not that it's the same thing but it's that there's a powerful link
opioids though so opioids are important
for instances of emotion but they are also important for every other category
of mental event that you brain can make other things too I agree so I you know
but you will see sometimes scientists will assign you know I'm an emotional
meaning to something like dopamine is for you know first hedonic pleasantness
and then it's for reward people like to assign single functions two biological entities
like a chemical or a brain region a cluster of neurons or it's just not
that's really I'm not saying every chemical does everything but whatever whatever opioids do they do in every
waking moment of your life not just in moments that are emotional for you
I should stop thank you very much yep I
think it is very interesting that you brought up the last topic of love because I think it actually brings up a
really important thing which is and what you were saying about connection that's
I mean the primary purpose of life is to procreate right I mean that that's what
our genes do so first of all right I'm not gonna be touching that yeah so yeah
and and it's of course in humans it's not just genetic it's mimetic I mean we
appropriated our ideas as well as you know physically and so a primary purpose
of our wanting our our reactions and interactions with other people and other
things is that goal is that we get
rewarded when we interact with other things in a way that creates something new whether it's art or you know a book
or technology or something like that and if we don't put that if there's no
inherent reason for a robot or a computer system to want to do that I
mean how do we how can we can even imagine
putting that into a system inherently that it wants to that it just
desperately wants to make something new I mean well here's what I would say you
know when we're we're first of all we're not we talk about we've been focusing a
lot on bodies I'm certainly not saying that's a sufficient condition right I'm just saying it's a Ceri one or something like a body but I
certainly have the motivation to create and I'm imagining that you do too I have
to tell you not everybody has that most adolescents don't have that many
adolescents don't have it okay that was a joke I have a teenage daughter I'm just telling you yeah but the point is
that that what people what people find rewarding is is remarkably diverse but
the property I think is that there has to be a feature of reward for for a lot
of people that's you know that's having an impact in some way having an impact on another person having an impact you
know on a building something that wasn't there before whatever you know in in
innovating or I'm discovering but it's not true for everybody it's just true for like a lot of people in this room
probably and certainly the people that we probably spend a lot of time with but not not for everybody
sure I mean look but if you want to you know if you want to if you you know we can certainly make a kinetic argument
here absolutely there there the there's nothing that
I've said today that is inconsistent with the idea that that you have to that
you have to pass your genes on to the next generation and actually make sure that that generation survives to
reproductive age it's not just enough to have offspring you have to make sure the
offspring survive to reproductive age and there's a whole argument about you
know the the learning of social reality
concepts of social reality that we've been talking about like money and emotions and so on that makes that
argument right that it's very expensive to have to encode everything in jeans or
to have to learn everything from scratch every generation so instead what we have
is a system a genetic system that allows us to wire the brains of the young with
what we know and that's what we do basically right so it's the I'm curious
about your analogy about intentionality that you talked about when you use the analogy between money and the perception
of red to the facts that we the fact that we have emotion because the
distinguishing feature it seems to me is the level of intentionality and as you said before our brain assigns meaning to
things but we don't or maybe and and my question is whether or not you agree with this I guess we don't always
deliberately assign me nothing I've said is about ever deliberate but sometimes
we do write often actually yeah so when you go back to like the question of what
makes something intelligent a lot of previous talks have been about you know we want to pick a goal and then we
create costs to achieve that goal but that goal is deliberately if assigned so
when you talk about like what makes something intelligent what do you think
the role of intentionality is and the spectrum therein so first of all we talk
about intentionality I think you have to really be careful that you are philosophers talk about intentionality in two ways they talk about
intentionality to mean a deliberate action the way you mean it but intentionality can also mean that
something has a referent out in the real we're out and outside of you really so that a word a word has a referent that's
the you know the intention of the of the word basically I think you have to be really careful I also think that you
have to make a distinction between a conscious deliberate explicitly a goal
that you can explicitly describe and I
mean you're you're sort of making you're sort of making I mean wait the kind of
question you're asking is getting very close to the question of free will so I would love to not have
disgust but but basically okay and and I what I'm about to say is gonna sound
very Cartesian unfortunately because that's English I don't know there's no other way to do it actually but what I
want to say is that your brain is always there is always a volition but it's not
always consciously experienced by you as as agency or will so yeah you're not
you're not a sea urchin you know your sensory neurons are not hardwired to
your motor neurons you have inter neurons that means you have choice do you consciously experience yourself as
making choices all the time no you don't but your brain is actually making choices all the time that's why people
who study decision making you know think they're studying the whole brain because they they are actually so I think you
have to be really careful about there are words that we use in English and in
science that have two meanings they can have a meaning that is about decision
making or choice that is just obligatory automatic and a function of how the
system works and then there's the kind of choice that feels deliberate and effortful and where we feel like we're
the agents we experience ourselves as the agents and intentionality usually
can be assigned to the second one but actually in truth in philosophical terms it can also be assigned to the first
that when you even if you're completely unaware of having made a choice you're
acting on something with some degree of volition because you you it's not a
reflex it's not like you know somebody hit your patellar tendon and nuke it and so it's really interesting so you know I
think you just have to you have to make that distinction and I probably should get to the next question we can question yeah thank you yep yeah so you've been
asked a lot of esoteric questions about AI but I think we might gain some insights by wondering about di that is a
dog intelligence oh sure so I believe I sort of understand what
my dog is feeling and III usually believe that my dog you know believes
the same but not the same with you know a cat because you're not a cat person
yeah sure I mean you know I'm not I don't know that much about monkeys but
I've never really seen a monkey be able to make the expressions that let's say a dog can and I was wondering if you had
any insights about you know why dogs are able to do this and why we're able to read dogs is it something just simple
like they're they have the right facial muscles or is it something some drive that allows them to learn this so I
think before I wrote my book I would have answered this question differently but now here's what I would say I think
what you know many creatures on this planet have effect right and we can
debate about whether you know I just came across a paper the other day about whether fly you know just your Safa I
have have effect and you know it's I mean it's actually really interesting question they certainly they have something like opioids and they you know
so it's consistory question but dogs dogs are really interesting because they
do seem to have some capacities that only that you only see in in great apes
and they may have capacities that great apes even other than us don't have and I
mean so they certainly have some capacities we don't have either but but here's my point we actually bred them we bred these
animals we selected them basically it's not natural selection it's it's you know
artificial selection and we selected them for a couple of things right if you look at the experiments on breeding
taking a fox you know taking foxes and breeding them into what looked like little dog like animals it's interesting
what they can do and one of the things they can do is they can move their facial muscles and in in in a lot of
ways that they have a lot more control over their facial muscles then you know chimpanzees and
do and and other apes and they also do
they do joint attention really well with gays so this is something that really no
other animal can do I think other than a dog other and humans and that is they
can they they meet your gaze and they use gaze for reference so you know
they'll look at something and they'll look back at you and you you know that's actually partly that's how we communicate with each other chimpanzees
lose that ability after about ten or eleven months a year overall of age but
but dogs continually do it and actually joint attention shared gaze is how we
communicate with our infants also that's actually partly how we teach infants about what's important in the world is
with gaze so I think that dogs may actually have some capacities probably
because we bred them to have those but we selected the animals that you know
and bred them to have those capacities that's very interesting again awesome well with that let's give these a big
hand thank you so much

----------

-----
--31--

-----
Date: 2018.02.16
Link: [# Sacha Arnoud, Director of Engineering, Waymo - MIT Self-Driving Cars](https://www.youtube.com/watch?v=LSX3qdy0dFg)
Transcription:

today we have the director of engineering head of perception at way mo a company that's recently driven over
four million miles autonomously and in so doing inspired the world in what
artificial intelligence and good engineering can do so please give a warm
welcome to Sasha our new [Applause]
thanks a lot Lex for the introduction well it's it's a pretty packed house
thanks a lot I'm really excited thanks a lot for giving me the opportunity to to
be able to come and share my passion with the Seb driving cars and be able to
share with you all the great work we've been doing at Weimer over the last 10
years and give you more details on the recent milestones we've reached
so as you see we'll cover a lot of different topics some more technical
some more about context but when either
the content I have three main objectives that that I'd like to convey today so
keep that in mind as we go through the through the presentation my first one is is to give you some background around
the self-driving space and what's happening there and what it takes to build self-driving cars but also give
you some some behind the scene views and tidbits on on the history of machine
learning deep learning and how it how it all came together within the big alphabet family from Google to way moe
another piece obviously another objective I have is to give you some
technical meat around the techniques that are working today on our self-driving cars so I think during the
the class you hear a lot you've heard a lot about different different deep
learning techniques models architectures algorithms and I try to put that in a
current hole so that you can you can see how those pieces fit together to build a system we have today and has been at
least I think as Lex mentioned it takes a lot more actually than
algorithms to build a sophisticated system such as our self-driving cars and
fundamentally it takes a a food industrial project to make that happen
and I'll try to give you some color with which hopefully is it are different from from what you've heard during the week
I'll try to give you some color on what it takes to actually pan out such an
industrial project in real life and make an essentially productionize machine learning so we hear a lot of talk we
hear a lot about self-driving cars it's a very hot topic and for very good
reasons I can tell you for sure that 2017 has been a great year for whammo
actually only a year ago in January 2017 when Moe became its own company so that
was a major milestone and a testimony to the to the robustness of distribution so that we could move to a product product
is Asian phase so what you see on the picture here is our latest generation
self-driving vehicle so it is based on on the chrysler pacifica you can already
see a bunch of sensors I'll come back to that and give you more more insights on what they do and how they operate but
that's that that's the latest and greatest so self-driving indeed is draws
a lot of attention and for very good reason I personally believe and I think
you will agree with me that self-driving really has as the potential to deeply
change the way we look about mobility and the way we move people and things
around so only to cover a few aspects here obviously that and I want to go
into too many details but safety is one of is one of the the main motivations
94% of us crashes today involve human errors a lot of those errors are around
distraction and things that could be avoided so safety is a big piece of it
disability and access to mobility is also a big motivation of ours
so obviously the the self-driving technology has the potential to make it
very available and cheaper for more people to to be able to move around and last but not least is efficiency a
collective efficiency so not only we spend a lot of time in our cars in in
long commute hours I personally spend a lot of time in on commit hours and that
time we spend in traffic probably could be better spent doing something else than having to drive to grab the coin in
complicated situations beyond beyond traffic obviously the self-driving
technology has the potential to deeply change the way we think about traffic parking spots urban environments city
design so that that's why it's a very exciting topic so that's why we made it
our our mission at Waco is fundamentally to to make it safe and easy to move
people and things around so that's a nice mission and we've been on it for a
very long time so actually the whole adventure started close to 10 years ago
in 2009 and at the time that was that starting under the umbrella of a Google
project that you may have heard of called chauffeur and back back back in
those days so remember we were before the deep learning days at least in the industry and so really back in those
days the the first the first objective of the project was to try and assemble first product a vehicle take
off-the-shelf sensors assemble them together and try to go and decide if
self-driving is even a possibility it is like it's one thing to to have some prototype somewhere but is that even a
thing that that that is worth pursuing which is a very common way for Google to to tackle problems so the genesis for
that work was to come up with a pretty aggressive objective
so the team the first milestone for the team was to essentially assembled 10100
my loops in Northern California around around Mountain View and try and figure
out so for a total of 1,000 miles and try and and see if they could build
first system that that would be able to go and drive those loops autonomously so
they were not afraid so the team was not afraid so those loops went through some
very aggressive patterns so you see that some of those loops go through the Santa
Cruz Mountains which is an area in California that as you'll see I'll show you a video that has very small roads
and two-way traffic and cliffs with negative obstacles and complicated
patterns like that some of those some of those paths were going on highways so
that and one of the the busiest highways some of those routes were going around
Lake Tahoe which is which is in the Sierras in California where you can
encounter different kinds of weather and again different kinds of roads conditions those routes were going
around bridges and the Bay Area has quite a few bridges to go through though
some of them were even going through a dense urban area so you can see San
Francisco being driven you can see Monterey some of the Monterey centers
being driven and as you see on the video those bring those truly bring dense
urban area challenges so since I promised it so here you're
gonna see some pictures of the driving and it's kind of working
so here with better quality so here you see the the roads I was talking about on
the mountain on the Santa Cruz Mountains driving in the night animals crossing the street freeway driving going through
patos just another area that is Charlie dance there's a aquarium there pretty
popular one that's the famous lombard street in san francisco that you may have heard of which in San Francisco
always brings unique set of challenges between fog and slopes and in that case even shop turns so that was all the way
back in 2010 so those ten loops were
successfully completed 100% autonomously back in 2010 so that's more than eight
eight years ago so on that on the heels of that success the team decided and
Google decided that self-driving was worth worth pursuing and moved and moved
forward with the development of the technology and and testing so we've been
at it for all those years and have been working very hard on it historically
way more and and and I think what the other companies out there have been relying on what we call safety drivers
to still sit behind the wheels even if the car is is driving autonomously you
still have a safety driver was able to take over at any time and make sure that we have very separations and and we've
been a committed my eyes and knowledge and developing the system many iterations of the system across all those enoguh lose years we reached a
major milestone as Lex mentioned back in back in November where for the first
time we reached a level of confidence and maturity in a system that we felt
confident and proved to ourselves that it was safe to remove the safety driver
as you can imagine that's that's a major milestone because it takes a very high
level of confidence to not have that backup solution of a safety driver to take over or something to arise so here
I'm gonna show you a small video a quick quick capture of that event so that the
video is from one of the first times we did that since then we've been continuously operating drug arrest cars
self-driving cars in the Phoenix area in Arizona to expand our testing so here
you can see the video swing so you can see our chrysler pacifica so here we
have members of the team who are acting as the passengers getting on a backseat there is you can notice that there is no
driver on the driver's seat so here we are running car having kind of service
so the passenger simply press the button the application knows where they want to go and the car goes nope no one on the
driver seat so we started with a fairly constrained geographical area in
Chandler close to Phoenix Arizona and we
we are hard working to expand testing and the scope of our operating area
since then so that goes well beyond a
single car a single day not only we do that continuously but we also have a growing fleet of self-driving cars that
we are deploying there all the way and looking for a product launch pretty quickly so I've talked about 2010 and we
are in 2018 and were getting there but what it took it took quite a bit of time so I think one of the one of the key
ideas that I'd like to convey here today and that I will I will go back to during
representation is how much work and how much work it takes to really take a demo
or something that's working in a lab into something that you feel safe to put on the roads and get all the way
to that to that depth of understanding that depth of perfection in your
technology that that you operate safely so one way to say that is that when you
are 90% done you still have 90 percent to go right so 90% of the technology takes only 10% of the time right in
other words you need to 10x right you need to 10x the the capabilities of your
technology you need to 10x your team size and find ways for more engineers and more researchers to collaborate
together you need to 10x the capabilities of your sensors you need to 10x fundamentally the overall quality of
the system right and your testing practices as we'll see and a lot of the aspects of the program and that's what
we've been that's what we've been working on so beyond the context of
self-driving cars I want to spend a little bit of time to give you kind of a
kind of an inside of view of the rise of deep learning Sumer I mentioned that
back in 2009 2010 deep learning was not ready available yet in full capacity in
the industry and so over those years actually it took a lot of breakthroughs
to to be able to reach that stage and one of them was the Agora algorithm
breakthrough that deep learning gave us and I'll give you a little bit of of backstage view on what happened at
Google during those years so as you know Google has been as committed itself to
machine learning and deep learning very early on you may have heard of the Google brain what we call internally the
Google brain team which is which is a team fundamentally hard at work to lead
the bleeding edge of research which is known but also leading the development
of the tools an infrastructure of the whole machine learning ecosystem at at Google and level to essentially low many
teams to develop machine learning at scale all the way to successful products so they've been working and pushing
that the deep learning technology has been pushing the field in many in many directions from computer vision to
speech understanding to NLP and all those directions are things that you can
see in Google products today so whether you're talking real assistant or Google photos speech recognition or even Google
Maps you can see the impact of deep learning in all those areas and actually
many years ago I was part of I myself was part of the street view team and I
was leading the what an internal program an internal project that we call the street smart and the good we had at
sweet smart was to use deep learning and machine learning techniques to go and
analyze Street imagery and as you know that that's a very big and varied corpus so that we could extract elements that
are core to our mapping strategy and build and that way build a better Google Maps so for instance in that picture so
that's that's a panorama or piece of a panorama from Street View imagery and
you can see that there are a lot of pieces in there that if you could find and and properly localized would
drastically help you build better maps so street numbers obviously that are really useful to map addresses street
names that when combined event on similar techniques from our views will
help you properly draw all the routes and give a name to them and those two combines actually allow you to do very
high quality address book apps which is a common query on Google Maps general text
and more specifically text on business facades that allow you to not only may be localized business listings that you
may have gotten by other means to actual physical locations weather so build some of those local listings directly from
scratch and and more traffic oriented patterns traffic whether it's traffic
lights traffic signs that can be used then for for ETA navigation ETA predictions and stuff like that so that
was our mission one of the as I mentioned one of the hot piece is to do
is to map addresses at Cal and so you can imagine that we had a breakthrough
when we first were able to properly find those street numbers out of the Street
View imagery and out of the facade solving that problem actually requires a lot of pcs not only you need to find
what where the the street number is on the facade which is if you think about
it a fairly hard semantic problem right what what's the difference between a street number versus another kind of
number versus other auto text but then obviously read it because there's no
point having pixels if you cannot understand the number that that's on on the facade all the way to properly draw
geo localizing it so that you can put it on on Google Maps and so the first
deepening application that that succeeded in production and that's all the way back to 2012 that we had the
first system in production was really the first breakthrough that we had across across alphabet on our ability to
properly understand real scene situations so here I'm gonna show you a
video that kind of sums it up so look every one of those segments is actually
a view from starting from the car going to the physical number of all those
house numbers that we've been able to detect and transcribe so here that's in Sao Paulo and well you can see that when
all that data is put together gives you a very consistent view of the addressing scheme so in in
so that's another example say similar things obviously we have more that in Paris where we are doing more imagery so
more views of those of those physical numbers that when you if you are going to triangulate you're able to do
localize them very accurately and have very accurate maps so the last example I'm going to show is in Cape Town in
South Africa where again the impact of that deep learning work has been huge in
terms of quality so many countries today actually have up tuned more than 95% of
addresses maps map to that way so doing similar things service you can see a lot
of parallelism between that work on 3d imagery and doing doing the same on the
real scene on the car but obviously doing that on the car is even harder is
even harder because you need to do that rigor time and and very quickly with low
latency and you also need to do that in in an embedded system right so the cars
have to be entirely autonomous you cannot rely on a connection to a Google
Data Centers and first you don't have the time in terms of latency to bring data back and forth but also you cannot
rely on a connection to for the safe operation of your system right so you need to do the processing within the car
but very so that's a that's a paper that you can read that dates all the way back
to 2014 where for the first time by using slightly different techniques we
were able to put deep learning at work inside inside that that constrained real-time environment and start to have
impact and in that case around a pedestrian detection so as I said there
are a lot of analogies you can see that to properly drive that scene like Street
View you need to find you need to see the traffic light you need to understand if the light is red or green and that's
what that's what essentially will allow you to to be at processing obviously driving is even more challenging beyond the
real-time and if you saw the cyclist going through so you have air stuff happening on the scene that you need to
detect and properly understand interpret and predict and at the same time he expressed explicitly took a night
driving example to show you that while you can choose when you take pictures of
street view and do it in in data I mean perfect conditions driving requires you
to take the conditions that they are and you have to deal with it so there has
been for from the very early beginning there's been a lot of cross pollenization
between the real scene work so here I took a few papers that we did in Street
View that obviously if you read them you see directly apply to some of the stuff we do on the cars well obviously that
collaboration between Google research and wham-o historically went well beyond
studio only and across all the resort groups and that still is a very strong collaboration going on that enables us
to be to stay on the bleeding edge right off of what we can do so now that we we
looked a little bit at how things happened I want to spend more time and and go into more of the details of
what's going on in the cars today and how deep learning is actually impacting
our current system so I think during the if I looked at the cursors properly I
think during the week you went through the major pieces that that you need to master to make a self-driving car so I'm
sure you heard about mapping localization so putting the car within those maps and understanding where you
are with it's pretty good accuracy perception scene understanding which is
a higher-level semantic understanding of what's going on in the scene starting to predict what the agents are going to do
around you so that you can do better motion planning obviously it is a whole robotics aspect at the end of the day
the car in many ways acts like a robot whether it's around the sensor data or
even the control interfaces to the car and for every one was was dead with
Holloway on robotics you will agree with me that that it's not a perfect world
and you need to deal with with with those errors other pieces that we may
have talked about is around simulation and essentially validation of whatever
system you put together so obviously machine learning and the planning have been having a deep impact in a in a
growing set of those areas but for the next for the next minutes here I'm going
to focus more on the on the perception piece which is which is a core element of what the self-driving car needs to do
so what is what is perception so fundamentally set perception is assist
in a system in the car that needs to build an understanding of the world around around it and it does that using
two major inputs the first one is prior on the scene so
for instance to give you an example it would be a little silly to to have to recompute the actual location of the
road the actual interconnectivity of the intersections of every intersection when
once you get on the scene because those things you can pre-compute you can pre-compute in advance and save your
onboard computing for all the tasks that are more critical so really so that's
often referred to as the mapping exercise but really it's about reducing the computation you're going to have to
do on on the car watch once it drives the other big input obviously is what
sensors are going to give you once you get on the spot so since your data is the is the the signal that's going to
tell you what is not like what you mapped and the things is the traffic light right or green where where are the
pedestrians where are the cars what are you doing so as we saw on the initial picture we
have quite a set of sensors on our self-driving cars so they go from vision
systems radar and later how the other three big families of sensors we have
one point to note here is is that they are designed to be complimentary right
so they are designed to be complimentary first in there in the localization on the car so we don't put them in the same
spot because obviously blind spots is is a major issues and and and you want to
have good coverage of the field of view the other piece is that there are
complementary India capabilities it's so for instance to give you an example cameras are going to be very good to
give you a dance representation it's like it is very dense set of information
it contains a lot of semantic information right you can you can see you can really see
a big number of a large number of details but Francis they are not really good to give you depth or it's much
harder computer and computer additionally expensive to get depth
information out of camera systems so systems like a lidar for instance will give you very good very good
when you hit when you hit objects will give you a very good depth estimation but obviously they're going to lack a
lot of the cementing information that you will find on camera systems so all those sensors are designed to be
complimentary in terms of their capabilities it goes without saying that
the better your sensors are the better your perception system is gonna be right
so that's why at way more we we took the path of designing our own sensors
in-house and and and and enhancing what's available of the shell today
because it's important for us to go all the way to be able to build a
self-driving system that we could believe in and so that's what perception
does take those two inputs and build a representation of the scene right so at
the end of the day you have to realize that that in nature that work of
perception is really what differentiates deeply differentiates what you need to
do in a safe driving system as opposed to a lower lower level driving
assistance system in many cases France we do speed control speed cruise or if
you do a lot of lower lower level drug resistance a lot of the strategies can
be around not bumping into things if you see things moving around you you group
them you segment them appropriately in blocks of moving things and you don't hit them you're good enough in most
cases when you don't have a driver on the dragon seat obviously the challenge totally changes scale so to give you an
example for instance if you're if you're on the lane and and you see a bicyclist going small slowly on the right on the
under on the lane right of you and there's a car and next to you you need to understand that there's a chance that
that car is going to want to avoid that bicyclist is going to swerve and you need to anticipate that behavior so that
you can you can properly decide whether you want to slow down give space for the car or speed up and have the car go
behind you those are the kinds of behaviors that go well beyond not bumping into things and that require
much deeper understanding of the world are going that's going on around you so
let me put it in picture and and we come back to that example in a court case so here is a typical scene that we
encountered at least so so he obviously you have a police car pulled over
probably pulled over someone there you have a cyclist on the road moving
forward and we need to drive through that situation so the first thing you
can do you have to do obviously is the basics right so out of your sensor data understand that a set of point clouds
and pixels belong to the cyclist find that you have two cars on the scene the
police car and the car park in front of it understand the policeman as a pedestrian so basic level of
understanding obviously you need a little more than that you need to go deeper in your semantics obviously you
need if you understand that the the flashing lights are on you understand
that the police car is becoming an Eevee and and it's performing something on the
scene if you understand that this car is parked and we see this a variable piece of information that's going to tell you
whether you can pass it or not something you may have not noticed is that there
are so cones so there are cones here on the scene that would prevent you for instance to go and drag that pathway if
you wanted to next level of getting closer to behavior prediction
obviously if you if you also understand that actually the police car has an open
door then all of a sudden you can start to expect it behavior where someone is gonna get over that car right and and
the way you swerve even if you were to decide to swerve or the way someone getting up out of that car would impact
the trajectory of the cyclist is something you need to understand in order to properly and safely Drive and
only then only when you have that that depth of understanding you can start to come up with realistic behavior
predictions and trajectory predictions for all those agents in the in on the scene so that you can come up with a
proper strategy for your planning control so how is a deep learning
playing into that whole space and how he is a deep learning impacting used to
solve many of those problems so remember when I said when you're 90%
down you still have 90% to go so I think that's not that starts to beat us I also
talked about how robotics and having sensors in real life is not a perfect world so actually it is
a big piece of the puzzle so I wish sensors would give us perfect data all
the time and we would give us a perfect picture that we can do reality use to do a deep learning but unfortunately that's
not how it works so here for instance you see an example where you have a
pickup truck so the imagery doesn't show it but you have a smoke coming off the out of the
exhaust and you have exhaust that's triggering a light our laser points
right not very relevant for your for any behavior prediction or for your driving
behavior so those points obviously and it's safe to go and drive through them
all right so those are very safe to ignore in terms of sin understanding
right so filtering the whole whole bunch of data coming off your sensors is is a
very important task because that reduces the computation you're gonna have to do whether Sookie to do to operator safely
a most more subtle one but important one are around reflections so we are driving
a scene there's a there's a car here on the camera picture the car is reflected in a bus and if you just do naive
detection especially that if the bus goes moves along with you and everything
move which is very typical and everything moves then you can have all of a sudden thing and have two cars on the scene and and if you take that car
too seriously all the way to impacting your behavior obviously you're gonna make mistakes right so here I showed you
an example of reflections on the on the visual range but obviously that affects
all sensors in slightly different matters but you could have the same effect for instance with a light our data where for instance when you drive
you drive a freeway and you have a road sign on top of the freeway that will reflect in the back window of the car in
front of you right and then showing a reflected sign on the road you better
understand that the thing you see on the road is actually a reflection and not try to swerve around and trying to avoid
that thing on the only sixty five miles per hour trajectory
so that's a big that's a big complicated challenge but assume we are able to get
to a proper sensor data that we can start the process with our machine
running so by the way a lot of the a lot of the the signal processing PC is
actually already used machine learning and deep learning too because as you can see Francis in the reflection space you
need to at the end of the day you can do some tricks to understand the difference in the signal but at the end of the day
at some point for some of them you're gonna have to understand to have a higher level of understanding of the scene and realize it's not possible that
the car is hiding behind the bus and given my field of view for instance but assuming you have do the sensor data
filter I would sensor data the very next thing I want to do is typically is apply
some kind of convolution layers on top
of that of that imagery so follow if you're not familiar with convolution
layers so that's that's a very popular way to do computer vision because it
relies on on connecting neurons with kernels that are gonna run that are
gonna learn layer after layer features of the imagery right so those kernels
typically work locally on this on this on region of the image and they can understand how they can understand lines
they can understand contours and as you build up layers are going to understand higher and higher levels of future
representations that ultimately will tell you what's happening on the on the image that's a very common technique and
much more efficient we slid and fully connected layers for instance that wouldn't work but unfortunately a lot of
a lot of the state of the art is actually in 2d convolutions right so again they've been developed on on
imagery and typically they require a fairly dense input rights so for an
imagery a crate is great because pixels are very dense you always have a pixel next to the next one there is not a lot
of void if you were for instance to think if you were to to plain convolutions on on a very sparse
laser point Swensen then you would have a lot of holes and those don't work nearly as well so typically what we do
is to first project sensor data into 2d planes and do processing on those so two
very typical views that we use the first one is a top-down so broad view is going
to give you a Google Maps kind of view of the scene so it's great for instance to to map up to map cars and objects
moving along along the scene but they don't it's harder to put imagery pixels
imagery you saw from the car into those top-down views so there's another famous
one common one that that is the driver view it's a projection onto the the
plane from the driver's perspective that are much better at utilizing imagery
because this essentially that's how imagery imagery got captured my name media news drone so here for instance you're gonna see
how you can if if your sensors are properly registered you can use both
lidar and imagery signals together to better understand the scene so the first
the first kind of processing you can do is is is what is called their
segmentation so once you have pixels or laser points you need to group them
together into together into objects that you can that you can then use for better
understanding and processing so unfortunately a lot of the objects you encounter while driving don't have a
predefined shape so here are two example of snow but if you think about vegetation or if you think about trash
bags for instance you can't you can't come up with
prior understanding on how they're gonna look like and so you have to be ready to have any shape of those objects so the
one of the techniques that works pretty well is to to build a smaller
convolution network that you're gonna slide across across Europe the protection of your sensor data so that's
the sliding window approach so here for instance if you have if you have a pixel
accurate snow detector that you slide across the image then you'll be able to
build a representation of those patches of snow and drag appropriately around
them so that works pretty well but as you can imagine is a little expensive
computation computation because it's like the if follow if you remember I
know if you if you've seen them actually it's like the old the whole the matrix printing it's like you had a printer and
it had to go and print the page point-by-point all right so it was pretty well but it's pretty slow
obviously but it's very analogous to that but it works pretty good so so that
was pretty well but you need obviously you need to be very conscious on which area of the of the of the scene you want
to apply it to to to stay efficient fortunately many of the objects you you
need to care about have predefined priors so Francis if you take a car from the bird from the top down view from the
birds view it's gonna be a rectangle you can you can take that that shape prior into consideration in most cases even on
the on the lanes on the driving lanes they're gonna go in in similar directions whether whether they go
forward or they come the other way they're gonna go in the direction of the lanes same for address and streets so
you can use those priors to actually do some more efficient deep learning that
in the literature is its convener the ideas of single-shot multi box constants
so so here again you would start with the convolution towers but what you do only one pass of convolution it's like
it's the same difference between a dot matrix printer and and press right that
would print a page at once it's not an allergy but I think that conveys the idea pretty well so here you
would train a deep deep net that would directly take the whole projection of just sensor data and output boxes that
that encode the pores you have so here for instance I can show you how such a
thing would work for cone detection so you can see that we don't have all the fidelity of the per pixel cone detection
but we not really care about that we just need to know there is a cone somewhere and we take a box prior and
obviously what what that image is also meant to show is that since it's a it's
a lot cheaper computed computationally you can obviously run that on a pretty wide range of space and and even if you
have a lot of them that's still easy the city is going to be a very efficient efficient way to get to get that data so
we talked about the member the flashing lights on top of the police car so even
if you if you properly detect and segment cars let's say on the road many
cars are very special semantics so here in that on that slide I'm showing you many examples of evie emergency vehicles
that you need to visually to understand you need to understand first that it is an Eevee and to whether the Eevee is
active or not so school births are not actually emergency vehicles but obviously whether the bus has lights on
or the bus has a stop sign open on the side carry heavy semantics that you need
to understand so how do you deal with that back to the deep learning techniques one thing you could do is is
take that patch build a new convolution tower and be the classifier on top of
that and essentially build a school bus classifier a school bus with light sound
classifier a school bus with stop sign open classifier I'm pretty sure that would work pretty well but obviously it
would be a lot of work and and pretty pretty expensive to run on the car because we need to and convolution
convolution layers typically are the most expensive pieces of a neural net so
one better thing to do is to use to use embeddings so if you're not familiar with it
embeddings essentially are vector representations of objects that you can
learn with deep nets that will that really carry some semantic meaning of those objects so for instance you've
given given a vehicle you can build a vector that's gonna carry the
information that that vehicle is a school bus whether the lights are on whether the stop sign is open and then
you you're back into a vector space which is much smaller much more efficient that you can operate in to do
further further processing so those embeddings have been actually historically they've been more closely
associated with word embeddings so in a typical text if you were able to build
those vectors with word alt of words right so out of every word in a piece of text you'll be the vector that
represents the meaning of that world and then if you look at the sequence of those words and operate in the vector
space you start to understand the semantics of those sentences right so
one of the early projects that you can look at is called work to Veck which was which was done in a DNP group at Google
where they were able to beat such things and and and they discovered that that embedding space actually carried some
interesting vector space properties such as if you took the vector for king- the
vector for man plus the vector for women actually you ended up with a vector whether the closest word to that vector
would be Queen essentially right so so that's to show you how those those vector representations can be very
powerful in the amount of information you can they can contain let's talk
about pedestrians so we talked about semantics image segmentation remember so
the ability to go pixel by pixel for for things that that don't really have a shape we talked about using shape priors
but pedestrians actually combine the complexity of those of those two
approaches for many reasons one is that they
obviously they are deformable and pedestrians come with many shapes and
poses as you can see here I think here you have a guy on someone on the on the
skateboard crouching more more unusual poses that you need to understand and
the recall you need to have on pedestrian is very high and pedestrians show up in many different situations so
for instance here you know clearly pedestrians that you need to see because that's a good chance when you when you
do your behavior prediction that that person here is gonna jump out of a car I need to be ready for that so last but
not least predicting the behavior of pedestrian is really hard because they
move in any direction that car moving that direction you can safely bet connect it's gonna it's not a drastic keychain angle in in a moment's notice
right but if you take children for instance it's a little more complicated right so they may not pay attention they
may jump in any direction and you need to be ready for that so it's harder in terms of shape prior it's harder in
terms of recall and it's also harder in terms of prediction right then you need to have a fine understanding of the
semantics to understand that another example here is that we encountered is
you get to an intersection and you have a visually impaired person that's jaywalking on the intersection and you
obviously need to understand all of that to know that you need to yield to that person pretty clearly so person on the
road maybe you should yield to it to him not easy so for instance here so there
is actually I don't I don't know if it's a real person or a mannequin or something right so but here we go
something that frankly really looks like a pedestrian that you should probably classify the pedestrian but lying on the
on the bed of a pickup truck so and obviously you shouldn't yield to that
person right because if you if you were to and yielding to a pedestrian at 35 miles per hour for instance ease is
hitting the brakes pretty hard right and with with the risk of where we are we New York Region so obviously you need to
understand that that that person is travelling with a truck and he's not
actually on the road and it's okay to not hear - to him so those are examples
of the rich region of the semantics you need to understand obviously one way to do that is to start and understand the
behavior of things over time everything we talked about up until now in the how we use deep learning to solve some of
these problems was on a pure friend basis but understanding that that person is moving with the truck versus the
jaywalker in the middle of the intersection viously do that kind of information you can get to if you
observe the behavior of a time back to the embeddings so if you had vector if
you have vector representations of those objects you can start and track them over time so a common technique that you can use
to get there is to use a recurrent neural networks that essentially are networks that will build a state that
gets better and better as it gets more observation sequential observations of for your pattern right so for instance
coming back to the to the world's example I gave her earlier you can you see you have one word you see its vector
representation another one the sentence saying you understand more but what did some what the author is trying to say
third word fourth word at the end of the sentence you had a good understanding and you can start to translate Winston's
right so he has a similar idea if you if you understand if you have a semantic
representation and coding in an embedding for the pedestrian and the car under it and track that over time and
build a state you that that gets more and more meaning as time goes by you're
going to get closer and closer to the to a good understanding of what's going on in the scene right so my my point here
is those vector representation combined with recurrent neural networks is a
common technique that that can help you figure that out
back to the point when you're 90% done you still have 90% to go and so to get
to the last leg of my talk here today I want to give you some appreciation for
what it takes to truly build a machine learning system at scale and in
sterilize it so up till now we talked a lot about algorithms as I said earlier
algorithms have been a breakthrough and and the efficiency of those algorithms has been a breakthrough for us to
succeed the self-driving task but it takes a lot more than algorithms to
actually get there the first piece that
you need to 10x is ease around labeling efforts so a lot of the algorithms we
talked about are supervised meaning that even if you have a strong Network attack
sure and you come up with the right one there are supervised in the sense that you need you need to give in order to to
train that network you need to come up with a representative set high-quality set of label data that's gonna map some
input to predict the output you want it to predict right so that's a pedestrian that's a car that's a pedestrian that's
a car and and the network will learn in a supervised way how to build the right
representations so there's a lot obviously the unsupervised space is a very active domain of research our own
team of research at wham-o and collaboration with Google is around either on that domain but today a lot of
it still is revised so to give you orders of magnitude so here represented in a logarithmic scale
the size of a couple data sets so you may be familiar with image net which i think is the 15 million of such labels
range that guy jumping represents number of seconds from birth to collect
correlation pre-cutting suing and so that's that's kind of that's more of an
historical tidbit but the first member the find I
the hustle the street number on the facade problem so in the back in those
days it took us a multi billion label data set to actually teach the network
right so those were very early days today we do a lot more a lot better obviously but that's to give you an idea
of scale so being able to put to have labeling operations that produce large
and high quality label data sets is key for your success and that's a big piece of the puzzle you need to solve
so obviously today we do a lot more better not only we require less data but
we also can generate those data set much much more efficiently you can use
machine running itself to come up with labels and use operators and more importantly use ibrain models where you
use labels to to more and more fix the discrepancies or the mistakes and I'll
have to label the whole thing from scratch so combining so that's a whole space of active learning and stuff like that
combining those those techniques together obviously you can get you can get to completion faster it's very
common to still need so that in the minions minions range kind of same pose to train a robust solution another piece
is around computation compute computing power so again that's that's that's kind
of a historical tidbit around the street number models so here it's a detection
model and here is the transcriber model so obviously comparison is not is only
worth what it's worth here but if you look at number of neurons or number of connections per neuron which are two
important parameters of a Fenny neural net that gives you an idea of scale it's
obviously it's many orders of magnitude away from what the human brain can do but you start to be competitive in
invent in some cases in the in the Mon space right so again historical historical data but
the main point here is that you need a lot of computation and you need a you need to have access to a lot of
computing to either train or an infer those train models on real time on the
sea and that requires a lot of very robust engineering an
infrastructure development to get to those to those scales but Google is
pretty good at that and and obviously we at Wayne who have access to the Google infrastructure and tools to essentially
get there so I know if you heard so the way the way it's happening at Google is around a
tensorflow so maybe you've heard about about it as a moral programming language
to program machine learning and and
encode network architectures but actually tensorflow is also becoming or
is actually the whole ecosystem that can combine combine all those pcs together
and do machine learning at scale at Google my mo so it's as I said it's a
language that allow teams that allows teams to collaborate and work together that's a data representation in which
you can represent your your label data sets for instance or your training batches
that's a runtime that that that you can deploy on to Google Data Centers and you
need you need it's good that we have access to that computing power another piece is his accelerators so back in the
early days when we had CPUs to 1d planning models at scale which is less
efficient and over time GPUs came into the mix and and and Google is proactive
into developing very advanced set of hardware accelerators so you have heard
about GPUs tensorflow processing units which has which are proprietary chipsets
that rule deploys in its data centers that are you to train and infer more efficiently this deep learning models
and tensorflow is the glue that allows you to deploy at scale across those
those pcs very important piece to get there so it's nice you're smart you
build we build a smart algorithm we were able to collect enough data to to train
it great ship it well self-driving system is pretty
sophisticated and that's a complex system to understand and that's a complex system
that that requires extensive testing and I think the last leg that you need to
cover to do machine learning at scale and and with a high safety bar is around
your testing program so we have three legs that that we that we use to make
sure that we our machine running is ready for production one is around we are what driving another one is around
simulation and the last one is around a structured testing so I'll come back to that in terms of we are about driving
obviously there is no way around it if if you want to encounter situations and
see and understand how you behave you need to drive so as you can see the
driving at way mo has been accelerating over time still accelerating so we crossed three million minds driven back
in May 2017 and only six months later back in November we reached four million
so that's an accelerating pace obviously not every mind is equal and what you
care about are the mice that carry new situations and important situations so what we do obviously is driving in many
different situations so those mice got acquired across 20 cities many weather conditions and many
environments it's forming a lot so to give you another of magnitude so that's when about 60 times around the globe
okay even more importantly it's not to point it's hard to estimate that's
probably around 300 years of human driving equivalent all right so so in
that data set potentially you have 300 years of experience that your machine learning can tap into to learn to learn
what to do even more importantly is your
ability to simulate obviously the software changes regularly so if for
each new revision of the software you need to go and we drive four million miles it's not very practical it's going
to take a lot of time so the ability to to good enough simulation that you can
replay all those miles that you've driven in any new iteration of the software is key for you to decide if the
new version is ready or not even more important is your ability to to make
those mozzie more even more efficient and tweak them so here is a screenshot of an internal tool that we call a car
craft that essentially gives us the ability to fast or change the parameters
of the actual scene we've driven so what if the cars were doing in a slightly different speed
what if there was an extra car that that was on the scene what if a pedestrian crossed in front of the car so so you
can use the actual live on Mars as a base and then augment them into new
situations that you can test your drive again your sub running system against so
that's a very powerful way to actually drastically multiply the impact of any animal you drive and simulation is
another of those massive scales project that you need to cover so a couple
orders of magnitude here so using Google's infrastructure we have the ability to run a vehicle fleet of 25,000
cars 24/7 in data centers so those those are those are software stacks that
immolate the driving across either roll miles that we've driven or modified miles that help us understand the
behavior of a software so do you another of magnitude last year alone we drove 2.5 billion of those
miles in in data centers right so remember four million driven miles total all the way to 2.5 so that's three
orders of magnitude of expansion in your in your ability to truly understand how
the system behaves but there's still a long tail there's a whole tail or a long
tail of situations that will happen very rarely so the way we decided to tackle
those is to set up our own testing facility that is a mark of of a city and
driving situation so we do that in a in a 90 acre testing facility on former Air
Force Base in to California that we set up with traffic lights railroad crossings I mean
truly trying to reproduce a real-life situation and where we set up very
specific scenarios that we haven't necessarily encountered during our guitar driving that but that we want to
test and again feedback into the simulation we augment using the same illumination strategies and an inject
into our 2.5 billion miles driven so here I'm gonna show you two quick examples of such tests so here just just
have a cab back up as the self-driving car get gets close and see what happens and use all those sensor data to and we
inject them into simulation another example is going to be around people dropping boxes so remember try to
imagine the kind of understanding segmentation you need to do to understand to understand what's
happening there and cementing understanding you have and to make it even more interesting note that the car
that has been put on the other side so that swerving is not an option right without hitting the car alright so
driving complex situations that go from perception to motion planning the whole stack and make sure that we are really
ball even in those long time ignore that examples and we're done it looks like a
lot of work I wish but no actually we still have we still have a lot of very
interesting work coming someone have much time to go into too many of those details but I'm just gonna give you two
the directions the first one is around growing our what we call OD d so
operating operating the main operating design domain so extending extending our
our fleet of the driving cars not only geographically so draw graphically meaning going into deploying into urban
cores deploying into different weather conditions so just as of this morning on
yesterday we announced that we we're gonna grow testing in San Francisco for instance with way more
cars that bring urban environments slopes fog as I said and so that's
that's obviously a very very important direction that we need to go into and
where machine learning is going to keep playing a very important role another area is around the cementing
understanding so in case you have an obvious haven't noticed yet I'm from
France that's a that's a famous roundabout in Paris + delete well which
seems pretty chaotic but I've driven it many times without any issues touching
wood but I know that it took a lot of semantics and an understanding for me to
do it safely I have a lot I had a lot of expectations and what people do had a
lot of communication visual gestures to essentially get it get through that that
thing safely right so and those require a lot of a lot of a lot deeper semantic
understanding of the scene around - for self-driving costume to get through so that's an example of a direction so back
to my objectives I hope I covered many of those at least you have you have directions to for further reading and
investigations on those those three objectives I had I had today first one
was around context context of the space context of the history at Google in way mo and and how and how deep the roots
the roots are of the way back in time my second objective was to give you to tie
in some of the technical algorithmic solutions that you may have talked about during that class into the practical
cases we need to solve in the production system and that's been at least really emphasize the scale and the engineering
infrastructure work that needs to happen to really take such a project into into
attrition in a production system last tweet
that's a scene with a kids on jumping on bags and as Frogger
of course the scene and I think we have time for a few questions
[Applause]
since tend to fail at this intersection between perception and planning so your planner might assume something about a
perfect world that perception cannot deliver so what's wondering if you use the simulation environment also to induce
these perception failures or whether that's really specific for scenario you're testing and whether you have
other validation arguments for the perception side very good question so
one thing I didn't mention is that the simulator obviously enables you to simulate many different layers in a
stack and one of the one of the hardcore engineering problems is to actually properly design your stack so that you
can isolate and test independently like like any dear August piece of software you need to have to have good aps and
layers so we have we have such a layer in in our system between perception and
planning and the way you write the way we test perception is more by measuring the performance of your part of your
perception system across more of the real miles and and use and tweak the the
output of the perception system with its mistakes so having having good understanding of the mistakes it make and reproduce those mistakes
realistically in the new scenarios you would come up with a part of your simulator to realistically test your
planning side of the planning side of the house conceived at scale and product
produced at scale do you have a systematic way of creating the
architectures of the embedded system you have so many choices for sensors algorithms each problem you showed has
many different solutions that's gonna create different interfaces between each elements so how do you choose which
architecture you put in a car that's that's true for any complex software
stack so there's a combination of different things so the first thing obviously that
I didn't talk too much here but it's around the vast amount of research that
we do that way mo but also we do in collaboration with Google teams to
actually understand even what building blocks we have at we are at our disposals to to even play with right and
come up with those production systems the other piece is obviously the one you
decide to take all the way to production so you're right so the the two big
elements here I would say the first one and the main element Frank frankly is is in your ability to so that that's that
that search actually will takes a lot of people to get to right so something I
try to say is that to really part of the the second 90% is your ability to grow
your team and essentially grow the number of people will be able to productively participate in your
engineering project and and that's where the the robustness we need to bring into
our development environment our testing is really key to be able to grow that
that team has that the biggest scale and essentially explore all those paths and come up with the best one right and at
the end of the day the the robustness of testing is the judge that's what tells
you whether an approach works not it's not a philosophical philosophical debate
thank you for your talk so the car is making a decision at every single stop
time you know on direction and speed and part of the reason why you have the simulation is so that you can test that
those decisions in every every like possible scenario so once self-driving cars become you
know production ready and out on the streets do you expect that the decision will be made based on prior
understanding of every single situation with which is possible or can the car
make a new decision in real time based on its scene understanding and everything around it
so I at the end of the day it's the goal of the system is not to to build a
library that a library of events that you can reproduce by one by one and make
sure that you encode if the analogy machine learning would be overfitting it's like if you if you if you
encountered five situations I'm pretty sure you can hard code the perfect thing
you need to do in those five situations but the sixth one happened if you don't generalize actually is gonna fall
through so the really the complexity of what you need to do is extract the core
principles that make that make you safely drive and and have the algorithms
learn those principles rather than the specifics of any situation because as
you said the parameter space of a real scene is infinite okay so we try to
first that a little bit with with the simulator what if the cars went little faster or slower but the goal is not to
enumerate all possibilities and make sure we dwell on those but the goal is to bring more diversity to the learning
of those general principles that will be run by the system or will be coded in a system for for the car to behave
properly and generalized when a new system new situation occurs okay okay
fantastic talk one of the questions I had was you mentioned the difficulty of
identifying snow because they could come in many different shapes one things that I immediately thought of was I know was
just an urban legend but it was that urban legend about the Inuit having like 150 different words for snow and you
mentioned embeddings of objects do you think one possible approach might be to
create a much wider array of object embeddings for things like snow I mean
if you're many different types of snow could actually have pretty different
impacts on driving whether it be just like a flurry or if it were be Thea kind
of like a really heavy blizzard like we we just had yeah I think from
if you look at it from from an algorithmic point of view that that may
make sense but maybe something I'd like to emphasize a little more is the the
very hard line to walk is to walk the line of what's a greatly possible
weather so what computationally feasible in the car right I think so too
to two points on on your on your remark so if you had the processing power to
process every point or every every to that to a load level of understanding and had the computing power to do that
maybe that would be an approach but that's that would be very expensive and that's a hard thing to do even more
importantly having fine sense it wouldn't make sense to have a behavior prediction of every snowflake of the
things you see on the side of the road right then and you need to group that's the whole point of segmentation you need to group what you see into semantic
objects that are likely to exhibit exhibit behavior as a whole and reason
at that level of abstraction to have a meaningful semantic understanding that you need to drive essentially right so
yeah it's an in-between last question
make it a good one thanks for the talk so if you're using perception for your
scene understanding are you worried about like adversarial examples or things that have been demonstrated or do
you can't believe that this like a real-world attack that could be used for perception based systems so generally
speaking yeah I think I think even beyond even before your saw your attacks
errors I mean errors can happen right there and Harris happen in every mode so
I think a prime example of that which is not adversarial is the reflection case it's like you could as well have put a
sticker on the car on the bus and say you're confused do you think it's a car it's not the car but you don't need to
put a sticker on the bus it's like the real life already brings a lot of those examples right so it's really the way
out is to way the first one is to to have sensors that complement each other
all right so I try to emphasize that but really different sensors or different
systems are not going to make the same mistakes and so they're gonna complement each other and that's a very important piece of redundancy that will be built
into the system the other one is is also even in a refraction case is is isn't
the understanding so so the way you as a human wouldn't be fooled is because you understand and you know it's not it's
not a thing as that can happen the same way you know that Cal reflecting in the bus there's no way you can see through
the bus and of a real car behind it so that level of an of semantic understanding is what is what he's gonna
tell you what what what is true and what is not or what is a mistake an error in your stack right and so similar patterns
apply we'd like to thank you very much Sasha Anu for coming to MIT

----------

-----
--30--

-----
Date: 2018.02.14
Link: [# Ray Kurzweil: Future of Intelligence | MIT 6.S099: Artificial General Intelligence (AGI)](https://www.youtube.com/watch?v=9Z06rY3uvGY)
Transcription:

welcome to MIT course 6 s 0 9 9 artificial general intelligence today we
have Ray Kurzweil he is one of the world's leading inventors thinkers and
futurists with a 30-year track record of accurate predictions called the Restless
genius by The Wall Street Journal and the ultimate thinking machine by Forbes magazine he was selected as one of the
top entrepreneurs by Inc magazine which described him as the rightful heir to Thomas Edison PBS selected him as one of
the 16 revolutionaries who made America Ray was the principal investigator of
the first ccd flatbed scanner the first omni font optical character recognition the first point to speech reading
machines for the blind the first text-to-speech synthesizer the first music synthesizer capable of creating
the grand piano and other orchestral instruments and the first commercially marketed large vocabulary speech
recognition among his many honors he received a Grammy Award for outstanding
achievements in music technology he's the recipient of the National Medal of Technology was inducted into the
National Inventors Hall of Fame holds 21 honorary doctorates and honors from
three u.s. presidents Ray has written five national best-selling books
including the New York Times bestsellers The Singularity is near from 2005 and how to create a mind from 2012 he is
co-founder and Chancellor of singularity University and a director of engineering at Google heading up a team developing
machine intelligence and natural language understanding please give ray a warm welcome
[Applause] [Music]
it's good to be back I've been in this lecture hall many times and walked the
infinite Carter I came here as an undergraduate in 1965 within a year of
my being here they started a new major called computer science it did not get
its own course number that's 6 1 even biotechnology recently got its own
course number but how many of you are CS majors ok how many of you do work in
deep learning how many of you have heard of deep learning here I came here first
in 1952 when I was 14 I became excited
about artificial intelligence it had only gotten its name six years earlier
the 1956 Dartmouth conference by Marvin Minsky and John McCarthy so I wrote
Minsky a letter there was no email back then and he invited me up he spent all
day with me as if he had nothing else to do he was a consummate educator
I then and the AI field had already bifurcated into two warring camps the
symbolic school which Minsk II was associated with and the connectionist
school was not widely known in fact I think it's still not widely known that Minsk II actually invented the neural
net in 1953 but he had become negative about it largely because there was a lot
of hype that these giant branes could solve any problem
so the first popular neural nets the perceptron was being promulgated by
Frank Rosenblatt at Cornell so Minsky set out what are you going now and saying I said to see Rosenblatt at core
now is that don't bother doing that and I went there and Rosenblatt was touting
the perceptron that it ultimately would be able to solve any problem so I brought some printed letters that had
the camera and it did a perfect job of recognizing them as long as they were courier ten different types I didn't
work at all and he said but don't worry we can take the output of the perceptron or feed it as the input to another
perceptron and take the output of that and feed it to a third layer and as we add more layers it'll get smarter and
smarter and generalize and so that's interesting if you even tried that well no but it's high on our research agenda
things did not move quite as quickly back then as they do now he died nine
years later never having tried that idea turns out to be remarkably prescient I
mean he never tried multi-layer neural nets and all the excitement we see now
about deep learning comes from a
combination of two things both many layer neural Nets and the law
of accelerating returns which I'll get to a little bit later which is basically the exponential growth of computing so
that we can run these massive nets and handle massive amounts of data it would
be decades before that idea was tried several decades later three level neural
nets were tried there were a little bit better they could deal with multiple type styles still weren't very flexible
that's not hard to add other layers it's a very straightforward concept there was
a math problem the disappearing gradient or the exploding gradient which I'm sure
many of you are familiar with basically you need to take maximum advantage of
the range of values in the gradients and
not let them explode or disappear and lose the resolution that's a fairly
straightforward mathematical transformation with that insight we could now go 200 layer neural nets and
that's behind sort of all the fantastic gains that we've seen recently
alphago trained on every online game and
then became a fair go player it then trained itself by playing itself and
soared past the best human alphago zero started with no human input at all
within hours of iteration sort Pascal
phago also soared past the best just programs they had another innovation
basically you need to evaluate the quality of the board at each point they used another hundred layer neural nets
to do that evaluation so there's still a
problem in the field which is there's a motto that life begins at a billion
examples one of the reasons I'm at Google is we have a billion examples for examples of
pictures of dogs and cats that are labeled so you got a picture of a cat and it says cat and then you can learn
from it and you need a lot of them alphago trained on a million online
moves that's how many we had of master games and that only created a sort of
fair go player a good amateur could defeated so they worked around that in
the case of go by basically generating an infinite amount of data by having the
system play itself had a chat with Denver's house office you know what kind
of situations can you do that with you have to have some way of simulating the world so go or chess are even though go
is considered a difficult game it's a-you know the definition of it can exist on one page so you can simulate it
that applies to math I mean amass axioms are can be contained on a page or two
it's not very complicated it gets more difficult when you have real-life situations like biology so we have
biological simulators but the simulators on perfect so learning from the simulators will only be as good as the
simulators that's actually the key to being able to do deep learning on biology
autonomous vehicles you need real-life data so the way mo systems have gone
three and a half million miles that's good that's enough data to then create a very good simulator so the
simulator is really quite realistic because they had a lot of real-world experience and the they've got a billion
miles in the simulator but we don't always have that opportunity to either
create the data or have the data around humans can learn from a small number of
examples your significant other your professor your boss your investor can
tell you something once or twice and you might actually learn from that some humans have been reported to do that
and that's kind of the remaining advantage of humans now there's actually
no back propagation in the human brain it doesn't use deep learning it uses a
different architecture that same year in 1962 I wrote a paper how I thought the
human brain worked there was actually very little neuroscience to go on there was one neuroscientist Vernon mount
Castle that had something relevant to say which as he did I mean there was a the common wisdom at the time and
there's still a lot of neuroscience that says say this that we have all these different regions of the brain they do different things they must be different
there's v1 in the back of the head where the optic nerve spills into that can
tell that that's a curved line that that's a straight line does these simple
feature extractions on visual images it's actually a large part of the neocortex does the fusiform gyrus up
here which can recognize faces we know that because if it gets knocked out
through injury or stroke people can't recognize faces they will learn it again with a different region of the neocortex
is the famous frontal cortex which does language in poetry and music so these
must work on different principles he did autopsies on the neocortex and all these different regions and found they all
looked the same they had the same repeating pattern same interconnections he said neocortex is neocortex so I had
that hint otherwise I can actually observe human brains in action which I
did from time to time and there's a lot of hints that you can get that way for
example if I ask you to recite the alphabet you actually don't do it from A to Z you do it as a sequence of
sequences ABCD efg hijk so we learn
things that secret forward sequences of sequences forward because if I ask you to recite the alphabet backwards you
can't do it unless you learn that as a new sequence so these are all interesting hints I wrote a paper that I
that the neocortex is organized as a hierarchy of modules in each module can
learn a simple pattern and that's how I got to meet President Johnson and that
initiated a half-century of thinking about this issue I came to MIT to study
with Marvin Minsky actually came for two reasons one the Minsky became my mentor
which was a mentorship that lasted for over 50 years the fact that MIT was so
advanced it actually had a computer which the other colleges I considered
didn't have it was an IBM 7090 for 32 K
of 36 bit words so it's 150 K of course storage to microsecond cycle time two
cycles for instructions or a quarter of a myth and that thousands of students
and professors shared that one machine in 2012 I wrote a book about this thesis
is now actually an explosion of neuroscience evidence to support it the European brain reverse engineering
project has identified a repeating module about a hundred neurons it's repeated three hundred million times
it's about 30 billion neurons in the neocortex the neocortex is the outer layer of the brain that's part where we
do our thinking and they can see in each module axons coming in from another
module and then the output acts the single output accent of that Jil goes as the input to another module
so we can see it organized as a hierarchy it's not a physical hierarchy
it's the hierarchy comes from these connections the neocortex is a very thin structure it's actually one module thick
there's six layers of neurons but it constitutes one module and we can see
that it learns in simple pattern and various reasons I cite in the book the
pattern recognition model that's using is basically a hidden Markov model how
many of you have worked with Markov models okay
that's usually no hands go open I asked that question but Markov model is not it
is learned but it's not back propagation it can learn local features so it's very
good for speech recognition and the speech recognition network I did in the 80s used these Markov models that became
the standard approach because it can deal with local variations so the fact
that a vowel is stretched you can learn that in a Markov model it doesn't learn
long distance relationships that's handled by the hierarchy and something
we don't fully understand yet is exactly how the neocortex creates that hierarchy but we have figured out how it can
connect this module to this module does it then grow I mean there's no virtual
communication or wireless communication it's actually connection so does it grow
an axon you know from one place to another which could be inches apart
actually they all all these connections are there from birth like the streets
and avenues of Manhattan there's vertical and horizontal connections so if the it decides and how it makes that
decision it's still not fully understood that it wants to connect this module to this module there's already a vertical
horizontal and a vertical connection it just activates them we can actually see
that now and I can see that happening in real time on non-invasive brain scans
so there's a current amount of evidence that's in fact the neocortex is a hierarchy of modules that can learn each
module learns a simple sequential pattern and even though the patterns we
perceived don't seem like sequences they may seem three-dimensional or even more complicated they are in fact represented
as sequences but the complexity comes in with the hierarchy so the neocortex
emerged 200 million years ago with mammals all mammals have a neocortex
it's one of the distinguishing features of mammals these first mammals were
small they were rodents but they were capable a new type of thinking other
non-mammalian animals had fixed behaviors but those fixed behaviors were very well adapted for their ecological
niche but these new mammals could invent a new behavior so creativity and
innovation was one feature of the neocortex so a mouse is escaping a predator its usual escape path is
blocked it will invent a new behavior to deal with it probably wouldn't work but
if it did work it would remember it and would have a new behavior and that behavior could spread virally through the community another Mouse watching
this was with say to itself that was really clever going around that rock I'm gonna remember to do that and it would
have a new behavior didn't help these early mammals that much because as I say
the non-mammalian animals were very well adapted to their niches and nothing much
happened for a hundred and thirty five million years but then 65 million years
ago something did happened there was a sudden violent change to the environment we now call it the Cretaceous extinction
event there's been debate as to whether it was a media or an asteroid I mean a
meteor or a volcanic eruption the asteroid or meteor hypothesis is in the
ascendancy but if you dig down to an area of rock reflecting 65 million years
ago the geologists will explain that it shows a very violent sudden change to the
environment we see it all around the globe so is a worldwide phenomenon the
reason we call it an extinction event is that's when the dinosaurs went extinct
that's when 75% of all the animal and plant species went extinct and that's
when mammals overtook their ecological niche so to anthropomorphize biological
evolution said to itself this neocortex is pretty good stuff and it began to grow it so-now mammals got bigger their
brains got bigger at an even faster pace taking up a larger fraction of their body the neocortex got bigger even
faster than that and developed these curvatures that are distinctive of a primate brain basically to increase its
surface area but if you stretched it out the human neocortex is still a flat structure it's about the size of a table
napkin just as thin and it's basically
created primates which became dominance in their ecological niche then something
else happened two million years ago biological evolution decided to increase the neocortex further and increase the
size of the enclosure and basically filled up the frontal cortex with our big skulls with more neocortex and up
until recently it was felt that as I said that this was the frontal cortex was different because it does these
qualitatively different things but we
now realize that it's really just additional neocortex so remember what we
did with it we're already doing a very good job of being primates so we put it at the top of the neocortical hierarchy
and we increased the size of the hierarchy it was maybe 20% more
neocortex but it doubled it tripled the number of levels because as you go up the hierarchy it's kind of like a
pyramid there's fewer and fewer modules and that was the enabling factor for us
to invent language and art music every human culture we've ever discovered has
music no primary culture really has music there's debate about that but it's really true
invention technology technology required another evolutionary adaptation which is
this humble appendage here no other animal has that if you look at a chimp and see it looks like they have a
similar hand but the thumb is actually down here doesn't work very well if you watch them trying to grab a stick so we
could imagine creative solutions yeah I could take that branch and strip off the
leaves and put a point on it and we could actually carry out these ideas and
create tools and then use tools to create new tools and it started a whole nother evolutionary process of
tool-making and that all came with the with the neocortex
so Larry Page read my book in 2012 and
liked it so I met with him in Essen for an investment in a company I'd started
actually a couple weeks earlier to develop those ideas commercially because that's how I went about things as a
serial entrepreneur and said well we'll invest but let me
give you a better idea what you do it here at Google we have a billion pictures of dogs and cats and we've got
a lot of other data and lots of computers and lots of talent all of which is true and says well I don't know
I just started this company to develop this is well by your company and how you
got a value a company that hasn't done anything just started a couple weeks ago and he said we can value anything so I
took my first job five years ago and I've been basically applying this model
this hierarchical model to understanding
language which i think really is the holy grail of AI I think Turing was
correct in designating basically text communication as what we now call a
turing-complete problem that requires there's no simple NLP tricks it you can apply to pass a valid Turing test with
an emphasis on the word valid mitch kapor and i had a six month debate on
what the rules should be because if you read Turing's 1950 paper he describes
this in a few paragraphs and doesn't really describe how to go about it but if it's a valid Turing test meaning it's
really convincing you through an interrogation and dialogue that it's a
human that requires a full range of human intelligence and I think that test
has to the test of time we're making very good progress on that I mean just
last week you may have read that two systems
asked paragraph comprehension test it's really very impressive winning came to
Google we were trying to past these paragraph comprehension tests we aced the first the first grade test second
grade tests were kind of got average performance and the third grade test had too much inference already you had to
know some common-sense knowledge as it's called and make implications of things
that were in different parts of the paragraph and there's too much inference and it really didn't didn't work so this
is now adult level it's just slightly surpassed average human performance but
we've seen that once something an AI does something it average human levels
it doesn't take long for it to soar past average human levels I think it'll take longer in language and it did in some
simple games like go but it's actually very impressive that it surpasses now
average human performance used at LST M long short temporal memory but if you
look at the adult test in order to answer these questions it has to put together inferences and implications of
several different things in the paragraph with some common sense knowledge is not explicitly stated so
that's I think a pretty impressive milestone so I I've been developing I've
got a team of about 45 people and we've been developing this hierarchical model
we don't use Markov models because we can use deep learning for each module
and so we create an embedding for each word and we create an embedding for each sentence this is we have a I can talk
about it because we have a published paper on it it can take into consideration context
if you use smart reply on G confused email on your phone you'll see it gives
you three suggestions for responses that's called Smart reply there are
simple suggestions but it has to actually understand perhaps a
complicated email and the quality of the suggestions is really quite good quite
on point that's for my team using this kind of hierarchical model so instead of
Markov models that uses embeddings because we can use back propagation we
might as well use it but I think what's missing from deep learning is this
hierarchical aspect of understanding because the world is hierarchical that's why evolution developed a hierarchical
brain structure to understand the natural hierarchy in the world
and there are several problems with big deep neural nets one is the fact that
you really do need a billion examples and we don't sometimes we can generate them it's in the case of NGO or if we
have a really good simulator as in the case of autonomous vehicles not quite the case yet in biology very often you
don't have a billion example if you suddenly have billions of examples of language but they're not annotated and
how would you annotate it anyway with more language that we can't understand in the first place so that's kind of a chicken and an egg problem so I believe
this hierarchical structures needed another criticism of deep neural Nets they don't explain themselves very well
it's a big black box that gives you pretty remarkable answers I mean in the
case of these games demos described it's playing in both go and chess is almost
an alien intelligence because we do things that were shocking to you and experts like sacrificing a queen and a
bishop at the same time or in close succession which shocked everybody but
then went on to win or early in a go game putting a piece at the corner of the board which is kind of crazy to most
experts because you really want to start controlling territory and yet it on reflection that was the brilliant move
that enabled it to win that game but it doesn't really explain how it does these
things so if yeah if you have a hierarchy it's much better at explaining it because you could look at the content
of the of the modules in the hierarchy and they'll explain what they're doing
and just and on the first application of
applying this to health and medicine this will get into high gear and we're going to really see us break out at the
linear extension to longevity that we've experienced I believe we're only about a
decade away from longevity escape velocity we're adding more time than is
going by not just the infant life expectancy but to your remaining life expectancy I think if someone is
diligent they can be there already I think I've at longevity escape velocity now a word
on what life expectancy means it used to be assumed that not much would happen so
whatever your life expectancy is with or without scientific progress it really
didn't matter now it matters a lot so life expectancy really means you know how long would you live what's the in
terms of a statistical likelihood if there were not continued scientific progress but that's a very inaccurate
assumption that scientific progress is extremely rapid I mean just as an AI in biotech there are advances now every
week is quite stunning now you can have a computed life
expectancies let's say 30 years 50 years 70 years from now you can still be hit
by the proverbial bus tomorrow we're working on that with self-driving vehicles but we'll get we'll get to a
point I think if you're diligent you can be there now in terms of basically advancing your own statistical life
expectancy at least to keep pace with the passage of time I think it would be there for
most of the population at least if they're diligent within about a decade
so if we can hang in there we may get to see the remarkable century ahead thank
you very much no question please raise
your hand we'll get your mic hi
so you mentioned both neural neural network models and symbolic models and I
was wondering how far have you been thinking about combining these two approaches creating a symbiosis between
neural models and symbolic ones I don't
think we want to use symbolic models as they've been used how many are familiar
with the psych project that was a very diligent effort in Texas
to define all of common-sense reasoning and it kind of collapsed on itself and
became impossible to debug because you fix one thing and it break three other
things that complexity ceiling has become typical of of trying to define
things through logical rules now it does seem that humans can understand logical
rules we have logical rules written down for things like law and game playing and
so on but you can actually define a connectionist system to have such a high
reliability on a certain type of action that it looks like it's a symbolic rule
even though it's represented in a connectionist way and connection systems
can both capture the soft edges because many things in life are not sharply
defined they can also generate exceptions so you you don't want to
sacrifice your queen in chess accept certain situations that might be a good idea so you can capture that kind of
complexity so we do want to be able to learn from accumulated human wisdom that
looks like it's symbolic but I think we'll do it with a connection system but
again I'm think the connection systems should develop a sense of hierarchy and
not just be one big massive neural net so I understand how we want you know use
the neocortex to extract useful stuff and commercialize that but I'm wondering how you know our middle brain and organs
that are below the neocortex will be useful for you know turn that into what
you want to do something well the cerebellum is an interesting case in point it actually has more neurons than
the neocortex and it's used to govern most of our behavior some things
if you write a signature that's actually controlled by the cerebellum so a simple sequence is stored in the cerebellum but
there's not many reasoning to it it's basically a script and most of our
movement now has actually been migrated from the center vellum to the neocortex cerebellum is still there some people
the entire cerebellum is destroyed through disease they still function
fairly normally their movement might be a little erratic as our movements is
largely controlled by the neocortex but some of the subtlety is a kind of pre-programmed script and so they'll
look a little clumsy but they're actually function okay a lot of other
areas of the brain control autonomic functions like breathing and but our
thinking really is is controlled by the neocortex in terms of mastering
intelligence I think the neocortex is the brain region we want to study I'm
curious what you think might happen after the singularity is reached in
terms of this exponential growth of information yes do you think it will
continue or will there be a whole paradigm shift what do you predict well
in the singularities near I talked about the atomic limits based on molecular
computing as we understand it and it can actually go well past 2045 and actually
go to trillions of trillions of times greater computational capacity than we have today
so I don't see that's stopping anytime soon and we'll go you know way beyond
what we can imagine and it becomes an
interesting discussion what the impact on human civilization will be so take it
may be slightly more mundane issue that comes up as a kind of eliminates most
jobs or jobs a point I make is it's not the first time in human history you've done
that how many jobs circa 1900 exist today and that was the feeling of the
Luddites which was an actual society that formed in 1800 the automation of the textile industry in England they
looked at all these jobs going away and felt that employment is going to be just limited to an elite indeed those jobs
didn't go away but new jobs were created so if I were oppression Futures to 1900
I would say well 38% of you work on farms and 25% work in factories it's 2/3
of the working force but I predict by
2015 115 years from now it's going to be 2% on farms and 9% factories and
everybody would go oh my God we're gonna be out of work and I said well don't worry for all these jobs we eliminate
through automation we're gonna invent new jobs and say oh really what new jobs and I'd say well I don't know we haven't
invented them yet that's the political problem we could see jobs very clearly
going away fairly soon like driving a car or truck and the new jobs haven't
been invented I mean just look at the last five or six years as many a lot of the increase in employment has been
through mobile app related types of ways of making money that just weren't
contemplated even six years ago if I really prescient I would say well you're
gonna get jobs creating mobile apps and websites and doing data analytics and
self-driving cars cars what's a car and nobody would have any idea what I'm
talking about now the new job some people say yeah we created new jobs
but it's not as many actually we've gone from 24 million jobs in nineteen hundred
242 million jobs today for 30 percent of the population to forty five percent of the population the new jobs pay eleven
times as much in constant dollars and they're more interesting and as I talk to people starting out their career now
they really want a career that gives them some life definition and purpose and gratification we're moving up Maslow's
hierarchy hundred years ago you were happy if you had a back-breaking job to put food on your family's table so and
we couldn't do these new jobs without enhancing our intelligence so we've been
doing that well for most of the last 100 years through education we've expanded
to K through 12 and constant dollars tenfold we've gone from 38,000 college students
in 1870 to 15 million today more
recently we have brain extenders and not yet connected directly in our brain but
they're very close at hand when I was here that my ta to take my bicycle across campus to get to the computer and
show an ID to get in the building now we carry them well you know in our in our
pockets and on our belts they're going to go inside our bodies and brains I think that's a notic really
important distinction but so we're basically going to be continuing to enhance our capability through merging
with AI and that's the I think ultimate answer to the kind of dystopian view we
see in futures movies where it's the AI versus a brave band of humans for control of humanity we don't have one or
two a eyes in the world today we have several billion three billion smartphones and last count will be six
billion in just a couple of years according to the projections so we're already deeply integrated with this and
I think that's going to continue and it's gonna continue to do things that you can't even imagine today just as we
are doing today things we couldn't imagine you know even twenty years ago you showed many graphs that goes through
exponential growth but I haven't seen one that isn't so I would be very interested in hearing you haven't seen
that what that is not exponential so tell me about regions that you've
investigated that have not seen exponential growth and why do you think that's the case well
price performance and capacity of information technology invariably follows a exponential when it impacts
human society it can be linear so for example the growth of democracy has been
linear but still pretty steady you can count the number of democracies on the fingers of one hand a century ago two
centuries ago you can count the number of democracies in the world on the fingers of one finger now there are
dozens of them that this and it's become kind of a consensus that that's how we should be governed
so the and I attributed all this to the growth and information technology
communication in particular for progression of social cultural
institutions but information technology because it ultimately depends on a
vanishingly small energy and material requirement grows exponentially and will
for a long time there's recently a criticism that well test scores have
it's actually a remarkably straight linear progression so humans think it's
like twenty eight hundred and it just sort passed out in 1997 with the blue and it's kept going and remarkably
straight and saying well this is linear not exponential but the chess score is a logarithmic measurement so it really is
exponential progression so if you're lhasa furs like to think a lot about the
meaning of things especially in the 20th century so for instance Martin Heidegger gave a couple of speeches and lectures
on the relationship of human society to technology and he particularly distinguished between the mode of
thinking which is calculating thinking and a mode of thinking which is reflective thinking or meditative
thinking and he posed this question what is the the meaning and purpose of
technological development and he couldn't find an answer he he recommended to remain open to what he
called and he called this an openness to the mystery I wonder whether you have
any thoughts on this is there is there a meaning of purpose to technological equipment and and is there a way for a
human success access that meaning well
we started using technology to shore up weaknesses and our own capabilities so
physically I mean who here could build this building so we've leveraged the power of our muscles with machines
and we're in fact very bad at doing things that you know the simplest computers can do like factor numbers or
even just multiply two eight digit numbers computers can do that trivially
we can't do it so we originally started using computers to make up for that
weakness I think the essence of what I've been writing about is to master the
unique strengths of humanity creating loving expressions in poetry and music
and the kinds of things we associate with the better qualities of humanity
with machines that's the to promise of AI that we're not there yet but we're
making pretty stunning progress just in the last year there's so many milestones that are really significant including in
language and but I think of technology
as an expression of humanity it's part of who we are and the human species is
already a biological technological civilization and it's part of who we are
an AI is it's part of humans so AI is
human and it's it's part of the technological expression of humanity and
we use technology to extend our reach you know I couldn't reach that fruit at
that higher branch a thousand years ago so we invented a tool to extend our physical reach we now extend our mental
reach we can access all of human knowledge with a few keystrokes and
we're going to make ourselves literally smarter by merging with AI hi
first of all honor to hear you speak here so I first read The Singularity is
near nine years ago or so and it changed the way I thought entirely but something
I think it caused me to over steeply discount was tail risk in geopolitics in
systems that span the entire globe and my concern is that there are there is
obviously the possibility of tail risk existential level events swamp in all of
these trends that are otherwise war proof climate proof you name it so my
question for you is what steps do you think we can take in designing
engineered systems in designing social and economic institutions to kind of
minimize our exposure to these tail risks and and and survive to make it to
UM you know a beautiful mind filled future yeah well the world was first
introduced to a human-made existential risk when I was in
elementary school we would have these civil defense drills to get under our desk and put our hands behind our head
to protect this from a thermonuclear war and it worked we made it through but
that was really the first introduction to an existential risk and those weapons
are still there by the way and they're still on a hair-trigger and they don't
get that much attention there's been a lot of discussion much of which I've
been in the forefront of initiating the existential risks of what sometimes referred to as GN rg4 genetics which is
biotechnology and for nanotechnology and gray goo robotics which is a
and I've been accused of being an optimist I think you have to be an
optimist to be an entrepreneur if you knew all the problems you were going to encounter you'd never start any project
but I've written a lot about the downsides I remain optimistic there are
specific paradigms and not foolproof that we can follow to keep these technologies safe so for example over 40
years ago some visionaries recognized the revolutionary potential both for
promise and peril of biotechnology neither the promise no peril was
feasible 40 years ago but they had a conference at the Asilomar conference
center in California and to develop both professional ethics and strategies to
keep biotechnology safe and they've been known as the Asilomar guidelines they've been refined through successive sell
more conferences much of that's baked into law and it in my opinion it's
worked quite well we're now as I mentioned getting profound benefit it's a trickle today it'll be a flood over
the next decade and the number of people who have been harmed either through intentional or accidental abuse of
biotechnology so far zero actually I take that back there was one boy who died in gene therapy trials but 12 years
ago and there's congressional hearings and they cancelled all research for gene
therapy for a number of years you could do an interesting master's thesis and demonstrate that you know 300,000 people
died as a result of that delay but you can't name them they can't go on CNN so we don't know who they are but it has to
do with the balancing of risk but in large measure virtually no one has been
hurt by biotechnology now that doesn't mean you can cross on our front list okay we took care of that one because
the technology keeps getting more sophisticated and Christopher's great opportunity there's hundreds of trials
of Christopher's technologies overcome disease but it could be abused you can
describe scenarios so we have to keep reinventing it January we had our first
Asilomar conference on AI ethics and so I think this is a good paradigm it's not
foolproof I think the best way we can
assure a democratic future that includes our ideas of Liberty is to practice that
in the world today because the future world of the singularity which is a merger of biological non-biological
intelligence it's not going to come from Mars I mean it's going to emerge from our society today so if we practice
these ideals today it's going to have a higher chance of us practicing them as we get more enhanced with technology if
that doesn't sound like a foolproof solution it isn't but I think that's the best approach in terms of technological
solutions I mean AI is the most daunting you can imagine there are technical solutions to
biotechnology and nanotechnology there's really no subroutine you can put in your
AI software there will assure that it remains safe intelligence it's
inherently not controllable there's some AI that's much smarter than you that's out for your destruction the
best way to deal with that is not to get in that situation in the first place if
if you are in that situation and find some AI that will be on your side but
basically it's going to eyeb Aleve we have been headed through technology
to event to a better reality look around the world and people really think things
are getting worse and I think that's because our information about what's wrong with the world is getting
exponentially better I say oh this is the most peaceful time on you in history if you say what are you crazy didn't you
hear about the event yesterday and last week and well a hundred years ago there
could be a battle that wiped out the next village in you wouldn't even hear about it for months of all these graphs on education and
literacy has gone from like 10% to 90% over a century and health wealth
poverty's declined 95% in Asia over the last 25 years document about the World
Bank all these trends are very smoothly getting better and everybody thinks things are getting worse but but but
you're right like on violence that curve could be quite disrupted there's an
existential event as I say I'm optimistic but I think that is something
if we need to deal with that a lot of it is not technological it's dealing with
our social cultural institutions so you
mentioned also exponential growth of software and IDs I guess related to software so one of the reasons for which
you said that all that information technology costs this exponential is because of fundamental properties of
matter and energy but in the case of ideas why would it have to be exponential well a lot of ideas produce
exponential gains they don't increase performance linearly there's actually
study during the Obama administration by his scientific advisory board on
assessing this question how much gains on 23 classical engineering problems
were gained through hardware improvements over the last decade and
software improvements and there's about a thousand to one improvement it's about doubling every year from Hardware there
was an averages of like twenty six thousand to one through softer improvements algorithmic improvements so
we do see both and apparently if you come up with in advance its it doubles
the performance or multiplies it by ten we see basically exponential growth from each innovation
so and we certainly see that in deep learning the architectures are getting
better while we also have more data and more computation and more memory to throw in these at these algorithms
thank you for being

----------

-----
--29--

-----
Date: 2018.02.08
Link: [# MIT AGI: Building machines that see, learn, and think like people (Josh Tenenbaum)](https://www.youtube.com/watch?v=7ROelYvo8f0)
Transcription:

today we have Josh Tenenbaum he's a professor here at MIT leading the computational cognitive science group
among many other topics and cognition and intelligence he is fascinated with
the question of how human beings learn so much from so little and how these
insights can lead to build AI systems that are much more efficient at learning from data so please give Josh a warm
welcome all right thank you very much
thanks for having me decided to be part of what looks like really quite a very impressive lineup especially starting
after today and it's I think quite a great opportunity to get to see perspectives on artificial intelligence
from many of the leaders in industry and other entities working on this this
great quest so I'm going to talk to you about some of the work that we do in our group but also I'm gonna try to give a
broader perspective reflective of a number of MIT faculty especially those who are affiliated with the Center for
brains minds and machines so you can see up there on my affiliation academically I'm part of brain and cognitive science
or course nine I'm also part of csail but I'm also part of the Center for brains minds and machines which is an
NSF funded Center Science and Technology Center which really stands for the bridge between the science and the
engineering of intelligence it literally straddles Vassar Street and that we have csail and DCs members we
also have partners at Harvard and other academic institutions and again what we stand for I want to try to convey some
of the specific things we're doing in the center and where we want to go with a vision that really is about jointly
pursuing the science the basic science of how intelligence arises in the human
mind and brain and also the engineering enterprise of how to build something increasingly like human intelligence in
machines and we deeply believe that these two projects have something to do with each other and our best pursued
jointly now it's really exciting time to be doing anything related to intelligence or certainly to AI for all
the reasons that you know brought you all here I don't have to tell you this we have all these ways in which AI is kind of finally here we finally live in
the era of something like real practical AI or for those who've been around for a
while and have seen some of the rises and falls you know AI is back in a big way but from my perspective and I think
maybe this reflects you know why we distinguish what we might call a GI from AI we we don't really have any real AI
basically we have what I like to call AI technologies which are systems that do things we used to think that only humans
could do and now we have machines that do them often quite well maybe even better than any human who's ever lived
right like a machine that plays go but none of these systems I would say are truly intelligent none of them have
anything like common sense none of them have anything like the flexible general-purpose intelligence that each
of you might use to learn every one of these skills or tasks right each of these systems had to be built by large
teams of engineers working together often for a number of years out often at great cost to somebody who's willing to
pay for it and each of them just does one thing so alphago might beat the worlds best but it can't drive to the
match or even tell you that go it what go is it can't even tell you the go is a
game because it doesn't even know what a game is right so what's missing why what what is it that makes every one of your
brains maybe you can't beat you know the world's best didn't go but any one of you can get behind the wheel of a car I
think of this because my daughter is gonna turn 16 tomorrow if she lived in California she'd have a driver's license
it's a little bit down the line for us here in Massachusetts but you know she
didn't have to be specially engineered by billion dollar startups and you know she got really into chess recently and
now she's taught herself chess by playing just you know a handful of games basically I mean she can do any one of
these activities and any one of us can so what is it what's that what makes up the difference well there's many things
right I'll talk about the the focus for us and our research and a lot of us
again in CBMM is summarized here um what what drives the success is right now in
AI especially in industry okay and all these AI technologies is many many things many things but what's what where
the progress has been made most recently and what's getting most of the attention is of course deep learning but other
kinds of machine learning technologies which essentially represent the maturation of a decades-long
for to solve the problem of pattern recognition that means taking data and finding patterns in the data that tells
you something you care about like how to label a class or how to predict some other signal okay
and pattern recognition is great it's an important part of intelligence and it's reasonable to say the deep learning as a
technology has really made great strides on pattern recognition and maybe even you know has coming close to solving the
problems of pattern recognition but intelligence is about many other things intelligence is about a lot more in
particular it's about modeling the world and think about all the activities that
a human does so model the world that that go beyond just say recognizing patterns and data but actually trying to
explain and understand what we see for instance okay or to be able to imagine things that we've never seen that never
seen maybe even very different from anything we've ever seen but might want to see and then to meet to set those as
goals to make plans and solve problems needed to make those things real or thinking about learning again the you
know some kinds of learning can be thought of as pattern recognition if you're learning sufficient statistics or weights in a neural net that are used
for those purposes but many activities of learning are about building out new models right either refining reusing
improving old models or actually building fundamentally new models as you've experienced more of the world and then think about sharing our models
communicating our models to others modeling their models learning from them all these activities of modeling these
are at the heart of human intelligence and it requires a much broader set of tools so I want to talk about the ways
we're studying these activities of modeling the world and something in a pretty non-technical way about what are
the kind of tools that allow us to capture these abilities now I think it's I want to be very honest up front and to
say this is just the beginning of a story right when you look at deep learning successes that itself is a
story that goes back decades I'll say a little bit about that history in a minute but where we are now is just looking forward to a future when we
might be able to capture these abilities you know at a really mature engineering scale and I would say we are far from
being able to capture the all the ways in which humans richly flexibly quickly build models of the world at the kind of
scale that say Silicon Valley wants either big tech companies like Google or soft or IBM or Facebook or small
startups right we can get there and I think what what I want to talk to you
about here is one route for trying to get there and this is the route that CBMM stands for the idea that by reverse
engineering how intelligence works in the human mind and brain that will give us a route to engineering these abilities in machines when we say
reverse engineering we're talking about science but doing science like engineers this is our fundamental principle that
if we approach cognitive science and neuroscience like an engineer where so the output of our science isn't just a
description of the brain or the mind in words but in the same terms that an engineer would use to build an
intelligence system then that will be both the basis for a much more rigorous and deeply insightful science but also
direct translation of those insights into engineering applications now I said before I talk a little about
history what I mean by that is is this again if if part of what brought you here is deep learning and I know even if
you've never heard of deep learning before which I'm sure is unlikely you saw some you know a good spectrum of
that in the in the overview session last night okay it's really interesting and
important to look back on the history of where did techniques for deep learning come from or reinforcement learning
those are the two tools in the in the current machine learning arsenal that are getting the most attention things like back propagation or end to end
stochastic gradient descent or temporal difference learning or cue learning here's a few papers from the literature
you know maybe some of you have read these original papers here's here's the original paper by rumelhart Hinton and
colleagues in which they introduced the back propagation algorithm for training multi-layer perceptrons right multi-layer neural networks here's the
original perceptron paper by Rosenblatt which introduced the one layer version of that architecture and the basic
perceptron learning algorithm here's the first paper on sort of the temporal difference learning method for
reinforcement learning from Sutton and Bartow here's the original Bolton machine paper also by Hinton and
colleagues which you know again is a those you don't know that architecture they give a kind of probabilistic
undirected multi-layer perceptron or for example before there were LS TMS if you
know about current recurrent neural network architecture earlier as much simpler versions of the same idea were proposed by Jeff Elman and his simple
recurrent networks the reason I want to put up the original papers here for you to look at both when they were
published and where they were published so if you look at the dates you'll see papers going back to you know the the
80s but even the 60s or even the 1950s and look at where they were published most of them were published in
psychology journals so the journal psychological review if you don't know it is like the leading journal of theoretical psychology and mathematical
psychology okay or cognitive science the Journal of the cognitive science Society or the the backdrop paper was published
in Nature which is a general interest science journal but by people who are mostly affiliated with an Institute for cognitive science in San Diego so what
you see here is already a long history of scientists thinking like engineers these are people who are in psychology
or cognitive science departments and publishing in those places but by formalizing even very basic insights
about how humans might learn or how you know brains might learn in the right kind of math that led to of course
progress on the science side but it led to all the engineering that we see now it wasn't sufficient right we needed we
needed of course lots of innovations and advances in computing hardware and software systems right but this is where
the basic the basic math came from and it came from doing science like an engineer so what I want to talk about in
our vision is what is the future of this look like if we were to look 50 years into the future what would we be looking back on now or you know over this time
scale well here's that here's a long-term research roadmap that reflects some of my ambitions and some of our
centers goals and many others too right we'd like to be able to address basic questions fundamental questions of what
it is to be and to think like a human questions for example of consciousness or meaning in language or real learning
right questions like you know even beyond the individual like questions of culture or creativity so our big ideas
up there and for each of these there are basic scientific questions right how do we become aware of the world in
ourselves in it starts with perception but it really turns into awareness awareness of yourself and of the world
and what we might call consciousness right or how does a word start to have a meaning what really is a meaning and how
does a child grasp it or how did children actually learn what do babies brains actually start with are they
blank slates or do they start with some kind of cognitive structure and then what is real learning look like these
are just some of the questions that were we're interested in working on or when we talked about culture we mean how do you learn all the things you
didn't directly experience right but that somehow you got from the accumulation of knowledge in society over many generations or how do you ever
think of new ideas or answers to new questions how do you think of the new questions themselves how do you decide what to think about these are all key
activities of human intelligence when we talk about how we model the world where our models come from what we do with our
models this is what we're talking about and if we could get machines that could do these things well again on the bottom row think of all the actual real
engineering payoffs now in our Center in both my own activities and a lot of what
my group does these days and what a number of other colleagues in the Center for brains minds and machines do as well as you know brought very broadly people
in VCS and csail one place where we work on the beginnings of these problems in the near term this is the long term like
think 50 years okay maybe short or maybe longer I don't know but think well beyond well beyond 10 years but in the
short term 5 to 10 years a lot of our focus is around visual intelligence and there's many reasons for that again we
can build on the successes of deep networks and a lot of pattern recognition and machine vision it's a
good way to put these ideas into practice when we when we look at the actual brain the visual system in the
brain in the human and other mammalian brains for example is really very clearly the best understood part of the
brain and at a circuit level it's the part of the brain that's most inspired current deep learning and neural network
systems but even there there's things which we still don't really understand like engineers so here's an example of a
basic problem in visual intelligence that we and others in the centre are trying to solve look around you and you
feel like there's a whole world around you and there is a whole world around you feel like your brain captures it but
what what the actual sense data that's coming in through your eyes looks more like this photograph here where you can
see there's a crowd scene but it's mostly blurry except for a small region of high resolution in the center so that
corresponds biologically to what part of the images in your fovea that's the central region of cells in the retina
where you have really high-resolution visual data the size of your phobia is roughly like if you hold out your thumb
at arm's length it's a little bit bigger than that but not much bigger right most of the image in terms of the actual
information coming in and a bottom-up sense to your brain is really quite blurry but somehow by looking at just one part
and then by secada around or making a few eye movements you get a few glimpses each not much bigger than the size of
your thumb at arm's length somehow you stitch that information together into what feels like and really is a rich representation of the whole
world around you and when I say around you I mean literally around you so here's another kind of demonstration um
without turning around nobody's allowed to turn around ask yourself what's behind you now the answer is going to be
different for different people depending on where you're sitting right for most of you you might think well there's I
think there's a person pretty close behind me all right you know you're in a crowded auditorium although you haven't seen that person you know that they're
there right for people in the very back row you know there isn't a person behind
you and you're conscious of being in the back row right you might be conscious that there's a wall right behind you but now for the people who are in the room
not in the very back think about how far behind you is the back like where's the nearest wall behind you so we can get
maybe we can call out try a little demonstration so I don't know I'm pointing to someone there can you see phrase say something if you think I'm
pointing at you well I could have been pointing at you but I'm pointing someone behind you okay I'll point to you yeah
I'm pointing to you all right so how far is the nearest wall no you can't turn around you've blown your chance right without turning around okay
so you you were laughs okay do you see I'm pointing to you there with the tie okay so without turning around how far
is the nearest wall behind you that's
sorry how far five meters okay well I mean that might be about right no other
people can turn around how about you how far is the nearest wall behind you
ten meters okay that might be right yeah how about here
how what do you think twenty okay see yeah since I didn't grow up in the metric system I barely know but yeah I
mean I mean the point is that like you're you're you each of you is is not
surely not exactly right but you're certainly within an order of magnitude and I guess if we actually tried to measure you know you're probably my
guess is you're probably right within you know fifty percent or less often you know maybe just twenty percent error okay so how do you know this I mean even
if it's not what did you say twenty meters even if it's not twenty meters it's probably closer to 20 meters than
it is to 5 or 10 meters and then it is 250 meters so how do you know this you haven't turned around in a while right
but some part of your brain is tracking the whole world around you right and how
many people are behind you yeah like a few hundred right I mean I don't know if it's 200 or 300 or but it's not a
thousand I mean I don't think so and it's certainly not ten or 20 or 50 right
so you track these things and you use them to plan your actions okay so again think about how instantly
effortlessly and very reliably okay your brain computes all these things so the people and objects around you and it's
not just you know approximations certainly when we're talking about what's what's behind you in space there's a lot of imprecision but when it
comes to reaching for things right in front of you very precise shape and physical property estimates needed to pick up and
manipulate objects and then when it comes to people it's not just the existence of the people but something about what's in their head right you
track whether someone's paying attention to you and you're talking to them what they might want from you what they might be thinking about you what they might be
thinking about other people okay so when we talk about visual intelligence this is the whole stuff we're talking about
and you can start to see how it turns into basic questions I think of not of what we might call the beginnings of
consciousness at least our awareness of ourself in the world and of ourselves as
a self in the world but also other aspects of higher-level intelligence and cognition that are not just about
perception like symbols right to describe even to ourselves what's around us and where we are and what we can do
with it you have to go beyond just what we would normally call the stuff of perception to say the thoughts in somebody's head and
your own thoughts about that okay so what we've been doing in CBMM is trying to develop an architecture for visual
intelligence and I'm not going to go into any of the details of how this works and this is just notional this is just a picture it's like a just a sketch
from a grant proposal of what we say we want to do but it's based on a lot of scientific understanding of how the
brain works there are different parts of the brain that correspond to these different modules in our architecture as well as some kind of emerging
engineering way to try to capture at the software and maybe even hardware levels how these modules might work so we talk
about a sort of an early module of a visual or perceptual stream which like bottom-up visual or other
perceptual input that's the kind of thing that is pretty close to what we currently have and say deep convolutional neural networks but then
we talk about some kind of the output of that isn't just pattern class labels but what we call the cognitive core core
cognition so we get an understanding of space and objects there physics other people their minds that's the real
stuff of cognition that has to be the output of perception but somehow we have to we have we have to have this is what
we call the brain OS in this picture we have to get there by stitching together the bottom-up inputs from glimpse here a
glimpse here a little bit here and there and accessing prior knowledge that comes from our memory systems to tell us how
to stitch these things together into the really core cognitive representations of what's out there in the world and then
if we're going to start to talk about it in language or to build plans on top of
what we have seen and understood that's where we talk about symbols coming into the picture ok the building blocks of
language and plans and so on so now we might say well ok this is an
architecture that is brain inspired and cognitively inspired and and we're planning to turn into real engineering
and you can say well do we need that maybe you know again I know this is a question you considered in the first lecture
maybe the engineering toolkit that's currently been making a lot of progress in let's say industry maybe that's good
enough maybe you know let's take deep learning but to stand for a broader set of modern pattern recognition based and
reinforcement learning based tools and say ok well maybe that can scale up to this and you might you know it but maybe
that's that's possible I'm happy in the question period of people want to debate this my sense is no I think that it's
not when I say no I don't mean like it can't happen or it won't happen what I mean is the highest value the highest
expected route right now is to take this more science-based reverse engineering approach and that if at least if you
follow the current trajectory that industry incentives especially optimized for it's not even really trying to take
us to these things so think about for example a case study of visual intelligence that is in some ways as
pattern recognition very much of a success it's again been mostly driven by industry it's something that if you read
in the Jews or even play around with in certain of it publicly available datasets feels
like we've made great progress and this is an aspect of visual intelligence which is sometimes called image
captioning it's bate or mapping images to text you know basically there's been
a bunch of systems here's a couple of press releases I guess this one's about Google Google's AI can now capture
images almost as well as humans here's ones about Microsoft a couple of years ago I think there were something
like eight papers all released onto archive around the same time from basically all the major industry computer vision groups as well as a
couple of academic partners okay which all driven by basically the same data set produced by some Microsoft
researchers and other collaborators trained a combination of deep convolutional neural networks you know
state of the art visual pattern recognition with recurrent neural networks which had recently been developed for you know basically kinds
of neural statistical language modeling glued them together and produced a system which which which made very
impressive results in a big training set and a held-out test set where the goal was to take an image and write a
sentence like a short sentence caption that that would seem like the kind of way a human would describe that image
and these systems you know surpassed human level accuracy on the held-out test set from a big training set but
what you can see when you really dig into these things is there's often a lot of what I would call data set overfitting it's not overfitting to the
training set but it's overfitting to whatever are the particular characteristics of this data set you know wherever ever came from certain set
of photographs and certain ways of captioning them okay which even a big data set it's not about quantity it's
more about the quality the nature of what people are doing all right so one way to test this system is to apply it
to what seems like basically the same problem but not within the a certain
curated or built data set and there's a convenient Twitter bot that lets you do this so there's something called the pic
desk bot which takes one of the state of the art industry AI captioning systems a very good one again this is not meant to
I'm not trying to critique these systems for what they're trying to do I'm just trying to point out what they don't really even try to do so this takes the
microsoft caption bot and just every couple of hours takes a random image from the web captions it and upload
the results to Twitter and a couple of months ago when I prepared a first version of this talk I just took a few
days in the life of this Twitter bot I didn't take every single image but I took you know most of the images in a
way that was meant to be representative of the successes and the kinds of failures that such a system will make so we can go through this and it's a little
bit entertaining and I think quite informative so here's just a somewhat random sample of a few days in the life
of one of these caption BOTS so here we have a picture of a person holding for
tonight my screen is very small here and I can't read up there so maybe you'll have to tell me was that but a person holding a cell phone I guess I'll just
read along with you so have a person holding a cell phone well it's not a person holding a cell phone but it's kind of close it's a person holding some
kind of machine so I don't even know what that is but it's some kind of musical instrument right
so that's a mixed success or failure here's some pretty good one a group of people on a on a field playing football
that's I would call that a you know a result maybe even A+ here's a group of
people standing on top of a mountain so less good there's a mountain but as far as I can tell there's no people but these systems like to see people because
of both the combination because in the data set they were trained on there's a lot of people and people often talk about people okay I mean and the fact
that you can appreciate both what I said and why it's funny that's there you did some of my cognitive activities that
this system is not even trying to do okay here we've got a building with the cake I'll go through these fast building
with the cake a large stone building with the clock tower I think that's pretty good I'd give that like a b-plus there's no clock but it's plausibly
right there might be a clock in there there's definitely something like that here's a truck parked on the side of a building I don't know maybe a b-minus
there there is a car on the side of a building but it's not a truck and it's and it's it's not doesn't seem like the
main thing in the image okay here's a necklace made of bananas here's
a large ship in the water this is pretty good I give this like an a-minus or b-plus because there is a ship in the
water but it's not very large it's really more of like a tugboat or something here's a sign sitting on the grass you know in some sense that's
great no but it but in another sense it's really missing what's actually interesting and important and meaningful to humans
here's a here's a garden is in the dirt a pizza
sitting on top of the building a small house with the red brick building that's pretty good although a kind of weird way of saying it a vintage photo of a pond
that's good they like vintage photos a group of people that are standing in the grass near a bridge again there's two people and there's some grass and
there's a bridge but it's really not what's going on a person in the yard okay kind of a group of people standing
on top of the boat there's a boat there's a group of people they're standing but again it's what the sentence that you see is is more based
on a bias of what people have said in the past about images that are only vaguely like this a clock tower is a
little at night that's really I think pretty impressive a large clock mounted to the side of the building a little bit less so a snow-covered feel very good a
building with snow on the ground a little bit less good there's no snow white some people who I don't know them
but I bet that's probably right because face identifying faces and recognizing people who are famous because they won
you know medals and the Olympics probably I would trust current pattern recognition systems to get that a painting of a base in front of a mirror
less good also a famous person there but we didn't get him a person walking in
the rain again there is sort of a person and there's some puddles but not you know a group of stuffed animals a car
parked in a parking lot that's good a car parked in front of a building less
good a plate with a fork and knife a clear blue sky okay so you get the idea again like if you actually go and play
with the system partly because I think Mike but my friends at Microsoft told me they've improved at some you know I this
is partly for entertainment values you know I chose what also would be the funnier example so I'm quite I want to
be quite honest about it and these are I'm not trying to take away what our impressive AI technologies but I think
it's clear that there's a sense of understanding any one of these images that it's important to see that even when it seems to be correct right if it
can make the kind of errors that it makes that even when it seems to be correct it's probably not doing what
you're doing and it's probably not even trying to scale towards the dimensions of intelligence that we think about when
we're talking about human intelligence okay another way to put this I'm going to show you a really insightful blog
post from one of your other speakers so in a couple of days I'm not sure you're going to have Andre Karpov a who's one of the leading people
in deep learning this is a really great blog post he wrote a couple of years ago
when he was I think still at Stanford he got his PhD from Stanford he did he worked at Google a little bit on some
early big neural net AI projects there he was an open AI he was one of the founders of open AI and recently he
joined Tesla as their director of AI research but about five years ago he was
looking at the state of computer vision from a human intelligence point of view and and lamenting how far away we were
okay so this is the title of his blog post the state of computer vision nai-nai we are really really far away
and he took this image which was a sort of a famous image in its own right it
was a popular image of Obama back when he was president kind of playing around as he liked to do when he was on tour so
if you take a look at this you can see you probably all can recognize the previous President of the United States
but you can also get the sense of where he is and what's going on and you might see people smiling and you might get the
sense that he's playing a joke on someone can you see that right so how do you know that he's playing a joke and
what that joke is well as Andre goes on to talk about in his blog post too if you think about all the things that that
you have to really deploy in your mind to understand that it's a huge list of course it starts with seeing people and
objects and maybe doing some face recognition but you have to do things like for example notice his foot on the
scale and understand enough about how scales work that when a foot presses down it exerts force that the scale is
sensitive doesn't just magically measure people's weight but it does that somehow through force you have to see who can
see that he's doing that and who can't who cannot see that he's doing that right in particularly the person on the scale and why some people can see that
he's doing that and can see that some other people can't see it why that makes it funny to them okay and someday we
should have machines that can understand this but hopefully you can see why what I would I what the kind of architecture
that I'm talking about would be the building blocks of the ingredients to be
able to get them to do that now I when I again I prepared a version of this talk a few months ago and I wrote to Andre
and I said I was gonna use this and I was curious if he how what you know if he had any reflections on this and where
he thought we were relative to five years ago because a certain a lot of progress has been made but he
said here's his email I hope he doesn't mind me sharing it but I mean again he's a very honest person and that's one of
the many reasons why he's such an important person right now in AI okay he's both very technically strong and honest about what we can do what we
can't do and as he says well what does he say it's nice to hear from you it's funny you should bring this up I was also thinking about writing a a return
to this and in short basically I don't believe we've made very much progress right he points out that in his long list of things that you'd need to
understand the image we have made progress on some the ability to again detect people and do face recognition for well-known individuals okay but
that's kind of about it all right and he wasn't particularly optimistic that the current route that's being pursued an industry is is anywhere close
to solving or even really trying to solve these larger questions um if we
give this image to that caption bot you know what we see is again represents the
same point so here's the caption bot it says I think it's a group of people standing next to a man in a suit and tie right so that's right right as far as it
goes it just doesn't go far enough and the current the current ideas of built a data set train a deep learning algorithm
on it and then repeat um aren't really even I would venture trying to get to
what we're talking about or here's another I'll just give you one other example of a couple of photographs from my recent vacation and a nice warm
tropical look how which I think illustrates ways in which again the gap where we have machines that can say beat
the world's best at go but can't even beat a child at tick-tack-toe now what do I mean by that well you know
of course we can build we don't even need reinforcement learning or deep learning to build a machine that can they can win or tie do is do optimally
in tic-tac-toe but think about this this is a real tic-tac-toe game which I saw on the grass outside my hotel right what
do you have to do to look at this and recognize that it's a tic-tac-toe game you have to see the objects you have to see what's you know in some sense
there's a three by three grid but it's but it's only abstract right it's only delimited by this these ropes or strings
okay it's not actually a grid in any simple geometric sense all right but yet
a child can look at that and indeed here's an actual child who was looking at it and recognized oh it's a game of tic-tac-toe and even know what they need
to do to win we put the X and completed and now they've got three in a row right that's that's literally child's play okay
you showed this sort of thing though to one of these you know image understanding caption BOTS and I think
it's a close-up of a sign okay again it's not like saying that this is a close-up of a sign is is not the same
thing I would venture as a as a cognitive or computational activity that's going to give us what we need to
say recognize the objects to recognize it as a game to understand the goal and how to plan to achieve those goals
whereas this kind of architecture is designed to try to do all of these things ultimately right and I bring in
these examples of games or jokes to really show where perception goes to
cognition you know that and all the way up to symbols right so to get objects and forces and mental states that's the
cognitive core but to be able to get goals and plans and what do I do or how do I talk about it that's symbols okay
here's another way into this and it's one that also motivates I think a lot of really good work on the engineering side
and a lot of our interest in the science side is think about robotics and think about what do you have to do to you know
what is the brain have to be light to control the body so again you're gonna hear from shortly I think maybe it's
next week from Mark raybert who's one of the founders of Boston Dynamics which is
one of my favorite companies anywhere they're without doubt the leading maker
of humanoid robots legged locomoting robots in industry they have all sorts of other really cool robots robots like
dogs robots that have all you know I think you'll even get to see a live demonstration of my new robots this
really awesome impressive stuff okay um but what about the minds and brains of
these robots well again if you ask mark ask them how much of human-like cognition do they have in their robots
and I think he would say very little in fact we have asked him that and he would say very little he has said very little
he's actually one of the advisors of our Center and I think in many ways were very much on the same page we both want
to know how do you build the kind of intelligence that can control these bodies like the way a human does alright
um here's another example of an industry robotics effort this is Google's arm farm where you know they've they've got lots
of robot arms and they're trying to train them to pick up objects using various kinds of deep learning and reinforcement learning techniques and I
think it's one approach I just think it's very very different from the way humans learn to say control their body
and manipulate objects and you can see that in terms of things that go back to what you were saying when you're introducing me right think about how
quickly we learn things right here you have these the arm farm is trying to generate you know effectively maybe if
not infinite but hundreds of thousands millions of examples of reaches and pickups of objects even with just a
single gripper and yet a child who in some ways can't control their body nearly as well as robots can be
controlled at the low level and is able to do so much more so I'll show you two of my favorite videos from YouTube here
which motivate some of the research that we're doing the one on the left is a one and a half year old and the other ones a
one year old so just watch this one and a half year old here doing a popular activity for many kids as a playing hmm
you see video up there I'd okay there we go okay so he's he's on doing this
stacking Cup activity alright he's stacking up cups to make a tall tower
he's got a stack of three and what you can see for the first part of this video is it looks like he's trying to make a
second stack and that he's trying to pick up at once basically he's trying to make a stack of two that'll go on the
stack of three and you know he's trying to debug his plan because it's it got a little bit stuck here but and think
about I mean again if you know anything about robots manipulating objects even just what he just did no robot can decide to do that and actually do it
right at some point he's almost got it it's a little bit tricky but at some point he's gonna get that stack of two
he realizes he has to move that object out of the way look at what he just did move it out of the way use two hands to
pick it up and now he's got a stack of two on a stack of three and suddenly you know subgoal completed he's now got a stack of five and he gives himself a
hand because he know he knows he accomplished a keyway point along the way to his final goal that's a kind of
early symbolic cognition right to understand that I'm trying to build a tall tower but a tower is made up of
little towers it's you know it can end and you can take a tower and put it on top of another tower or stack a stack on us
a can you have a bigger stack right so think about how he goes from bottom up perception to the objects of the physics
needed to manipulate the objects to the ability to make even those early kinds of symbolic plans at some point he keeps
doing this he puts another stack on there I'll just jump to the end oops sorry you missed it so he he gets
really excited and he gives himself another big hand but falls over okay again Boston Dynamics now has robots
that could pick themselves up after that that's really impressive again but all the other stuff to get to that point we
don't really know how to do in a robotic setting or think about this baby here this is a younger baby this is one of
the Internet's very most popular videos because it features a baby and a cat and
but the babies doing something interesting he's got the same cups but he's decided he's again decided to try a
new thing so this think about creativity he's decided that his goal is to stack up cups on the back of a cat I guess
he's asking how many cups can I fit on the back of a cat well three let's see can I fit more let's try another one
okay well he can't fit more than three it turns out and then he then does it's
not working so he changes his goal now his goal appears to be to get the cups on the other side of the cat now watch
that part when he reaches back behind him there that's I'll just pause it there for a moment umm someone he just reached back there
that's a particularly striking moment in the video it shows a very strong form of what we call in cognitive science object
permanence okay that's the idea that you represent objects as these permanent enduring entities in the world even when
you can't see them in this case he hadn't seen or touched that object behind him for like at least a minute
right maybe much longer I don't know and yet he still knew it was there and he was able to incorporate it in his plan
right there's a moment before that when he's about to reach for it but then he sees this other one right and it's only when he's now exhausted all the other
objects here that he can see he's like okay now time to get this object and bring it into play right so think about
what has to be going on in his brain for him to be able to do that right that's like the analog of you understanding
what's behind you okay um it's not that these things are impossible to capture machines far from it it's just that like
training a deep neural network or any kind of pattern recognition system we don't think is going to do it but we think by reverse engineering how it
works in the brain we might be able to do it I think we can can do it okay it's not just humans that do this kind of activity here's a couple
of again rather famous videos you can watch all of these on YouTube crows are famous object manipulators and
tool users but also orangutangs other primates rodents we can watch if we just
hey let me pause this one for a second if we watch this orangutan here he's got a bunch of big legos and over the course
of this video he's building up a stack legos it's really quite impressive
you're just jumping to the end there's actually some controversy out there of
whether this video is a fake but the controversy isn't about you know it's not like whether it was I don't know
dumb with computer animation some people think the video was actually filmed backwards that a human built up the
stack and the orangutan just slowly disassembled it piece by piece and it turns out it's remarkably hard to tell whether it's played forward or backwards
in time and people have argued over little details because you know it would be quite impressive if an orangutan actually was able to build up this
really impressive stack of Legos but I would submit that it would be almost as impressive if he disassembled it think
about the activity I mean if I wanted to disassemble that the easiest thing to do would just be to knock it over that's really all most robots could do
but to piece by piece disassemble it even if it's played backwards like this that's still a really impressive act of
symbolic planning on physical objects or here you've got this this famous Mouse this you can find on the internet under
the mouse versus cracker video and what you'll see here over the course of this video is a mouse valiantly and mostly
hopelessly struggling with a cracker that they're hoping to bring back to their nest I guess it's a very appealing
big meal and at some point after just trying to get it over the over the wall
at some point the mouse just gives up because it's just never gonna happen and he just goes away except that because
even Mouse's can dream or mice can dream some point he decides okay I'm just
gonna come out for one more try and he tries one more time and this time valiantly gets it over yeah isn't that
very impressive congratulations guys okay you don't have to clap form you can clap for me at the end or clap for
whoever later okay but I want to applaud the mouse there every time I see that okay but again think what had to be
going on in his brain able to do that all right it's a crazy thing and yet he formulated the goal and
was able to achieve it I'll just show one more video that is really more about science these other ones are you know
some of them actually were from scientific experiments but this is one that motivates a lot of the science that
I do and it's to me it sets up kind of a grand cognitive science challenge for AI and robotics it's from an experiment
with humans again eighteen month olds or one-and-a-half year old so the the kids in this experiment were the same age is the first baby I showed you the one who
did the stacking and 18 months is really a very very good age to study if you're interested in intelligence for reasons
we can talk about later if you're interested this is from a very famous experiment done by two psychologists
Felix Warren akin and Michael Tomasello and it was studying the spontaneous helping behavior of young children it
also contrasted humans and chimps and the punchline is that chips sometimes do things that are kind of like what this
human did but not nearly as reliably or as flexibly okay so not nearly it is and
I'll show you a particular kind of unusual situation where human kids had
relatively little trouble figuring out kind of what to do or even whether they should do it whereas basically no chimp
did what you're gonna see humans sometimes doing here so the experimenter in this movie I'll turn on the sound
here if you can hear it the experimenter is the tall guy and the participant is
the little kid in the corner there there there's sound but no words right and at
some point he stops and then the kid just does whatever they want to do so watch what he does he goes over he opens
the cabinet looks inside then he steps back and he looks up at felix and then
looks down okay and then the action is completed now well wonder I want you to watch it one more time and think about
what's gotta be going inside the kid's head to understand this to understand like so it seems like what it looks like
to us is the kid figured out that this guy needed help and helped him and the paper is full of many other situations like this this is just one OK but the
key idea is that the situation is somewhat novel people have seen people holding books and opening cabinets but
probably it's very rare to see this kind of situation exactly right it's different in some important details from
what you might have seen before and there's other ones in there that are really truly novel because they just made up a machine right there okay but somehow he has to understand
causally from the way the guy's banging the books against the thing that it's it's sort it's sort of both a symbol but
it's also somehow he's got to understand what he can do and what he can't do and then what the kid can do to help and
I'll show this again but really just watch the main part I want you to see is
I'll just sort of skip ahead so watch this part here let's say I'll just jump
right when he watch right now he's about to look up he looks up and makes eye contact and then his eyes look down so
again he looks up he looks up and then a saccade a sudden rapid eye movement down
down to his hands up down okay so that's again that's this brain OS in action
right he's making one glance small glance at the big guy's eyes just to
make eye contact to see to get a signal did I understand what you wanted and did
you did you register that joint attention and then he makes a prediction about what the guy's gonna do so he
looks right down he doesn't just like look around randomly he looks right down to the guy's hands to track the action
that he expects to see happening if I did the right thing to help you then I expect you're gonna put the books there okay so you can see these things
happening and we want to know what's going on inside the mind that guides all of that all right so that's the sort of
big scientific agenda that we're working on over the next few years where we think some kind of human understanding
of human intelligence in scientific terms could lead to all sorts of AI payoffs in particular suppose we could
build a robot that could do what this kid and many other kids and these experiments do just say help you out around the house without having to be
programmed or even really instructed just to kind of get a sense oh yeah you need to have at that shirt let me help
you out okay even 18 month olds will do that sometimes not very reliably or effectively sometimes they'll try to
help and really do the opposite right but imagine if you could take the the flexible understanding of humans actions
goals and so on and make those reliable engineering technology that would be very useful and it would also be related
to say machines that you can actually start to talk to and trust in some ways right that shared understanding so how
are we gonna do this well let me spend the rest of the time talking about how we try to do this right some of the some of the technology
that we're building both in our group and more broadly to try to make these kinds of architectures real and I'll
talk about two or three technical ideas again not in any detail all right um what is the idea of a probabilistic
program so this is a kind of a you think of it as a computational abstraction
that we can use to capture the common-sense knowledge of this core cognition so when I say we have an intuitive understanding of physical
objects in people's goals how do I build a model of that model you have in the head probabilistic programs a little bit
more technically our one way to understand them is as a generalization of Bayesian networks or other kinds of
directed graphical models if you know those okay but where instead of defining a probability model on a graph you
define it on a program and thereby have access to a much more expressive toolkit
of knowledge representation so data structures other kinds of algorithmic tools for representing knowledge okay
but you still have access to the ability to do probabilistic inference like in a graphical model but also causal
inference in a directed graphical model so for those of you who know about graphical models that might make some
sense to you but just more broadly what this is think of this as as a toolkit that allows us to combine several of the
best ideas not just of the recent deep learning era but over if you look back over the whole scope of AI and as well
as cognitive science I think there's three or four ideas there and more but definitely like three ideas we could
really put up there that have proven their worth and have have had have risen and fallen in terms of each of these had
ideas when the mainstream of the field thought this was totally the way to go and every other idea was was obviously a
waste of time and also had its time when many people thought it was a waste of time okay and these three big ideas I
would say are first of all the idea of symbolic representation or symbolic languages for knowledge representation
probabilistic inference in generative models to capture uncertainty ambiguity learning from sparse data and in their
hierarchical setting learning to learn right and then of course the recent developments with neural inspired
architectures for pattern recognition okay each of these things each of these ideas symbolic languages
probabilistic inference and neural networks has some distinctive strengths that are real weak points of the other
approaches right so to take one example but I haven't really talked about here people in the but I but you but you
mentioned as an outstanding challenge for neural networks transfer learning we're learning to take knowledge across
a number of previous tasks to transfer to others this is a real challenge and has always been a challenge in a neural net ok but is something that's addressed
very naturally and very scalable in for example a hierarchical Bayesian model and if you look at some of the recent
attempts really interesting attempts within the deep learning world to try to get kinds of transfer learning and learning to learn they're really cool ok
but many of them are in some ways kind of reinventing within a neural network paradigm ideas that people you know
maybe just 10 or 15 years ago developed in very sophisticated ways in let's say hierarchical Bayesian models ok and a
lot of attempts to get sort of symbolic algorithm like behavior in neural networks again are really you know
they're very small steps towards something which is a very mature technology in computer systems and
programming languages probabilistic programs I'll just sort of advertise mostly are a way to combine the
strengths of all of these approaches to have knowledge representations which are as expressive as anything that anybody
ever did in the symbolic paradigm that are as flexible at dealing with uncertainty and sparse data as anything
in the probabilistic paradigm but that also can support pattern recognition tools to be able to for example to do
very fast efficient inference in very complex scenarios and there's a number of probably that's that that's the kind
of conceptual framework there's a number of actually implemented tools I'm point two here on the slide a number of
probablistic programming languages which you can go explore for example there's one that was developed in our group a few years ago
almost 10 years ago now called church which was the antecedent of some of these other languages built on a functional programming course a church
is a probablistic programming language built on the lambda calculus or really in Lisp basically but there are many
other more modern tools especially if you are interested in neural networks there are tools like for example pyro or
prob torch or Bayes flow that try to combine all these ideas in a or for
example Jen here which is a project of the Koch men's singles probably the computing group these are all things
which are just in the very beginning stages very very alpha but you can find out more
about them online or by writing to their creators and I think this is a this is a very exciting place where the
convergence of a number of different AI tools are happening and when and this will be absolutely necessary for making
the kind of architecture that I'm talking about work another key idea which we've been building on in our lab
and I think again many people are using some version of this idea but maybe a little bit different from the way we're
doing it is what what version of this idea that I'd like to talk about is what I call the game engine in the head so
this is the idea that it's really what the programs are about when I talk about problems tick programs I haven't said
anything about what kind of programs we're using we're just basically these probablistic programming languages at their best and Church the language that
that was developed by Noah Goodman and Vikash and others and Dan Roy and our group some 10 years ago was intended to
be a turing-complete probabilistic programming language so any probability model that was computable or for whose
inferences conditional inferences are computable you could represent in these languages but that that leaves
completely open what what I'm actually gonna what what kind of proto I'm gonna write to model the world and I've been
very inspired in the last few years by thinking about the kinds of programs that are in modern video game engines so
again I'm probably most of you are familiar with these but if you're and increasingly they're playing a role in all sorts of ways an AI but these are
tools that were developed by the video game industry to allow a game designer to make a new game with without having
to do most of in some sense many must have the hard technical work bison from scratch but rather to focus on the
characters the world the story okay the things that are more interesting for designing a novel game in particular we
if we want a player to explore some so new three-dimensional world but to have them be able to interact with the world
in real time and to render nice looking graphics in in real time in an interactive way as the player moves
around and explores the world or if you want to populate the world with non-player characters that will behave in a even vaguely intelligent way okay
game engines give you tools for doing all of this without having to write all of graphics from scratch or all of
physics the rules of physics from scratch so what are called game physics engines
and in some sense are a set of principles but also hacks from Newtonian mechanics and other areas of physics
that allow you to simulate plausible looking physical interactions in very complex world very approximately but
very fast there's also what's called game AI which are basically very simple planning models so let's say I want to
have an AI in the game that is like unguarded that gardens of base and a player is gonna attack the space so back
in the old Atari days like when I was a kid you know the guards would just be like random things that would fire missiles kind of randomly in random
directions at random times right but let's say you want a guard to be a little intelligent so to actually look
around him oh and I see the player and then to actually start shooting at you and to even maybe pursue you so that requires putting a little AI in the game
and you do that by having basically simple agent models in the game so what we think and some of you might think
this is crazy and some of you might think this is very natural idea I get both kinds of reactions what we think is
that these tools of you know past approximate renderers physics engines and sort of very simple kinds of AI
planning are an interesting first approximation to the kinds of common-sense knowledge representations
that evolution has built into our brains so when we talk about the cognitive core or how do babies start what's what you
know ways in which a baby's brain isn't a blank slate one interesting idea is that it starts with something like these
tools and then wrapped inside a framework for probabilistic inference that's what we mean by promising programs that can support many
activities of common sense perception and thinking so I'll just give you one example what we call this intuitive
physics engine okay so this is work that we did in our groups that Pete Battaglia and Jess Hamrick did started this work
about five years ago now where we showed people you know in some
sense and this is this is also an illustration of a kind of experiment that you might do what you might keep talking about science like I'll show you
now a couple of experiments right so we would show people simple physical scenes like these blocks world scenes and ask
them to make a number of judgments and the model we built does it basically a little bit of probabilistic inference in
a game style physics engine it perceives the physical state and imagines a few different possible ways the world could
go over the next one or two seconds to answer questions like will the stack of blocks fall
or if they fall how far will they fall or which way will they fall or what would happen if say one of the colored
one color of blocks are one material like the green stuff is ten times heavier than the gray stuff or vice
versa how will that change the direction of fall or look at those red and yellow stack blocks some of which look like
they should be falling but aren't so why can you infer from the fact that they're not fall in that one color block is much
heavier than the other let me show you a sort of a slightly weird task it's in a
behavioral experiment sometimes we we do weird things so that we can test ways in
which you use your knowledge that you didn't just you know learn from pattern recognition but use it to do new kinds
of tasks that you'd never seen before so here's a task which you know many of you have maybe seen me talk about these
things so you might have seen this task but probably only if you saw me give a talk around here before we call this the red yellow task and again we'll make
this one interactive so imagine that the blocks on the table are knocked hard enough to bump the tables bumped hard
enough to knock some of the blocks onto the floor so you tell me is it more likely to be red blocks or yellow blocks what do you say red okay good
how about here yellow good how about here uh-huh here here okay here here
okay so you just experience for yourself what it's like to be an objective one of
these experience we just did the experiment here the data is all captured on video sort of right okay you could see that sometimes people were very
quick other times people were slower sometimes there was a lot of consensus sometimes there was a little bit less consensus right
that reflects uncertainty so again there's a long history of studying this scientifically that you know you could
but you can see something you can see the probabilistic inference at work probabilistic inference over what well I
would say one way to describe it is over one or a few short low precision simulations of the physics of these
scenes so here is what I mean by this I'm gonna show you a video of a game engine reconstruction of one of these
scenes that simulates a small bump so here's a small bubble here's the same scene with a big bump okay now notice
that at the micro level different things happen but at the cognitive or macro level that matters for common sense
reasoning the same thing happened namely all the yellow blocks went over onto one side of the table and few or none of the
red blocks did so it didn't matter reach of those simulations you ran in your head you'd get the same answer in this case right this is one that's very easy
and high confidence and quick also you didn't have to run the simulation for very long you only have to run it for a
few time steps like that to see what's gonna happen or similarly here you only have to run it for a few time steps okay
and it doesn't have to be even very accurate even a fair amount of imprecision will give you basically the
same answer at the level that matters for common sense so that's the kind of thing our model does it runs a few low
precision simulations for a few time steps but if you take the average of what happens there and you compare that
with people's judgments you get results like what I show you here the scatterplot shows on the y-axis the
average judgments of people on the x-axis the average judgments of this model and it does a pretty good job it's not perfect but the model basically
captures people's graded sense of what's going on in this scene and many of these others okay and it doesn't do it with
any learning but I'll come back to that in a second it just does it by probabilistic reasoning over a game physics simulation now we can use and we
have used the same kind of technology to capture in very simple forms really just proofs of concept at this point the kind
of common-sense physical scene understanding in child in a child playing with blocks or other objects or in what might go on in a young child
understanding of other people's actions what we called the intuitive psychology engine where now the probabilistic programs are
defined over these kind of very simple planning and perception programs and I won't go into any details I'll just
point to a couple of papers that my group played a very small role in but we provided some models which together with
some infant researchers people working on both of these are experiments that that were done with 10 or 12 month
infants so younger than even some of the babies I showed you before but basically like that youngest baby the one with the
cat here's an example of showing simple physical scenes these are moving objects to 12 month olds where they saw a few
objects bouncing around inside a gumball machine and after some point in time the scene gets occluded you'll see the scene
is occluded and then after another period of time one of the objects will appear at the bottom and the question is
is that the object you expected to see or not is its expected or surprising the standard way you study what infants know
is by is by what's called looking time methods just like an adult if I show you something that's surprising you might
look longer okay if you're bored you'll look away all right so you can do that
same kind of thing with infants and by measuring how long they look at a scene you can measure whether you've shown
them something surprising or not all right people have there are literally hundreds of studies if not more using looking
time measures to study what infants know but only with this paper that we published a few years ago did we have a
quantitative model we're able to show a relation between inverse probability in this case and surprise so things which
were objectively lower probability under one of these probabilistic physics simulations across a number of different manipulations of how fast the objects
were where they were when the scene was occluded how long the delay was various physically relevant variables how many objects there were one type or another
infants expectations connected with this model or another paper that we published that one was was done that the
experiments that were done by era note eggless and Luca bananas lab here is a study that was done just recently by
sherry Lu inless spell keys lab at there at Harvard but they're part they're partners with us and CBMM which was
about infants understanding of goals so this is more like again understanding of agents and intuitive psychology we're in again in very simple cartoon
scenes you show an infant an agent that seems to be doing something like an animated cartoon character but it jumps
over a wall or rolls up a hill or it jumps over a gap and the question is basically how much
does the agent want the goal that it seems to be trying to achieve and what this study showed okay and the models
here we're done by Tomer omen was that infants appeared to be sensitive to the physical work done by the agent the more
work the agent did in a sense of the integral of force applied over a path the more the infant's thought the agent
wanted the goal we think of this as representing what we've sometimes called the naive utility calculus so the idea
that there's a basic calculus of cost and benefit you know we take actions
which are a little bit costly to achieve goal states which give us some reward that's the most basic way the oldest way
to think about rational intentional action and it seems that even ten-month-old understand some version of that where
the cost can be measured in physical terms okay I see I'm running a little bit behind on time and and I wanted to leave
some time for discussion so I'll I'll just go very quickly through a couple of other things and and Lee and happy to stay around at the end for discussion
okay the what I showed you here was the science where does the engineering go so
one way one thing you can do with this is say build a machine system that can look not a little animated cartoon like
these baby experiments but a real person doing something and again combine physical COFF and constraints of actions
with some understanding of the agents utilities that's the math of planning to
figure out what they want it so look in this scene here and see if you can judge
which object that the woman is reaching for so you can see there's there's a grid of four by four objects there's
sixteen objects here and she's gonna be reaching for one of them raise it's gonna play in slow motion but raise your
hand when you know which one she's reaching for ok so just watch and raise your hand when you know which one she wants okay so most of
they're up by now alright and notice I was looking at your hands not here but went but what happened is most of the
hands were up at the about the time when that gray or the one that - line shot up okay
that's not human data you provided the data this is our model so our model is predicting more or less when you're able
to say what her goal was okay it's well before she actually touched the object how does the model work again I'll skip
the details but it does the same kind of thing that that our models of those infants did namely it but in this case
it does it with a full body model from robotics so we use what's called the mu Joko physics engine which is a standard
tool in robotics for planning physically efficient reaches of say a humanoid robot and we say we can give this
planner program a goal object as input we can give it each of the possible goal objects as input and say plan the most
physically efficient action so the one that uses like the least energy to get to that object and then we can do a
Bayesian inference this is the probabilistic inference part the program is them is the MU Joker planner okay but
then we can say I want to do Bayesian inference to work backwards from what I observed which was the action to the
input to that program what goal was provided as input to the planner and here you can see the full array of four
by four possible inputs and those bars that are moving up and down that's the Bayesian posterior probability of how
likely each of those was to be the goal and what you can see is it converges on the right answer at least well it turns
out to be the ground truth right answer but it's also the right answer according to what people think with about the same kind of data that people took now you
might say well okay I'm sure if I just wanted to build a system that could detect what somebody was reaching for I could generate a training data set of
this sort of scene and train something up to analyze patterns of motion but again because the engine in your head
actually does something we think more like this it does what we call inverse planning over a physics model it can
apply to much more interesting scenes that you haven't really seen much of before so take the scene on the left right where again you see somebody
reaching for one of a four by four array of objects but what you see is a strange kind of reach can you see why he's doing
that strange reach up there it's a little small but what is you can see that he's reaching over something right
it's actually a pane of glass right you see that and then there's this other guy who's helping him who sees what he wants
and hands the thing he wants so how does the firt the guy in the foreground see the other guy's goal
how does he and for his goal and know how to help him and then how do we look at the two of them and figure out who's
trying to help who or that in a scene like this one here that it's not somebody trying to help somebody but
rather the opposite okay so here's a model on the left of how that might work right and we think this is the kind of model needed to
tackle this sort of challenge here right basically it's a model it's a we take this model of planning sort of maximal
expected utility planning which you can run backwards but then we recursively nest these models inside each other so
we say an agent is helping another agent if this agent is acting apparently to us seems to be maximizing an expected
utility that's a positive function of that agents expectation about another agents expected utility and that's what
it means to be a helper hindering is sort of the opposite if one seems to be trying to lower somebody else's utility
okay and we've used these same kind of models to also describe infants understanding of helping and hindering
in a range of scenes I'll just say one last word about learning because everybody wants to know about learning
and and the the key thing here and it's definitely part of any picture of AGI but the thought I want to leave you on
is really about what learning is about ok I'll be just a few more slides and then I'll stop I promise none of the
models I showed you so far really did any learning they certainly didn't do any task specific learning ok we set up
a probable state program and then we let it to inference now that's not to say that we don't think people learn to do these things we do but the real learning
goes on when you're much younger right everything I showed you in basic form even a one-year-old baby can do ok the
basic learning goes on to support these kinds of abilities not that there isn't learning beyond one year but the basic
way you learn to say solve these physics problems is what goes on in your baton in the brain of a child between 0 and 12
months so this is just an example of some phenomena that come from the literature on infant cognitive development these are very rough
timelines you can take pictures of this if you like this is always a popular slide because it really is quite inspiring I think and I can give you
lots of literature pointers but I'm summarizing in very broad strokes with big error bars what we've learned in the
field of infant cognitive development about when and how kids seem to have to at least come to certain understand
of basic aspects of physics so if you really want to study how people learn to be intelligent a lot of what you have to
study are kids at this age you have to study what's already in their brain at zero months and what they learn and how
they learn between four six eight ten twelve and so on and on up beyond that okay now well effectively what that
amounts to we think is if what you're learning is something like a let's say an intuitive game physics engine to
capture these basic abilities then what we need if we're gonna try to reverse-engineer that is what we might
think of as a program learning program if your knowledge is in the form of a program then you have to have programs that build other programs right this is
what I was talking about the beginning about learning as building models of the world or ultimately if you think what we
start off with is something like a game engine that can play any game then what you have to learn is the program of the
game that you're actually playing or the many different games that you might be playing over your life so think of learning as like programming the game
engine in your head to fit with your experience and and to fit with the possibilities that you seem like you can
take now this is what you could call the hard problem of learning if you come to learning from say neural networks or
other tools and machine learning right so what makes machine makes most of machine learning go right now and
certainly what makes neural network so appealing is that you can set up a basically a big function approximator
that can approximate many of the functions you might want to do in a certain application or task but in a way
that's end-to-end differentiable and with a meaningful cost function so you can have one of these nice optimization
landscapes you can compute the gradients and basically just roll downhill until you get to an optimal solution but if
you're talking about learning as something like search in the space of programs we don't know how to do anything like that yet we don't know how
to set this up as any kind of a nice optimization problem with any notion of smoothness or gradients okay rather what
we need is a instead of learning as like rolling downhill effectively right a process which just if you're willing to
wait long enough you know some you know simple algorithm will take care of think
of what we call the idea of learning as programming there's a popular metaphor in cognitive development called the
child of scientists which emphasizes children as active theory builders and children's play as a kind of kind of
casual experimentation but this is the algorithmic complement to that what we could call the child as
or around MIT will say the child is hacker but the rest of the world if you say child is hacker they think of
something someone who breaks into your email and steals your credit card numbers we all know that hacking is you know making your code more awesome right
if your knowledge is some kind of code or legal library of programs then learning is all the ways that a child
hacks on their code to make it more awesome that more awesome can mean more accurate but it can also mean faster
more elegant more transportable to other applications or their tasks more explainable to others maybe just more
entertaining okay children do all of them have all of those goals and learning and the activities by which they make their code more awesome also
correspond to many of the activities of coding alright so think about all the ways on a day-to-day basis you might
make your code more awesome all right you might tune you might have a big library of existing functions with
some parameters that you can tune on a data set that's basically what you do with backprop or stochastic gradient descent in training a deep learning
system but think about all the ways in which you might actually modify the underlying function so write new code or
take old code from some other thing and map it over here or make a whole new library of code or refactor your code to
some other you know some other basis for that that will work more robustly and be more extensible or transpiling or
compiling right or even just commenting your code or asking someone else for their code ok again these are all ways
that we make our code more awesome and children's learning has analogs all of these that we would want to understand
as an engineer from an algorithmic point of view so in our group we've been working on on various early steps
towards this and again we don't have anything like program writing programs at the level of children's learning
algorithms but one example of something that we did in our group which you might not have thought of being about this but
it's definitely the AI work we did that got the most attention in the last couple of years from our group we had
this paper that was in science it was actually on the cover of science sort of just hit the market at the right time if
you like and it got about a hundred times more publicity than anything else I've ever done which is partly a testament to the really great work that
Brendan Lake who was the first author did for his PhD here but much more so just about the hunger for AI systems at
the time when we published this in 2015 and we built a machine system that the way we described it what
doing human level concept learning four simple concept very simple visual concepts these handwritten characters in
many of the world's alphabets for those of you who know the famous Emnes data set in the data set of handwritten digits 0 through 10 or 30 through 9
sorry that drove so much good research in deep learning and pattern recognition it did that not because Jana Kuhn who
put that together or Geoff Hinton who did a lot of work on deep learning with M Nez they were interested fundamentally
in character recognition that they saw that as a very simple testbed for developing more general ideas and
similarly we did this work on getting machines to do what we kind of one-shot learning of generative models also to
develop more general ideas we saw this as learning very simple little mini probabilistic programs in this case what
are those programs they're the programs you use to draw a character so ask yourself how can you look at any one of these characters and see in a sense how
somebody might draw it the way we tested this in our system was this little visual Turing test where we showed
people one character in a novel alphabet and we said draw another one and then we compared nine people like say on the
left and nine samples from our machine say on the right and we said we asked other people could you tell which was
the human drawing another example or imagining another example in which was the machine and people couldn't tell
when I said ones on the left ones on the right I don't actually remember and on different ones you can see if you can tell it's very hard to tell can you tell
which is for each one of these characters which new set of examples were drawn by a human versus a machine
here's the right answer and probably you couldn't tell the way we did this was by
assembling a simple kind of program learning program right so we basically said when you draw a character you're assembling strokes and
sub strokes with goals and sub goals that produce ink on the page and when you see a character you're working
backwards to figure out what was the program the most efficient program that did that so you're basically inverting a
probabilistic program doing Bayesian inference to the program most likely to have generated what you saw this is one
small step we think towards being able to learn programs to being able to learn something ultimately like a whole game
engine program the last thing I'll leave you with is just a pointer to sort of work in action right so this is some work being done by
a current PhD student who works partly with me but also with armando salar Lezama and cecil this is kevin Ellis
it's an example of what's now I think again a urging exciting area and AI well beyond
anything that we're doing is the is combining techniques from where amando comes from which is the world of
programming languages not machine learning or AI but tools from programming languages which can be used
to automatically synthesize code okay with the machine learning toolkit in this case a kind of Bayesian Men and a
minimum description length idea to be able to make again what is really one small step towards machines that can
learn programs by basically trying to efficiently find the shortest simplest program which can capture some data set
so we think by combining these kinds of tools in this case let's say from Bayesian inference over programs with a
number of tools that have been developed in other areas of computer science that don't look anything or haven't been
considered to be machine learning or AI like programming languages it's one of the many ways that going forward we're
gonna be able to build smarter more human-like machines so just to end then what I've tried to tell you here is
taught first of all identify the ways in which human intelligence goes beyond pattern recognition to really all these
activities of modeling the world okay to give you a sense of some of the domains where we can start to study this in
common sense scene understanding for example or you know something like
one-shot learning for example like what we were just doing there or learning is programming the engine in your head okay
and to give you a sense of some of the technical tools probabilistic programs program synthesis game engines for
example as well as a little bit of deep learning that bringing together we're starting to be able to make these things
real okay now that's the science agenda and the reverse engineering agenda but think about for those of you who are
interested in technology what are the many big AI frontiers that this opens up so the one I'm most excited about is
this idea which is which I've highlighted here in our big research agenda this is one I'm most excited about to work on for the you know it
could be the rest of my career honestly but it's really what is what is the oldest and maybe the best dream of AI
researchers of how to build a human-like intelligence system a real a GI system it's the idea that Turing proposed when
he proposed the Turing test or Marvin Minsky proposed this at different times in his life or many people have proposed this right which is to build a system
that grows into intelligence the way a human does that starts like a baby and learns like a child
tried to show you how we're starting to be able to understand those things what a baby's mind starts with how children
actually learn and looking forward we might we might imagine that someday we'll be able to build machines that can
do this I think we can actually start working on this right now and we're and that's something that we're doing in our
group so if that kind of thing excites you then I encourage you to work on it maybe even with us or if any one of
these other activities of human intelligence excite you I think taking the kind of science-based reverse
engineering approach that we're doing and then trying to put that into engineering practice it's it's this is
this is a this is not just a possible route but I think it's it's quite possibly the most valuable route that
you could work on right now to try to actually achieve at least some kind of artificial general intelligence
especially the kind of intelligence AI system that's going to live in a human
world and interact with human there's many kinds of AI systems that could live in worlds of data that none of us can understand or will ever live in
ourselves but if you want to build machines that can live in our world and interact with us the way we are used to
interacting with other people then I think this is a route that you should consider okay thank you
[Applause]
hi there so early in the talk you expressed some skepticism about whether or not industry would get us to
understanding human level intelligence it seems that there's a couple of trends that favor industry one is the industry
is better than that academia accumulating resources and plowing back into the topic and it seems at the
moment we've got a bit of brain drain going on form academia into industry and that seems like a on going trend yeah if
you look at something like learning to fly or learning to fly into space then it looks like a story is one of Industry
kind of taking over the field and going off on its own yeah a little bit academia academics still have a role but
industry kind of dominates so yes is industry going to overtake the field you think well that's a really good question
and it's got several good questions packed into one there right I didn't mean to say I didn't this wasn't meant
to say go academia bad industry right what I was taught what I what I tried to say was the approaches that are
currently getting the most attention in industry and they're really because they're really the most valuable ones right now for the short term you know
any industry is really focused on what it can do what are the value propositions on basically a two year
time scale at most I mean if you ask say Google researchers to take the most prominent example it's pretty much what
they'll all tell you okay maybe maybe things that might you know pay off
initially in two years but maybe take five years or more to really develop but if if you can't show that it's gonna do
something practical for us in two years in a way that matters for our bottom line then it's not really worth doing okay so what when we say what I'm
talking about is the technologies which right now industry sees as meeting that specification and what I'm saying is
right now I think those that's that's not where the route is to something like
human-like but not the most valuable promising route to human-like kinds of AI systems all right but I hope that
like in the cases you said you know the basic research that we're doing now will be successful enough that it will get
the attention of industry when the time is right but I think so you know I mean I hope at some point you know it won't
it will only at least the engineering side will have to be done in industry not just in academia but you're also
pointing to issues of like brain drain and other things like that but I think it's these are real issues confronting our community I think
everybody knows this and I'm this will come up multiple times here which is you know I think we have to
find ways to even now to combine the best of the idea of the energy and the resources of academia and industry if we
want to keep doing basically something interesting right if we will if we just want to redefine AI to be well whatever
people currently call AI but scaled up well then then then fine forget about it and or if we just want to say let me and
people like me do what we're doing at what industry would consider a snail's pace on toy problems okay fine but if
but if we want to if you know if I want to take what I'm doing to the level that that will really be you know paying off
that level the industry can appreciate or just that really has technological impact on a broad scale right or I think
if industry wants to take what it's doing and really build machines that are actually intelligent right our machine
learning that actually learns like a person then I think we need each other now and not just in some point in the future so this is a general challenge
for MIT and for everywhere and for Google I mean we just spent a few days talking to Google about exactly this
issue that this was a talk I prepared partly for that purpose so we wanted to raise those issues and and it's just I
mean really there I don't know what I mean well rather I can think of some solutions to that problem of what you
could call brain drain from the academic point of view or what you could call just narrowing in into certain local minima in the industry point of view but
they will require the leadership of both academic institutions like MIT and companies like Google being creative
about how they might work together in ways that are a little bit outside of their comfort zone I hope that will start to happen including at MIT and at
many other universities and at companies like Google and many others and I think we need it to happen for the health of all parties concerned okay thank you
very much things I'm curious about sort of the premise that you gave that one of
the big gaps missing at determining intelligence is the fact that we need to teach machines how to recognize models
and I'm curious as to what you think sort of non goal oriented cognitive
activity comes into play they're things like feelings and emotions and and y-you don't think that might not
necessarily be like that the no I'm I was born in questo the only reason
emotions didn't appear on my slide is because there's a few reasons but the slide is only so big I wanted the font
to be big readable for such an important slide I've had versions of my slide in which I do talk about that okay it's not
that I think feelings or emotions aren't important I think they are important and I used to not have many insights on it
about what to do about them but actually partly based on some of my colleagues here at MIT BCS Laura Schultz and
Rebecca Saxe two of my cognitive colleagues in who I work closely with they've been starting to do research on
how people understand emotions both their own and others and we've been starting to work with them on computational models so that's actually
something I'm actively interested in and even working on but I would say and again for those of you who study emotion
to know about this actually you're gonna have Lisa coming in right oh so she's gonna basically say a version of the same thing I think the deepest way to
understand she's one of the world's experts on this the deepest way to understand emotion is very much based on
our mental models of ourselves of the situation we're in and of other people right think about for example all of the
different I mean if you you know if you think about it I mean again Lisa will talk all about this but if you think
about emotion as just a very small set of what are sometimes called basic emotions like being happy or angry or
sad or you know those are a small number of them right there's usually only few
right you might not say you might see that it's somehow like very basic things
that are opposed to some kind of cognitive activity but think about all the different words we have for emotion
right for example think about an a famous cognitive emotion like regret
what does it mean to feel regret or frustration right just to know both for
yourself when you're not just feeling kind of down or negative but you're feeling regret that that means something
like I have to feel like there's a situation that came out differently from how I hoped and I realize I could have
done something differently right so that means you have to be able to understand you have to have a model you have to be
able to do a kind of counterfactual reasoning and to think oh if only I had acted to differ way then I can predict that the world
would have come out differently and that's the situation I wanted but instead it came up this other way right or think about frustration again that
requires something like understanding okay I've tried a bunch of times I thought this would work but it doesn't seem to be working maybe I'm ready to
give up though those are all those are those are very important human emotions we have to understand to understand
ourselves we need that to understand other people to understand communication but those are all filtered through the kinds of models of action that I was
just the ones I was talking about here with these say cost-benefit analyses of action so what I'm so I'm just trying to
say I think this is very basic stuff that will be the basis for building I think better engineering style models of
the full spectrum of human emotion beyond just like well I'm feeling good or bad or scared okay and if I think when you see Lisa she
will in her own way say something very similar interesting thanks yeah thanks
Josh for your nice talk so all is about human cognition and try to build a model to mimic those cognition but you don't
how much could help you to understand how the circuit implement those things hmm I mean like these circuits in the
brain yeah yeah that's the is that what you work on by any chance is that what you work on by any chance yeah yeah yeah
so so in the Center for brains minds of machines as well as in brain and cognitive science yeah we I have a
number of colleagues who study the actual hardware basis of this stuff in the brain and that includes like the
large-scale architecture of the brain say like what Nancy kanwisher Rebecca Saxe studied with functional brain imaging or the more detailed
circuitry which usually requires recording from say non-human brains right at the level of individual neurons
and connections between neurons all right so I'm very interested in those things although it's not mostly what I
work on right but I would say you know again liking in many other areas of science certainly in neuroscience the
kind of work I'm talking about here in a sort of classic reductionist program sets the target for what we might look
for like if I if I just want to go I mean I I would I would I would assert right or my working conjecture is that
if if you do the kind of work that I'm talking about here it gives you the right targets or gives you a candidate
set of targets to look for what are the neural circuits computing right whereas if you just go in and just say
poking around in the brain or have some idea that what you're gonna try to do is find the neural circuits which underlie
behavior without a sense of the computations needed to produce those behaviors I don't I think it's gonna be
very difficult to eat to know what to look for and to know when you've found even viable answers so I think that's
you know that's the standard kind of reductionist program but it's not that's it's not I also think it's it's not one
that is I'm divorced from the study of neural circuits it's also one if you look at the broad picture of reverse
engineering it's one where we're neural circuits and understanding the circuits in the brain play an absolutely critical
role okay I would say the mate as an when you look at the brain at the hardware level as an engineer I'm mostly
looking at the software level right but when you look at the hardware level there are some remarkable properties one
remarkable property again is you know how much parallelism there is and in many ways how fast the computations are
okay neurons are slow but the computations intelligence are very fast so how do we get elements that are in some sense
quite slow in their time constant to produce such intelligent behavior so quickly that's a great mystery and I
think if we understood that it would have payoff for building all sorts of you know Apple basically application
embedded circuits okay but also maybe most important is the power consumption and again many people have-have have
noted this right if you look at the power consumption the power that the brain consumes like what did I eat today
okay almost nothing um my daughter who's again she's doing an internship here she
literally yesterday all she ate was a burrito and yet she wrote 300 lines of code for her internship project on
really cool computational linguistics projects so somehow she turned a burrito into you know a model of child language
acquisition okay but how did she do that or how do any of us do this right um we're if you look at the power that we
consume when we simulate even a very very small chunk of cortex on our conventional hardware or we do any kind
of machine learning thing we have systems which are very very very very far from the power of the human brain
computationally but in terms of physical energy consumed way way past what any
individual brain is doing so how do we get circuitry of any sort biological or just any physical circuit
to be as smart as we are with as little energy as we are this is this is a huge
problem for basically every area of engineering right if you want to if you want to have any kind of robot the power
consumption is a key bottleneck same for self-driving cars if we want to build AI without contributing to global warming
and climate change let alone use AI to solve climate change we really need to address these issues and the brain is a
is a huge guide there right I think there are some people who are really starting to think about this how can we
say for example build somehow brain inspired computers which are very very
low-power but maybe only approximate so I'm thinking here of Joe Bates I don't know if none of you know Joe he's he's
been around MIT and other places for quite a while can I tell them about your company so so Joe has a start-up in
Kendall Square called singular computing and they have some very interesting ideas including some actual implemented technology for low power approximate
computing in a sort of a brain like way that might lead to possibly even like the ability to build something this is
Joe's dream to built in this about the size of this table but that has a billion course a billion cores and runs
on a reasonable kind of power consumption I would love to have such a machine if anybody wants to help Joe
build it I think he'd love to talk to you but that's it's one of a number of ideas I mean Google X people are working
on similar things probably most of the major chip companies are also inspired by this idea and I think even if you
don't didn't think you were interested in the brain if you want to build the kind of AI were talking about and run it on physical Hardware of any sort and
understanding how the brain circuits compute what they do what what I'm talking about with as little power as
they do I don't know any better place to look it seems like a lot of the improvements in AI have been driven by
increasing like computational power yeah how far you would you say me like GPUs or CMU yeah yeah how far would you say
we are from hardware that could run a general artificial intelligence of the
kind that I'm talking about yeah I don't know I'll start with a billion cores and then we'll see I mean I I think we're I
think we're I mean I think I think there's no way to answer that question in a way that software independent I don't know how to do that right but I
think that it's and and you know I don't know like
when you say how far are we you mean how far am i with the resources I have right now how far am i if if
Google decides to put all of its resources at my disposal like they might if I were working at deepmind I don't know the answer to that question
I but I think the I think what we can say is this um individual neurons I mean
again this goes back to another reason to study neural circuits um if you look at what we currently call neural networks in the AI side the model of a
neuron is this very very simple thing right individual neurons are not only much more complex but have a lot more
computational power it's not clear how they use it or whether they use it but I think it's just as likely that a neuron
is something like a rail you write is that a neuron is something like a computer like under one neuron in your
brain is more like a CPU node okay maybe and thus the ten billion or trillion you
know the large number of neurons in your brain I think it's like 10 billion cortical pyramidal neurons or something
might be like 10 billion cores okay for example that's at least as plausible I think to me as any other estimate so and
I think so I think we're on the definitely on the underside with very big error bars so I completely agree
that or if this is what you might be suggesting and may you know going back to my answer to your question I don't
think we're gonna get to what I'm talking about that anything like a real brain scale without major innovations on
the hardware side and you know it's it's interesting that what drove those innovations in that support current a I
was mostly not AI it was the video game industry I'm when I point to the video
game engine in your head that's a similar thing that was driven by the video game industry on the software side I think we should all play as many video
games as we can and contribute to the growth of the video game industry because no because I mean I mean you can
see this in very like there are companies out there for example there's a company called improbable which is a
London company London based startup a pretty sizable start-up at this point which is building something that they
call spatial OS which is it's a it's not a it's not a hardware idea but it's a kind of software idea for very very big
distributed computing environments to run much much more complex realistic simulations of the world for
much more interesting immersive permanent videogames I think that's one thing that might hopefully that will lead to more fun new kinds of games but
that's one example of where we might look to that industry to drive some of the you know just computer systems
really hardware and software systems that we'll take we'll take our game to
the next level just understanding on the algorithmic level or cognitive level is just to
understanding the learning the meaning of learning would be how to predict but on the circuit level is different but at
the what level on the circuit level well of course it's different right but already you I think you made a mistake
there honestly like you said the cognitive level is learning how to predict but I'm not sure what you mean by that there's many things you could
mean and are what our cognitive science is about is learning which of those versions like I don't think it's learning how to predict I think it's learning what you need to
know to plan actions and to a map you know all those things like it's not just about predicting it's because there are
things we can imagine so that you would never predict because there never happen unless we somehow make the world
different so generalizations are you're not predicting okay when your model could generalize but especially in the
transfer learning that you are interested in a few hundred of neurons in prefrontal cortex they have generalize a lot yes but not kind of a
Bayesian model do that you said but a thean model won't do that or they don't
do it the way a Bayesian model does for sure because that's in the abstract level well I mean how do you really know
like and what does it mean to say that some neurons do it like so maybe another way to put this is to say look we have a
certain math that we use to capture these you could call it abstract I call it software level abstractions right I
mean all engineering is based on some kind of abstraction but you might have a circuit level abstraction a certain kind
of hardware level that you're interested in describing the brain at and I'm mostly working out or starting from a more software level of abstraction right
they're all distractions we're not talking about molecules here right we're talking about some abstract notion of maybe a circuit or of a program okay
right now it's a really interesting question if I look at some circuits how do I know what program they're
implementing right if I look at the circuits in this machine could I tell what program they're implementing well maybe but certainly it would be a lot
easier if I knew something about what programs they might implementing before I start to look at the circuitry if I just looked at the circuitry without knowing what a program
was or what programs the thing might be doing or what kind of programming components would be mapable to circuits
in different ways right I don't even know how to begin to answer that question so I think you know we've made some
progress at understanding what neurons are doing in certain low-level parts of sensory system and certain parts of the
motor system like primary motor cortex like basically the parts of the neurons that are closest to the inputs and outputs of the brain right where we
don't eat when you can say we don't need the kind of software abstractions that I'm talking about or where we sort of
agree on what those things already are so we can make enough progress on knowing what to look for and how to how
to know when we found it but if you want to talk about flexible planning things that are more like cognition that you know go on in prefrontal cortex right I
this point I don't I don't think that just by recording from those neurons we're gonna be able to answer those
questions in a meaningful engineering way a way that that any engineer software a hardware whatever could
really say yeah okay I get it I get those insights in a way that I can engineer with and that's what my goal is right so my goal that's my goal to do at
the software level the hardware level or the entire systems level connecting them and I think that you know we can do that
by taking what we're doing and bringing into contact with people studying neural circuits but I don't think you can you can leave this level out and just go
straight to the neural circuits and I think the more you have the more progress we make the more we can help people who are studying at the neural
circuit level and they can help us address these other engineering questions that we don't really have access to like the power issue or the
speed issue thank you okay thanks that was great I thought maybe it'd give Jessica Han

----------

-----
--28--

-----
Date: 2018.02.03
Link: [# MIT AGI: Artificial General Intelligence](https://www.youtube.com/watch?v=-GV_A9Js2nM)
Transcription:


Intro
welcome to course six as $0.99 artificial general intelligence we will
explore the nature of intelligence from as much as possible and engineering
perspective you will hear many voices my voice will be that of an engineer our
MIT AGI Mission: Engineer Intelligence
mission is to engineer intelligence the
MIT motto is mind in hand what that means is we want to explore the
fundamental science of what makes an intelligence system the core concepts
behind our understanding of what is intelligence but we always want to
ground it in the creation of intelligent systems we always want to be in the now
in today in understanding how today we can build artificial intelligence
systems that can make for a better world that is the core for us here at MIT
first and foremost we're scientists and engineers our goal is to engineer
intelligence we want to provide with
this approach a balance to them very important but over represented view of
artificial general intelligence the black box reasoning view where the idea
is once we know how to create a human level intelligence system how will
society be impacted will robots take over and kill everyone will we achieve a
utopia that will remove the need to do any of the messy jobs that will make us
all extremely happy those kinds of beautiful philosophical concepts are
interesting to explore but that's not what we're interested in doing I believe that from an engineering perspective we
want to focus on the black box of a GI start to build insights and intuitions
about how we create systems that approach human level intelligence I believe we're very far away from
creating anything resembling human level intelligence however the dimension of
the metric behind the word Farr may not be time in time perhaps through a few
breakthroughs maybe even one breakthrough everything can change but as we stand now our current methods as
we will explore from the various ideas and approaches and the guest speakers coming here over the next two weeks and
beyond our best understanding our best intuition and insights are not yet at
the level of reaching without a major leap and breakthrough a paradigm shift
towards human level intelligence so it's not constructive to consider the impact
of artificial intelligence to consider questions of safety and ethics
fundamental extremely important questions we it's not constructive to
consider those questions without also deeply considering the black box of the
actual methods of artificial intelligence human level artificial intelligence and that's what I see what
Balance Between Paralyzing Technophobia and Blindness to Big Picture Consequences
I hope this course can be its first iteration its first exploratory attempt
to try to look at different approaches of how we can engineer intelligence
that's the role of MIT it's tradition of mine in hand it's to consider the big
picture the future impact of society 10 20 30 40 years out but fundamentally
grounded in what kind of methods do we have today and what are their
limitations and possibilities of achieving that the black box of a GI
in the future impact on society of creating artificial intelligence systems
that get become increasingly more intelligent the fundamental disagreement
lies in the fact the the very core of that black box which is how hard is it
to build an AGI system how hard is it to create a human level artificial
intelligence system that's the open question for all of us from from Josh
Tenenbaum to Andrey Carpathia to folks from open AI to Boston Dynamics to the
brilliant leaders in various fields of artificial intelligence that will come here that's the open question how hard
is it there's been a lot of incredibly impressive results in deep learning in
neuroscience and computational cognitive science in robotics but how far are we
still to go to the AGI that's the fundamental question that we need to explore before we consider the questions
the future impact on society and the goal for this class is to build
intuition one talk at a time a project at a time build intuition about where we
stand about what the limitations of current approaches are how can we close the gap a nice meme that I caught on
Twitter recently of the difference between the engineering approach at the
very simplest of a Google intern typing a for loop that just does a grid search
on parameters for a neural network and on the right is the way media would
report this for loop the Google AI created its own baby AI I think it's
easy for us to go one way or the other but we'd like to do both our first goal
is to avoid the pitfalls of black box thinking of the few tourism thinking that results in hype
that's detached from scientific engineering understanding of what the actual systems are doing that's what the
media often reports that's what some of our speakers will explore in a rigorous
way it's still an important topic to explore Ray Kurzweil's on Wednesday we'll look we'll explore this topic next
week talking about AI safety and autonomous weapon systems we'll explore this topic the future impact 10 20 years
out how do we design systems today that would lead to safe systems tomorrow still very important but the reality is
a lot of us need to put a lot more emphasis on the left on the four loops
on creating these systems at the same time the second goal of what we're
trying to do here is not emphasize the silliness the simplicity the naive basic
nature of this for loop in the same way as was the process in creating nuclear
weapons before during World War two the idea that as an engineer as a
scientist that I'm just the scientist is also a flawed way of thinking we have to
consider the big picture impact the near-term negative consequences that are
preventable the low-hanging fruit that can be prevented through that very engineering process we have to do both
and in this engineering approach we
always have to be cautious that just because we don't understand
we're just because we our intuition our best understanding of the capabilities
of modern systems that learn that act in this world seem limited seem far from
human level intelligence our ability to learn and represent common sense reasoning seems limited the exponential
potentially Exponential's could be argued and he will growth of technology
of these ideas means that just around the corner is a singularity is a breakthrough idea that will change
everything we have to be cautious of that moreover we have to be cautious of
the fact that every decade over the past century our adoption of new technologies
has gotten faster and faster the the rate at which a new technology from its
birth to its wide mass adoption has shortened and shortened and shortened
that means that new idea the moment it
drops into the world can have widespread effects overnight so as and I think the
in the engineering approach is fundamentally cynical on artificial general intelligence because every
aspect of its is so difficult we have to always remember that overnight everything can change through this
question of beginning to approach from a deep learning perspective deep reinforcement learning from brain
simulation computational cognitive science from computational neuroscience
from cognitive architectures from robotics from legal perspectives and
autonomous weapon systems as we begin to approach these questions we need to
start to build intuition how far away are we from creating intelligent systems
the singularity here is that spark that moment when we're truly surprised by the
intelligence of the systems we create I'd like to visualize it by the by a
certain analogy that we're in this dark room looking for a light switch with no
knowledge of where the light switch is there's going to be people that say well it's a smaller the rooms are all smaller
right there and say anywhere we'll be able to find it in any time the reality is we know very little so we have to
stumble around feel our way around to build the intuition a far far away we
really are many will speakers here will talk about
Human Drive to explore and Uncover the Mysteries of the Universe
how we define intelligence how we can begin to see intelligence what are the
fundamental impacts of creating intelligence systems I'd like to sort of see the positive reason for this little
class and for these efforts that have fascinated people throughout the century
of trying to create intelligent systems is that there's something about human beings that one that craves to explore
to uncover the mysteries of the universe fundamental in itself a desire to
uncover the mysteries of the universe not for a purpose and there's often an
underlying purpose of money of greed of the power craving for power and so on
but there's seems to be an underlying desire to explore nice little book an
exploration a very short introduction by Stewart Weaver he says for all the different forms it takes in different
historical periods for all the worthy and unworthy motives that lie behind it
exploration travel for the sake of discovery and adventure is a human
compulsion a human obsession even it is defining element of a distinctly human
identity and it will never rest at any frontier whether terrestrial or
extraterrestrial from 325 BCE with a
long 7500 mile journey on the ocean to
explore the Arctic to Christopher Columbus and his flawed harshly
criticized the modern scholarship trip that ultimately paved the way didn't
discover pave the way to colonization of the Americas to the DAR
trip the voyage of the Beagle whilst this planet has gone cycling on
according to the fixed law of gravity from so simple a beginning endless forms most beautiful and most wonderful have
been and are being evolved to the first
venture into space by Yuri Gagarin first
human in space in 1961 what he said over
the radio is the earth is blue it is amazing this these are the words they
think drive our exploration in the sciences in engineering and today an AI
and the first walk on the moon and now
the desire to colonize Mars and beyond
that's where I see this desire to create intelligent systems talking about the
positive or negative impact of AI on society talking about the business case of the jobs lost jobs gain jobs created
diseases cured the autonomous vehicles the ethical questions the safety of
autonomous weapons of the misuse of AI in the financial markets underneath it
all and they're people many people are spoken about this what drives myself and
many in the community is the desire to explore to uncover the mystery of the universe and I hope that you join me in
that very effort with the speakers that come here in the next two weeks and beyond the website for the course is a
GI that MIT died edu I am a part of an amazing team many of whom you know AGI
at MIT that edu is the email where on slack deep - MIT does slack for
registered MIT students you create account on the website and submit five new links and vote on ten to
vote AI which is an aggregator of information a material we've put together for the topic of AGI and submit
a entry to one of the competitions one of the three competitions projects that
we have in this course and the projects are dream vision I'll go over them in a
little bit dream vision angel ethical car and the aggregator of material vote
AI we have guest speakers incredible guest speakers I will go over them today and as before with a deep learning for
self-driving cars course we have shirts and they're free for in-person for
people that attend in person for the last lecture most likely or you can order them online
okay dream vision we take the Google G dream idea we explore the idea of
creativity where it is I ins view of intelligence the mark of intelligence is creativity this this idea is something
we explore by using neural networks and interesting ways to visualize what the
networks see and in so doing create beautiful visualizations in time through
video so taking the ideas of deep dream and combining them together with
multiple video streams to mix dream and reality and the competition is through
Mechanical Turk we set up a competition of who produces the most beautiful
visualization will provide code to generate this visualization and ideas of
how you can make it more and more beautiful and how to submit it to the
competition angel the artificial neural generator of emotion and language is a
ANGEL: Artificial Neural Generator of Emotion and Language
different twist on the Turing test where we don't use words we all
using motions to speak expression of those emotions and we create we use an
age a face customizable we're 26 muscles
all of which can be controlled with an LS TM we use a neural network to train
the generation of emotion and the
competition in you submitting the code to the competition is you get 10 seconds
to impress with the these expressions of emotion the viewer
it's a be testing your goal is to impress the viewer enough to where they
choose your agent versus another agent and those that are most loved the agents
most loved will be the ones that are declared winners in a twist we will add
human beings into this mix so we've created a system that map's our human
faces myself and the TAS to where we ourselves enter an outcome in the
competition and try to convince you to keep us as your friend that's the Turing test ethical car
EthicalCar: Machine Learning Approach
building and the ideas of the trolley problem and the moral machine done here
in the Media Lab the incredible interesting work we take a machine learning approach to it and take what
we've developed the deep reinforcement learning competition for success 0 9 for
the deep traffic and we add pedestrians into it stochastic astok irrational
unpredictable pedestrians and we add human life to the loss function where
there's a trade-off between getting from point A to point B so in deep traffic
the deep reinforcement learning competition the goal was to go as fast as possible here it's up to you to
decide what what your agents goal is there's a parade of front
trade-off between getting from point A to point B as fast as possible and
hurting pedestrians this is not a
ethical question it's an engineering question and it's a serious one because
fundamentally in creating autonomous vehicles that function in this world we
want them to get from point A to point B as quickly as possible the United States government insurance
companies put a price tag on human life we put that power in your hands
in designing these agents to ask the question of a how can we create machine
learning systems where the objective function the loss function has human life as part of it and vote AI is an
aggregator of different links different
articles papers videos on the topic of artificial general intelligence where people vote on vote quality articles up
and down and choose on the sentiment of
positive and negative we'd like to explore the different ways to the different arguments for and against
artificial general intelligence there is
an incredible list of speakers the best in their disciplines from Josh Tenenbaum
PN MIT to Ray Kurzweil at Google to lisa Feldman Barrett and Nader Pinsky from
Northeastern University Andre karpati Stephen Wolfram Richard
Moyes mark Robert Ilya sutskever and myself Josh Tenenbaum
Josh Tenenbaum, MIT Computational Cognitive Science
tomorrow I'd like to go through each of these speakers and talk about the
perspectives they bring that to try to see the approach the
ideas they bring to the table they're not in most cases interested in the
discussion of the future impact on society without grounding it into the
expertise into the actual engineering into creating these intelligent systems so josh is a computational cognitive
science expert professor faculty here at MIT he will talk about how we can create
common-sense understanding systems that see a world of physical objects and
their interactions and our own possibilities to act interact with others the intuitive physics how do we
build into systems the intuitive physics of the world more than just the deep
learning memorization engines that take patterns and learn through supervised
way to map those patterns to classification actually begin to
understand the intuitive the common-sense physics of the world and learn rapid model based learning learn
from nothing learn from very little just like we do as children just like we do as human being successfully often only
need one example to learn a concept how do we create systems that learn from
very few sometimes a single example and integrate ideas from various disciplines
of course from neural networks but also probabilistic generative models and symbol processing architectures it's
going to be incredible of course from a from a different area of the world
another incredible thinker intellectual speaker is Ray Kurzweil he'll be here on
Wednesday and 1:00 p.m. and he will do a whirlwind discussion of where we stand
with intelligence creating intelligent systems how we see natural intelligence our own human intelligence how we define
it how we understand it and how that transfers to the increasing exponential
growth of development of artificial general intelligence
Lisa Feldman Barrett, NEU Emotion Creation
something I'm myself very excited about is Lisa Feldman Barrett coming here on
Thursday she's written a book I believe how emotions are made she argues that
emotions are created that there is a distinction there's a detachment between
what we feel in our bodies the physical state of our bodies and the expression
of emotion from from body to the contextually grounded to the face
expressing that emotion which means now why is there as a person who is
psychology person in a fundamental engineering computer science topic like AGI because if emotions are created in
the way she argues and she'll systematically break it down that means we're learning societal as
human beings were learning societal norms of how to express emotion the idea of emotional intelligence is learned
which means we can have machines learn this idea it's a machine learn like it's
a human learning problem it's a machine learning problem in a little bit of a
twist she asked that instead of giving it talk I have a conversation with her
so it's going to be a little bit challenging and fun and she's great
looking forward to it and we'll explore different ways that we can get emotion
Re-Enacting Intelligence
expressed through video through audio through the project the angel project
that I mentioned so there's been worked in reenacting intelligence so well
reenacting mapping face to face mapping different emotions on video that was
previously recorded so if you can imagine that means we can take emotions
that we've created the kind of emotion creation we've been discussing and remap it on previous video that's one way to
see intelligence is taking raw human data that we already have and mapping
new computer-generated the the underlying fundamentals of human
but the surface appearance the representation of emotion visual or
auditory is generated by a computer it
could be in the embodied form
Sophia: Embodied Re-Enactment
[Music] very important to note for those
captivated by Sofia in the press or have seen these videos Sofia is an art exhibit she's not a
strong natural language processing system this is not an AGI system but
it's a beautiful visualization of embodying of Hollow it's a beautiful visualization of how
easy it is to trick us human beings that there's intelligence underlying something that the emotional expression
the physical embodiment and the emotional expression that has a that has
some degree of humor that has some degree of wit and intelligence is enough
to captivate us so that's an argument for not creating intelligence from scratch but having machines at the very
surface the display of that emotion the generation the mapping of the visual and
auditory elements worth underneath it is really trivial technology that's
fundamentally relying on humans like in the sophia's case and in the simplest
form we remove all elements of Hajus a
attractive appearance from from an agent we really keep it to the simplest
muscles characteristics of the face and see with 26 muscles controlled by a neural
network through time so recurrent neural network I was TM how can we explore the
generation of emotion can we get this thing and this is an open question for us too we just created the system we
don't know if we can can we get it to make us feel something make us feel
something by watching it express its feelings can it become human before our
eyes can I learn to by competing against other agents a be testing on Turk a
Mechanical Turk can the winners be very convincing to make us feel entertained
pity love maybe some of you will fall in love with angel here Nate dibinsky on
Nate Derbinsky, NEU Cognitive Modeling
Friday will talk about cognitive modeling architectures so you will speak about the cognitive modeling aspect can
we have a ma can we model cognition in some kind of systematic way to try to
build intuition of how complicated cognition is Andre karpati famous for
being the state-of-the-art human on the imagenet challenge the representative
the 95% accuracy performance among other things he's also famous for his now a
Tesla he will talk about the role the limitations the possibilities of deep
learning we'll talk as I have spoken
about in the past few weeks and throughout about our misunderstanding or
our flawed intuition about what are the difficult and what are the easy problems in deep learning and the power of
Deep Learning is Representation Learning Talca Feature Learning
representational learning the ability of neural networks to form deeper and deeper representations of the underlying
raw data that ultimately forms
takes complex information that's hard to make sense of and convert it into useful
actionable knowledge that is from a
certain lens in a certain a certain lens in a certain problem space can be
clearly defined as understanding of the complex information understanding is
ultimately taking complex information and reducing it to its simple essential elements representational learning is in
the trivial case here in drawing having to draw a straight line to separate the
blue and the red curves that's impossible to do in the in initial input
space on the Left what the act of learning is for deep neural networks in this formulation is to construct a
topology under which there exists a straight line to accurately classify blue versus red that's the problem and
for a simple blue and red line it seems trivial here but this works in the general case for arbitrary input
spaces for arbitrary nonlinear highly dimensional input spaces and the ability
to automatically learn features to learn hierarchical representations of the raw
sensory data means that you could do a lot more with data which means you can expand further and further and further
to create intelligent systems that operate successfully with real-world
data that's what representational learning means that deep learning allows because the arbitrary number of features
that can be automatically determined you can learn a lot of things about a pretty
complex world unfortunately there needs to be a lot of supervised data there still needs to be
a lot of human input Andre and others
Josh will talk about the difference between our human brain our biological
neural network and the artificial neural network the full human brain with 100
billion neurons 1000 trillion synapses and the biggest neural networks out
there the artificial neural network having much smaller 60 million synapses
for ResNet 152 the biggest difference the parameter is a human brain being
Neuron: Biological Inspiration for Computation
several orders of magnitude more synapses the topology being much more
complex chaotic the asynchronous nature of the human brain and the learning
algorithm of artificial neural networks is trivial and constrained with
backpropagation is essentially an optimization function over over a clearly defined loss function from the
output to the to the input using back propagation to teach to adjust the
weights on that network the learning algorithm for our human brain is mostly
unknown but it's certainly much more complicated than back propagation the
power consumption the human brain is a lot more efficient than artificial neural networks and there's a very kind
of artificial trivial supervised learning process for training artificial
neural networks you have to have a training stage and you have to have an evaluation stage and once the network is
trained there's no clear way to continue training it or there's a there's a lot of ways but they're inefficient it's not
designed to do online learning naturally to always be learning is designed to be
to learn and then be applied obviously our human brains are always learning but
the beautiful fascinating thing is that they're both distributed computation
systems on a large scale so it's not a there's it doesn't ultimately boil down
to a single compute unit the computation is distributed the back propagation
learning process is distributed can be paralyzed in a GPU massively paralyzed the underlying computational unit of a
neuron is trivial but can be stacked together to form forward neural networks recurrent neural networks to represent
both spatial information with images and temporal information we
the audio speech text sequences of images and video and so on mapping from
one-to-one one-to-many many-to-one so the mapping any kind of structure vector
and time data as an input to any kind of classification regression sequences
captioning video audio as output learning in the general sense but in a
domain that's precisely defined for the supervised training process we can think
Deep Learning from Human and Machine
of the in deep learning case you can think of the supervised methods where
humans have to annotate the data as memorization of the data we can think of the exciting new and growing field of
semi-supervised learning where most of the data through or through generative adversarial networks or through
significant data augmentation clever data augmentation most of it is done automatically the annotation process or
through simulation and then reinforcement learning where most of the most of the labels are extremely sparse
and come rarely and so the system has to figure out how to operate in the world with very little human input very little
human data we can think of that as reasoning because you take very little
information from our teachers the humans and transfer it across generalize it across to reason about the world and
finally unsupervised learning the excitement of the community that promise the hope you could think of that as
understanding because ultimately it's taking data with very little or no human input and forming representations that
that data is how we think of understanding requiring making sense of
the world without strict input of how to make sense of the world the kind of
process of discovering information maybe discovering new ideas new ways to
simplify the world to represent the world that you can do new things with it the new is the key element there
understanding and Andre and Ilya and others will talk
Past and Future of Deep Learning Breakthroughs
about the certainly the past but the future of deep learning where is it going to go
is it overhyped underhyped what is the future will the compute of cpu GPU si
Asics continue with the breakthroughs the Moore's law in its various forms of
massive parallelization continue and the large datasets with tens of millions of
images grow to billions and trillions will the algorithms improve is there a
groundbreaking idea that's still coming look with Jeff hiddens capsule networks
is there fundamental architectural changes to neural networks that we can come up with that will change everything
that will ease the learning process they'll make the learning process more efficient or we'll be able to represent
higher and higher orders of information such that you can transform knowledge
between domains and the software architectures that support intensive
florida pi torch i would say last year and this year will be the year of deep learning frameworks so those will
certainly keep coming in their various forms and the financial backing is growing and growing the open challenges
Current Challenges
for deep learning really a lot of this course is kind of connected to deep learning because that's where a lot of
the recent breakthroughs that inspire us to think about intelligence systems come
from but the challenges of many the need the ability to transfer between different domains as in reinforcement
learning and robotics the need for huge data in an official learning that we
still need supervised data an ability to learn in an unsupervised way is a huge
problem and not fully automated learning there's still a degree a significant
degree of hyper parameter necessary with the reward functions the loss functions are ultimately defined by humans and
therefore are deeply flawed when we release those systems into the real
world there is no ground truth for the testing set and the goal isn't achieving a class
high classification on a trivial image classification localization detection
problem but rather to have a autonomous vehicle that doesn't kill pedestrians or
an industrial robot that operates in jointly with other human beings and all
the edge cases that come up how does deep learning methods how do machine learning methods generalize over the
edge cases the weird stuff that happens in the real world those are all the problems there Stephen Wolfram will be
Stephen Wolfram Knowledge-Based Programming
here on Monday evening at 7 p.m. has done a lot of amazing things I would say
is very interesting from his recent interest in knowledge based programming Wolfram Alpha I think is the fuel for
most middle school and high school students now for the first time taking
calculus I pray probably go to Wolfram Alpha to answer their own questions but more seriously there is a a deep
connected graph of knowledge is being built there with the Wolfram or Wolfram
Alpha and Wolfram language that steel will explore in terms of language an
interesting thing he was part of the team on arrival that worked on the
language if for those of you are familiar the arrival were a alien
species spoke with us us humans through a very interesting beautiful complicated
language and he was brought in as a representative human to interpret that language just like in the movie he was
represent that in real life and you used the skills that him and his son Christopher used to analyze this
language very interesting that process is extremely interesting I hope he talks about it and his background with
"Artificial Life Simulation": Cellular Automata and Emerging Complexity
Mathematica and new kind of science the
sort of another set of ideas that have
inspired people in terms of creating
intelligence systems is the idea that from very simple things were very simple
rules extremely complex patterns can emerge his work was cellular automata
did just that taking extremely simple mathematical constructs here with
cellular automata these are these are grids of computational units that switch
on and off in some kind of a predefined way and only operate locally based on their local neighborhood and somehow
based on different kinds of rules different patterns emerge here's a three dimensional cellular automata with a
simple rule starting with nothing with a single cell they grow in really interesting complex ways this emergent
complexity is inspiring it's the same kind of thing that inspires us about
neural networks that you can take a simple computational unit and when combined together in arbitrary ways can
form complex representations that's also very interesting you can see knowledge
from a knowledge perspective you can see knowledge formation in the same kind of way simplicity at a mass distributed
scale resulting in complexity next Tuesday
Richard Moyes, Article36 Al Safety and Autonomous Weapon Systems
Richard Moyes from article 36 coming all the way from UK for us we'll talk about
it works with autonomous weapons systems works with also nuclear weapons but
primarily autonomous weapon systems and concern legal policy and technological
aspects of banning these weapons there's been a lot of agreement about the safety
hazards of autonomous systems that make decisions to kill a human being
mark Robert CEO of Boston Dynamics previously a long time ago faculty here
Marc Raibert, CEO, Boston Dynamics Robots in the Real World
at MIT will talk about will bring robots and talk to us about his work of robots
in the real world as doing a lot of exciting stuff with humanoid robotics and any kind of robots operating on legs
it's incredible work extremely exciting and gets to explore the idea of how difficult it is
to build these robot systems that operate in the real world from both the
control aspect and from the way the
final result is perceived by our society it's very interesting to see when
intelligence in robotics is embodied and then taking in by us and what that
inspires fear excitement hope concern and all the above Ilya sutskever is
expert in many aspects of machine learning he is the co-founder of open AI I'll
talk about their different aspects of game playing that they've recently been
exploring about using deeper enforcement learning to play our K games in D on the
deep mind side using deep reinforcement learning to beat the best in the world that the game of go in 2017 the big
fascinating breakthrough achieved by that team with alphago zero training an agent that through self play playing
itself not on expert games so truly from scratch learning to beat the best in the
world including the previous iteration of alphago we will explore what aspects of the
stack of intelligent robotic systems intelligent agents can be learned in
this way so deep learning the memorization the supervised learning memorization approach it looks at the
sensor data feature extraction representation learning aspect of this taking the sensor data from camera light
our audio extracting the features forming higher-order representations and
on those representations learning to actually accomplish some kind of classification regression task figuring
out based on the representation what is going on in the raw sensory data and then combining that data together to
reason about it and finally in the robotic domains taking it all together
as with human industrial robotics autonomous vehicles
taking all together and actually acting in this world with the effectors and the
open question is how much of this AI stack can be learned that's something
for us to discuss to think about that a Leo will touch on with deeper
enforcement learning we can certainly learn representations and perform classifications state-of-the-art better
than human and image classification imagenet and segmentation tasks and the
excitement of deep learning is what's highlighted there in the red box can be done end to ends raw sensory data out to the knowledge to
the output to the classification can we begin to reason is the open question with the knowledge based programming
that Stephen Wolfram will talk about can we begin to take these automatically generated high order representations and
combine them together to form knowledge bases to form aggregate graphs of ideas
that can then be used to reason and can we then combine them together to act in
the world for whether in simulation with arcade games or simulation of autonomous
vehicles or biotic systems or actually in the physical world with robots moving about can that end end from raw sensory
data to action be learned that's the open question for for artificial general
intelligence for this class can this entire process be end to end can we
build systems and how do we do it that achieve this process end to end in the
same way that humans do we're born in this raw sensory environment taking in very little information and learn to
operate successfully in arbitrary constraints arbitrary goals and to do so
we have lectures we have three projects and we have guest speakers from various
disciplines I hope that all these voices will be heard and will feed a
conversation artificial intelligence and it's positive and it's concerning effects in
society and how do we move forward from an engineering approach the topics will
be deep learning deep reinforcement learning cognitive modeling computational cognitive science emotion
creation knowledge based programming AI safety with autonomous weapon systems and personal robotics with human
centered artificial intelligence that's for the first two weeks of this class that's the the part where if you're
actually registered students that's where you need to submit the project that's when we all meet here every every
night with the incredible speakers but this will continue we already have several speakers scheduled in the next
couple of months yet to be announced but they're incredible and we have
conversations on video and we have new projects I hope this continues throughout 2018 on
the topics of IAI ethics and bias there's a lot of incredible work in we
now have a speaker there coming on the topic of how do we create artificial intelligence systems that I do not
discriminate do not form the kind of biases that us humans do in this world
that are operating under social norms but our reasoning beyond the flawed
aspects of those social norms with bias creativity as with a project of dream
vision and beyond there is so much exciting work in charin using machine
learning methods to create beautiful art and music brain simulation neuroscience
competition in neuroscience shockingly in the first two weeks we don't have a competition neuroscience speaker which
is a fascinating perspective brain simulation or neuroscience in general
computational neuroscience is a fascinating approach from the from the muck of actual brain work to get the
perspective of how our brain works and how we can create something that mimics that resembles the fundamentals of what
makes our brain intelligent and finally the touring test the traditional definition of intelligence defined by
Alan Turing was grounded in natural language processing creating chat BOTS that impress us that amaze us and trick
us into thinking they're human we will have a project and a speaker on natural
language processing in March with that I'd like to thank you for coming today
and look forward to seeing your submissions for the three projects thank you very much

----------

-----
--27--

-----
Date: 2018.01.30
Link: [# MIT 6.S094: Deep Learning for Human Sensing](https://www.youtube.com/watch?v=Z2GfE8pLyxc)
Transcription:


Intro
today we will talk about how to apply the methods of deep learning to
understanding the sense of the human being the focus will be on computer vision the visual aspects of a human
being of course we humans express ourselves visually but also through audio voice
and through text beautiful poetry and novels and so on we're not going to
touch those today we're just going to focus on computer vision how we can use computer vision to extract useful
actionable information from video images video of human beings in particular in
the context of the car so what are the
requirements for successfully applying deep learning methods in the real world so when we're talking about human
sensing we're not talking about a basic face recognition of celebrity images
we're talking about using computer vision deep learning methods to create
systems that operate in the real world and in order for them to operate in the real world there are several things they
sound simple some are much harder than they sound first and the most important
here for most to less more to less critical ordered is data data is
everything real world data we need a lot of real world data to form the data set
on which these supervised learning methods can be trained I'll say this
over and over throughout the day today data is everything that means data collection is the hardest part and the
most important part we'll talk about how that data collection is carried out here in our group at MIT all the different
ways to capture human beings in the driving context in the road user context pedestrians cyclists but the data it
starts and ends at data the fun stuff is the algorithms but the data is what
makes it all work real world data okay then once you have the data okay data
isn't everything I lied because you have to actually annotate it so what do we mean by data there's raw data video
audio lidar all the types of sensors we'll talk about to capture real world
you wrote user interaction you have to reduce that into meaningful
representative cases of what happens in that real world in driving 99% of the
time driving looks the same it's the it's the 1% the interesting cases that we're interested in and what we want is
algorithm to train learning algorithms on those 1% so we have to collect 100
percent we have to collect all the data and then figure out and automated semi-automated ways to find the pieces
of that data that could be used to train your own networks and that a representative of the general thing
kinds of things that happen in this world efficient annotation annotation
isn't just about drawing bounding boxes on images of cats annotation tooling is
key to unlocking real world performance
systems that successfully solve some problem accomplish some goal in real
world data that means designing annotation tools for a particular task annotation tools that are used for
glance classification for determining where drivers are looking it's very different than annotation tools used for
body pose estimation is very different than the tooling use that we use for
psyche views investing thousands of dollars for the competition for this class to annotate fully scene
segmentation where every pixel is colored there's needs to be tooling for each one of those elements and they're
key that's HCI question that's a design question there's no deep learning
there's no robotics in that question it's how do we leverage human
computation human the human brain to mow effectively label images such that we
can train y'all networks on them hardware in order to train these
networks in order to parse the data we collect and we'll talk about we have now
over five billion images of data of driving data in order to parse that you
can't do it on a single machine you have to do large-scale distributed compute
and large-scale distributed storage and finally the the stuff that's the most
exciting that people that there's this class and many classes and much of the
literature is focused on is the algorithms the deep learning algorithms the machine learning algorithms the
algorithms that learn from data of course that's really exciting and important but what we find time and time
again in real world systems is that as long as these algorithms learn from data
so as long as this deep learning the data is what's much more important of
course it's nice for the algorithms to be calibration free meaning they learn
to calibrate self calibrate we don't need to have the sensors in an exact same position every time that's a very
nice feature the robustness of the system is then generalizable across multiple multiple vehicles and multiple
scenarios and one of the key things that comes up time again time and time again
and we'll mention today is a lot of the algorithms developed in deep learning are really focused for computer vision
are focused on single images now the real world is happens in both space and
time and we have to have algorithms that both capture the visual characteristics but also look at the sequence of images
sequence of those digital characteristics that form the temporal dynamics the physics of this world so
it's nice when those algorithms are able to capture the physics of the scene
the big takeaway I would like if you leave with anything today
unfortunately it's that the painful boring stuff of collecting data of
cleaning that data of annotating that data in order to create successful
systems is much more important than good algorithms or great algorithms it's important to have good algorithms as
long as you have neural networks that learn from that data okay so today I'll
Human Imperfections
talk I like to talk about human imperfections and the various detection
problems the pedestrian body pose glance and motion cognitive load estimation
that we can use to help those humans as they operate in the driving context and
finally try to continue with the idea of
the vision that fully autonomous vehicles as some of our guest speakers have spoke about and sterling anis will
speak about tomorrow is really far away that the humans will be an integral part
of the operating cooperating with the AI systems and I will continue on on that
line of thought to try to motivate why we need to continuously approach the
autonomous vehicle the self-driving car paradigm in the human centered way okay
first before we talk about human imperfections let's just pause and
acknowledge that humans are amazing we're actually really good at a lot of
things that's sometimes sort of fun to talk about how much called terrible of
drivers who are how distracted we are how irrational we are but we're actually really damn good at driving here's a
video of stadia our soccer player messi the best soccer player in the world
obviously and the state-of-the-art robot on the right same thing
well there's it's not playing but I assure you the American Ninja Warrior
Casey is is uh is far superior to the
DARPA humanoid robotics systems shown on the right okay so continuing and the
line of thought to challenge to challenge us here that humans are amazing is you know there's record high
in 2016 in the United States there was over forty thousand since uh many years
it's across the forty thousand fatalities mark more than forty thousand people died in car crashes in the United
States but that's in three point two trillion miles traveled so that's one
fatality per eighty million miles that's one in 625 chance of dying in a car
crash in your lifetime interesting side fact for anyone in the United States
folks who live in Massachusetts are the least likely to die in a car crash
Montana is the most likely so for every
one that thinks of Boston drives is terrible maybe that adds some
perspective here's a visualization of ways data across a period of a day
showing you the rich blood of the city that the the traffic flow of the city the people getting from A to B and a
mass scale and doing it surviving doing it okay humans are amazing but they're
also flawed texting sources of distraction with a smartphone the eating
the secondary tasks of talking to other passengers grooming reading using
navigation system yes sometimes watching video and manually adjusting or
adjusting the radio and 3,000 people were killed and 400,000 were injured in
motor vehicle crashes vaulted involving distraction in 2014 distraction is a it's a very
serious issue for safety texting every day more and more people text
smartphones are proliferating our society 170 billion text messages are
sent in the United States every month that's in 2014 you can only imagine what it is today
eyes off road for five seconds is the average time your eyes off the road while texting five seconds if you're
traveling 55 miles an hour in that five seconds that's enough time to cover the length of a football field
so you're blindfolded you're not looking at the road in five seconds the average time of texting you're covering the
entire football field eight so many things can happen in that moment of time
that's distraction drunk driving 31% of
traffic fatalities involve a drunk driver drunk driving 23% of nighttime
drivers tested positive for a legal prescription or over-the-counter medication distracted driving as I said
is a huge safety risk drowsy driving people driving tired nearly three
percent of all traffic fatalities involve a drowsy driver if you are
uncomfortable with videos that involve risk I urge you to look away these are
videos collected by Triple A of teenagers a very large-scale naturalistic driving data set and it's
capturing clips of teenagers being distracted on their smartphone
[Music]
once you take it in the problem we're against
so in the cutting context of human imperfections we have
to ask ourselves is the human centered approach to autonomy in systems autonomous vehicles that are using
artificial intelligence to aid the driving task do we want to go as I mentioned a couple of lectures ago the
human centered way or the full autonomy way the tempting path is towards full autonomy where we removed this imperfect
flawed human from the picture altogether and focus on the robotics problem of
perception and control and planning and driving policy or do we work together
human and machine to improve the safety to alleviate distraction to bring drive
our attention back to the road and use artificial intelligence to increase safety through collaboration human robot
interaction versus removing the human completely from the picture as I've
mentioned as as sterling will certainly talk about tomorrow and and rightfully
so and yesterday or on Tuesday Emilio has talked about the elf four-way is
grounded in literature it's grounded in common sense since in some sense it's
you can count on the fact that humans the the natural flaws of human beings to
over trust to misbehave to be irrational about their risk estimates will result
in improper use of the technology and that leads to what I've showed before
the public perception of what drivers do and semi autonomous vehicles they begin to over trust the moment the system
works well they begin to over trust they begin to do stuff they're not supposed to be doing in the car taking it for
granted a recent video that somebody posted this is a common sort of more
practical concern that people have is while the traditional ways to ensure the
physical engagement of the driver is by saying they should touch the wheel the the steering wheel every once in a while
and of course there's ways to buy the need to touch the steering wheel
some people hang objects like I can off of the steering wheel in this case
brilliantly I have to say they shove an orange into the into the wheel to make
the touch sensor fire and therefore be able to take their hands off the autopilot and that that kind of idea
makes us believe that there's no way that you know humans will find a way to misuse this technology however I believe
that that's not giving the technology enough credit artificial intelligence
systems if are they're able to perceive the human being are also able to work with the human being and that's what I'd
like to talk about today teaching cars to perceive the human being and it all
starts with data it's all about data as I mentioned data is everything in these
real world systems with the MIT naturalistic driving data set of 25
vehicles of which 25 and 21 and equipped with Tesla autopilot we instrument them
this is what we do the data collection two cameras on the driver will see the cameras on the face capturing
high-definition video of the face that's where we get the glance classification the emotion recognition cognitive load
everything coming from the face that we have another camera or a fisheye that's looking at the body of the driver and
that from that comes the body pose estimation hands on wheel activity
recognition and then one video looking out for the full scene segmentation for all the scene perception tasks and
everything is being recorded synchronized together with GPS with audio with all the can covered from the
car on a single device synchronization of this data is critical so that's one
road trip in the data where thousands like it traveling hundreds of miles
sometimes hundreds of miles under automated control and autopilot that's
the data again as I said data is everything and from this data we can both gain
understanding what people do which is really important to understand how
autonomy successful autonomy can be deployed in the real world and to design
algorithms as for training for training the deep learning the deep neural
networks in order to perform the perception tasks better twenty five
beagles 21 Tesla's Model S Model X and
now model three over a thousand miles collected a day every single day we have
thousands of miles in the Boston Massachusetts area driving around all of that video being recorded now over five
billion video frames there are several
ways to look at autonomy one of the big ones is safety that's what everybody
talks about how do we make these things safe but the other one is enjoyment do
people actually want to use it it we can create a perfectly safe system we can
create it right now we've had it for ever before even cars a car that never
moves is a perfectly safe system well not perfectly but almost and but it
doesn't provide a service that's valuable it doesn't provide an enjoyable driving experience so okay what about
slow moving vehicles that's an open question the reality is with these Tesla
vehicles and l2 systems doing automated driving people are driving 33% of miles
using Tesla autopilot what does that mean that means that people are getting
value from it they a large fraction of their driving is done an automated way
that's value that's enjoyment the glance
suffocation algorithm we'll talk about today is used as one example that we use
to understand what's in this data shown with the bar graphs there and the red and the blue red is during manual
driving blues during autopilot driving and we look at glance classification regions of where drivers are looking on
road and off-road and if that distribution changes with automated driving or manual driving and would
these glass classification methods we can determine that there's not much difference at least until you dig into
the details which we haven't done and the aggregate there's not a significant difference that means people are getting
value enjoying using these technologies but yet they're staying attentive or at
least not attentive but physically engaged when your eyes are on the road
you might not be attentive but you're at the very least physically your body's
position in such a way your head is looking at the forward roadway that you're physically in position to be
alert and to take in the forward roadway so they're using it and they don't over
trust it and that's I think the sweet spot that human-robot interaction needs
to achieve is the human gaining through
experience through exploration through trial and error exploring and understanding the limitation of the
system to a degree that over trust can occur that seems to be happening in this system and using the computer vision
methods I'll talk about we can continue to explore how that can be achieved in other systems when the when the when the
fraction of automated driving increases from 30% to 40% to 50% and so on it's
all about the data and I'll I'll harp on this again the algorithms are interesting you know I will mention of
course it's the same convolution neural networks it's the same networks that
take in raw pixels and extract features of interest it's 3d convolutional neural
networks that take into sequences of images and extract the temporal dynamics along with the visual characteristic for
the individual images it's RN and zoella's TMS that use the convolutional
neural networks to extract features and over time look at the dynamics and the
images these are pretty basic architecture is the same kind of deep neural network architectures but they
rely fundamentally and deeply on the data on real-world data so let's start
Pedestrian Detection
where perhaps on the human sensing side it all began which is pedestrian detection decades ago to put it in con
texe pedestrian detection here shown from left to right on the left is green showing the easier human sensing tasks
tasks of sensing some aspect to a human being but as for your detection which is detecting the full body of a human being
in an image or video is one of the easier computer vision tasks and on the
right under in the red microcircuits these are the tremors of the eye or
measuring the pupil diameter or measuring the cognitive load or the fine blink dynamics of the eye the velocity
of the blink micro glances and I pose are much harder problems
so you think body pose estimation pedestrian detection phase classification detection recognition
head pose estimation all those are easier tasks anything that starts getting smaller looking at the eye and
everything that start getting fine-grained there's much more difficult so we start at the easiest pedestrian
detection and as the usual challenges of all of computer vision we've talked
about as the various styles of appearance so the inter class variation
the different possible articulations of put it of our bodies superseded only
perhaps by cats but as humans are pretty flexible as well the presence of
occlusion from the accessories that we wear to occluding self occlusion and including each other but that crowded
scenes have a lot of humans in them and they include each other and therefore to be able to disambiguate to figure out
each individual pedestrians is a very challenging problem so how do people approach this problem well there is I
need to extract features from raw pixels
whether that was hot cascades hog or CNN the through the decades the sliding
window approach was used because the pedestrians can be small in an image or big so there's the problem of scale so
you use a sliding window to detect where that pedestrian is you have a classifier
that's given a single image such as this that's you're not you take that classify you slide across the image to find where
all the pedestrians of scene are so you can use non neural network methods or you can use convolution neural networks
for that classifier it's extremely inefficient then came along our CNN fast
our CNN fast our CNN these are networks that as opposed to doing a complete
sliding window approach are much more intelligent clever about generating the
candidates to consider so as opposed to considering every possible position of a window different scales of the window
they generate more a small subset of candidates that are more likely and
finally using a CNN classify for those candidates whether there's a pedestrian or not whether the there's an object of
interest or not a face or not and using that maximum suppression because there's
overlapping bounding boxes to figure out what is the most likely bounding box around this pedestrian around this
object that's our CNN and there's a lot of variants now with masks our CNN
really the state-of-the-art localization Network mask also adds to this on top of
the body box also performed segmentation there's voxel net which does three-dimensional and light our data
uses localization and point clouds so it's not just using it to the images but in 3d but it's it's it's all kind of
grounded in the our CNN framework ok data so we have large-scale data
collection going on here in Cambridge if you've seen cameras a lidar various intersections throughout MIT we're part
of that so for example here's one of the intersections to collecting about 10 hours a day instrumenting it with
various sensors I'll mention but we see about 12,000 pedestrians a day across
that particular intersection using 4k cameras using stereo vision cameras 360
now the insta 360 which is an 8k 360 camera gopro lidar various sizes the 64
channel of the 6 and recording this is where this is the
this is where the data comes from this is from the 360 video this is from the
lidar data of the same intersection this is for the 4k camcorders pointing at a
different intersection and the different than capturing the entire 360 view with
the vehicles approaching in the pedestrians making crossing decisions this is understanding the negotiation
that pedestrian is the nonverbal negotiation that pedestrians perform and choosing to cross or not especially when
they're jaywalking and everybody jaywalks especially if you're familiar
with this particular intersection there's more Jay walkers than non jaywalkers it's a fascinating one and so
we record everything about the driver and everything about the pedestrians
again our CNN this is where it comes in is you do Bonney box detection of the
pedestrians here are the vehicles as well and allows you to convert this raw data into hours of pedestrian crossing
decisions and begin to interpret it that's pedestrian detection bounding box
for body pose estimation is the more
Body Pose Estimation
difficult task body pose estimation is also finding the joints the hands the
elbows the shoulders the hips knees feet the landmark points in the image XY
position that marked that those joints that's body pose estimation so why is
that important in driving for example it's it's important to determine the vertical position or the alignment of
the driver the seatbelts and the sort of the the airbag testing is always
performing the seatbelt testing is performed with the dummy considering the frontal position in a standard dummy
position the the greater greater degrees of automation comes more capability and
flexibility for the driver to get misaligned from the standard corner dummy position and so body pose or at
least upper body pose estimation allows you to determine how often these drivers get out of line from the standard
position the general movement and then you can look at hands on wheel smartphone smartphone detection activity
and help add context to glance estimation that which we'll talk about
so some of the more traditional methods were sequential is detecting first the
head and then stepping detecting the shoulders the elbows the hands the
depot's holistic view which has been the very powerful successful way for multi
person pose estimation is performing a regression of detecting body parts from
the entire image it's not sequentially stitching bodies together it's detecting
the left elbow the right elbow the hands individually it's performing that
detection and then stitching everything together afterwards allowing you to deal
with the crazy deformations of the body that happened the occlusions and so on
because you don't need all the joints to be visible and with this cascade of pose
regressors meaning these are convolutional neural networks had taken a raw image and produce an XY position
of their estimate of each individual joint input as an image output is an
estimate of a joint of elbow shoulder whatever one of several landmarks and
then you can build on top of that every estimation zooms in on that particular
area and performs a finer and finer grain estimation of the exact position
of the Joye repeating it over and over and over so through this process we can
do part detection and multi-person and multi-person scene that contain multiple
people so we can detect the the head the neck here the hands the elbows shown in
the various images on the right that don't have an understanding who the head the elbows the the hands belong to
it's just performing a detection without trying to do individual person detection
first and then finally connecting or not
finally but next step is connecting with part affinity fields is connecting those
parts together so first you detect individual parts then you connect them together and then through bipartite
matching you determine which is who is that each individual body part most likely belonging to so you kind of
stitch the different people together in the scene after the detection is performed with the CNN
we use this approach for detecting the upper body specifically the shoulders
the neck and the head eyes nose ears that is used to determine the the
position of the driver relative to the standard dummy position for example looking during autopilot driving
30-minute periods we can look at on the x-axis is time and the y-axis is the
position of the neck point that I pointed out in the previous slide that the the the midpoint between the two
shoulders the neck is the position over time relative to where it began this is
the slouching the sinking into the seat allowing the car to know that
information and allowing us or the designers of safety systems and all that information is really important we can
use the same body pose algorithm to from the perspective of the vehicle outside the vehicle perspective so the vehicle
looking out is doing the as opposed to just plain pedestrian detection using body pose estimation again here in
Kendall Square vehicles crossing observing pedestrians making crossing
decisions and performing body pose estimation which allows you to then
generate visualizations like this and gain understanding like this on the
x-axis is time on the y-axis is on the top plot in blue is the speed of the
vehicle the speed of the vehicle the ego vehicle from which the camera is
observing the scene and on the bottom in green up and down as a binary value
whether the Podesta when the pedestrian is not looking at the car one when the pedestrian is looking at the car so we
can look at thousands of episodes like this crossing decisions nonverbal communication decisions and determine
using body pose estimation the dynamics of this nonverbal
here just nearby by media lab crossing there's a pedestrian approaches we can
look in green there when the pedestrian glasses looks away glasses the car looks away fascinating glance behavior that
happens interesting most people look away before they cross same thing here
this is just an example we have thousands of these body pose estimation allows you to get this fine-grained
information about the pedestrian glance behavior pedestrian body behavior
hesitation glass classification one of
Glance Classification
the most important things in driving is determining where drivers are looking it
if there's any sensing that I advocate
and is has the most impact in the driving context is for the car to know
where the driver is looking and at the very crude region level information of
is the driver looking on road or off road that's what we mean by glance classification it's not the standard
gaze estimation problem of X Y Z determining where the eye pose and the head pose combined to determine where
the driver is looking no this is classifying two regions on road off-road
or six regions on road off road left right center stack rearview mirror and
instrument cluster so it's region based glance allocation not the geometric gaze
estimation problem why is that important it allows you to address it as a machine
learning problem it's a subtle but critical point every problem we try to solve in human sensing in driver sensing
has to be learn about from data otherwise it's not it's not amenable to
application in the real world we can't design systems in the lab that are
deployed without learning if they involve a human it's possible to do slam
localization by having really good sensors and doing localization using
those sensors without much learning it's not possible to design systems that deal with lighting variability and the full
variability of human behavior without being able to learn so gaze estimation
the geometric approach of finding the landmarks in the face and from those landmarks determining the the Jeremie
the orientation of the head and the orientation of the eyes there's no learning there outside of actually
training the systems to detect the different landmarks if we convert this
into a gaze classification problem shown here glass classification is when taking
the raw video stream determining in post so humans are annotating this video is
the driver which region the driver is looking at that's we're able to do by
converting the problem into a simple variant of classification on-road off-road left-right the same can be done
for pedestrians left forward right it can annotate regions of where they are
looking and using that kind of classification approach determine are
they looking at the cars or not are they looking away are they looking at their smartphone without doing the 3d gaze
estimation again it's a subtle point but think about it if you wanted to estimate exactly where they're looking
you need that ground truth you don't have that ground truth unless you there
there's no in the real world data there's no way to get the information about where exactly people were looking
you're only inferring so you have to convert it into a region based classification problem in order to be
able to train your networks on this and the pipeline is the same the source
video here the face the the 30 frames a second video coming in of the drivers
face of the human face there is some degree of calibration that's required you have to determine approximately
where the sensor is that's taking in the image especially for the glance classification task because its region
based needs to be able to estimate where the forward roadway is where the the camera
frame is relative the world frame the video stabilization and the face front
elevation all the basic processing they've removed the vibration of the noise that remove the physical movement
of the head that removed the shaking of the car in order to be able to determine
stuff about eye movement and blink dynamics and finally with the neural networks there is nothing left except
taking in the raw video of the face for the glass classification tasks and the
eye for the cognitive load tasks raw pixels that's the input to these networks and the output is whatever the
training data is and we'll mention each one so whether that's cognitive load
glance emotion drowsiness the input is the raw pixels and the output is
whatever you have data for data is everything here the face an alignment
problem which is a traditional geometric approach to this problem is designing
algorithms that are able to detect accurately the individual landmarks in the face and from that estimate the
geometry of the head pose for the class
of in version we perform the same kind of alignment or with the same kind of face
detection in alignment to determine where the head is but once we have that we pass in just the raw pixels and
perform the classification on that as opposed to doing the estimation its
classification allowing you to perform what's shown there on the bottom is the
real-time classification of where the driver is looking Road left right center
stack instrument cluster and rearview mirror and as I mentioned annotation
tooling is key so we have a total 5 billion video frames one and a half
billion of the face that would take tens
of millions of dollars to annotate just for the glass classification fully so we
have to figure out what to annotate in order to trade and you'll networks to perform this task and what we annotate
is the things that the network is not confident about the moments of highlighting variation the partial
occlusions from the light or self occlusion and the moving out of frame the outer frame occlusions all the
difficult cases going from frame to frame to frame here and the different pipeline starting at the table going at
the bottom whenever the classification has a low confidence we pass it to the
human it's simple we rely on the human only when the classifier is not confident and the fundamental trade-off
in all of these systems is what is the accuracy we're willing to put up with
here in red and blue and red is human choice decision and blue as a machine
tasks in red we select the video we want to classify in blue the the the neural
network performs the face detection task localizing the camera choosing what is the angle of the camera
and provides a trade opportunity and percent frames it can annotate so
certainly and you'll networking at a glance for the entire data set they would achieve accuracy in the case of
glass classification of nine low 90% classification on the sixth glass task
now if you want a higher accuracy that it will only be able to achieve that for us for a smaller fraction of frames
that's the choice and then a human has to go in and
perform the annotation of the frames that the algorithm was not confident
about and it repeats over and over the algorithm is then trained on the frames
that were annotated by the human and repeats this process over and over on the frames until everything is annotated
yes yes absolutely
the question was do you ever observe that the classifier is highly confident about the incorrect class yep right
question was hot well then how do you how do you deal with that how do you account for that how do you account for
the fact that highly confident predictions can be highly wrong yeah
false positives false positives that you're really confident in there there's
no at least in our experience there's no good answer for that except more more and more training data on the things
you're not confident about that usually seems to deal generalize over cases we
don't encounter obvious large categories of data where you're really confident
about the wrong thing usually some degree of human annotation fixes most
problems annotating the low the low confidence
part of the data solves all incorrect issues but of
course that's not always true in the general case that you can imagine a lot of scenarios whether that's not true for
example one one one thing they always perform is for each individual person we
usually entertain a large amount of the data manually no matter what so we have to make sure that the neural network has
seen that person in the various and the various ways their face looks like with
glasses with different hair with different a lighting variation so we
want to manually annotate that it's overtime we're allowing the machine to do more and more of the work so what's resulting in this in the
glance classification cases you can do real-time classification you can give the car information about whether the
driver is looking on road or off road this is critical information for the car to understand and you want to pause for
a second to realize that when you're driving a car for those our driver for those that driven any kind of car with
any kind of automation it has no idea about what you're up to at all there's
no it doesn't have any information about the driver except if they're touching the steering wheel or not more and more
now with the GM supercruise vehicle and Tesla now has added a dryer facing camera that slowly started to think
about moving towards perceiving the driver but most vehicles on the road
today have no knowledge of the driver this knowledge is almost common sense and trivial for the car to have the it's
common sense how important this information is where the driver is looking that's the glance classification
problem and again emphasizing that we've converted it's been three decades of
work on gaze estimation yet gaze estimation is doing head pose estimation so the geometric orientation of the head
combining the orientation of the eyes and using that combined information to determine where the person is looking
will convert that into a classification problem so the standard gaze estimation definition is not a machine learning
problem classification is a machine learning problem this transformation is key
Emotion Recognition
emotion human emotion is a fascinating thing so the same kind of pipeline
stabilization cleaning of the data raw pixels in and then the classification is
emotion the problem with emotion if I may speak as an expert human not am NOT
an expert in emotion is just an expert of being human is that there is a lot of ways that's a sodomize emotion to
categorize emotion to define emotion whether that's for the the primary
emotion of the para scale would love joy surprise anger sadness fear there's a lot of ways to mix those together to
break those apart into hierarchical taxonomies and the way we think about it
in the driving context at least there is a general emotion recognition task sort
of I mentioned I'll mention it but it's kind of how we think about primary
emotions is detecting the the broad categories of emotion of joy and anger
of disgust and surprise and then there is application specific emotion
recognition where you're using the facial expressions that all the various ways that we can deform our face to
communicate information to determine the
specific question about the interaction of the driver so I'll first for the
general case these are the building blocks I mean there's there's countless ways of deforming the face that we use
to communicate with each other there's 42 individual facial muscles that can be
used to form those expressions one of
our favorite work with is the effective SDK this is their their their task with the general
emotion recognition task is taking in raw pixels and determining categories of
emotion very subtleties of that emotion in the general case producing a classification of anger disgust fear
surprise so on and then mapping I mean essentially what these algorithms are
doing whether whether they using deep neural networks or not whether using face alignment to do the landmark
detection and then tracking those landmarks over time to do the facial actions they're determined they're
mapping the expressions the component their various expressions who can make with their eyebrows or their nose and
mouth and eyes to map them to the emotion so I'd like to highlight one
because I think it's an illustrative one for joy an expression of joy is smiling
so there's an increased likelihood that you observe a smiling expression on the
face when joy is experienced or vice versa if there's an increased probability of a smile there's an
increased probability of emotion of joy being experienced and then joy an
experience has a decreased probability likelihood of brow raising and brow following so if you see a smile that's a
that's a plus for joy if you see brow raised bright for Oh brow furrow is a minus for joy that's
for the general emotional recognition task that's been well studied that's sort of the core of affective computing movement from from the visual
perspective again from the computer vision perspective from the application of specific perspective which were
really focused on again data is everything what what are you annotating we can take here we have a large-scale
data set of drivers interacting with a voice based navigation system so they're
tasked with in various vehicles to enter a navigation so with they're talking to
their GPS using their voice this is for depending on the vehicle depending on the system in most cases an incredibly
frustrating experience so we have them perform this task and then the annotation is self-report after the
task they say on a scale of 1 to 10 how frustrating was this experience and when
you see on top is is the expressions detected and associated with a satisfied
a person who said a a 10 on the satisfaction so a 1 in the frustration
scale was perfectly satisfied with a voice based interaction on the bottom is
frustrated as a believin 9 on the frustration scale so the feature the
strongest there the expression remember joy smile was the strongest indicator of
frustration for all our subjects that was the strongest expression smile was the thing that was always there for
frustration there's other various frowning that followed and shaking the
head and so on but smiles were there so that shows you the kind of clean difference between general emotion
recognition tasks and the application-specific here perhaps they enjoyed an absurd
moment of joy at the frustration that were experiencing you can sort of get philosophical about it but the practical
nature is they were frustrated with the experience and we're using the 42 most of the face to make expressions to do
classification of frustrated or not and their data does the work not the
algorithms it's the annotation a quick mention for the AGI class next week for
the artificial general intelligence class one of the competition's we're doing is we have a JavaScript face
that's trained with a neural network to form various expressions to communicate
with the observer so we're interested in creating emotion which is a nice mirror
coupling of the emotional recognition problem it's gonna be super cool
cognitive load we're starting to get to the eyes
Cognitive Load Estimation
cognitive load is the degree to which a human being is accessing their memory or
as Lawson thought how hard they're working in their mind to recollect
something to think about something as cognitive load and to do a quick pause
of eyes as the window to cognitive load eyes the window to the mind there's a
different ways the eyes move so there's pupils the black part of the eye they can expand and and contract based on
various factors including the lighting variations in the scene but they also expand and contract based on cognitive
load that's a that's a strong signal they can also move around there's ballistic movement saccades when
we look around eyes jump around the scene they can also do something called smooth pursuit when you and connecting
to our animal past you can see a delicious meal flying by or running by that your eyes
can follow it perfectly they're not jumping around so when we read a book our eyes are using saccadic movements
where they jump around and when the purse muth pursuit the eye is moving perfectly smoothly those are the kinds
of movements who have to work with and cognitive load can be detected by
looking at various factors of the eye the blink dynamics the eye movement and
the eye the pupil diameter the problem is in the real world and real world data
with lighting variations everything goes out the window in terms of using pupil diameter which is the standard way to
measure non-contact way to measure cognitive load in the lab when you can control lighting conditions and use
infrared cameras when you can't all that goes out the window and all you have is the blink dynamics and the eye movement
so neural networks to the rescue 3d convolutional neural networks in this case we take a sequences of images that
I through time and use 3d convolutions as opposed to 2d convolutions on the
left is everything we've talked about previous to this as 2d convolutions when
the convolution filter is operating on the XY 2d image every channel is operated on
by the filter individual separately 3d convolutions combine those convolve
across the across multiple images across multiple channels therefore being able
to learn the dynamics of the scene through time as well not just spatially
temporal and data data is everything for
a cognitive load we have in this case 92 drivers so how do we sort of perform the
cognitive load classification task we have these drivers driving on the highway and performing the what's called
the n-back task zero back one back to back and that task involves hearing
numbers being read to you and then recalling those numbers one at a time so
one zero back the system gives you a number seven and then you have to just say that number back seven and it keeps
repeating that's easy it's supposed to be the easy task one back is when you hear number you have to remember it and
then that for the next number you have to say the number previous to that so
you kind of have to keep one number in your memory always and not get distracted by the new information coming
up but to back you have to do that two numbers back so you have to use memory more and more went to back so cognitive
load is higher and higher okay so what do we do we use face alignment face
front elevation and detecting the eye closest to the camera and extract the eye region and now we have this nice raw
pixels of the eye region across six seconds of video and we take that and
put that in as a 3d convolutional neural network and classify simply one of three
classes zero back one back and two back so we have a ton of data of people on the highway performing these tasks and
back tasks and that forms the classification supervised learning training data that's it the input is 90
images it's at 15 frames a second and the output is one of three classes
face fronto ization i should mention is the technique developed under for face recognition because most face
recognition tasks require frontal face orientation is also what we use here to normalize everything that we can focus
in on the exact blink it's taking the it's taking whatever the orientation of
the face and projecting into the frontal position taking the raw pixels of the
face is detecting the eye region zooming in and grabbing the eye where you find
and this is where the intuition builds it it's a fascinating one what's being
plotted here is the relative movement of the pupil the relative movement of the eye based on a different cognitive loads
for cognitive load on the left of zero so when your mind is not that lost in thought and cognitive load of two on the
right when it is lost in thought the eye moves a lot less eye is more focused on
the forward roadway that's an interesting finding but it's only in aggregate and that's what the neural
neural network is task would do it with extracting an a frame-by-frame basis
this is a standard 3d convolutional architecture again taking in the image
sequence is the input cognitive load classification is the output and classifying on the right is the accuracy
that's able to achieve of 86% that's pretty cool from real-world data the
idea is that you can just plop in a webcam get the video going in going into
the neural network and this predicting it continued
a stream from zero to two of cognitive load because every single zero want back
one back to back classes are have a confidence that's associated with them so you can turn that into a real value
between zero and two and when you see here's a plot of three of the people on
the team here driving a car performing a task of conversation and in white
showing the cognitive load frame by frame a thirty frames a second estimating the cognitive load of each of
the drivers on from zero to two on the y-axis so these are high cognitive load
and showing in on the bottom red and yellow of high medium cognitive load and
when everybody's silent the cognitive load goes down so we can perform now with this simple neural network with the
training data that we formed we can extend that to any arbitrary new data set and generalize okay those are some
Human-Centered Vision for Autonomous Vehicles
examples of Chania neural networks can be applied and why is this important again is while we focus on the sort of
the perception tasks of using neural networks of using sensors and signal
processing to determine where we are in the world where the different obstacles are informed trajectories around those
obstacles we are still far away from completely solving that problem I would
argue 20 plus years away the human will have to be involved and so when it's the
system is not able to control when the system is not able to perceive when there's some flawed aspect about the
perception or the driving policy the human has to be involved and that's where we have to know let the car know
what the human is doing that's the essential element of human robot interaction the most popular car in the
United States today is the Ford f-150 no automation the thing that sort of
inspires us and makes us think that transportation can be fundamentally
transformed is the Google self-driving mo our and although our guest speakers and
all the folks work in the autonomous vehicles but if you look at it the only
people who are at a mass scale or beginning to are actually injecting automation into our daily lives is the
ones in between it's the Tesla's the l2 systems it's the tesla system the supercruise the audio
as 90s the the vehicles that are slowly adding to some degree of automation and
teaching human beings how to interact with that automation and here's again
the the the path towards mass scale
automation we're steering wheels removed the consideration that humans removed I
believe is more than two decades away on the path to that we have to understand
and create successful human robot interaction approach autonomous vehicles
autonomous systems in a human centered way the mass scale integration of these
systems of the human center systems like to test the vehicles a Tesla is just a small company right now the the kind of
l2 technologies have not truly penetrated the the market have not penetrated that our vehicles even the
Brittain the new vehicles being released today I believe that happens in the early 2020s and that's going to form the
core of our algorithms that will eventually lead to the full autonomy all
of that data what I mentioned with Tesla with a 32% miles being driven all of
that is training data for the algorithms the edge cases arise there that's where we get all this data in our data set at
MIT is 400,000 miles Tesla has a billion miles so that that's all training data
on the way on the stairway to mass scale automation why is this
important beautiful and fundamental to the role of AI in society I believe that
self-driving cars when they're in this way are focused on a human robot interaction our personal robots they're
not perception control systems tools like a Roomba performing a particular
task when human life is a steak when there's a fundamental transfer between
of life of a human being giving their life over to an AI system directly one
on one is a transfer that is kind of a relationship that is one indicative of a
personal robot this is it requires all the things of understanding
communication of trust these are fascinating to understand how a human
and robot can form trust enough to create a really an almost
one-to-one understanding of each other's mental state learn from each other oh
boy so one of my favorite movies Good Will
Hunting we're in Boston Cambridge have two have two gonna regret this one this
is Robin Williams speaking about human imperfections so I'd like you to take
this quote and replace every time you mentioned girl with car people call
those things imperfections Robin Williams is talking about his wife who passed away in the movie talking about
her imperfections they call these things imperfections but they're not that's the good stuff and then we'll get to choose
who we let into our weird little worlds you're not perfect sport and let me save
you the suspense this girl you met she isn't perfect to you there you know what let me just
the video sequences that only I know about that's what made her my wife when she
had a puts on me - she all my pet dogs people call these things into fashions
suffice no need to choose we learn to obviate the words in my breath explore
things in suspense he has an air attack but the question is
what am i perfect for each other [Music]
[Music]
so the approach we're taking in building
the autonomous vehicle we are here at MIT in our group it's the human centered approach the autonomous vehicles they
were going to release in March of 2018 in the streets of Boston those who would
to help please do I will talk run a
course on deep learning for understanding the humans of Chi 2018 will be going through tutorials that go
far beyond the visual the convolutional neural network based detection of
various aspects of the face and body would look at natural language
processing voice recognition and Gans if you're going to Chi please join next
week we have an incredible course that's aims to understand to begin to explore
the nature of intelligence natural and artificial we have Josh Tenenbaum Ray
Kurzweil Lisa Barret Nate Dubinsky
looking at cognitive modeling architectures Andre karpati Stephen Wolfram Richard Moyes talking about
autonomous weapon systems and AI safety mark Robert from Boston Dynamics and the
amazing incredible robots I have and Ilya sutskever from open AI and myself
so what next for folks register for this course you have to submit by tonight a
deep traffic entry that achieves a speed of 65 miles an hour and I hope you
continue to submit more that win the competition the high performer award
will be given to folks the very few folks who achieved 70 miles an hour
faster we will continue rolling out seg fuse having hit a few snags and invested
a few thousands of dollars in the sanitation process of annotating a
large-scale data set for you guys we'll continue this competition that will take
us into into a submission to his nips where we'd hope to submit the results for this
competition and deep crash the deeper enforcement learning these competitions will continue through May 2018 I hope
you stay tuned and participate there's upcoming classes the a GI class
I encourage you to come to is going to be fascinating and there's so many cool
interesting ideas that we're going to explore it's gonna be awesome there's an introduction to deep learning
course that I'm also part of will get a little bit more applied and get folks
who are interested in the the very basic algorithms of deep learning how to get
started with those hands-on and there's an awesome class that ran last year for
those who took this class last year we also talked about it on the the global
business of AI and robotics the slides are online I encourage you to click a link on there and register it's in the
spring it's once a week and it's truly brings together a lot of cross-disciplinary folks to talk about
ideas of artificial intelligence and the role of AI and robotics and society it's an awesome class and if you're
interested in applying deep learning methods in the automotive space come work with us we have a lot of
fascinating problems to to solve or collaborate so with that I'd like to
thank everybody here everybody across the community that's been contributing
we have thousands of submissions coming in for deep traffic and I'm just truly humbled by the support we've been
getting and the team behind this class is incredible thank you to Nvidia Google Amazon Alexa auto live in Toyota and
today we have shirts extra large extra
extra large medium over there small and large over there the big and small
people over here and then the medium-sized people over here so just grab it grab one and enjoy thank you
very much [Applause]

----------

-----
--26--

-----
Date: 2018.01.27
Link: [# MIT 6.S094: Computer Vision](https://www.youtube.com/watch?v=CLOAswsxudo)
Transcription:


Computer Vision and Convolutional Neural Networks
today we'll talk about how to make machines see computer vision and we'll
present Thank You Claire said yes and today we will present a competition that
unlike deep traffic which is designed to explore ideas teach you about concepts
of deep reinforcement learning seg fuse the deep dynamic driving scene
segmentation competition that I'll present today is at the very cutting edge whoever does well in this
competition is likely to produce a publication or ideas that would lead the
world in the area of perception perhaps together with the people running this
class perhaps in your own and I encourage you to do so even more cats
today computer vision today as it stands is deep learning majority of the
successes in how we interpret form representations understand images and
videos utilize to a significant degree neural networks the very ideas we've
been talking about that applies for supervised unsupervised and reinforcement learning and for the
supervised case is just the focus of today the process is the same the data
is essential there's annotated data where the human provides the labels that serves as the ground truth in the
training process then the neural network ghost's through that data learning to
map from the raw sensory input to the ground truth labels and then generalize
or the testing data set and the kind of raw sensors were dealing with their
numbers I'll say this again and again that for human vision for us here would
take for granted this particular aspect of our ability is to take in raw sensory information through our eyes and
interpret but it's just numbers that's something whether you're an expert computer vision
person or new to the field you have to always go back to meditate on is what
kind of things the Machine is given what what what is the data that is tasked to
work with in order to perform the tasks you're asking it to do perhaps the data is given is highly
insufficient to do what you want it to do that's the question I'll come up again and again our images enough to
understand the world around you and given these numbers the set of numbers
sometimes with one channel sometimes with three RGB where every single pixel have three different colors the task is
to classify or regress produce a
continuous variable or one of a set of class labels as before we must be
careful about our intuition of what is hard and what is easy in computer vision
let's take a step back to the inspiration for neural networks our own
biological neural networks because the human vision system and the computer
vision system is a little bit more similar in these regards this
and visual cortex is in layers and as information passes from the eyes to the
to the parts of the brain that makes sense of the raw sensor information higher and higher order representations
have formed this is the inspiration the idea behind using deep neural networks
for images higher and higher order representations of form through the layers there early layers taking in the
very raw and sensory information then extracting edges connecting those edges
forming those edges to form more complex features and finally into the higher-order semantic meaning that we
hope to get from these images in computer vision deep learning is hard
I'll say this again the illumination variability is the biggest challenge or at least one of the
one of the biggest challenges in driving for visible light cameras pose
variability the objects as I'll also discuss about some of the advances geoff
hinton and the capsule networks the idea with the neural networks as they're
currently useful computer vision are not good with representing variable pose
these objects in images and this 2d plane of color and texture look very
different numerically when the object is rotated and the object is mangled and
shaped in different ways the deformable will truncated cat intraclass variability the for the classification
task which would be an example today throughout to introduce some of the networks over the past decade that have
received success in some of the intuition and insight that made those networks work classification there is a
lot of variability inside the classes and very little variability between the classes all of these are cats on top all
of those are dogs are bottom they look very different and the other I would say
the second biggest problem in driving perception visible light camera perceptions occlusion when part of the object is
occluded due to the three-dimensional nature of our world some objects in
front of others and they occlude the background object and yet we're still
tasked with identifying the object when only part of it is visible and sometimes that part told you there's cats is very
hardly visible here we're tasked with classifying a cat with just an ears visible just the leg and in
the philosophical level as we'll talk about the motivation for our competition here here's a cat dressed as a monkey
eating a banana on a philosophical level most of us understand what's going on in
the scene in fact a neural network it's to today successfully classify this
image this video as a cat but the
context the humour of the situation and in fact you could argue it's a monkey is
missing and what else is missing is the dynamic information the temporal
dynamics of the scene that's what's missing in a lot of the perception work
that has been done to date in the autonomous vehicle space in terms of
visible light cameras and we're looking to expand on that that's what psyche fuse is all about
image classification pipeline there's a bin with different categories inside
each class cat dog mug hat those bins there's a lot of examples of each and
your task with when a new example comes along you never seen before to put that image in a bin it's the same as the
machine learning tasks before and everything relies on the data that's
been ground truth that been labeled by human beings amnesty is a toy data set of handwritten
digits often used as examples and Koko safar imagenet places and a lot of other
incredible datasets rich data sets of a hundred thousands millions of images out
there represent scenes people's faces and different objects those are all
ground truth data for testing algorithms and for competing architectures to be
evaluated against each other see far ten one of the simplest almost toy datasets
of tiny icons with ten categories of airplane automobile bird cat deer dog
for our course ship and truck is commonly used to explore some of the basic convolution neural networks we'll
discuss so let's come up with a very trivial classifier to explain the concept of how we could go about it in
fact this is maybe if you start to think about how to classify an image if you don't know any of these techniques this
is perhaps the approach you would take is you would subtract images so in order to know that an image of a cat is
different than image of a dog if to compare them when given those two images what what's the what's the way you
compare them one way you could do it is you just subtract it and then sum all
the pixel wise differences in the image just subtract the intensity of the image pixel by pixel sum it up if that intent
if that difference is really high that means the images are very different using that metric we can look at C for
10 and use it as a classifier saying based on this difference function I'm
going to find one of the 10 bins for a new image that that is that has the
lowest difference find an image in this data set that is most like the image I
have and put it in the same bin as that images in so there's 10 classes if we
just flip a coin the accuracy of our classifier will be 10% using our image
difference classifier we can actually do pretty good much better than random much better than 10%
we can do 35 38 percent accuracy that's a classifier we have our first
classifier K nearest neighbors let's
take our classifier to a whole new level instead of comparing it to just fight
trying to find one image that's the closest in our data set we tried to find K closest and say what is what class do
the majority of them belong to and we take that k and increase it for 1 to 2 to 3 to 4 to 5 and see how that changes
the problem with seven years neighbors which is the optimal under this approach
for CFR 10 we achieve 30% accuracy
human level is 95% accuracy and with
convolutional neural networks will get very close to 100% that's where you'll
networks shine this very task of bending images it all starts at this basic
computational unit signal in each of the signals are weighed summed bias added
and put an input into a nonlinear activation function that produces an
output the nonlinear activation function is key all of these put together and
more and more hidden layers form a deep neural network and that deep neural
network is trained as we've discussed by taking a forward pass and examples have
garage with labels seeing how close those labels are to the real ground truth and then punishing the weights
that resulted in the incorrect decisions and rewarding the weights that resulted
in correct decisions for the case of 10 examples the output of the network is
different values the input being handwritten digits from 0 to 9 for 10 of
those and we wanted our network to classify what is in this image of a
handwritten digit is it 1 is 0 1 2 3 through 9 the way it's often done is
there's ten outputs of the network and each of the neurons on the output is
responsible for getting really excited when it's number is called and everybody
else is supposed to be not excited therefore the number of classes is the
number of outputs that's how it's commonly done and you assign a class to
the input image based on the highest the neuron which produces the highest output
but that's for a fully connected network that we've discussed on Monday there is
in deep learning a lot of tricks that make things work that make training much
more efficient on large class problems where there's a lot of classes on large
data sets when the representation that the neural network is tasked with learning is extremely complex and that's
where convolutional neural neural networks step in the trick they use a spatial invariance they use the idea
that a cat in the top left corner of an image is the same as a cat in the bottom
right corner of an image so we can learn the same features across the image
that's where the convolution operation steps in instead of the fully connected
networks here there's a third dimension of depth so the blocks in this neural
network as input take 3d volumes and as output produced 3d volumes
a slice of the image a window and slide it across applying the same exact
weights and we'll go through an example the same exact weights as in the fully connected network on the edges that are
used to map the input to the output here are used to map this slice of an image
this window of an image to the output and you can make several many of such
convolutional filters many layers many different options of what kind of
features you look for in an image what kind of window you slide across in order to extract all kinds of things all
kinds of edges all kind of higher-order patterns in the images the very
important thing is the parameters on each of these filters the subset of the image these windows are shared if the
feature that defines a cat is useful in the top left corner it's useful in the top right corner it's useful in every
aspect of the image this is the trick that makes convolutional neural networks save a lot of a lot of parameters reduce
parameter significantly it's the reuse the spatial sharing of features across
the space of the image the depth of
these 3d volumes is the number of filters the stride is the skip of the
filter the step size how many pixels you skip when you apply the filter to the
input and the padding is
they're padding the zero padding on the outside of the input to a convolutional
layer let's go through an example so on
the left here and the slides are now available online you can follow them along and I'll step through this example
on the left here is a input volume of three channels the left column is the
input the three block the three squares there are the three channels and there's
numbers inside those channels and then
we have a filter in red two of them two
channels of filters with a bias and we those filters are three by three each
one of them is size three by three and what we do is we take those three by
three filters that are to be learned these are our variables our weights that
we have to learn and then we slide it across an image to produce the output on
the right the green so by applying the filters in the red there's two of them
and within each one there's one for every input channel we go from the left
to the right from the input volume on the left to the output volume green on
the right and you can look it you can
pull up the slides yourself now if you can't see the numbers on the screen but the the operations are performed on the
input to produce the single value that's highlighted there in the green and the output and we slide this convolution no
filter along the image with a stride in this case of to skipping skipping along
they sum to the to the right the two channel output in green that's it
the convolutional operation that's what's called the convolutional layer neural networks and the parameters here
besides the bias are the read values in the middle that's what we're trying to
learn and there's a lot of interesting tricks we'll discuss today on top of
those but this is at the core this is the spatially invariant sharing of
parameters that make convolutional neural networks able to efficiently
learn and find patterns and images to build your intuition a little bit more
about convolution here's an input image on the left and on the right the
identity filter produces the output you see on the right and then there's different ways you can different kinds
of edges you can extract with the activate or the resulting activation map
seen on the right so when applying the filters with those edge detection
filters to the image on the left you produce in white are the parts that
activate the convolution the results of these filters and so you can do any kind
of filter that's what we're trying to learn any kind of edge any kind of any
kind of pattern you can move along in this window and this way that's shown here you slide along the image and you
produce the output you see on the right and depending on how many filters you have in every level you have many of
such slices VC on the right the input on the left the output on the right if you
have dozens of filters you have dozens of images on the right each with
different results that show where each of the individual filter patterns were
found and we learned what patterns are useful to look for in order to perform
the classification task that's the task for the neural network to learn these
filters and the filters have higher and higher order of representation going from the
very basic edges to the high semantics meaning that spans entire images and the
ability to spend images can be done in several ways but traditionally has been successfully done through max pooling
through pooling of taking the output of
convolutional operation and reducing the resolution of that byte by condensing
that information by for example taking the maximum values the maximum activations therefore reducing the
spatial resolution which has detrimental effects as we'll talk about in the scene segmentation but it's beneficial for
finding higher order representations and the images that bring images together that bring features together to form an
entity that we're trying to identify and classify okay so that forms a
convolution Yool network such convolutional layers stacked on top of each other is the only addition to a
neural network that makes for a convolutional neural network and then at the end the fully connected layers or
any kind of other architectures allow us to apply particular domains
Network Architectures for Image Classification
let's take image net as a case study an
image net the data set an image net the
challenge the task is classification as I mentioned the first lecture image net is
a data set one of the largest in the world of images with 14 million images
21,000 categories and a lot of depth to
many of the categories as I mentioned 1200 granny smith apples
these allow - these allow the newer networks to learn the rich
representations in both pose lighting variability and intraclass class variation for the particular things
particular classes like granny smith apples so let's look through the various
networks let's discuss them let's see the insights it started with Alex net the first
really big successful GPU trained neural network on image net that's achieved a
significant boost over the previous year and moved on to vgg net Google net ague
Lynnette ResNet see you image and as Annette in 2017 again the numbers will
show for the accuracy are based on the top five error rate we get five guesses
and it's a one or zero if you get guess if one of the five is correct you get a one for that particular guess otherwise
it's a zero and human error is five
point one when a human tries to achieve the same tries to perform the same task
as the machinist task of doing the air is five point one the human annotation is performed on the images based on
binary classification Granny Smith apple or not cat or not the actual tasks that
the machine has to perform and that the human competing has to perform is given an image is provide one of the many
classes under that human errors 5.1% which was surpassed in 2015 by ResNet to
achieve four percent error so let's
with Alex net I'll zoom in on the later networks they have some interesting insights but Alex net and vgg net both
fall at a very similar architecture very uniform throughout its depth vgg net in
2014 is convolution convolution pooling
convolution pooling convolution pooling and fully connected layers at the end
there's a certain kind of beautiful simplicity uniformity to these architectures because you can just make
it deeper and deeper and makes it very amenable to implementation in a layer
stack kind of way and in any of the deep learning frameworks it's clean and
beautiful to understand in the case of eg gina was 16 or 19 layers with 138
million parameters not many optimizations and these parameters therefore the number of parameters is
much higher than the networks that followed it despite the layers not being that large Google Net introduced the
inception module starting to do some interesting things with the small
modules within these networks which allow for the training to be more efficient and effective the idea behind
the inception module shown here with the previous layer on bottom and the
convolutional layer here with the inception module on top produced on top
is it used the idea that different size
convolutions provide different value for the network smaller convolutions are
able to capture or propagate forward features that are very local a high
resolution in in in texture larger convolutions are better able to
represent and capture and catch highly abstracted features higher-order
features so the idea behind the inception module is to say well as
opposed to choosing and high in a high pair tuning process or architecture design
process choosing which convolution size we want to go with why not do all of
them together while several together in the case of the Google net model there's
the one by one three by three and five by five convolutions with the old trusty
friend of max pooling still left in there as well which has lost favor more
and more over time for the image classification task and the results is there's fewer parameters are required if
you pick the placing of these inception modules correctly the number of
parameters required to achieve a higher performance is much lower res net one of
the most popular still to date
architectures that we'll discuss in scene segmentation as well came up and
use the idea of a residual block the initial inspiring observation which
doesn't necessarily hold true as it turns out but that network depth
increases representation power so these residual blocks allow you to have much
deeper networks and I'll explain why in a second here but the thought was they
work so well because the network's so much deeper the key thing that makes these blocks so effective is the same
idea that's that reminiscent of recurrent neural networks that I hope
would get a chance to talk about the training of them is much easier they
take a simple block repeated over and over and they pass the input along
without transformation along with the ability to transform it to learn to
learn the filters learn the weights so you're allowed to you're allow every
layer to not only take on the processing of previous layers but to take in the
wrong transform data and learn something new the ability to learn something new
allows you to have much deeper networks and the simplicity of this block allows
for more effective training the state of
the art in 2017 the winner is squeezed and excitation networks that unlike the
previous year will see you image which simply took ensemble methods and combined a lot of successful approaches
to take a marginal improvement se net got a significant improvement at least
in percentages I think there's a 25% reduction in error from 4 percent to 3
percent something like that by using a very simple idea that I think is
important to mention a simple insight it added a parameter to each channel and
the convolutional layer in the convolutional block so the network can
now adjust the weighting on each channel based for for each feature map based on
the content based on the input to the network this is kind of a take away to think about about any of the networks
who talk about any of the architectures is a lot of times your recurrent neural
networks and convolutional neural networks have tricks that significantly
reduce the number of parameters the bulk the sort of low-hanging fruit they use
spatial invariants a temporal invariants to reduce the number of parameters to represent the input data but they also
leave certain things not parameterize they don't allow the network to learn it allow in this case the network to learn
the weighting on each of the individual channels so each of the individual filters is something that you learn as
along with the filters takes it makes a huge boost the cool thing about this is it's
applicable to any architecture this kind of block that's kind of what the the squeeze and excitation block is
applicable to any architecture and because obviously it it just simply
permit Rises the ability to choose which filter you go with based on the content it's a subtle but crucial thing I think
it's pretty cool and for future research it inspires to think about what else can
be parameterize in your own networks what else can be controlled as part of the learning process including hiring
higher-order hyper parameters which which aspects of the training and the
architecture of the network can be part of the learning this is what this network inspires another network has
been in development since the 90s ideas with geoff hinton but really received
has been published on received significant attention 2017 that i won't go into detail here we are going to
release an online-only video about capsule networks it's a
little bit too technical but they inspire a very important point that we
should always think about with deep learning whenever it's successful is to think about what as I mentioned with the
cat eating a banana on a philosophical and the mathematical level you have to
consider what assumptions these networks make and what through those assumptions
they throw away so neural networks due to the spatial with convolutional neural networks due to their spatial invariants
throw away information about the relationship between the the hierarchies
between the simple and the complex objects so the face on the left and the face on the right looks the same to
accomplish a neural network the presence of eyes and nose and mouth is the
central aspect of what makes classification tasks work for
convolution Network where it will fire and say this is definitely a face but
the spatial relationship is lost is ignored which means there's a lot of
implications to this but for things like pose variation that information is lost
we're throwing away that away completely and hoping that the pooling operation
that's performing these networks is able to sort of mesh everything together to
come up with the features that are firing of the different parts of the face that then come up with the total
classification that it's a face without representing really the relationship between these features at the low level
and and the high level at the low level of the hierarchy at the simple and the complex level this is a super exciting
field now that's hopefully will spark developments of how we design your own networks that are able to learn this the
rotational the orientation invariance as well ok so as I mentioned you take these
Fully Convolutional Neural Networks
combos in your networks chop off the final layer in order to apply to a
particular domain and that is what we'll do with fully convolutional neural networks the ones that we task to
segment the image at a pixel level as a
reminder these networks through the convolutional process are really
producing a heat map different parts of the network are getting excited based on
the different aspects of the image and so it can be used to do the localization of detecting not just classifying the
image but localizing the object and they could do so at a pixel level so the
convolutional layers are doing the encoding process they're taking the rich
raw sensory information in the image and encoding them into an interpretable set
of features representation that can then be used for classification but we can
also then use it kotor up sample that information and produce a map like this fully
convolutional neural network segmentation semantic scene segmentation image segmentation the goal is to as
opposed to classify the entire image you classify every single pixel its pixel level segmentation you color every
single pixel with what that pixel what object that pixel belongs to in this 2d
space of the image the 2d projection the in the image of a 3-dimensional world so
the thing is there's been a lot of advancement in the last three years but
it's still an incredibly difficult problem if you if you think if you think about the amount of data that's used for
training and the task of pixel level of megapixels here of millions of pixels
that are tasked with having a scientist single label it's an extremely difficult problem why is this interesting
important problem to try to solve as opposed to bounding boxes around cats well it's whenever precise boundaries of
objects are important certainly medical applications when looking at imaging and detecting in particular for example
detecting tumors in the in in medical
imaging of different different organs and in driving in robotics when objects
are involved it's a done scene of all those vehicles pedestrians cyclists we need to be able to not just have a loose
estimate of where objects are we need to be able to have the exact boundaries and then potentially through data fusion
fusing sensors together fusing this rich textural information about pedestrians cyclists and vehicles
to lidar data that's providing us the three-dimensional map of the world or have both the semantic meaning of the
different objects and their exact three-dimensional location
a lot of this work successfully a lot of the work in the semantic segmentation
started with fully convolutional networks for semantic segmentation paper FCN that's where the name of FCN came
from in november 2014 now go through a few papers here to give you some intuition where the field is
gone and how that takes us to seg fuse the segmentation competition so FCM
repurposed the image net pre-trained nets the nets that were trained to classify what's in an image the entire
image and chopped off the fully connected layers and then added decoder
parts that that up sample there the image to produce a heat map here shown
with a tabby cat a heat map of where the cat is in the image it's a much slower
much coarser resolution than the input image 1/8 at best
skip connections to improve coarseness of up sampling there's a few tricks if
you do the most naive approach the up sampling is going to be extremely coarse because that's the whole point of the
neural network the encoding part is you throw away all the useless data the
YouTube the most essential aspects that represent that image so you're throwing away a lot of information that's
necessary to then form a high resolution image so there's a few tricks where you
skip a few of the final pooling operations to go in similar way and this
is a residual block to go to go to the output produce higher and higher resolution heat map at the end segment
in 2015 applied this to the driving context and really taking it to kitty
data set and have have shown a lot of interesting results and really explored
the encoder decoder or formulation of the problem
really solidifying this the place of the encoder/decoder framework for the
segmentation task dilated convolution I'm taking you through a few components
which are critical here to the state of the art dilated convolutions so the
convolution operation as the pooling operation reduces resolution
significantly and dilated convolution has a certain kind of gritting as
visualized there that maintains the local high resolution textures while
still capturing the spatial window necessary
it's called dilated convolutional layer and that's in a 2015 paper proved to be
much better at up sampling a high resolution image deep lab with a be v1
v2 Navi 3 added conditional random fields which is the final piece of the
of the state-of-the-art puzzle here a lot of the successful networks today
that do segmentation not all do post
process using CRFs conditional random fields and what they do is they smooth
the segmentation the up sample segmentation that results from the FCN by looking at the underlying image
intensities so that's the key aspects of
the successful approaches today you have the encoder decoder framework of a fully accomplished in your network it replaces
the fully connected layers with the convolutional layers deconvolution layers and as the years progress from
2014 to today as usual than underlying
networks from alex net to vgg net and to now ResNet have been one of the big
reasons for the improvements of these to be able to perform the segmentation so naturally they mirrored the imagenet
challenge performance in adapting these networks so the state-of-the-art uses ResNet or similar networks conditional
random fields for smoothing based on the input image intensities and the dilated
convolution that maintains the computational cost but increases the
resolution of the up sampling throughout the intermediate feature Maps and that
takes us to the state of the art that we used to produce the images to produce
the images for the competition present that do you see for dance up sampling
convolution instead of bilinear up sampling you make the up sampling learn
about you learn the upscaling filters that's on the bottom that's really the
key part that made it work there should be a theme here sometimes the the
biggest addition they can be done this parameter izing one of the aspects of the network they've taken for granted
letting the network learn that aspect and the other I'm not sure how important
it is to the success but it's a it's a cool little addition is a hybrid dilated convolution as I showed that
visualization where the convolution is spread apart a little bit in the input
from the input to the output the steps of that dilated convolution filter when
they're changed it produces a smoother result because when it's kept the same
there certain input pixels get a lot more attention than others so losing
that favoritism is what's achieved by using a variable different dilation rate
those are the two tricks but really the biggest one is the parameterization of the upscaling filters okay so that's
what we're that's what we used to generate that data and that's what we provides you the code with if you're interested in competing in psyche views
Optical Flow
the other aspect here that everything we've talked about from the classification to the segmentation to
making sense of images is it there the information about time the temporal
dynamics of the scene is thrown away and for the driving context of the robotics
contest and what we'd like to do with psyche fuse for the segmentation dynamics scene segmentation context of
when you try to interpret what's going on in the scene over time and use that information time is essential thus the
movement of pixels is essential through time that that understanding how those
objects move in a 3d space through the 2d projection of an image it's
fascinating and us there's a lot of set of open problems there so flow is what's
very helpful to as a starting point to help us understand how these pixels move
flow optical flow dense optical flow is the computation that our best of a best
approximation of where each pixel in image one and moved in the in
temporarily following image after that there's two images in 30 frames a second
there's one image at time zero the other is 33.3 milliseconds later and the
idents optical flow is our best estimate of how each pixel in the input image moved to in the output image the optical
flow for every pixel produces a direction of where we think that pixel moved and the magnitude of how far moved
that allows us to take information that we detected about the first frame and
try to propagate it forward this is the competition it's to try to segment an
image and propagate that information forward for manual annotation of a
image so this kind of coloring book annotation where you color every single pixel in the state-of-the-art dataset
for driving cityscapes that it takes 1.5
ninth and 1.5 hours 90 minutes to do that coloring that's 90 minutes per
image that's extremely long time that's why there doesn't exist today dataset
and in this class we're going to create one of segmentation of these images
through time through video so long
videos where every single frame is fully segmented that's still an open problem
that we need to solve flows a piece of that and we also provide you the this
computer state-of-the-art flow using flow net 2.0 so flow net 1.0 in May 2015
used neural networks to learn the optical flow the dense optical flow and
it did so with two kinds of architectures flow net s flowing that simple and flow net core flow net see
the simple one is simply taking the two images so what's what's the task here
there's two images and you you want to produce from those two images they follow each other in time thirty-three
point three milliseconds apart and your task is the output to produce the dense
optical flow so for the simple architecture you just stack them together each are RGB so it produces a
six channel input to the network there's a lot of convolution and finally it's the the same kind of process as the
fully convolution your networks to produce the optical flow then there is flow net correlation architecture where
you perform some convolution separately before using a correlation layer to combine the feature Maps both
effective in different data sets and different applications so flow net 2.0
in December 2016 is one of the
state-of-the-art frameworks code bases that we used to generate the data all
show combines the flow net Assam flow net C and improves over the initial flow
net producing a smoother flow field preserves the fine motion detail along
the edges of the objects and it runs extremely efficiently depending on the
architecture there's a few variants either eight to a hundred forty frames a
second and the process there is essentially one that's common across
various applications deep learning is stacking these networks together the
very interesting aspect here that we're
still exploring and again applicable in all of deep learning in this case it
seemed that there was a strong effect in taking sparse small multiple data set
and doing the training the order of which those data sets were used for the training process mattered a lot that's
very interesting so using flow net 2.0 here's the data
SegFuse Dynamic Scene Segmentation Competition
set we're making available for psych fuse the competition cars that mit.edu
slash psych fuse first the original video us driving in high-definition
1080p and a 8k 360 video original video
driving around Cambridge we're providing the ground truth for a
training set for that training set for every single frame 30 frames a second
we're providing the segmentation frame to frame to frame segmented on
Mechanical Turk we're also providing the output of the network that I mentioned
the state of their our segmentation network that's pretty damn close to the ground truth but still not and our task
is this is the interesting thing is our task is to take the output of this
network well there's two options one is to take the output of this network and
use use other networks to help you propagate the information better so what
this segmentation the output of this network does is it only takes a frame by
frame by frame it's not using the temporal information at all so the question is can we figure out a way can
we figure out tricks to use temporal information to improve this segmentation so it looks more like this segmentation
and we're also providing the optical flow from frame to frame to frame so the optical flow based
on flowing at 2.00 of how each of the pixels moved okay and that forms a seg
fuse competition 10,000 images and the task is to submit code we have starter
code in Python and on github to take in
the original video take in for the training set the ground truth the segmentation from the state-of-the-art
segmentation Network the optical flow from the state-of-the-art optical flow Network and taking that together to
improve the the stuff on the bottom left the segmentation to try to achieve the ground truth and on the top right okay
with that I'd like to thank you tomorrow at 1 p.m. is way mo in Stata 32 one two
three the next lecture next week will be on deep learning for a sense in the human understanding the human and we
will release online only lecture on capsule networks and Gans general
adversarial networks thank you very much [Applause]

----------

-----
--25--

-----
Date: 2018.01.25
Link: [# MIT 6.S094: Deep Reinforcement Learning](https://www.youtube.com/watch?v=MQ6pP65o7OM)
Transcription:


AI Pipeline from Sensors to Action
today we will talk about deep reinforcement learning the question we
would like to explore it's to which degree we can teach systems to act to
perceive and act in this world from data so let's take a step back and think of
what is the full range of tasks then artificial intelligence system needs to accomplish here's the stack from top to
bottom top the input bottom output the environment at the top the world that
the agent is operating in sensed by sensors taking in the world outside and
converting it to raw data interpretable by machines sensor data and from that
raw sensor data you extract features you extract structure from that data such
that you can input it make sense of it discriminate separate understand the
data and as we discussed you form higher
and higher order representations a hierarchy of representations based on which the machine learning techniques
can then be applied once the machine
learning techniques the understanding as I mentioned converts the data into
features into higher order representations and into simple actionable useful information we
aggregate that information into knowledge we take the pieces of knowledge extracted from the data
through the machine learning techniques and to build a taxonomy a library of
knowledge and with that knowledge we reason an aging estas to reason to
aggregate to connect pieces of data it's seen in the recent past or the distant
past to make sense of the world that's operating in and finally to make a plan
of how to act in that world based on its objectives based on what it wants to accomplished as I mentioned a simple but
commonly accepted definition of intelligence is a system that's able to accomplish complex goals so system
that's operating in the environment in this world must have a goal must have an objective function a reward function and
based on that it forms a plan and takes action and because there operates in
many cases in the physical world it must have tools effectors with which
it applies the actions to change something about the world that's the full stack of an artificial intelligence
system that acts in the world and the question is what kind of task can such a
system take on what kind of task can an artificial intelligence system learn as
we understand AI today we will talk about the advancement of deeper
enforcement learning approaches and some of the fascinating ways it's able to take much of the stack and treat it as
an end-to-end learning problem but we look at games we look at simple
formalized worlds while it's still impressive beautiful and unprecedented accomplishments it's nevertheless formal
tasks can we then move beyond games and into expert tasks of medical diagnosis
of design and into natural language and
finally the human level tasks of emotion imagination consciousness let's once
again review the stack in practicality in the tools we have the input for
robots operating in the world from cars to humanoid to drones as light our
camera radar GPS stereo cameras audio microphone networking for communication
and the various ways to measure kinematics with IMU
the raw sensory data is then processed features of form to representations are
formed and multiple higher and higher order representations that's what deep learning gets us before
neural networks before the advent of before the recent successes of neural
networks to go deeper and therefore be able to form high order representations of the data that was done by experts by
human experts today networks are able to do that that's the representation piece
and on top of the representation piece the final layers these networks are able
to accomplish the supervised learning tasks the generative tasks and the
unsupervised clustering tasks through machine learning that's what we talked
about a little in lecture one and we'll continue tomorrow and Wednesday
that's supervised learning and you can think about the output of those networks
as simple clean useful valuable information that's the knowledge and
that knowledge can be in the form of single numbers it could be regression
continuous variables it could be a sequence of numbers it can be images audio sentences text speech once that
knowledge is extracted and aggregated how do we connect it in multi resolution
always form hierarchies of ideas connect ideas the trivial silly example is
connecting images activity recognition and audio for example if it looks like a
duck quacks like a duck and swims like a duck we do not currently have approaches
that effectively integrate this information to produce a higher confidence estimate that is in fact the
duck and the planning piece the task of taking the sensory information fusing
the sensory information and making action control and longer-term plans based on that information as we'll
discuss today are more and more amenable to the learning approach to the deep learning
approach but to date have been the most successful and non learning optimization based approaches like with the several
of the guest speakers we have including the creator of this robot Atlas in Boston Dynamics so the question how much
of the stack can be learned and to end from the input to the output we know we can learn the representation and the
knowledge from the representation and to knowledge even with the kernel methods of SVM and certainly with with neural
networks mapping from representation to information has been where the primary
success of machine learning over the past three decades has been mapping from
raw sensory data to knowledge that's where the success the automated
representation learning of deep learning has been a success going straight from
raw data to knowledge the open question for us today and beyond is if we can
expand the red box there of what can be learned and to end from sensory data to
reasoning so aggregating forming higher representations of the extracted knowledge and forming plans and acting
in this world from the raw sensory data we will show the incredible fact that we're able to do CERN exactly what's
shown here and to end with deeper enforcement learning on trivial tasks in a generalizable way the question is
whether that can then move on to real-world tasks of autonomous vehicles
of humanoid robotics and so on that's
the open question so today let's talk about reinforcement learning there's three types of machine learning
Reinforcement Learning
supervised unsupervised are the categories at the
extremes in relative to the amount of human and human input that's required
for supervised learning every piece of data that's used for teaching these systems is first labeled by human beings
and unsupervised learning on the right is no data is labeled by human beings in
between is some sparse input from humans semi-supervised learning is when only
part of the data is provided by humans ground truth and the rest must be inferred generalized by the system and
that's what reinforcement learning Falls reinforcement learning has shown there
with the cats as I said every successful presentation must include cats they're
supposed to be Pavlov's cats and ringing a bell and every time they ring a bell
they're given food and they learn this process the goal of reinforcement
learning is to learn from sparse reward
data from learn from sparse supervised data and take advantage of the fact that
in simulation or in the real world there is a temporal consistency to the world there is a temporal dynamics that
follows from state to state the state through time and so you can propagate information even if the information that
you're received about the the supervision the ground truth is sparse you can follow that information back
through time to infer something about the reality of what happened before then even if your reward signals were weak so
it's using the fact that the physical world evolves through time and some some
sort of predictable way to take sparse information and generalize it over the
entirety of the experience as being learned so we apply this the two problems today we'll talk about deep
traffic as a methodology deep reinforcement learning so deep traffic
is a competition that we ran last year and expanded significantly this year and
I'll talk about some of the details and how the folks in this room can on your smart phone today or if you have a
laptop training agent while I'm talking training a neural network in the browser
some of the things we've added our we've added the capability we've now turned it
into a multi agent deeper enforcement learning problem where you can control up to ten cars within your network
perhaps less significant but pretty cool is the ability to customize the way the
agent looks so you can upload and people have to an absurd degree have already
begun doing so uploading different images instead of the car that's shown there as long as it maintains the
dimensions shown here is a SpaceX rocket the competition is hosted on the website
self-driving cars that MIT ID you slash deep traffic will return to this later
the code is on github with some more information a starter code and a paper
describing some of the fundamental insights that will help you win at this
competition is an archive so from supervised learning in lecture
one to today supervised learning we can think of as memorization of ground truth
data in order to form representations that generalizes from that ground truth
reinforcement learning is we can think of as a way to brute force propagate
that information the sparse information through time to to assign quality reward
to state that does not directly have a reward to make sense of this world when
the rewards are sparse but are connected through time you can think of that as
reasoning so the through time is modeled in most
reinforcement learning approaches very simply that there's an agent taking an
action in a state and receiving a little reward and the agent operating in an environment execute an action receives
an observed state and new state and receives their reward this process continues over and over and some
examples we can think of any of the video games some of which we'll talk about today like Atari breakout as the
environment the agent is the paddle each
action that the agent takes has an influence on the evolution of the
environment and the success is measured by some reward mechanism in this case
points are given by the game and every game has a different point scheme that
must be converted normalized into a way that's interpreted by the system and the
goal is to maximize those points maximize the reward the continuous
problem of card pole by balancing the goal is to balance the pole on top of a moving cart the state is the angle the
angular speed the position of horizontal velocity the actions are the horizontal
force applied to the cart and the reward is one at each time step if the pole is still upright
all the first-person shooters the video games is now Starcraft the strategy games in case
of first-person shooter and doom what is the goal the environment is the game the
goal is to eliminate all opponents the state is the raw game pixels coming in the actions is moving up down left right
and so on and the reward is positive when eliminating an opponent and
negative when the agent is eliminated
industrial robotics been packin with a robotic arm the goal is to pick up a
device from a box and put it into a container the state is the raw pixels of the real world that the robot observes
the actions are the possible actions of the robot the different degrees of freedom are moving through those degrees
moving the different actuators to realize of the position of the arm and the reward is positive when placing a
device successfully and negative otherwise everything could be modeled in
this way Markov decision process there's a state as zero action a zero and reward
received a new state is achieved again action rewards state action rewards
state until a terminal state is reached and the major components of
reinforcement learning is a policy some kind of plan of what to do in every
single state what kind of action to perform a value function a some kind of
sense of what is a good state to be in of what is a good action to take in a state and sometimes a model that the
agent represents the environment with some kind of sense of the environment its operating in the dynamics of that
environment that's useful for making decisions about actions let's take a
trivial example a grid world of three by four twelve
squares we start at the bottom left and their task with walking about this world
to maximize reward they're awarded at the top right is a plus 1 and a 1 square
below that is a negative 1 and every step you take is a punishment or is a
negative reward of 0.04 so what is the optimal policy in this world now when
everything is deterministic perhaps this is the policy when you start the bottom
left well because every step hurts every step has a negative reward then you want to take the shortest path
to the maximum square with a maximum reward when the state space is
non-deterministic as presented before with a probability of 0.8 when you
choose to go up you go up but with probability 0.1 you go left and point 1
you go right unfair again much like life that would be the optimal policy what is
the Keith observation here that every single state in the space must have a plan because you can't because then a
non-deterministic aspect of the control you can't control where you're going to
end up so you must have a plan for every place that's the policy having an action an optimal action to take in every
single state now suppose we change the reward structure and for every step we
take there's a negative reward is a negative 2 so it really hurts there's a
high punishment for every single step we take so no matter what we always take
the shortest path the optimal policy is to take the shortest path to the to the only spot on the board that doesn't
result in punishment if we decrease the
reward of each step to negative 0.1 the policy changes whether
some extra degree of wandering encouraged and as we go further and
further in lowering the punishment as before to negative 0.04 more wandering
and more wandering is allowed and when we finally turn the reward into positive
so every step it every step is increases
the reward then there's a significant incentive to to stay on the board
without ever reaching the destination kind of like college for a lot of people
so the value function the way we think about the value of a state or the value
of anything in the environment is the reward were likely to receive in the
future and the way we see the reward were likely to receive as we discount
the future award because we can't always count on it
here Gama further and further out into the future more and more discounts
decreases the reward the importance of the reward received and the good
strategy is taking the sum of these rewards and maximizing it maximizing the scoundrel ward
that's what reinforcement learning hopes to achieve and with cue learning we use
any policy to estimate the value of taking an action in a state so off
policy forget policy we move about the world and use the bellman equation here
on the bottom to continuously update our estimate of how good a certain action is
in a certain state so we don't need this
this allows us to operate in a much larger state space in a much larger action space we move about this world
through simulation or in the real world taking actions and updating our estimate of how good certain actions are over
I'm the new state at the left is the is the updated value the old state is the
starting value for the equation and we update that old state estimation with the sum of the reward received by taking
action s tax action a and state us and
the maximum reward that's possible to be received in the following states
discounted that update is decreased with
a learning rate the higher the learning rate the more value we the the faster
will learn the more value we assigned to new information that's simple that's it
that's Q learning the simple update rule allows us to to explore the world and as
we explore get more and more information about what's good to do in this world
and there's always a balance in the various problem spaces we'll discuss there's always a balance between
exploration and exploitation as you form
a better and better estimate of the Q function of what actions are good to take you start to get a sense of what is
the best action to take but it's not a perfect sense it's still an approximation and so there's value of
exploration but the better and better your estimate becomes the less and less exploration has a benefit so usually we
want to explore a lot in the beginning and less and less so towards the end and when we finally release the system out
into the world and wish it to operate its best then we have it operate as a
greedy system always taking the optimal action according to the q2 key value function and everything I'm talking
about now is permit rised and our parameters that are very important for
winning the deep traffic competition which is using this very algorithm with
a neural network at its core so for sin
table representation of a cue function where the y-axis is state four states s
one two three four and the x-axis is actions a one two three four we can
think of this table as randomly initiated or initiated initialized in any kind of way that's not
representative of actual reality and as we move about this world and we take actions we update this table with the
bellman equation shown up top and here slides now are online you can see a
simple pseudocode algorithm of how to update it how to run this bellman
equation and over time the approximation becomes the optimal cue table
the problem is when that cue table it becomes exponential in size when we take
Deep Reinforcement Learning
in raw sensory information as we do with cameras with deep crash or with deep
traffic it's taking the full grid space and taking that information the raw the
raw grid pixels of deep traffic and when you take the arcade games here they're
taking the raw pixels of the game or when we take go the game of go when it's
taking the units the the board the raw state of the board as the input the
potential state space the number of possible combinations of what states it
possible is extremely large larger than we can certainly hold the memory and
larger that we can ever be able to accurately approximate through the bellman equation over time through
simulation through the simple update of the bellman equation so this is where
deep reinforcement learning comes in neural networks are really good approximate errs they're really good at
exactly this task of learning this kind of cue table
so as we started with supervised learning or neural networks helped us memorize patterns using supervised
ground true data and we'll move to reinforcement learning that hopes to propagate outcomes to knowledge deep
learning allows us to do so on much larger state spaces are much larger
action spaces which means it's generalizable it's much more capable to
deal with the raw stuff of sensory data which means it's much more capable to
deal with the broad variation of real world applications and it does so
because it's able to learn the representations as we discussed on
Monday the understanding comes from
converting the raw sensory information into into simple useful information
based on which the action in this particular state can be taken in the same exact way so instead of the cue
table instead of this cue function we plug in a neural network where the input is the state space no matter how complex
and the output is a value for each of the actions that you could take input is
the state output is the value of the function it's simple this is deep Q
Network DQ one at the core of the success of deep mind a lot of the cool
stuff you see about video games D queuing or variants of DQ and our play
this is water first with a nature paper a deep mind the success came of playing
the different games including Atari games
how are these things trained very similar to supervised learning the
bellman equation up top it takes the reward and the discounted
expected reward from future states the
loss function here for neural network and you'll now work learners with a loss function it takes the reward received at
the current state does a forward pass through a neural network to estimate the value of the future state of the best
action to take in the future state and then subtract that from the forward pass
through the network for the current state in action so you take the difference between what your a Q
estimator then you'll network believes the value of the current state is and what it more
likely is to be based on the value of the future states that are reachable
based on the actions you can take here's
the algorithm input is the state output is the Q value for each action or in
this diagram input is the state in action and the output is the Q value it's very similar architectures so given
a transition of s a are s prime s
current state taking an action receiving reward and achieving US prime state the
the update is to a feed-forward pass through the network for the current
state do a feed-forward pass for each of the possible actions taken in the next
state and that's how we compute the two parts of the loss function and update
the weights using back propagation again loss function back propagation is how
the network is trained this has actually been around for much longer than the
deep mind a few tricks made it made it
really work experience replays the
biggest one so as the games are played through simulation or if it's a physical
system as it acts in the world it's actually collecting the observations
into a library of experiences and that training is performed by randomly
sampling the library in the past by randomly sampling the previous
experiences and batches so you're not always training on the natural
continuous evolution of the system you're training on randomly picked batches of those experiences that's like
huge it's a it's a seems like a subtle trick but it's a really important one so
the system doesn't over fit a particular evolution of this of the game of the
simulation another important again
subtle trick as in a lot of deep learning approaches the subtle tricks make all the difference is fixing the
target network for the loss function if you notice you have to use the neural
network thick the singly neural network the gqi network to estimate the value of the current state and action pair and
next so using it multiple times and as
you perform that operation you're updating the network which means the
target function inside that loss function is always changing so you're the very nature your loss function is
changing all the time as you're learning and that's a big problem for stability that can create big problems for the
learning process so this little trick is to fix the network and only update it
every safe thousand steps so as you
train the network the the network that's used to compute the target function
inside the loss function is fixed it produces a more stable computation on a
loss function so the ground doesn't shift under you as you're trying to find
a minimal for the loss function the loss function doesn't change in unpredictable
difficult to understand ways and reward clipping which is always true with
general systems that are operating it's seeking to operate in the generalized
way is for very for these various games the points are different some some
points are low some points are high some go positive and negative and they're all normalized to a point where the good
points or the positive points are a 1 and negative points are a negative 1
that's reward clipping simplify the reward structure and because a lot of
the games are 30 FPS or 60 FPS and the actions are not it's not valuable to
take actions at such a high rate inside of these as particularly Atari games then you only take an action every four
steps while still taking in the frames as part of the temporal window to make decisions tricks but hopefully gives you
a sense of the kind of things necessary for both seminal papers like this one
and for the more important accomplishment of winning deep traffic is that
the tricks make all the difference here on the bottom is the circle is when the
technique is used in the x1 it's not looking at replay and target takes target network and experience replay
when both are used for the game of breakout River raid sea quests and Space
Invaders the higher the number the better it is the more points achieved so when it
gives you a sense that when replay and target both gives significant improvements in the performance of the
system order of magnitude improvements two orders of magnitude for breakup and
here is pseudocode of implementing dq1
the learning the key thing to notice and you can look to the slides is the the
loop the while loop of playing through the games and selecting the actions to
play is not part of the training it's it's part of the saving the observations
the state action reward next state observation is saving them into replay
memory into that library and then you sample randomly from that replay memory
to then train the network based on the loss function and with probability up up
top with the probability epsilon select a random action that epsilon is the
probability of exploration that decreases that's something you'll see in
deep traffic as well is the rate at which that exploration decreases over
time through the training process you want to explore a lot first and less and less over time so this algorithm is
being able to accomplish in 2015 and since a lot of incredible things things
that made the AI world think that we
were onto something that general AI is within reach for the first
time that raw sensor information was used to create a system that acts and makes sense of the world make sense of
the physics of the world enough to be able to succeed in it from very little information but these games are trivial
even though there is a lot of them this
dqn approach has been able to outperform a lot of the Atari games that's what's been reported on
outperform the human level performance but again these games are trivial what I
AlphaGo
think and perhaps biased I'm biased but one of the greatest accomplishments of
artificial intelligence in the last decade at least from the philosophical
or the research perspective is alphago 0
first alphago and then alphago 0 its
deepmind system that beat the best in the world in a game of go so what's the
game of go it's simple I won't get into
the rules but basically it's a 19 by 19 board shown on the bottom of the slide
for the bottom row of the table for a board of 19 by 19 the number of legal
game positions is 2 times 10 to the power of 170 it's a very large number of
possible positions to consider any one time especially the game evolves the
number of possible moves is huge much larger than in chess so that's why AI
the community thought that this game is not solvable until 2016 when alphago
used this use human expert position play to seed in a supervised way
reinforcement learning approach and I'll describe in a little bit of detail and a
couple of slides here to beat the best in the world and then
alphago 0 that is the accomplishment of
the decade for me in AI is being able to play with no training data on human
expert games and beat the best in the
world in an extremely complex game this is not Atari this is and this is a much
higher order difficulty game and that and the quality of players that is
competing in is much higher and it's able to extremely quickly here to
achieve a rating that's better than alphago and better than the different
variants of alphago and certainly better than the best of the human players in 21
days of self play so how does it work all of these approaches much much like
the previous ones the traditional ones that are not based on deep learning are
using Monte Carlo tree search MCTS which is when you have such a large
state space you start at a board and you play and you choose moves with some
exploitation exploration balancing choosing to explore totally new
positions or to go deep in the positions you know are good until the bottom of the game is reached until the final
state is reached and then you back propagate the quality of the choices you
made leading to that position and in that way you learn the value of of board positions and play that's been
used by the most successful go playing engines before and alphago since but you
might be able to guess what's the difference with alphago verse to the previous approaches they use the neural
network as the intuition quote-unquote - what
are the good states what are the good next board positions to explore and the
key things again the tricks make all the difference that made alphago zero work
and work much better than alphago is first because there was no expert play
instead of human games alphago used that very same Monte Carlo
tree search algorithm MCTS to do an intelligent look ahead based on the
neural network prediction of where dove the good States to take it checked that
instead of human expert play it checked how good indeed are those states it's a
simple look ahead action that does the ground truth that does the target
correction that produces the loss function the second part is the multitask learning what's now called
multitask learning is the networkers is quote-unquote two-headed in the sense
that first it outputs the probability of which move to take the obvious thing and it's also producing a probability of
winning and there's a few ways to combine that information and continuously train both parts of the
network depending on the choice taken so you want to take the best choice in the short term and achieve the positions
that are highly a slightly hood of winning for the player that's whose turn it is and another big step is that they
updated from 2015 the updated of the state-of-the-art architecture which are
now the architecture that one imagenet as the residual networks ResNet for
imagenet those that's it and those little changes made all the
DeepTraffic
difference so that takes us to deep traffic and the eight billion hours
stuck in traffic America's pastime so we tried to
simulate driving that behavior layer of driving so not the immediate control not
the motion planning but beyond that on top on top of those control decisions
the human interpretable decisions of changing lane of speeding up slowing down modeling that in a micro traffic
simulation framework that's popular in traffic engineering the kind of shown here we apply deep reinforcement
learning to that I'll call it deep traffic the goal is to achieve the highest average speed over a long period
of time weaving in and out of traffic for students here the requirement is to
follow the tutorial and achieve a speed of 65 miles an hour and if you really
want to achieve a speed over 70 miles an hour which is what's acquired to win and
perhaps upload your own image to make sure you look good doing it what you
should do clear instructions to compete read the tutorial you can change
parameters in the code box on that website cars done on mighty dad you size deep traffic click the white button that
says apply code which applies the code that you write these are the parameters that you specify then you'll network it
applies those parameters creates the architecture do you specify and now you have a network written in JavaScript
living in the browser ready to be trained then you click the blue button that says run training and that trains
the network much faster than one's actually being visualized in the browser
a thousand times faster by evolving the game making decisions taking in the grid
space as I'll talk about here in a second the speed limit is 80 miles an hour based on the various adjustments
were made to the game reaching 80 miles an hour is certainly impossible an average and reaching some of the speeds
that we've achieved last year it's much much much more difficult finally when you're happy and the
training is done submit the model to competition for those super eager
dedicated students you can do so every five minutes and to visualize your
submission you can click the request visualization specifying the custom
image and the color okay so here's the simulation speed limit 80
miles an hour cars 20 on the screen one of them is a red one in this case that's that one is
controlled by a neural network its speed it's allowed the actions of speed up slow down change lanes left-right or
stay exactly the same the other cars are
pretty dumb they speed up slow down turn left right but they don't have a purpose
in their existence they do so randomly or at least purpose has not been
discovered the road the car the speed the road is a grid space an occupancy
grid that specifies when it's empty it's set to a B meaning that the the
grid value is whatever speed is achievable if you were inside that grid
and when there's other cars that are going slow the value in that grid is the
speed of that car that's the state space that's the state representation and you can choose how much what slice that
state space you take in that's the input to the neural network for a visual
Asian purposes you can choose normal speed or fast speed for watching the
network operate and there's display options to help you build intuition
about the network takes in and what space that car is operating in the default is no extra information is added
then there's the learning input which visualizes exactly which part of the
road the is serves as the input to the network then there is the safety system
which I'll describe in a little bit which is all the parts of the road the car is not allowed to go into because it
would result in a collision and that with JavaScript would be very difficult to animate and the full map here's a
safety system you could think of this system as a CC basic radar ultrasonic
sensors helping you avoid the obvious collisions to obviously detectable
objects around you and the task for this red car for the steel Network is to move about this space is to move about the
space under the constraints of the safety system the red shows all the
parts of the grid it's not able to move into so the goal for the car is to not
get stuck in traffic it's make big sweeping motions to avoid crowds of cars
the input like DQ n is the state space the output is the value of the different
actions and based on the epsilon parameter through training and through
inference evaluation process you choose how much exploration you want to do
these are all parameters the learning is done in the browser on your own computer
utilizing only the CPU the action space there's five giving you some of the
variables here perhaps you go back to the slides to look at it the brain quote unquote is the thing that takes in the
state and the reward takes a four passed through the state and produce to
the next action the brain is where the neural network is contained both of the
training and the evaluation the learning input can be controlled in width forward
length and backward length lane side number of lanes to the side that you see patches ahead as the patches ahead that
you see patches behind as patches behind the you see mu this year can control the
number of agents that are controlled by the neural network anywhere from one to
ten and the evaluation is performed
exactly the same way you have to achieve the highest average speed for the agents
the very critical thing here is the agents are not aware of each other so
they're not jointly jointly planning the network is trained under the joint
objective of achieving the average speed for all of them but the actions are taking in a greedy
way for each it's very interesting what can be learned in this way because this
kinds of approaches are scalable to an arbitrary number of cars and you could imagine us plopping down the best cars
from this class together and having them compete in this way the best neural
networks because they're full in their greedy operation the number of networks
that can concurrently operate is fully scaleable there's a lot of parameters
the temporal window the layers the many
layers types that can be added here's a fully connected layer with tenure ons the activation functions all of these
things can be customized as specified in the tutorial the final layer a fully
connected layer with output a five regression giving the value of each of
the five actions and there's a lot of more specific parameters some of which
have this just from gamma to epsilon to experience
replay size to learning rate in temporal window the optimizer the learning rate
momentum batch size l2 l1 to K for regularization and so on there's a big
white button that says apply code that you press that kills all the work you've done up to this point so be careful
doing it it should be doing it only at the very beginning if you happen to
leave your computer running in training for several days as as folks have done the blue training button you press and
it trains based on the parameters you specify and the network state gets shipped to the main simulation from time
to time so the thing you see in the browser as you open up the web site is running then the same network that's
being trained and regularly it updates that network so it's getting better and better even if the training takes weeks
for you it's constantly updating the network you see on the left so if the car for the network that you're training
is just standing in place and not moving it's probably time to restart and change
the parameters maybe add a few layers to your network number of iterations is
certainly an important parameter to control and the evaluation is something
we've done a lot of worked on since last year to remove the degree of randomness to remove the the incentive to submit
the same code over and over again to hope to produce a higher reward a higher evaluation score the method for
evaluation is we collect the average speed over ten runs about 45 seconds of
game each not minutes 45 simulated seconds and there is five hundreds of
those and we take the median speed of the 500 runs it's done server-side so
extremely difficult to cheat I urge you to try you can try it locally there's a
start evaluation run but that one doesn't count that's just for you to feel better by you network that's that should
produce a result that's very similar to the one we were produced on the server it's to build your own intuition and as
I said we significantly reduce the influence of randomness so the the score the speed you get for the network you
design should be very similar with every valuation loading is saving if the
network is huge and you want to switch computers you can save the network it saves both the architecture of the
network and the weights and the on the network and you can load it back in
obviously when you load it in it's not saving any of the data you've already
done you can't do transfer learning with javascript in the browser yet submitting
your network submit model to competition and make sure you run training first otherwise it'll be initiated the way to
initiate it randomly and will not do so well you can resubmit us off and you like and the highest score is what counts the
coolest part is you can load your custom image specify colors and request the
visualization we have not yet shown the visualization but I promise you it's
going to be awesome again read the tutorial change the parameters in the code box click apply code run training
everybody in this room on the way home on the train hopefully not in your car
should be able to do this in the browser and then you can visualize request visualization because it's an expensive
process you have to want it for us to do it because we have to run in server-side
competition link is there github starter code is there and the details for those
that truly want to win is in the archive paper so the question that will come up
Conclusion
throughout is whether these reinforcement learning approaches are at all or rather if action planning control
is amenable to learning certainly in the case of driving we can't do it alpha go
zero did we can learn from scratch from self play
because that will result in millions of crashes in order to learn to avoid the
crashes unless we're working like we are deep crash on the RC car or we're
working in a simulation so we can look at export data we can look at driver data which we have a lot of and learn
from it's an open question whether this is applicable to date and I'll bring up
two companies because they're both guest speakers deep IRL is not involved in the
most successful robots operating in the real world in the case of Boston
Dynamics most of the perception control
and planning like in this robot does not involve learning approaches except with
minimal addition on the perception side best of our knowledge and certainly the
same is true with Wei MO as the speaker on Friday will talk about deep learning
is used a little bit in perception on top but most of the work is done from the sensors and the optimization base
the model-based approaches trajectory generation and optimizing which trajectory trajectory is best to avoid
collisions deep IRL is not involved and
coming back and back again the unexpected local POC is a high reward which arises in all of these
situations and apply in the real world so for the cat video that's pretty short
where the cats are ringing the bell and they're learning that the ring in the bell is is mapping to food I urge you to
think about how that can evolve over time in unexpected ways they may not
have a desirable effect where the final reward is in the form of food and the
intended effect is to ring the bell that's
ASAT comes in for the artificial general intelligence course in two weeks that something will explore extensively its
how these reinforcement learning planning algorithms will evolve in ways
they're not expected and how we can constrain them how we can design reward
functions that result in safe operation so I encourage you to come to the talk
on Friday at 1:00 p.m. as a reminder so 1:00 p.m. not 7:00 p.m. in Stata 32 one
two three and two the awesome talks in two weeks from Boston Dynamics to Ray
Kurzweil and so on for AGI now tomorrow we'll talk about computer vision and
psyche fuse thank you everybody [Applause]

----------

-----
--24--

-----
Date: 2018.01.20
Link: [# MIT Self-Driving Cars (2018)](https://www.youtube.com/watch?v=_OCjqIgxwHw)
Transcription:


welcome back to six at zero night for deep learning for self-driving cars
today we will talk about autonomous vehicles also referred to as driverless
cars autonomous cars Robo cars first the
utopian view where for many autonomous
vehicles have the opportunity to transform our society into a positive direction 1.3 million people die every
year in the automobile crashes globally thirty five thirty eight forty thousand
died every year in the United States so the one opportunity that's huge that's
one of the biggest focus for us here and MIT for people who truly care about this
it's to design autonomous systems our artificial intelligence system that saves lies
and those systems help work with deal
with or take away what nitsa calls the four DS of human folly drunk drugged
distracted and drowsy driving autonomous vehicles have the ability to take away
drunk driving distracted drowsy and drugged eliminate car ownership
so taking shared mobility to another level
eliminating car ownership from the business side is the opportunity to save
people money and increase mobility and
access making vehicles removing ownership makes vehicles more accessible
because the cost of getting from point A to point B drops an order to magnitude
and the insertion of software and intelligence into vehicles makes those
vehicles makes the idea of Transportation makes the way we see moving from A to point B a totally
different experience much like with our smart phone it makes it a personalized
efficient and reliable experience now for the negative view for the dystopian
view eliminate jobs any technology
throughout its history throughout our history of human civilization has always created fear that jobs that rely on the
prior technology will be lost this is a huge fear especially in trucking because
so many people in the United States and across the world rely work in the
transportation industry transportation sector and the possibility that AI will
remove those jobs has potential catastrophic consequences the idea one
that we have to struggle with in the 21st century of the role of intelligence
systems that aren't human beings being further and further integrated into our
lives is the idea that a failure of an autonomous vehicle even if they're much
rare if they're even if they're much safer that there is a possibility for an AI algorithm designed by probably one of
the engineers in this room will kill a person where that person would not have
died if they were in control of the vehicle the idea of an intelligent system one
indirect interaction with a human being killing that human being is one that we have to struggle with in a philosophical
ethical and technological level artificial
systems in popular culture lesson
engineering concerns may not be grounded ethically grounded at this time much of
the focus of building these systems as we'll talk about today and throughout this course that focuses on the
technology how do we make these things work but of course decades out years or
decades out the ethical concerns starts arising for Rodney Brooks one of the
seminal people from MIT those ethical concerns will not be an issue for another several decades at least five
decades but they're still important it continues the thought the idea of what
is the role of AI in our society when that car gets to make a decision about human life
what is it making that decision based on especially when it's a black box what is the ethical grounding of that
system does it conform with our social norms does a goal go against them and
there's many other concerns security is definitely a big one a car that's not
even artificial intelligence based a car that's software basis they're becoming more and more millions most of the cars
on road today are run by millions of lines of source code the idea that those
lines of source code written again by some of the engineers in this room get
to decide the life of a human being means then a hacker from outside of the
car can manipulate that code to also
decide the fate of that human being that's a huge concern for us from the
engineering perspective the truth is somewhere in the middle we want to find
what is the best positive way we can build these systems to transform our society to improve the quality of life
of everyone amongst us
but there's a grain of salt to the hype of autonomous vehicles we have to
remember as we discussed in the previous lecture and it will come up again and again our intuition about what is
difficult and what is easy for deep learning for autonomous systems is
flawed if we use our if use ourselves in this example human beings are extremely
good at driving this will come up again and again our intuition has to be
grounded in the understanding of what is the source of data what is the annotation and what is the approach what
is the algorithm so you have to be careful while using our intuition extending it decades out and making
predictions whether it's towards the utopian or dystopian view and as we'll talk about
some of the advancements of companies working in the space today you have to
take what people say in the media what the companies say some of the speakers
that will be speaking at this class say about their plans for the future and their current capabilities I think us a
guy that can provide is when there's a promise of a future technology future
vehicles there are two years out or more that has to be that's a very doubtful
prediction one that is within a year as we'll give a few examples today is
skeptical the real proof comes in actual
testing of public roads or in the most impressive the most amazing the reality
of it is when it's available to consumer purchase I would like to use Rodney
Brooks as a so it doesn't come from my mouth but I happened to agree his
prediction is no earlier than 2032 a driverless taxi service in a major US
city will provide arbitrary pick up and drop off locations fully autonomously that's 14 years away and
bite one 45 it will do so in multiple cities across the United States so think about
that that a lot of the engineers working in the space a lot of folks are actually building these systems agree with this
idea and that is the earliest I believe this will happen and Rodney believes but
as all technophobes have been wrong who could be wrong this is a map on the
x-axis a plot on the x axis of time throughout the 20th century and the
adoption rate and the y axis from zero to 100% of the various technologies from electricity to cars to radio the
telephone and so on and as we get closer to today the technology adoption rate
when it goes from zero to a hundred percent the number of years it takes to
adopt that technology is getting shorter and shorter and shorter as a society
we're better at throwing away the technology of old and accepting the technology of new so if a brilliant idea
to solve some of the problems were discussing comes along it could change everything overnight so let's talk about
Different approaches to autonomy
different approaches to autonomy we'll talk about sensors afterwards we'll talk
about companies players in this space and then we'll talk about AI and the
actual algorithms and how they can help solve some of the problems of autonomous
vehicles levels of autonomy here's a useful tech solemnization of levels of
autonomy useful for initial discussion for legal discussion and for policy
making and for blog posts and media reports but it's not useful I would
argue for design and engineering of the underlying intelligence and the system
viewed from a holistic perspective the entire thing creating an experience that
safe and enjoyable so let's go over those levels the five the six levels
this is presented by SAE report J three zero one six the most widely accepted
taxonomies ation of autonomy no automation at level zero level 1 and
level 2 is increasing levels automation level one is cruise control level two is
adaptive cruise control lane keeping level three I don't know what level
three is there's a lot of people that will explain that level three is conditional automation meaning it's
constrained to certain geographical location I will explain that from an engineering perspective I'm personally a
little bit confused of where that stands I'll try to redefine how we should view
automation level four and level five is high full level automation level four is
when the vehicle can drive itself fully for part of the time there's certain
areas in which it can take care of everything no matter what no human interaction input safekeeping is
required level five automation is the car does everything everything I would
argue that those levels aren't useful for designing systems that actually work
in the real world I would argue that there's two systems but first a starting
point that every system to some degree involves a human it starts with manual control from a
human human getting in the car and a human electing to do something so that's
the manual control what we're talking about when the human engages the system when the system is first available and
the human chooses to turn it on that's when we have to AI systems human
centered autonomy when the human is needed is involved and full autonomy
when AI is fully responsible for everything from the legal perspective that
means a to full autonomy means the car they designer the I system is liable is
responsible and for the human-centered autonomy the human is responsible what
does this practically mean for human center autonomy and we'll discuss
examples of all of these when a human interaction is necessary the question
then becomes is how often is the system available is it available on in traffic
conditions so for traffic bumper-to-bumper is available on the highway is it sensor based like in the
tesla vehicle meaning based on the visual characteristics to the scene the vehicle is confident enough to be able
to control to make control decisions perception control decisions the other
factor poor not discussed enough and I
think poorly imprecisely discussed when it is is the number of seconds given to
the driver not guaranteed but provided as a sort of feature to the driver to
take over in the tesla vehicle in all vehicles on the road today that time is
zero zero seconds are guaranteed zero seconds are provided there is some there's some room sometimes it's
hundreds of milliseconds sometimes it's multiple seconds but really there's no standard of how many seconds you get to
say wake up take control then tally up
operation something that some of the companies will mention are playing with is when a human being is involved
remotely controlling the vehicle remotely so being able to take over control the vehicle when you're when
you're not able to control it so support by a human that's not inside the car that's a very interesting idea to
explore but for the human centered autonomy side all of those features are
not required they're not guaranteed the human driver the inside the car is always responsible at
the end of the day they must pay attention to a degree that's required to take over when the system fails and no
matter under this consideration under this level of autonomy the system will
fail at some point that is the that is the point this is a collaboration between human and robot as the system
will fail and the human has to catch it when it does and then full autonomy is
AI is fully responsible now that doesn't again as will present some
companies in the marketing material and the PR side of things they might present
that there is significant degrees of autonomy if you're talking about l3 or l4 or l5 you have to read between the
lines you're not allowed to have teleoperation if a human is remotely
operating the vehicle a human is still in the loop a human is still evolved
it's still a human senator autonomy system you don't get the ten second rule
which is just because you give the
driver ten seconds to take control that somehow removes liability for you if you
say that that's it as an AI system I can't take can't resolve can't deal
can't control the vehicle in this situation and you have ten seconds to take over that's not good enough the
driver might be sleeping that driver may have had a heart attack they're not able to control the vehicle full autonomous
systems might must find safe harbor they must get you full stop from point A to
point B that point B might be your desired destination or might be a safe parking lot but it has to bring you to a
safe location this is a clear definition of the two systems in the human of
course as far as our certain current conception of artificial intelligence in
cars today is a human always overrides the AI system so we should for them for
the in the general case the human gets to
choose to take control they I can't take control the human except when danger is
imminent meaning sudden crashes like in a bee events we're not yet ready for the I
systems to say as a society to say no no you're drunk you can't drive so beyond
the traditional levels from level zero to level five the starting point is level zero no automation all cars start
here level one level two and level three I would argue fall into human senator
autonomy systems a1 because they did
involve some degree of a human then l4 l5 to some degree there's some crossover
fall into full autonomy even though with l4 with way mo as you can ask on Friday
and anyone Cruz uber playing in the space there's very often a human driver
involved one of the huge accomplishments of way mo over the past month incredible
accomplishment where in Phoenix Arizona they drove without the car drove without
a driver the meaning there was no safety driver to catch there was no engineer
staff member there to catch the car a human being that doesn't work for Google
or way mo got into that car and got from A to point B without a safety driver that's an incredible accomplishment and
that particular trip was a fully autonomous trip that is full autonomy
well there's no human to catch the car
no way I press station is good without cats it's a full
autonomy a to system its when you do
nothing but right along human Senate autonomy system is when you have some
control I'm sorry I had to so the two
paths for autonomous systems they want to need to in blue on the left is a one
human centered on the right is a two full autonomy and then blue is from the
artificial intelligent perspective is easy easier and then red is harder
easier meaning we do not have to achieve a hundred percent accuracy harder means
everything that's off of a hundred percent accuracy no matter how small has
a potential of costing human lives and huge amounts of money for companies so
let's discuss we'll discuss later in the lecture about the algorithms behind each
of these methods and the left on the right but this summarizes the two approaches the localization mapping for
the car to determine where it's located for the human centered autonomy it's
easy it still has to do the perception it has to localize itself within the lane it has to find all the neighboring
pedestrians and the vehicles in order to be able to control the vehicle to some degree but because the human is there it
doesn't have to do so perfectly when it fails a human is there to catch it scene understanding perceiving everything in
the environment from the camera from whether its lidar radar ultrasonic the
planning of the vehicle whether it's just staying within lane or for adaptive cruise control controlling the
longitudinal movement of the vehicle or its changing lanes is the Tesla autopilot or higher degrees of
automation all of those movement planning decisions can be autonomous Lee when the human is there
to catch it's easier because you're allowed to be wrong rarely but wrong the
hard part is getting the human robot interaction piece right that's next Wednesday lecture as we'll
discuss about how deep learning can be used to interact first perceive everything about the driver and second
to interact with the driver that part is hard because you can't screw up on that
part you have to make sure you help the driver know where your flaws are so they can take over if the driver is not
paying attention you have to bring their attention back to the road back to the interaction you have to get that piece
right because for a flawed system one that's rarely flawed the rarity is the
challenge in fact has to get the interaction right and then the final
piece communication the autonomous vehicle fully autonomous vehicle must
communicate extremely well with the external world with the pedestrians that
jaywalkers the humans in this world the cyclists that communication piece one at
least that is part of a safe and enjoyable driving experience is extremely difficult on the taught a way
Moe vehicle I wish them luck if they come to Boston from getting from point A to point B because pedestrians will take
advantage a vehicle must assert itself in order to be able to navigate Boston
streets and that assertion is communication that piece is extremely
difficult for Tesla vehicle for for a human centered autonomy vehicle l2 l3
the way you deal with Boston pedestrians is you take over roll down the window yell something and
then speed up getting the piece for an artificial
intelligence system to actually be able to accomplish something like that as we'll discuss on the ethics side and the
engineering side is extremely difficult that said most of the literature and the
human factors field in the autonomous vehicle field anyone that studied autonomy in aviation and in vehicles is
extremely skeptical about the human centered approach they think it's deeply responsible it's deeply responsible
because as argued because human beings
when you give them a technology which will take control part of the time they
will get lazy they would take advantage of that technology they will over trust that technology they'll assume will work
perfectly always this is the idea that this this idea extended beyond further
and further means that the better the system gets the better the car gets it driving itself the more the humans will
sit back and be completely distracted it will not be able to re-engage themselves in order to safely catch when the system
fails this is Chris Urmson the founder of the Google self-driving cars program and now the co-founder of one at the
other co-founders a speaker this class on next Friday sterling Anderson of a company called Aurora a start-up he was
one of the big proponents or the I
should say opponents the idea that human senator autonomy could work they tried
it publicly is spoken about the fact that at Google as in the early
self-driving car program they've tried shared autonomy they've tried l2 and it
failed because they're engineers that people driving their vehicles fell asleep and that's the belief that people have
and we'll talk about why that may not be true there's a fascinating truth in the
way human beings can interact with artificial intelligence systems that may
work in this case as I mentioned it's the human robot interaction building
that deep connection between human and machine of understanding of communication this is what we believe
happens so there's a lot of videos like this as it's it's fun but it's also
representative of what what society believes happens when automation is
allowed to enter the human experience and driving or the human life is a stake
that you can become completely disengaged it's kind of it's kind of a
natural thing to think but the question is does this actually happen what
actually happens on public roads the amazing thing that people don't
often talk about is that there is
hundreds of thousands of vehicles on the road today
equipped with autopilot Tesla autopilot that have a significant degree of
autonomy that's data that's information so we can answer the question what
actually happens so many of the people behind this team have instrumented 25
vehicles 21 of which are Tesla autopilot vehicles now with over collected
recording everything about the driver 2 cameras 2 HD cameras on the driver 2
cameras on the external camera on the external roadway and collecting everything about the car including audio
the state that pulling everything from the cam bus the kinematics of the vehicle I am you GPS all of that
information over now over 300,000 miles over 5 billion video frames all as we'll
talk about analyze the computer vision you extract from that video of the driver of
everything they're doing that level distraction the allocation of attention
the drowsiness emotional states the hands on wheel hands off wheel body pose
activity smartphone usage all these factors all of these things that you
would think would fall apart when you start letting autonomy into your life
we'll talk about what the initial reality is that should be inspiring and
thought-provoking as I said three cameras single board computer recording
all the data over a thousand machines in Holyoke and distributed computation
running the deep learning algorithms I've I've mentioned on these five plus
billion video frames going from the raw data to the actionable useful
information the slides are up online if you'd like to look through them oh fly
through some of them and this is the video of one of thousands of trips we
have in autopilot in our data a car driving autonomously a large fraction of
the time on highways from here to California from here to Chicago to
Florida and all across the United States we take that data and using the
supervised learning algorithms semi-supervised the number of frames
here is huge for those that work in computer vision five billion frames is
several orders of magnitude larger than any data set that people are working
with in computer vision actively
annotated so we want to use that data
for understanding the behavior of what people actually doing in the cars and we
want to train the algorithms that do perception and control a quick summary over three hundred thousand miles twenty
five vehicles the colors are true to the actual colors of the vehicles little fun facts Tesla
Model X Model S and now model three five
hundred thousand five hundred plus sorry miles a day and growing now most days in
2018 are over a thousand miles a day this is a quick GPS map in red is manual
driving across the Boston area in blue cyan is autonomous driving this is
giving you the sense of just the scope of this data this is a huge number of miles with automated driving several
orders of magnitude larger than what Wei Mo's doing that what Cruise is doing and
what Ober is doing the miles driven in
this data with autopilot confirming what
y'all muska stated it's 33% of miles of
driven autonomously this is a remarkable number for those of you who drive and
for those of you who are familiar with these technologies that is remarkable adoption rate that 33 percent of the
miles are driven in autopilot that means these drivers are getting use out of the
system it's working for them that's an incredible number it's also incredible
because under the the decades of literature from aviation to automation
and vehicles to to Chris Urmson and way mo the belief is such high numbers are
likely to lead to crashes to fatalities to at the very least highly responsible
behavior drivers over trusting the systems and getting in trouble we can run the glance
classification algorithms again this is for next Wednesday discussion to the
actual algorithm it's the algorithm that tells you the region that the driver is looking at and it's comparing road
instrument cluster left rearview center stack and right does the allocation of glance change with autopilot or with
manual driving it does not appear to in any significant noticeable way meaning
you don't start playing chess you don't start you don't get in the backseat to sleep you don't start texting in your
smartphone watching a movie at least in this data set there's promise here for
the human centered approach the observation to summarize this particular
data is that people are using it a lot the percentage of miles the percentage of hours is incredibly high at least
relative to what was will be expected from these systems and given that
there's no crashes there's no near crashes in autopilot the row type is
mostly highway traveling at high speeds the mental engagement looked at 8,000
transyl of control from machine to human so human beings taking control of the vehicle saying you know what I'm going
to take control now I'm not comfortable with the situation for whatever reason either not comfortable or electing to do
something that the vehicle is not able to like turn off the highway make a right or left turn stop for a stop sign
these kinds of things physical engagement as I said glance remains the
same and what do we take from this it says something that I'd like to really
emphasize this we talked to was we talked about autonomous vehicles in this class and the guest speakers who are all
on the other side so I'm representing the human center side all our speakers
are focused on the full autonomy side because that's the side roboticists know
how to solve that's the fascinating algorithm nerd side and that's the side I love as
well just my belief stands that the solving the perception control problem
is extremely difficult and to three decades away so in the meantime we have to utilize the human robot interaction
to actually bring these AI systems onto the road to successfully operate and the
way we do that counter-intuitively is we have to have we have to let the
artificial intelligence systems reveal their flaws one of the most endearing
things to human beings can do to each other friends is reveal their flaws to
each other now from an automotive perspective from a company perspective it's perhaps not appealing for an AI
system to reveal what it sees about the world and what it doesn't see about the world where it succeeds and where it
fails but that is perhaps exactly what it needs to do in the case of autopilot
the way the very limited but I believe successful way is currently doing that
is allowing you to use autopilot basically anywhere so what people are doing is they're trying to engage their
turn on autopilot in places where they really shouldn't rural rural roads curvy
with terrible road markings with in in
heavy rain conditions with snow with lots of cars driving at high speeds all
around they turn autopilot on to understand to experience the limitations of the system to interact that
human-robot interaction is through its tactile by turning it on and seeing is
it going to work here how's it gonna fail and the human is always there to catch it that interaction that's
communication that intimate understanding is what creates successful integration of AI in the car before
we're able to solve the full autonomy puzzle learn the limitations by exploring it starts with this guy
and hundreds of others if you search on YouTube first time with autopilot the
amazing experience of direct transfer of control of your life to an artificial
intelligence system in this case giving control to Tesla autopilot system this
is why in the human centered camp of autonomy I believe that autonomous vehicles can
be viewed as personal robots with which you build build a relationship or the
human robot interaction is the key problem not the perception control and
they're the flaws of both humans and machines must be clearly communicated
and perceived perceived because we use the computer vision algorithms to detect
everything about the human it communicated because on the displays of the car or even through voice it has to
be able to reveal when it doesn't see different aspects of the scene from the
human centered approach then we can focus on the left the perception and
control side perceiving everything about the external environment and controlling the vehicle without having to worry
about being 99.99999% correct approaching a hundred percent correct
because in the cases where it's extremely difficult we can let the human catch the system we can reveal the flaws
and let the human take over when the system can't so let's get to the sensors
Sensors
the sources of raw data that we'll get to work with there
three there's cameras so image sensors RGB infrared visual data does radar and
ultrasonic and there's lidar let's
discuss the strengths first to discuss really what these sensors are the strengths the weaknesses and how they
can be integrated together through sensor fusion so radar is the trust of
the old trusted friend the sensor that's commonly available in most vehicles that
have any degree of autonomy on the left is a visualization of the kind of data on high-resolution radar that's able to
be extracted it's cheap both radar which works with
electromagnetic waves and ultrasonic which works with sound waves sending a
wave letting it bounce off the obstacles knowing the speed of that wave being able to calculate the distance to the
obstacle based on that it does extremely well in challenging weather rain snow
the downside is low resolution compared to the other sensors we'll discuss but
it is the one that's most reliable and used in automotive industry today and it's the one that's in sense of fusion
is always there lidar visualized on the
right the down size it's expensive but
it produces an extremely accurate depth information and a high resolution map of the environment that has 360 degrees of
visibility it has some of the big
strengths of radar in terms of reliability but with much higher resolution and accuracy the downside is
cost here is the visualization comparing the two of the
kind of information get to work with the the the density and the quality of
information with lidar is much higher and lighter has been the successful
source of ground truth the reliable sensor relied upon on vehicles that
don't care about cost and camera the
thing that most people here should be passionate about because machine learning deep learning has the most
ability to have a significant impact there why first it's cheap so it's everywhere
second it's the highest resolution so there's the most the most highly dense amount of information which means
information is something that could be learned and inferred to interpret the
external scene so that's why it's the best source of data for understanding
the scene and the other reason it's awesome for deep learning is because of
the huge eNOS of data involved the its
many orders of magnitude more data available for driving in camera visible
light or infrared than it is in lidar the and our world is designed for
visible light our eyes work in similar ways the cameras at least crudely so the
source data is similar the lane markings the traffic signs of traffic lights the
other vehicles the other pedestrians all operate with each other in this RGB
space in terms of visual characteristics the downside is cameras are bad at depth
estimation it's noisy and difficult even with stereo vision cameras to estimate depth relative to lidar and they're not
good in extreme weather and they're not good at least visible light cameras at night
compare the ranges here's a plot and meters on the x-axis of the range and
acuity and the y-axis with ultrasonic
lidar radar and camera passive visual
sensor plotted the range of cameras is the greatest this is looking at we're
going to look at several different conditions this is for clear well-lit conditions so during the day no rain no
fog lighter and radar have a smaller range under 200 meters and ultrasonic
sensors used mostly for Park assistance and these kinds of things and blind spot warning has terrible range is designed
for extremely close as high resolution distance estimation for extremely close
distances here a little bit small but looking at up top is clear well-lit
conditions the plot we just looked at and on bottom is clear dark conditions so just a clear night day no rain but
it's night and on the bottom right is heavy rain snow or fog vision falls
apart in terms of range and accuracy under dark conditions and in rain snow
or fog radar our old trusted friend stay strong the same range just under
two hundred meters and at the same acuity same with sonar lighter doesn't
works well at night but it does not do well with rain or fog or snow one of the
biggest downsides of lidar other than cost so here's another interesting way
to visualize this that I think is productive for our discussion of which sensor will win out is it the Elon Musk
prediction of camera or is that the way more prediction of lidar for
I'd are in this kind of plot that will look for every single sensor the greater
the radius of the blue the more successful that sensor is at
accomplishing that feature with a bunch of features lined up around the circle
so range for lidar is pretty good not great but pretty good resolution is also
pretty good it works in the dark it works in bright light but it falls apart
in the snow it does not provide color information texture information contrast
it's able to detect speed but the sensor size at least to date is huge the sensor
cost at least to date is extremely expensive and it doesn't do well in
proximity where ultrasonic shines speaking of which ultrasonic same kind
of plot does well in proximity detection it's cheap the cheapest sensor of the four and sensor size you can get it to
be tiny it works and snow and fog and rain but its resolution is terrible
its range is non-existent and it's not able to detect speed
that's where radar steps up it's able to detect speed it's also cheap it's also
small but the resolution is very low and
it's just like lidar is not able to provide texture information color information camera the sensor cost is
cheap the sensor size is small not good up close proximity the range is the longest
of all of them resolution is the best of all of them it doesn't work in the dark it works in
bright light but not always one of the biggest downfalls of camera senses is
the sensitivity to the lighting variation it works it doesn't work in
the snow fog rain so suffers much like lidar from that
but it provides rich interesting sectional information the very kind that
deep learning needs to make sense of this world so let's look at the cheap
sensors ultrasonic radar and cameras
which is one approach putting a bunch of those in a car and fusing them together
the cost there is low one of the nice
ways to visualize using this visualization technique when they're fused together on the bottom it gives
you a sense of them working together to complement each other as strengths and
the question is whether the camera or lidar will win out for partial autonomy
or full autonomy on the bottom showing this kind of visualization for a lidar
sensor and on top showing this kind of visualization for fused radar ultrasonic
and camera at least under these considerations the fusion of the cheap
sensors can do as well as lidar now the open question is whether lidar in the
future of this technology can become cheap and its range can increase because then lidar can win out
solid-state light our and a lot of developments with a lot of startup ladder companies are promising to
decrease the cost and increase the range of these sensors but for now we plow
along with dedication on the camera front the annotated driving data grows
exponentially more and more people are beginning to annotate and study the
particular driving perception and control problems and the very algorithms
for the supervised and semi-supervised and generative networks that we use to
work with this data are improving so it's a race and of course radar and ultrasonic I was
there to help so companies that are playing in the space some of them are
Companies in the self-driving car space
speaking here lame-o in April 2017 they
exited their testing their extensive impressive testing process and allow the
first rider in Phoenix Public rider in November 2017
it's an incredible accomplishment for a company and for an artificial intelligence system in November 2017 no
safety driver so the car truly achieved full autonomy under a lot of constraints
but it's full autonomy it's a step it's an amazing step in the direction towards
full autonomy much sooner than people would otherwise predict and the miles
four million miles driven autonomously by November 2017 and growing quickly
growing in terms of full autonomous driving if I can say so cautiously
because most of those miles have a safety driver so I would argue it's not
full autonomy but however they define full autonomy it's four million miles
driven incredible uber in terms of miles
second on that list they have driven two million miles autonomously by December
of this of last year 2017 the quiet
player here in terms of not making any
declarations of being fully autonomous just quietly driving in a human censored
way l2 over 1 billion miles in autopilot
over three hundred thousand vehicles today are equipped with autopilot
technology with the ability to drive control the car laterally and longitudinally and if anyone believes
the CEO of Tesla there'll be over 1 million such vehicles by the end of 2018
but no matter what the 300,000 is an incredible number and the 1 billion
miles is an incredible number autopilot was first released in September 2014 one
of the first systems on the road to do so autopilot and I call myself as one of
the skeptics in October 2016 autopilot
decided to let go of an incredible work done by Mobil I now Intel we're
designing their perception control system they decided to let go of it completely and start from scratch using
mostly deep learning methods the DRI px 2 system from Nvidia and 8 cameras they
decided to start from scratch that's the kind of boldness the kind of risk-taking
that can come with naivety but in this case it worked
incredible audio 8 system is going to be
released at the end of 2018 and it's promising one of the first vehicles that's promising what they're calling l3
and the definition of l3 according to
Thorsten Lionheart the head of the automated driving and Oddie in a naughty
is when the function is operate as intended if the customer turns the
traffic jam pilot on now this l3 system is designed only for traffic jazz
bumper-to-bumper traffic under 60 kilometers an hour if the customer
returns a traffic jam pilot on and uses it as intended and the car was in control at the time of the accident the
driver goes to the insurance company and the insurance company will compensate the victims of the accident and
aftermath they come to us we will pay them so that means the cars
liable the problem is under the
definition of l2 l3 perhaps there is some truth to this being an l3 system
the important thing here is it's nevertheless deeply and fundamentally human centered because even as you see
here in this demonstration video with a reporter the car for a poorly understood
reason transfer control to the driver says that's it I can't I can't take care
of the situation you take control how how much time do you have in terms of
seconds before you really need to know to take over well this is the new thing about level 3 with level 3 the system
allows the driver to give the prompt to take over vehicle control again ahead of
time which is in this case up to 10 seconds ok so if the traffic jam
situation clears up or any failure in the system occurs everything you might
think of the system still needs to be able to drive automatically because the driver has this time to take over
you might ask what its new about this so why is Howdy saying this is the first
level 3 system worldwide on the market when talking about these levels of
automation there's a classification which starts at lower zero which is basically the drivers doing everything
there's no assistance nothing and then it gradually becomes into partly
automation and when we're talking about these assistance functions like lane-keeping and distance keeping we're
talking about level 2 assistance function ok which is meaning that the
driver is obliged to permanently monitor the traffic situation to keep the hands
on the wheel even though there's a support and an assistance and to intervene immediately if anything is not
quite right so you know that from laying assistance systems when the steering is
not perfectly in the right lane we have to intervene and correct immediately and
that is the main difference now we got a takeover request so what so let's let's
talk about what that means this is still a human Center system it still struggles
that still must solve the human robot interaction problem and there's many
others playing in the space I'm the on the full autonomy side way mo uber GM
crews new tana me the CTO of which he'll speak here on Tuesday optimist ride its
annuity voyage the CEO of which will speak here next Thursday and Aurora not
listed this the founder of which will speak here next Friday and the human
centered autonomy side the reason I am speaking about us so much today is we
don't have any speakers I'm the speaker the Tesla autopilot is for several years
now doing incredible work on that side we are also working with Volvo pilot assist as a lot of different approaches
they're more concerned of interesting the audio traffic jam assist as I mentioned the a8 being
released at the end of this year the Mercedes drive pollicis in the e-class
an interesting vehicle that I got to drive quite a bit as the Cadillac supercruise the ct-6 which is very much
constrained geographically to highway driving and the loudest proudest of them
all george hotz of the comma a open pilot let's just
leave that there so where can a I help
Opportunities for deep learning
we'll get into the details of the coming lectures on each individual component I'd like to give some examples the key
areas problem spaces that we can use machine learning to solve from data his
localization and mapping so being able to localize yourself in the space the very first question that a robot needs
to answer where am I seen understanding taking the scene in and interpreting
that scene detecting all the entities in the scene detecting the class of those
entities in order to then do movement planning to move around those entities
and finally driver state essential element for the human robot interaction perceive everything about the driver
everything about the pedestrian and the cyclists and the cars outside the human element of those the human perception
side so first the where am I visual odometry using camera sensors which is
really where once again deep learning is most that a vision sensor is the most
amenable to learning based approaches and visual odometry is using camera to
localize yourself to answer the where am I question the traditional approaches
slam detect features in the scene and
track them through time from frame to frame and from the movement
those features are able to estimate thousands of features tracking estimate
the location the orientation of the vehicle or the camera those methods with
stereo vision first requires taking two camera streams on distorting them
competing disparity map from the different perspectives of the two camera computing the matching between the two
the feature detection thus if too fast or any of the methods of extracting non
deep learning methods of the extracting features strong detectable features that
can be tracked through from frame to frame tracking those features and estimating the trajectory the
orientation of the camera that's the traditional approach to visual odometry
in the recent years since 2015 but most success in the last year has been the
end end deep learning approaches either stereo or monocular cameras deep vo is
one of the most successful the antenna method has taken a sequence of images
extracting with a CNN from each image the central features from each image and
then using RNN recurrent neural network to track over time the trajectory the
pose of the camera image to pose and to
end here's the visualization on a kitty data set using deep vo again taking the
video up on the top right as an input and estimating what's visualized is the
position of the vehicle in red is the estimate based again and to end with a
CNN and RNN the in red is the estimate in blue is the ground truth in the kitty
dataset so this removes a lot of the modular parts a slam a visual odometry
and allows it to be and to end which means it's learner bull which means it
gets better with data that's huge
vision alone this is one of the exciting opportunities for AI or people working
in AI is the ability to use a single sensor and perhaps the most inspiring
because that sensor is similar to our own the sensor that we ourselves use of
our eyes to use that alone as the primary sensor to control a vehicle
that's really exciting and the fact that deep learning that the vision visible
light is the most amenable to deep learning approaches makes this particularly an exciting area for deep
learning research scene understanding of course who can do a thousand slides on this traditionally object detection
pedestrians vehicles there is a bunch of different types of classifiers of feature extractions harlech features and
deep learning has basically taken over and dominated every aspect of scene
interpretation perception understanding tracking recognition classification
detection problems and audio can't forget audio that we can use audio as
source of information whether that's detecting honks or in this case using the audio of the tires microphones on
the tires to determine visualize there's a spectrogram of the audio coming in
for those of you who are particularly have a particularly tuned ear can listen
to the different audio coming in here of wet road and dry road after the rain so
there's no rain but the road is nevertheless wet and detecting that is extremely important for vehicles because
they still don't have traction control estelle have poor control in road to road surface tired road surface
connection and being able to detect that from just audio is a very interesting approach
finally we're not finally next for the perception control side finally is the
movement planning getting from A to point from point A to point B traditional approaches the optimization
based approach determine the optimal control try to reduce the problem
formalize the problem in a way that's amenable to optimization based
approaches there's a lot of assumptions that need to be made but once those
assumptions are made you're able to determine to generate thousands or
millions of possible trajectories and have an objective function we determine which of the trajectories to take here's
a race car optimizing how to take a turn at high speed with deep learning
reinforcement learning the application mule networks
reinforcement learning is particularly exciting for both the control and the
planning side so that's where the two of
the competitions we're doing in this class come into play the simplistic two-dimensional world of deep traffic
and the high mood high speed moving
high-risk world of deep crash will
explore those tomorrow tomorrow's lectures on deeper enforcement learning
and finally drivers state detecting everything about the driver and then
interacting with them on the left and green are the easier problems on the right and red are the harder problems in
terms of perception in terms of how amenable they are to deep learning methods body pose estimation is a very
well studied problem we have extremely good detectors for estimating the pose
the hands the elbows the shoulders every aspect visible aspect of the body head
pose the orientation of the head or extremely good at that and as we get
smaller and smaller in terms of size blink rate blink duration I pose and
blink dynamics start getting more and more difficult all of these metrics all of these metrics extremely important for
detecting things like drowsiness or as components of detecting emotion or word people are looking in driving where your
head is turned is not necessarily where you're looking in regular life
non-driving life when you look somewhere you usually turn your head to look with
your eyes in driving your head often stay still or moves very subtly your
eyes do a lot more moving it's the kind of effect that we described as the
lizard owl effect some fraction of people a small fraction or owls meaning
they move their head a lot and some people most people are lizards
moving eyes to allocate their attention the problem with eyes is from the
computer vision perspective they're much harder to detect in lighting variation than real-world conditions they get
harder and we'll discuss how to deal with it of course that's where deep learning steps up and really helps with
real-world data cognitive load we'll discuss as well estimating the cognitive load of the
driver to give a quick clip is this as the driver glance we've seen before
estimating the very most important problem on driver stateside is
determining whether they're looking on road or off road it's the dumbest simplest but most important aspect are
they looking are they in the seat and looking on the road or are they not that's driver glance classification not
estimating the X Y Z geometric orientation where they're looking but actually binary class classification on
road or off road body pose estimation determining if the hands are on wheel or
not determining if the body alignment is standard is good for seatbelt for safety
this is one of the important things for autonomous vehicles if there's an imminent danger to the driver the driver
should be asked to return to a position that is safe for them in case of a crash driver in motion on the top is
satisfied on the bottom as a frustrated driver they self-reported satisfied this
is with a voice based navigation one of the biggest sources of frustrations for people in cars is voice based navigation
trying to tell an artificial intelligence system using your voice alone where you would like to go huge
source of frustration one of the interesting things in our large data set that we have from the effective
computing perspective is determining which of the features are most commonly
associated with frustrated voice based interaction and that's a smile as shown there it's the counter intuitive notion
that emotion in particularly emotion in the car is very context dependent that
smiling is not necessarily a sign of happiness and the stoic board look of
the driver up top is not necessarily a reflection of unhappiness he is indeed a
10 out of 10 in terms of satisfaction with the experience if he has ever been
satisfied with anything happens to be Dan Brown one of the
amazing engineers in our team cognitive load estimating from the eye region and
sequences of images 3d convolutional neural networks taking in a sequence of
images from the eye looking at the blink dynamics and the eye position to determine the cognitive load from 0 to 2
how deep in thought you are two paths to autonomous future again I would like to
maybe for the last time but probably not argue for the one on the left because
our brilliant much smarter than me guest speakers will argue for the one on the right the human centered approach allows
us to solve the problems of 99% accuracy of localization scene understanding movement planning those are the problems
were taking on in this class the scene segmentation that we'll talk about on Thursday the control they will talk
about tomorrow and the driver state that we'll talk about next Wednesday these problems can be solved with deep
learning today the problems on the right solving them to close to 100% accuracy are extremely difficult and may be
decades away because for full autonomy to be here we have to solve this
situation I've shown this many times octave Triomphe we have to solve this situation I give you just a few examples
what do you do you have to solve this situation a sort of subtler situation
here is a it's a busy crosswalk where no
autonomous vehicle will ever have a hope of getting through unless it asserts itself and that there's a couple of
vehicles here that kind of nudge themselves through or at least when they have the right-of-way don't necessarily
nudge but don't hesitate when a pedestrian is present an ambulance flying by even though if you use a
trajectory so and pedestrian intent modeling algorithm to predict the
momentum of the pedestrian to estimate where they can possibly go you would
then autonomous vehicle will stop but these vehicles don't stop they assert themselves they move forward now for a
full autonomy system this may not be the last time I show this video but because
it's taking full control it's following a reward function an objective function
and all of the problems the ethical and the AI problems that arise like this
Coast Runner problem will arise so we have to solve those problems we have to
design that objective function so with that I'd like to thank you and encourage
you to come tomorrow because you get a chance to participate in deep traffic a deep reinforcement learning competition
thank you very much [Applause]

----------

-----

--23--

-----
Date: 2018.01.15
Link: [# MIT 6.S094: Deep Learning](https://www.youtube.com/watch?v=-6INDaLcuJY)
Transcription:

Thank you everyone for braving the cold, and the snow To be here
This is 6.S094: Deep Learning for Self-Driving Cars
And, it's a course where we cover the topic of Deep learning
Which is a set of techniques, that have taken a leap in the last decade For our understanding
Of what artificial intelligence systems are capable of doing And self-driving cars, which is systems,
that can take these techniques, and integrate them In a meaningful, profound way into our daily lives
In a way that transforms society. So that's why both of these topics, are extremely important
And extremely exciting. My name is Lex Fridman, And I'm joined by an amazing team of engineers,
In Jack Terwilliger, Julia Kindelsberger, Dan Brown
Michael Glazer, Li Ding, Spencer Dodd and Benedikt Jenik,
Among many others... We build autonomous vehicles, here at MIT,
Not just ones that perceive, and move about the environment,
But ones that interact, communicate, and earn the trust, And understanding of human beings inside the car,
The drivers and the passengers, And the human beings outside the car the pedestrians and other drivers and cyclists.
The website for this course: selfdrivingcars.mit.edu if you have questions, email at: deepcars@mit.edu
Slack: deep-mit For registered MIT students, you have to register on the website
And, by midnight, Friday, January 19th
build a neural network, and submit it to the competition. That achieves the speed of 65 miles per hour
On the new deep traffic 2.0 It's much harder and much more interesting
than last year's for those of you who participated. There's three competitions in this class:
Deep Taffic,SegFuse DeepCrash There's guest speakers, that come from:
Waymo, Google, Tesla And, those are starting new, autonomous vehicle startups
In Voyage, NuTonomy and Aurora
And then use a lot today from CES. And, we have shirts! For those of you who braved the snow
and continued to do so towards the end of the class there will be free shirts.
Yes, I said free and shirts in the same sentence, You should be here.
Okay. First: The Deep Traffic competition There's a lot of updates, and we'll cover those on Wednesday.
it's a deep reinforcement learning competition. Last year we received over 18,000 submissions,
This year we're going to go bigger! Not only can you control one car, with your neural network
You can control up to ten This is multi agent deep renforcement learning. This is super cool!
Second: SegFuse - Dynamic Driving Scene Segmentation competition
Where, you're given the raw video,
The kinematics of the vehicles, the movement of the vehicle, The state-of-the-art segmentation.
For the training set you're given: Ground truth labels, pixel level labels Scene segmentation, and optical flow.
And with those pieces of data, You're tasked to try to perform better than the state-of-the-art
In image based segmentation. Why is this critical,
And fascinating, in an open research problem? Because, robots that act in this world,
In the physical space, not only must interpret, Use these deep learning methods to interpret, The spatial visual characteristics of a scene,
They must also interpret, understand, and track The temporal dynamics of the scene. This competition is about temporal propagaton of information,
Not just scene segmentation. You must understand the space, and time.
And finally Deep Crash Where we use deep reinforcement learning,
To slam cars thousands of times, Here, at MIT, at the gym.
You're given data on a thousand runs, where car Or a car knowing nothing is using a monocular camera's
Single input, driving over 30 miles an hour, Through a scene, it has very little control through
Very little capability to localize itself It must act very quickly. In that scene you're given a thousand runs, to learn anything.
We'll discuss this, in the coming weeks. This competition will result in four submissions
That; We evaluate everyone's in simulation But the top four submissions, we put head-to-head at the gym.
And, until there is a winner declared, we keep slamming cars ...at 30 miles an hour.
Deep crash, and also on the website is from the last year, And on GitHub there's DeepTesla.
Which is using the large-scale naturalistic driving data set We have to train a neural network to do enter and steering
That takes in monocular video from the forward roadway, And produces steering commands,
Steering commands for the car. Lectures: Today we'll talk about deep learning,
Tomorrow we'll talk about autonomous vehicles, Deep RL is on Wednesday,
Driving scene understanding So segmentation That's Thursday.
On Friday, we have Sacha Arnoud, The Director of Engineering at Waymo.
Waymo is one of the companies, that's truly taking Huge strides in fully autonomous vehicles.
They're taking the fully L4, L5, autonomous vehicle approach. and it's fascinating to learn,
he's also the head of perception for them. To learn from him; What kind of problems they're facing?
And what kind of approach they're taking on? We have Emilio Frazzoli, Who's one of last year's speakers
Sertac Karaman said Emilio is the smartest person he knows, So Emilio Frazzoli's the CTO of nuTonomy
An autonomous vehicle company, that was just acquired by Delphi
For a large sum of money. And they're doing a lot of incredible work in Singapore, and here in Boston.
Next Wednesday, we are going to talk about the topic of our research, or my personal fascination,
is deep learning for driver state sensing. Understanding the human, perceiving everything about the human
being inside the car, and outside the car. One talk, I'm really excited about,
is Oliver Cameron on Thursday. He is now the CEO of autonomous vehicle startup Voyage.
He's previously the director of the self-driving car program, for Udacity He will talk about: how to start a self-driving car company,
For those, who said that MIT folks are entrepreneurs. If you want to start one yourself, he'll tell you exactly how.
It's super cool! And then, Sterling Anderson! Who was the director previously, Tesla Autopilot team.
And now is a co-founder of Aurora, The self-driving car startup, that I mentioned,
...that has now partnered, with NVIDIA and many others. So, why self-driving cars?
Self-Driving Cars
This class is about applying data-driven learning methods, To the problem of autonomous vehicles.
Why self-driving cars are fascinating, And an interesting problem space?
Quite possibly, in my opinion, This is the first wide reaching, and profound integration
Of personal robots, in society. Wide-reaching, because there's one billion cars on the road,
Even a fraction of that, will change, ...the face of transportation, and how we move about this world.
Profound, and this is an important point, ...that's not always understood.
There's an intimate connection, between a human, And a vehicle, when there's a direct transfer of control.
It's a direct transfer of control... That takes that, his or her life, into the hands,
Of an artificial intelligence system. I showed a few quick,
Quick clips here, you can Google first time with Tesla autopilot, On YouTube and watch people, perform that transfer of control,
There's something magical... About a human and a robot working together,
That will transform, what artificial intelligence is, In the 21st century.
And this particular autonomous system, AI system, self-driving cars, is on the scale.
And the profound,the life-critical nature of it, is profound. In a way that, it will truly test the capabilities of AI.
There'a a personal connection, That will argue throughout these lectures, That we cannot escape considering the human being. That will argue throughout these lectures,
That we cannot escape considering the human being. That autonomous vehicle, must not only perceive and control
It's movement through the environment. You must also perceive everything about the human driver and the passenger
And interact, communicate, and build trust with that driver.
Because,... In my view, As I will argue throughout this course,
An autonomous vehicle is more of a personal robot, than it is a perfect perception controled system.
Because, perfect perception and control, For this world, full of humans,...
Is extremely difficult. And could be, two-three-four decades away.
Full autonomy. Autonomous vehicles are going to be flawed.
They're going to have flaws... And we have to design systems, that are effectively caught
That effectively transfer control to human beings, When they can't handle the situation.
And that transfer of control... Is an... Is a fascinating opportunity for AI.
Because the obstacle avoidance, Perception of obstacles, and obstacle avoidance,
It's the easy problem. It's the safe problem. Going 30 miles an hour Navigating through streets of Boston,
It's easy. It's when you have to get, to work, and you're late.
Or you're sick of the person in front of you, ...that you want to go in the opposing lane,
and speed up. That's human nature. And we can't escape it. Our artificial intelligence systems,
Can't escape human nature, they must work with it. What's shown here, is one of the algorithms,
We'll talk about next week, for cognitive load. Or we take, the raw,...
3D convolutional neural networks, Take in the eye region, the blinking, and the pupil movement
To determine the cognitive load of the driver. We'll see how we can detect everything about the driver,
Where they're looking? Emotion? Cognitive load? Body pose estimation?
Drowsiness. The movement towards full autonomy
...is so difficult... I would argue That it almost requires human level intelligence.
That the.... As I said, 2-3-4 decade out journey...
For artificial intelligence researchers, to achieve full autonomy Will require achieving, solving, some of the problems
Fundamental problems of creating intelligence. And... That's something we'll discuss,
In much more depth, In a broader view in two weeks, For the artificial general intelligence course,
Where we have Andrej Karpathy, from Tesla, Ray Kurzweil, Marc Raibert, from Boston Dynamics
Who asked for the dimensions of this room, because he's bringing robots
Nothing else was told to me... It'll be a surprise.
So that is why I argue the human centered Artificial intelligence approach In every algorithm of a design considers the human.
For autonomous vehicle on the left, the perception Scene understanding, and the control problem,
As we'll explore through the competitions, And the assignments, of this course Can handle 90, and increasing
...percent of the cases. But it's the 10, 1.1 percent of the cases as we get better and better,
That we have to... We're not able to handle through these methods And that's where the human, perceiving the human
is really important. This is the video from last year. Of Arc de Triomphe Thank you
Didn't know it last year, I know now. That is one of millions of cases,
Where human to human interaction is the dominant driver.
Not, the basic perception control problem
So why deep learning in this space? Because deep learning
Deep Learning
Is a set of methods, that do well from a lot of data.
And to solve these problems Where human life is at stake, We have to be able to have techniques
That learn from data, learn from real-world data. This is the fundamental reality of artificial intelligent systems
That operate in the real world. They must learn from real world data. Whether that's on the left for the perception, the control side,
Or on the right, for the human The perception, and the communication, Interaction
And collaboration with the human, And the human robot interaction.
Ok. So what is deep learning?
It's a set of techniques, if you allow me the definition, of intelligence Being the ability to accomplish complex goals,
Then I would argue, definition of understanding Maybe a reasoning is...
The ability to turn complex information Into simple, useful, actionable information.
And that is what deep learning does. Deep learning is representation learning,
Or feature learning, if you will. It's able to take raw information,
Raw complicated information, That's hard to do anything with, And construct hierarchical representations of that information,
To be able to do something interesting with it. It is the branch of artificial intelligence,
Which is most capable and focused, on this task. Forming representations from data,
Whether it's supervised or unsupervised, Whether it's with the help of humans, or not; It's able to construct structure,
Find structure in the data; Such that you can extract Simple, useful, actionable information.
On the left, From Ian Goodfellow's book,
Is the basic example of a misclassification. The input of the image,
On the bottom, with the raw pixels And as we go up the stack as we go up the layers,
Higher and higher order representations are formed. From edges, to contours The corners, to object parts
And then finally, The full object semantic classification, of what's in the image
This is representation learning A favorite example for me
Is, one from four centuries ago. Our place in the universe,
And representing that place in the universe, Whether it's relative to Earth, Or relative to the Sun.
On the left is our current belief, On the right is the one, that was held widely,
Four centuries ago Representation matters! Because,what's on the right
Is much more complicated than what's on the left.
You can think of, in a simple case here When the task is to draw a line that separates, Green triangles and blue circles
In the Cartesian coordinates space, on the left The task is much more difficult. Impossible, to do well
On the right, it's trivial, in polar coordinates. This transformation is exactly
Whan we need to learn, this is representation learning. So you can take the same task,
Of having to draw a line that separates The blue curve, and the red curve on the left . If we draw a straight line, it's going to be a high
There's no way to do it with zero error. With 100% accuracy
Shown on the right, is our best attempt. But what we can do with deep learning,
With a single hidden layer network done here, Is form the topology, the mapping of the space,
In such a way, in the middle, That allows for a straight line to be drawn, That separates the blue curve, and the red curve.
The learning of the function in the middle, Is what we're able to achieve with deep learning.
It's taking raw, complicated information, And making it simple, actionable, useful.
And the point is, that, this kind of ability to learn, From raw sensory information
Means that, we can do a lot more, with a lot more data. So, deep learning gets better with more data.
And that's important, for real world applications. Where edge cases are everything.
This is us driving, with two perception control systems. One is in Tesla vehicle, with the autopilot
Version one system that's using a monocular camera, To perceive the external environment, And produce control decisions.
And our own, neural network Running on adjacent TX2, that's taking in the same.
With a monocular camera, and producing control decisions. And, the two systems argue, and when they disagree
They raise up a flag, to say that this is an edge case That needs human intervention.
There is... Covering such edge cases, using machine learning, Is the main problem, of artificial intelligence, and...
When applied to the real world, It is the main problem to solve.
Okay. So what are neural networks? Inspired very loosely, and I'll discuss
About the key difference between, Our own brains and artificial brains Because there's a lot of insights, in that difference.
But inspired loosely by biological neural networks, Here, as a simulation of a...
Thalamocortical brain network, Which is only 3 million neurons, 476 million synapses... The full human brain,
Is a lot more than that. A 100 billion neurons, ...1,000 trillion synapses.
There's inspirational music, with this one That I didn't realize was here, it should make you think.
Artificial neural networks, yeah... Let's Just let it play...
The human neural network is, a hundred billion neurons, right? 1,000 trillion synapses.
One of the state-of-the-art, Neural network is ResNet-152, which has...
60 million synapses. That's a difference, of about...
A seven order of magnitude difference That's a difference, of about... A seven order of magnitude difference The human brains have, 10 million times more synapses,
Than artificial neural networks. Plus or minus one order of magnitude, depending on the network.
So, what's the difference, between A biological neuron, and an artificial neuron?
The topology of the human brain have no layers. Neural networks are stacked in layers
They're fixed, for the most part. There is chaos!
Very little structure in our human brain In terms of how neurons are connected.
They're connected, often, to 10,000 plus other neurons. The number of synapses, from individual neurons
That are... Input into the neuron is huge! They're asynchronous. The human brain works asynchronously.
Artificial neural networks work synchronously. The learning algorithm for artificial neuron networks,
The only one, the best one... Is back propagation.
And we don't know, how human brains learn...
Processing speed, this is one of the... The only benefits we have with artificial neural networks is...
Artificial neurons are faster. But they're also extremely power inefficient,
And... There is a division into stages, Of training and testing with neural networks.
With biological neural networks, as you're sitting here today They're always learning.
The only profound similarity, the inspiring one The captivating one, is that both are,
Distributed computation at scale. There is an emergent aspect to neural networks,
Where the basic element of computation: A neuron, Is simple. Is extremely simple.
But when connected together, beautiful Amazing, powerful approximators can be formed.
A neural network is built up with these computational units, They're the inputs, There's a set of edges, with weights on them.
The edges... The weights are multiplied by this input signal, A bias is added, with a nonlinear function.
That determines whether the network gets activated or not Well, the neuron gets activated or not.
Visualized here. And these neurons can be combined in a number of ways.
they can form a feed-forward neural network, Or they can feed back into itself,
To form... To have state memory. In Recurrent neural networks.
The ones on the left, are the ones that are most successful, For most applications, in computer vision.
The ones on the right are very popular, and specific. One temporal dynamics, or dynamics time series
of any kind are used. In fact, the ones on the right, are much closer
To the way our human brains are Than the ones on the left, But that's why, they're really hard to train.
One beautiful aspect, of this emergent power, For multiple neurons being connected together
Is the universal property That with a single hidden layer These networks can learn any function
Learn to approximate any function. Which is an important property to be aware of, because
The limits here, are not in the power of the networks The limit in... ...is in the methods by which
We construct them, and train them.
What kinds of machine learning, deep learning are there? We can separate into two categories.
Memorizers, The approaches, that essentially memorize patterns in the data.
And approaches that, we can loosely say Are beginning to reason
To generalize over the data, with minimal human input. On top, on the left are the, quote/unquote "Teachers",
Is how much human input in blue, is needed To make the method successful for supervised learning,
Which is what most of deep learning successes come from Or most of the data is annotated by human beings,
The human is at the core of the success. Most of the data, that's part of the training
Needs to be annotated by human beings. With some additional successes, coming from augmentation methods,
That extend that... Extend the data, based on which these networks are trained
And the semi-supervised reinforcement learning, And unsupervised methods, That we'll talk about, later in the course,
That's where the near-term successes we hope are. And with the unsupervised learning approaches,
that's where, the true excitement, About the possibilities of artificial intelligence lie.
Being able to make sense, of our world With minimal input from humans,...
So, we can think of two kinds of deep learning impact spaces.
One is a special purpose intelligence. It's taking a problem, formalizing it.
Collecting enough data on it, and being able to, Solve a particular case, that provides value.
Of particular interest here is a network That estimates apartment costs in the Boston area.
So you could take the number of bedrooms, The square feet, and the neighborhood... And provide as output, the estimated cost.
On the right is the actual data, Of apartment cost. We're actually standing,
In an area, that has over 3000 dollars for a studio apartment
Some of you may be feeling that pain. And then there's general-purpose intelligence.
Or something that feels like... Approaching general-purpose intelligence.
Which is reinforcement, and unsupervised learning. Here with Andrej, from Andrej Karpathy's, Pong to Pixels.
A system that takes in, 80 by 80 pixel image And with no other information is able to beat,
Is able to win at this game. No information except a sequence of images, Raw sensory information,
The same way, the same kind of information, That human beings take in, from the visual Audio, touch, sensory data.
The very low-level data, and be able to learn to win. And it's very simplistic, And it's very artificially constructed world,
But nevertheless, A world where no feature learning is performed. Only raw sensory information is used to win.
With very sparse minimal human input. We'll talk about that on Wednesday.
With deep reinforcement learning. So. But for now we'll focus on supervised learning.
Where there is input data, There is a network we're trying to train,
A learning system, and there's a correct output, That's labeled by human beings. That's the general training process for a neural network.
Input data, labels... And the training of that network , that model.
So that, in a testing stage, A new input data, that has never seen before, It's tasked with producing guesses, and is evaluated based on that.
For autonomous vehicles, that means being released Either in simulation, or in the real world, to operate.
And how they learn, how neural networks learn, Is given, the forward pass,
Of taking the input data, whether it's from the training stage In the training stage, taking the input data,
Producing a prediction. And then given that there's ground truth in the training stage, We can have a measure of error, based on a loss function.
That then punishes... The synapses, the connections, the parameters,
That were involved with making that wrong prediction.
And it back propagates the error, through those weights. We'll discuss that in a little bit more detail, in a bit here...
So what can we do with deep learning? You can do one-to-one mapping. Really you can think of input as being anything,
It can be a number, a vector of number, a sequence of numbers A sequence of vector of numbers...
Anything you can think of, from images to video, To audio, to text can be represented in this way. And the output can, the same, be a single number,
Or it can be images, video, text, audio. One-to-one mapping on the bottom,
One-to-many, many-to-one, many to many, and... Many to many with different starting points for the data.
Asynchronous. Some quick terms, that will come up
Deep learning is the same as neural networks, It's really deep neural networks, large neural networks
It's a subset of machine learning, that has been Extremely successful in the past decade.
Multi-layer perceptron, deep neural network , Recurrent neural network Long short-term memory network LSTM
Convolution neural network and deep belief networks, All of these will come up to the slides...
And, there is specific operations, Layers within these networks of Convolution, pooling, activation, and back propagation.
This concept that we'll discuss, In this class. Activation functions, there's a lot of variants.
On the left is the activation function, the left column, And the x-axis is the input,
On the y-axis is the output. The sigmoid function, the output.
If the font is too small, the output is... Not centered at zero.
For the Tanh function, it's centered at zero; But it still suffers from vanishing gradients.
Vanishing gradients is when the value, The input is low or high.
The output of the network, as you see in the right column, There, the derivative of the function is very low.
So the learning rate is very low. For ReLU,
Not, it's also not zero centered, But it does not suffer from vanishing gradients.
Back propagation is the process of learning It's the way we take goal from error, Compute as the loss function,
At the bottom right of the slide, Taking the actual output of the network with a forward pass,
Subtracting it from the ground truth, Squaring, dividing by two,
And than using that loss function. that back propagate, Through, to construct a gradient, to back propagate the error.
To the weights that were responsible, For making either a correct, or an incorrect decision.
So the subtasks are there, there's a forward pass, There's a backward pass, and...
A fraction of the weight's gradient subtracted from the weight. That's it! That process is modular,
So it's local to each individual neuron, Which is why it's extremely,... We're able to distribute it across multiple,
Across the GPU. Parallelize across the GPU.
So, learning for a neural network, These competition units are extremely simple.
They're extremely simple to then... Correct when they make an error, when they're Part of a larger network, that makes an error.
And, all that boils down to, Is essentially an optimization problem. Where the objective, utility, function is
The loss function, and the goal is to minimize it. And we have to update the parameters The weights, and the synapses,
And the biases to decrease that loss function.
And that loss function is highly nonlinear. Depending on the activation function's different properties,
Different issues arise. There's vanishing gradients, for sigmoid.
Where the learning can be slow There's dying ReLU's...
Where the derivative is exactly zero, For inputs less than zero.
There are solutions to this, like leaky ReLU's And a bunch of details, you may discover When you try to win the deep traffic competition
But, for the most part These are the main activation functions And it's the choice of the neural network designer
Which one works best... There's saddle points, all the problems From your miracle, non-linear optimization
That arise, come up here. It's hard to break symmetry,
And stochastic gradient descent Wthout any kind of tricks to it,
Can take a very long time, to arrive at the minima One of the biggest problems in all of machine learning
And certainly deep learning, is overfitting You can think of the blue dots and a plot here
As the data, to which we want to fit a curve We want to design a learning system that approximates
The regression of this data. So, in green, is a sine curve
Simple. Fits well. And then, there's a ninth degree polynomial Which fits even better, in terms of the error
But it clearly overfits this data If there's other data
That it has not seen yet that it has to fit It's likely to produce a high error So it's overfitting the training set
This is a big problem for small data sets And so we have to fix that, with regularization
Regularization is a set of methodologies That prevent overfitting Learning the training too well, in order
And then to not be able to generalize To the testing stage
And overfitting, the main symptom Is the error decreases in training set But increases in the test set.
So there's a lot of techniques and traditional machine learning That deal with this; Cross validation, and so on... But because of the cost of training
for neural networks Its traditional to use what's called a validation set
So you create a subset of the training That you keep away For which you have the ground truth
And use that, as a representative of the testing set. So you...
Perform early stoppage, or more realistically Just save a checkpoint. Often.
To see how, as the training evolves, The performance changes on the validation set,
And so you can stop, when the performance In the validation set is getting a lot worse It means you're overtraining on the training set.
In practice, of course, We run training much longer And see when, what is the best performing
What is the best performing Snapshot checkpoint of the network?
Dropout, is another very powerful regularization technique. Where we randomly remove part of the network
Randomly remove some of the nodes in the network Along, with it's incoming and outgoing edges
So what that really looks like, Is a probability of keeping a node. And in many deep learning frameworks today
It comes with a dropout layer So it's essentially a probability That's usually greater than 0.5
That a node will be kept. For the input layer The probability should be much higher,
Or, more effectively, what works well is just adding noise What's the point here?
You want to create enough diversity in the training data Such that it is generalizable, to the testing.
And as you'll see with deep traffic competition, There's L2 and L1 penalty, Weight decay, weight penalty
Where, there's a penalisation on the weights that get too large The L2 penalty keeps the weight small
Unless the error derivative is huge And produces a smoother model,
And prefers to distribute When there is two similar inputs It prefers to put half the weights on each
Distribute the weights As opposed to putting the weight on one of the edges.
Makes the network more robust L1 penalty has the one benefit That, for really large weights
They're allowed to be, to stay. So it allows for a few weights to remain very large.
These are the regularization techniques And I wanted to mention them because they're useful To some of the competitions, here in the course.
And I recommend to go to playground To tensorflow playground To play around with some of these parameters
Where you get to, online in the browser Play around with different inputs, different features
Different number of layers, and regularization techniques And to build your intuition about classification
Regression problems, given different input data sets.
So what changed? Why over the past many decades
Neural networks that have gone through two winters Are now again Dominating the artificial intelligence community
CPUs, GPUs, ASICs, So, computational power has skyrocketed
From Moore's law to GPUs There is huge data set, including ImageNet, and others
There is research; Back propagation In the 80's, The convolutional neural networks
LSTMs, there's been a lot of interesting breakthroughs About how to design these architectures
How to build them, such that they're trainable efficiently Using GPUs.
There is the software infrastructure From being able to share the data, or get; To being able to train networks, and share code
And effectively view neural networks as a stack of layers As opposed to having to implement stuff from scratch
With TensorFlow, PyTorch and other deep learning frameworks And there's huge financial backing from Google, Facebook, and so on...
Deep learning... ..is...
In order to understand, why it works so well And where it's limitations are...
We need to understand where our own intuition comes from About what is hard, and what is easy The important thing about computer vision
Which is a lot of what this course is about Even in deep reinforcement learning formulation
Is that visual perception for us human beings Was formed 540 million years ago
That's 540 million years worth of data
An abstract thought Is only formed about a 100 thousand years ago
That's several orders of magnitude less data So we can make, with the neural networks
Predictions that seemed trivial
Trivial to us human beings But completely challenging and wrong to neural networks
Here, on the left, showing a prediction of a dog With a little bit of a distortion and noise added to the image
Producing the image on the right And your network is confidently 99 percent plus accuracy, Predicting that it's an ostrich
And there's all these problems to deal with Whether it's in computer vision data, Whether it's in text data, audio...
All of this variation arises In vision, It's illumination variability
The set of pixels and the numbers look completely different Depending on the lighting conditions It's the biggest problem in driving
Is, lighting conditions, lighting variability. Pose variation Objects need to be learned from every different perspective
I'll discuss that for when sensing the driver Most of.... Most of the deep learning work that's done in the face
On the human, is done on the frontal face Or semi frontal face. There's very little work done on the full 360 pose
Variability that a human being could take on.
Intraclass variability for the classification problem, For the detection problem... There is a lot of different kinds of objects
For cats, dogs, cars, bicyclists, pedestrians.
So that brings us to object classification. And I'd like to take you through where deep learning
Has taken big strides for the past several years Leading up to this year, to 2018
So let's start at object classification Is when you take a single image,
And you have to say... One class, that's most likely to belong in that image.
The most famous variant of that is the ImageNet competition ImageNet challenge. ImageNet data set is a data set of 14 million images
With 21,000 categories And... For, say, the category of fruit
There's a total of 188,000 images of fruit And there is 1200 images of Granny Smith apples.
It gives you a sense, of what we're talking about here So this has been, the source
Of a lot of interesting breakthroughs in deep learning And a lot of the excitement, in deep learning
It's first, the big successful network At least, one that became famous
In deep learning is AlexNet in 2012 That took a leap of...
A significant leap in performance on the ImageNet challenge. So it was one of the first neural networks
That was successfully trained on the GPU And achieved an incredible performance boost Over the previous year on the ImageNet challenge.
The challenge is: ...and I'll talk about some of these networks... It's to given a single image, give five guesses,
And you have five guesses to guess For one of them to be correct
The human annotation is a question often comes up So how do you know the ground truth? Human level performance is 5.1 percent accuracy, on this task.
But, the way the annotation for ImageNet is performed, is There's a Google search, where you pull the images
Already labeled for you, and then the annotation that Mechanical Turk, other humans perform
Is just binary: Is this a cat, or not a cat So they're not tasked with performing The very high-resolution semantic labeling of the image.
Okay. So, through, from 2012 with AlexNet, to today
And the big transition in 2018 of the ImageNet challenge Leaving Stanford and going to Kaggle.
It's sort of a monumental step Because in 2015 with the ResNet network Was the first time
That the human level performance was exceeded And I think this is, a very important
Map of where deep learning is. For particularly what I would argue is a toy example
Despite the fact that it's 14 million images So we're developing state-of-the-art techniques here
And in next stage, as we are now exceeding Human level performance, on this task Is how to take these methods into the real world.
To perform scene perception, to perform driver state perception.
In 2016, and 2017 CUImage and SENnet has a very unique new addition
To the previous formulations that has achieved An accuracy of 2.2 percent error
2.25 percent error on the ImageNet classification challenge. It's an incredible result.
Ok, so you have this image classification architecture That takes in a single image, and produces convolution
And takes it through pooling convolution, and at the end Fully connected layers and performs
A classification task, or regression task. And you can swap out that layer to perform any kind of other task
Including with recurrent neural networks of Image captioning, and so on... Or localization of bounding boxes
Or, you can do fully convolutional networks Which we'll talk about on Thursday
Which is when you take an image as an input, And produce an image as an output.
But where the output image, in this case,is a segmentation. Is, where a color indicates what the object is.
The category of the object. So it's pixel level segmentation, Every single pixel in the image is assigned,
A class, a category, where that pixel belongs to. This is, the kind of task,
That's overlaid on top of other sensory information, Coming for the car in order to
Perceive the external environment You can continue to extract information
From images in this way To produce image to image mapping For example to colorize images
And take from grayscale images to color images
Or you can use that kind of heat map information To localize objects in the image So as opposed to just classifying that this is an image of a cow
R-CNN, Fast and Faster R-CNN, And a lot of other localization networks
Allow you to propose different candidates For where exactly the cow is located in this image
And thereby being able to perform object detection Not just object classification.
In 2017 there has been a lot of cool applications Of these architectures
One of which is background removal Again mapping from image to image Ability to remove background from selfies
Of humans or human-like pictures of faces
The reference is, with some incredible animations, Are in the bottom of the slide,
And the slides are now available online
Pix2pixHD There's been a lot of work in GANs
In Generative Adversarial Networks In particular in driving
GANs have been used to generate examples That generate examples from source data
Whether that's from raw data Or in this case with pix2pixHD Is taking coarse semantic labeling of the images
Pixel level, and producing Photorealistic, high-definition images of the forward roadway
This is an exciting possibility For being able to generate A variety of cases for self-driving cars
For autonomous vehicles to be able to learn To generate, to augment the data And be able to change the way different roads look
Road conditions, To change the way vehicles look cyclists, pedestrians.
Then we can move on to recurrent neural networks Everything I've talked about was one-to-one mapping
From image to image, or image to number Recurrent neural networks work with sequences
We can use sequences to generate handwriting
To generate text captions from an image Based on the localization, as the various detections, in that image.
We can provide video description generation So taking a video
And combining convolutional neural networks With recurrent neural networks Using convolutional neural networks to extract features
Frame to frame And using those extracted features To input into our RDRN ends, to then generate labeling
A description of what's going on in the video A lot of exciting approaches for autonomous systems
Especially in drones Where the time to make a decision is short
Same with the RC car traveling 30 miles an hour Attentional mechanisms For steering the attention of the network
Have been very popular For the localization tasks and for just saving
How much interpretation of the image How many pixels need to be considered In the classification task
So we can steer, we can model the way A human being looks around an image
To interpret it And use the network to do the same. And we can use that kind of steering
To draw images, as well.
Finally the big breakthroughs in 2017 Came from this Pong to Pixels
The reinforcement learning using sensory data Raw sensory data And use reinforcement learning methods
Deep are all methods of which we'll talk about on Wednesday I'm really excited about... The underlying methodology of deep traffic, and deep crash
Is using neural networks as the approximators
Inside reinforcement learning approaches. So AlphaGo in 2016, have achieved
a monumental task. That when I first started in artificial intelligence Was told to me is impossible for a system to accomplish
Which is to win at the game of Go Against the top human player in the world.
However that method was trained on human expert positions The Alphago system, was trained on previous games
Played by human experts. And in an incredible accomplishment
AlphaGo Zero in 2017 Was able to beat AlphaGo,
And many of it's variants By playing itself, from zero information
So no knowledge of human experts No games, no training data very little human input
And what more, it was able to generate Moves, that were surprising to human experts.
I think it's Einstein that said that intelligence That the key mark of intelligence is imagination.
I think it's beautiful to see an artificial intelligence system Come up with something that surprises human experts
Truly surprises... For the gambling junkies, DeepStack
And a few other variants Have been used in 2017 to win a heads-up poker.
Again another incredible result! I was always told an artificial intelligence would be impossible
For Deep, For any machine learning method to achieve And was able to beat a professional player
And several competitors have come along since We're yet to be able to beat
To win, in a tournament setting, so multiple players For those unfamiliar heads-up poker is one-on-one.
It's a much much smaller, easier space to solve. There's a lot more human-to-human dynamics going on,
For when there's multiple players. But that's the task for 2018
And the drawbacks! It's one of my favorite videos I show it often, of Coast runners.
For these deep reinforcement learning approaches The learning of the reward function
The definition of the reward function Controls how the actual system behaves
And this will come... This would be extremely important for us, with autonomous vehicles
Here the boat is tasked with Gaining the highest number of points,
And it figures out that it does not need to race, Which is the whole point of the game, In order to gain points
But instead, pick up green circles That regenerate themselves, over and over.
This is the... The counterintuitive behavior of a system
That would not be expected When you first designed the reward function
And this is a very formal simple system Nevertheless Is extremely difficult to come up with a reward function
That makes it operate in the way you expect it to operate Very applicable for autonomous vehicles
Of course in the perception side As I and mentioned with the ostrich and the dog A little bit of noise, with 99.6 percent confidence
We can predict That the noise up top is a robbing, a cheetah, Armadillo, lesser Panda...
These are outputs from actual state-of-the-art neural networks
Taking in the noise, and producing a confident prediction It should build our intuition, to understand that we don't
That the visual characteristics, The spatial characteristics of an image
Did not necessarily convey the level of hierarchy Necessary to function in this world.
In a similar way, with a dog and the ostrich And everything and an ostrich Network confidently, with a little bit of noise
Can make the wrong prediction Thinking that school bus, is an ostrich And a speaker is an ostrich
They're easily fooled But not really... Because they perform the task that they were trained to do, well
So we have to make sure we keep our intuition
Optimized to the way machines learn Not the way humans have learned
Over the 540 million years of data That we've gained Through developing the eye through evolution
The current challenges we're taking on First: Transfer learning There's a lot of success in transfer learning
Between domains that are very close to each other So, image classification from one domain to the next.
There's a lot of value in forming representations Of the way scenes look, in order Natural scenes look,
In order to do scene segmentation The driving case, for example. But we're not able to do any bigger leaps,
In the way it would perform transfer learning The biggest challenge for deep learning Is to generalize
Generalize across domains. It lacks the ability to reason, In the way that we've defined understanding previously
Which is the ability to turn complex information Into simple useful information.
Convert domain specific, Complicated sensory information.
That doesn't relate to the initial training set. That's the open challenge for deep learning
Train on very little data, and then go and reason, And operate in the real world.
Right now, you'll know, it's very inefficient They require big data
They require supervised data Which means they need human. Cost a human input
They're not fully automated, Despite the fact that the feature learning Incredibly the big breakthrough
Feature learning is performed automatically, You still have to do a lot of design, Of the actual architecture of the network
And all the different hyper parameter tuning needs to be performed. Human input
Perhaps a little bit more educated human input, A former PhD students, postdocs faculty
Is required to tune these hyper parameters. But nevertheless, human input is still necessary.
They cannot be left alone. For the most part...
The reward. Defining the reward As we saw with coast run Is extremely difficult For systems that operate in the real world
Transparency Quite possibly it's not an important one But neural networks, currently, are a black box.
For the most part. They're not able to accept Through a few successful visualization methods
That visualize different aspects of the activations They're not able to reveal, to us humans
Why they work, or where they fail And this is a philosophical question,
For autonomous vehicles, That we may not care as human beings If a system works well enough.
But I would argue that, it will be a long time, Before systems work well enough,
Or we don't care. We'll care, And we'll have to work together with these systems
And that's where transparency, communication, ...collaboration is critical. Edge cases. It's all about edge cases.
In robotics, in autonomous vehicles... The 99.9 percent of driving is really boring,
It's the same. Especially highway driving. Traffic driving. It's the same.
The obstacle avoidance, the car following the lanes... ...centering. All these problems are trivial.
It's the edge cases. Trillions of edge cases, They need to be generalised over,
On a very small amount of training data.
So again I return to: Why deep learning?
I mentioned a bunch of challenges, And this is an opportunity! It's an opportunity, to come up with techniques,
that operate successfully in this world. So I hope the competitions we present in this class,
And the autonomous vehicle domain, Will give you some insight, and an opportunity to apply...
In some of these cases are open research problems, Wth semantic segmentation of external perception,
With control of the vehicle, and deep traffic And, with deep crash,
Of control of the vehicle, and under actuated High speed conditions, and the driver state perception.
So with that, I wanted to introduce deep learning to you today, Before we get to the fun tomorrow of autonomous vehicles.
So, I would like to thank: Nvidia, Google, Autoliv, Toyota. And, at the risk of setting off people's phones:
Amazon Alexa, Auto... But, truly, I would like to say, that I've been humbled
Over the past year, by the thousands of messages were received By the attention. By the 18,000 competition entries.
By the many people across the world, not just here at MIT, That are brilliant, that I got a chance to interact with.
And I hope we go bigger, And do some impressive stuff in 2018. Thank you very much, and tomorrow is self-driving!

----------

-----
--22-- 

-----
Date: 2017.12.24
Link:  [# MIT Sloan: Intro to Machine Learning (in 360/VR)](https://www.youtube.com/watch?v=s3MuSOl1Rog)
Transcription: 

the video you're watching now is in 360 resolution is not great but we wanted to
try something different so if you're on a desktop or laptop you can pan around with your mouse or if you're in a phone
or tablet you should be able to just move your device to look around of course it's best viewed with a VR
headset the video that follows is a guest lecture on machine learning that I gave an MIT Sloan course on the business
of artificial intelligence the lecture is non technical and intended to build intuition about these ideas amongst the
business students in the audience the room was a half circle so we thought why not film the lecture in 360 we recorded
a screencast of the slides and pasted it into the video so that the slides are more crisp let me know what you think
and remember it's an experiment so this course is talking about the broad
Course Overview
context the impact of artificial intelligence the global there's global which is the global impact of artificial
intelligence says the business which is when you have to take these fun research ideas that I'll talk about today a lot
of them are cool on toy examples when you bring them to reality you face real challenges which is what I would like to
really highlight today that's the business part when you want to make real
impact when you Miller make these technologies of reality so I'll talk about how amazing the technology is for
a nerd like me but also talk about how when you take that into the real world
what are the challenges you face so machine learning which is the technology
at the core of artificial intelligence will talk about the promise the
excitement that I feel about it the limitations will bring it down a little bit what are the real capabilities the
technology where for the first time really as a civilization exploring the
meaning of intelligence it is if you pause for a second and just think you
know maybe of many of you want to make money out of this technology many of you want to save lives help people but also
in the philosophical level we get to explore what makes us human so while I'll talk about
the low-level technologies also think about the incredible opportunity here we
get to almost psychoanalyze ourselves by trying to build versions of ourselves in
the machine alright so here's the open question how powerful is artificial
How Powerful is Artificial Intelligence
intelligence how powerful is machine learning that lies at the core of artificial intelligence is it simply a
helpful tool a special-purpose tool to help you solve simple problems if your which is what it currently is currently
machine learning artificial intelligence is a way if you can formally define the
problem you can formally define the tools you're working with you can formally define the utility function where you want to achieve with those
tools as long as you can define those things we can come up with algorithms that can solve them as long as you have
the right kind of data which is all I'll talk about data is key and the question
is into the future can we break past
this very narrow definition of what machine learning can give us which is solve specific problems to something
bigger to where we approach the general intelligence that we exhibit as human beings when we're born we know nothing
and we learn quickly from very little data the right answer is we don't know
Supervised Learning
we don't know what are the limitations of technology what kind of machine learning are there there are several
flavors the first two is what's really the first is what's achieved success
today supervised learning what I'm showing here on the left of the slide is the teachers is the data that is fed to
the system and on the right is the students which is the system itself for machine learning so they're supervised
learning whenever everybody talks about machine learning today what for the most part they're referred to supervised
learning which means every single piece of data that is used to train the model is seen by human eyes and those human
eyes with an accompanying brain label that data in a way that makes it useful to the
machine this is this is critical because that's one the blue box the human is
really costly so whenever every single piece of data that needs to be that's used to train the machine needs to be
seen by a human you need to pay for that human and second you're limited to just the the time there's the amount of data
necessary to label what it means to exist in this world is humongous
Augmented Learning
augmented supervised learning is when you get machine to really to help you a little bit there's a few tricks there
but still it's still only tricks it's still the human is at the core of it and the promise of future research that
we're pursuing that I'm pursuing and perhaps in the applications if we get to discuss or some of the speakers here get
to discuss they're pursuing in semi-supervised and reinforcement learning where the human starts to play
a smaller and smaller role in how much they get to annotate they have to annotate the data and the dream of the
sort of Wizards of the dark arts of deep learning are all excited about
unsupervised learning that has very few actual successes in application in the
real world today but it is the idea that you can build a machine that doesn't
require a human teacher a human being to teach it anything is fills us artificial
intelligence researchers with excitement there's a theme here machine learning is
Machine Learning
really simple the learning system in the middle there's a training stage where
you teach it something all you need is some data input data and you need to
teach it the correct output for that input data so you have to have a lot of
pairs of input data and correct output there'll be a theme of cats throughout
this presentation so if you want to teach in a system difference being a cat
and a dog you need a lot of images of cats you need to tell it that this is a cat this bounding box here and the images of
cat you have to give it a lot of images of dogs and tell it ok for this in this
in these pictures they're dogs and then then there's a spelling mistake on the
second stage is the testing stage when you actually give it new input it has never seen before and you hope that it
has given for cat versus dog enough data to guess is this new image that I've
never seen before a cat or a dog now the
one of the open questions do you want to keep in mind is what in this world can
we not model in this way what activity what task what goal my I offer to you
that there's nothing you can't model in this way so let's think about what in
terms of machine learning can be so it
starts small what can be modeled in this way first on the bottom of the slide left is one-to-one mapping where the
input is an image of a cat and the output is a is a label that says cat or dog you can also do one-to-many where
the image the input is a image of a cat and the output is a story about that cat
captioning of the image you can first of all you can do the other way many to one
mapping where you give it a story about a cat and it generates an image there's
many to many this is Google Translate we translate a sentence from one language to another and there's various flavors of that
again same theme here input data provided with correct output and then
let it go into the wild where it runs on
input data hasn't seen before to provide guesses and it's as simple as this
whatever you can come into one of the following four things numbers vector of numbers so bunch of
numbers sequence of numbers or the temporal dynamics matters so like audio
video where the sequence the ordering matters or sequence of vector numbers
just a bunch of numbers if you can convert it into numbers and I propose to you that there's nothing you can't
convert it to numbers if you can convert it to numbers you can have a system
learn to do it and the same thing with the output generate numbers vectors and numbers sequence the numbers or sequence
of vectors and numbers first
Questions
is there any questions at this point well we have a lot of fun slides to get
through but I'll pause every once in a while to make sure we're on the same page here so what kind of input are we
talking about just to fly through it images so faces or medical applications for looking looking at scans of
different parts of the body to determine if they're to diagnose any kind of medical conditions text so conversations
your texts article blog posts for sentiment analysis question and answering so you ask it a question where
the output you hope is answers sound so voice recognition any kind of anything
you could tell from audio time series data so financial data stock market can
use it to predict anything you want about the stock market including whether
to buy or sell I think if you're curious doesn't work quite well as a machine
learning application physical world so cars or any kind of object any kind of
robot that exists in this world so location of where I am location of where
other things are the actions of others that could be all the input all of it can be converted to numbers and the
correct output same thing classification a bunch of numbers classification is saying is it's a cat or dog regression
is saying to what degree I turn the steering wheel sequence is generating
audio generating video generating stories captioning text images generate anything you could
think of as numbers and at the core of it is a bunch of data agnostic machine
learning algorithms there's traditional ones nearest neighbors Navy base support machine support vector machines a lot of them
are limited in all describe how and then
there's neural networks there's nothing special and new about neural networks
and I'll describe exactly the very subtle thing that is powerful that's
always been there all along and certain things have now been able to unlock that
power about neural networks but it's still just the flavor of a machine learning algorithm and the inspiration
for neural networks as Jonathan showed last time is our human brain as perhaps why the media perhaps why the hype is
captivated by the idea of neural networks is because you immediately jump to this feeling like because there's
this mysterious structure to them that scientists don't understand artificial neural networks I'm referring
to and the biological ones we don't understand them and the similarity
captivates our minds that we think well this approach is perhaps as limited as our as limitless as our own human mind
but the comparison ends there in fact the artificial neuron their artificial
Artificial Neuron
neural networks are much simpler computational units at the core of
everything is this neuron if this is a computational unit that does a very two
very simple operations on the left side it takes a set of numbers as inputs it
applies weights to those inputs sums them together applies a little bias and
provides an output somewhere between 0 and 1 so you can think of it as
computational entity that gets excited when it sees certain inputs and gets
totally turned off when it gets other kinds of inputs so maybe this neuron
with a zero with a point seven point six one point four weights it gets really
excited when it sees pictures of cats and totally doesn't care about dogs some
of us are like that so that's the job of this neuron it's to detect cats now what
Building an Artificial Neuron
the way you build an artificial neural network the way you release the power
that I'll talk about in the following slides about the applications what could
be achieved it's just stacking a bunch of these together think about it this is this is
a extremely simple computational unit there so you need to sort of pause
whenever we talk about the following slides and think that there there's a
few slides that I'll show that say neural networks are amazing now I want you to think back to this slide that
everything is built on top of these really simple addition operations with
the a simple nonlinear function applied at the end just a tiny math operation we
stack them together within a feed-forward way so there's a bunch of layers and when people talk about deep
neural networks it means there's a bunch of those layers and then there's
recurrent neural networks that are also a special flavor that's able to have memory so as opposed to just pushing
input into output directly it's also able to do stuff on the inside in a loop where it remembers things this is useful
for natural language processing for audio processing whenever the sequence is not the length of the sequence is not
defined okay slide number one in terms of neural networks are amazing
Neural Networks
this is this is perhaps for the math nerds but also I want you to use your
imagination there's a universality to neural networks means that this simple
computational unit on the left is an input on the right is the output of this network with just a single hidden layer
it's called a hidden layer because it sits there in the middle of the input and the output layers a single hidden
layer with some number of notes can represent any function any function that
means anything you want to build in this world everyone in this room can be
represented with a neural network with a single hidden layer so the power and
this is just one hidden layer the power of these things is limitless the problem of course is how do you find the network
so how do you build a network that as as
clever as many of the people in this room but the fact that you can build
such a network is incredible it's amazing I want you to think about that and the way you train a network so it's
born as a blank slate some random weights assigned to the edges again a
network is represented the numbers at the core the parameters of the core of this network are the numbers on each of
those arrows each of those edges and you start knowing nothing this is a baby
Network and the way you teach it something unfortunately currently as I
said in a supervised learning mechanism you have to give it pairs of input and output you have to give it pictures of
cats and labels on those pictures saying that they're cats and the basic
fundamental operation of learning is when
you compute the measure of an error and
you back propagate it to the network what I mean everything is easier with cats I
apologize I apologize too many cats and
so the input here is a cat and the neural network we trained
it's just guessing it doesn't know say I don't know it's guessing cat well it
happens to be right so we have to this is the measure of error yes you got a
right and you have to back propagate that error you have to reward the network for doing a good job and all you
do what I mean by a reward there's weights on each of those edges and so the the node that individual neurons
that were responsible that back to that cat neuron that cat neuron needs to be rewarded for seeing the cat so you just
increase the weights on the neurons that were associated with producing the correct answer now you give it a picture
of a dog and the neural networks is cat well that's an incorrect answer so no
there's a high error needs to be back propagated to the network so the weights are responsible with classifying this
out of this picture as a cat need to be punished they need to be decreased simple and you
just repeat this process over and over this is what we do as kids when we're first learning i I'm you know for the
most part that we have to we're also supervised learning machines in the sense that we have our parents
and we have the environment the world that teaches about what's correct and
what's incorrect and we back propagate this error and reward through our brain to learn the problem is as human beings
we don't need too many examples and I'll talk about some of the drawbacks of these approaches we don't need too many
examples you fall off your bike once or twice and you learn how to ride the bike unfortunately neural networks needs need
tens of thousands of times when they fall off the bike in order to learn how to not do it that's one of the
limitation and one key thing I didn't mention here is when we refer to input data it's when
Representation
we refer to input data we usually refer to sensory data raw data we have to
represent that data in some clever way in some deeply clever way where we can
reason about it whether it's in our brains or in the neural network in a
very simple example here to illustrate what representation of data matters so
the way you represent the data can make the discrimination of one class from
another a cat versus dog either incredibly difficult or incredibly
simple here is a visualization of the same kind of data and Cartesian coordinates and polar coordinates on the
right you can just draw a simple line to separate the two what you want is a
system that's able to learn the polar coordinate representation versus the
Cartesian representation automatically and this is where deep learning has
stepped in and revealed the incredible power of this approach which deep
learning is the smallest circle there is a type of representational learning
machine learning is the bigger second to the biggest up so this class is about the biggest circle AI includes robotics
includes all the fun things that are built on learning and I'll discuss while machine learning I think will close this
entire circle into one but for now AI is the biggest circle then a subset of that
is machine learning and a smaller subset of that is representation learning so
deep learning is not only able to say given a few examples of cats and dogs to
discriminate between a cat and a dog it's able to represent what it means to
be a cat it's so it's able to automatically determine what are the
fundamental units at the low level and the high level talking about this very Plato what it
means to represent a cat from the whiskers to the high level shape of the
head to the the fuzziness and the deformable aspects of the cat not a cat
expert but I hear this these are the features of a cat verses that are essential to discriminate between a cat
and a dog learning those features as opposed to having to have experts this
is the drawback of systems that Jonathan talked about from the 80s and 90s where you have to bring in experts for any
specific domain that you try to solve you had to have them encode that information deep learning this is this
is simply the only big difference between deep learning and other methods
is that it learns the representation for you it learns what it means to be a cat nobody has to step in and help it figure
out what what that cats have whiskers and dogs don't what does this mean the
fact that it can learn these features these whisker features is as opposed to
having five or ten or a hundred or five hundred features that are encoded by brilliant engineers with PhDs it can
find hundreds of thousands millions of features automatically hundreds of
millions of features so stuff that that can't be put into words are described in
fact it's one of the limitations the neural networks is they find so many fundamental things about what it means
to be a cat that you can't visualize what it really knows it just seems to know stuff and it finds that stuff
automatically what what does this mean it's the critical thing here is because
it's able to automatically learn those hundreds of millions of features it's able to utilize data it doesn't start
the diminishing returns don't hit on until what we don't know when they hit
the point is with the classical machine learning algorithms you start hitting a wall when you have tens of thousands of
images of cats with deep learning you get better better with more data neural networks
General Intelligence
are amazing slide two here's here's a game a simple arcade game where there's
two paddles the bouncing a ball back and forth okay great you can figure out an artificial
intelligence agent that can play this game it can and not even that well just kind of it kind of learns to do all
right and eventually win here's the
fascinating thing with deep learning as opposed to encoding the position of the
paddles the position of the ball having an expert in this game as many come in
and encode the physics of this game the input to the neural network is the raw
pixels of the game so it's learning in
the following way you give it an evolution of the game you give it a
bunch of pixels pixels are you know images are built up of pixels they're
just numbers from 0 to 256 so there's this array of numbers that represent
each image and then you give it several tens of thousands of images they're represented game so you have the stack
of pixels and stack of images that represents a game and the only thing you
know this giant stack of numbers the only thing he knows at the end you won or lost that's it so based on that you
have to figure out how to play the game you know nothing about games you know nothing about colors or balls or paddles
or winning or anything that's it so this is it's why is this amazing that it even
works and it works too it wins it's amazing because that's exactly what we do as human beings this is general
intelligence so I need you to pause and think about this well we'll talk about
special intelligence - the usefulness and it ok there's cool tricks here and there that we can do to get you an edge
on your high-frequency trading system but this is general intelligence general
intelligence is the same intelligence we use as babies when we're born what we get is an input sensory input of image
sensory input right now all of us most of us are seeing hearing feeling with
touch and that's the only input we get we know nothing and with that input we have to learn something
nobody is pre teaching us stuff and this is an example of that a trivial example
but one of the first examples where this is truly working I sorry to linger on
this but it's a fundamental fact the fact that we have systems that and now
outperform human beings in these simple arcade games is incredible this is the research side of things but
let me step back these again the takeaways that previous slide is why I
think machine learning is limitless in the future currently it's limited again
the representation of the data matters and if you want to have impact we
currently can only tackle the small problems what are those problems image
recognition we can classify given the entire image of a leopard of a boat of a
mite with pretty good accuracy of what's
in that image that's image classification what else we can find exactly where in that image
each individual object is that's called image segmentation again the same the
process is the same as the learning system in the middle and neural network
as long as you give it a set of numbers as input and the correct set of labels
as output it learns to do that for data hasn't seen the best let me pause a
second and maybe if you have any questions does anyone have any questions about
the techniques of neural networks yes
Data Representation
so that's a great question and in a couple of slides I'll get to it exactly
so the the the data representation I'll
elaborate in a little bit but loosely the data representation is for a neural network is in the weights of each of
those arrows that connecting your ons that's where the representation is so
I'll show to really clarify that example of what that means the Cartesian versus
polar coordinates is just the visual very simple visualization of the concept
but you want to be able to represent the data in an arbitrary way where there's
no limits to the representation it could be highly nonlinear highly complex any
other questions
Pattern Recognition
so I have a couple of slides almost asking this questions because there's no good answers but one could argue and I
think somebody in last class brought up that you know is machine learning just pattern recognition it's possible that
reasoning thinking is just pattern
recognition and I'll describe sort of an intuition behind that so we tend to
respect thinking a lot because we've recently as human beings learned to do
it in our evolutionary time we think that it's somehow special from for
example perception we've had visual perception for several orders of magnitude longer in our evolution
evolution as a living species we've started to learn to reason I think about
a hundred thousand years ago so we think it's somehow special from the same kind
of mechanism we use for seeing things perhaps it's exactly the same thing it's so perception is pattern recognition
perhaps reasoning is just a few more layers of that that's the hope that's an
open question it's
yes that's a great question there there's been very few breakthroughs in
your networks since through the AI winters that we discussed through a lot
of excitement in spurts and even recently there's been a very few
algorithmic innovations the big gains came from compute so improvements in GPU
and better faster computers the you can't underestimate the power of
community so the ability to share code and the internet ability to communicate
together through the internet and work on code together and then digitization of data so like ability to have large datasets
easily accessible and downloadable all of those little things but I think it in
terms of the future of deep learning and machine learning it it all rides on
compute I think meaning continued bigger and faster computers that doesn't
necessarily mean Moore's law in making small and smaller chips it means getting clever in different directions massive
parallelization coming up with ways to do super efficient power efficient
implementations and neural networks and so on so let me just fly through a few
Machine Learning Examples
examples of what we can do with machine learning just to give you a flavor I think in future lectures as possible
we'll discuss different speakers the different specific applications really
dig into those so we can as opposed to working with just images you can work with videos and segments those I
mentioned image segmentation we do video segmentation so through video segments the different parts of a scene that's
useful to a particular application here and driving you can segment the road from cars and
vegetation and lane markings you can
also a subtle but important point these
very small piece of information that we just we know are important like there is a red light like I have to stop I have
to slow down so hard questions so the
How to Detect Traffic Lights
question was how do you detect the traffic light and lights so how do we do
it as human beings first of all let's start there the way we do it is by the
knowledge we'll bring to the table so we we know what it means to be on the road there's a lot of the huge network of
knowledge that you come with and so that makes the perception problem much easier this is pure perception you take an
image and you separate different parts based purely on tiny patterns of pixels
so first it finds all the edges and it learns that traffic lights have certain
kinds of edges around them and then zoom out a little bit they have a certain
collection of edges that make up this black rectangle type shape so it's all
about shapes it kind of build up knowing this this shape structure of things but
it's a purely perception problem and one of the things that argue is that if it's purely a perception approach and you
bring no knowledge to the table about the physics of the world the three-dimensional physics and the temporal dynamics that you are now going
to be able to successfully achieve near 100% accuracy and some of the
so that's exactly the right question is you for all of these things think about
how you as a human being would solve these problems and what is lacking in the machine learning approach what data
is lacking in the machine learning approach in order to achieve the same kind of results the same kind of
reasoning required to that you would use as a human so there is also image
detection image detection which means the subtle but important point the stuff
I've mentioned before image classification is given them image of a cat you find the cat noting the side you
don't find the cat you say this images of a cat or not and then detection or localization is when you actually find
where in the image that is that problem is much harder but also doable with
machine learning with with deep neural networks now as I said inputs outputs
can be anything the input could be a video the output could be video and you could do anything you want with these
videos you can colorize the video you can add take an old black-and-white film
and produce color images again in terms
of being out in terms of having an impact in the world using these applications you have to think this is a
cool demonstration but how well does it actually work in the real world translation whether that's from text to
text or image to image you can translate here dark-chocolate from one language to
another it's class global business of artificial intelligence there's a
How to Generate Text
reference below there you can go and generate your own text you can generate the writing of the act of generating
handwriting so you can type in some text and given different styles that it learns from other handwriting samples it
can generate any kind of text using handwriting again the input is language the output
is a sequence of writing of pen movements on the screen you can complete
sentences this is kind of a fun one where if you start
so you can generate language you can generate language where you start you feed the system some input first so in
black there's says life is and then have the neural network complete those sentences life is about kids life about
life is about the weather there's a lot of knowledge here I think being conveyed and you can start
the sentence with the meaning of life is the meaning of life is literary recognition true for us academics or the
meaning of life is the tradition of ancient human production also true but
these are all generated by a computer you can also caption this has been become very popular recently is caption
generation given us input as an image the output is a set of text the cap captures the content of the image you
find the different objects in the in the image that's a perception problem and
once you find the different objects you stitch them together in a sentence that makes sense you generate a bunch of sentences and
classify which sentence is the most likely to fit this this image and you
EndtoEnd Approach
can so certainly in the I tried to avoid mentioning to driving too much because
it is my field with this what I'm excited about what then the moment I
start talking about driving it'll all be about driving so but I should mention of course the deep learning is critical to
driving applications for the both the perception and what is really exciting to us now is the end-to-end the
end-to-end approach so whenever you say end-to-end in any application what that means is you start from the very raw
inputs that the system gets and you produce the very final output that's
expected of the system so supposed to in the self-driving car case as opposed to breaking a car down into each individual
components of perception localization mapping control planning and just taking
the whole stack and just ignoring all the super complex problems in the middle
just taking the external scene as input and as output produced steering and acceleration of braking commands
and so in this way taking this input is the image of the external world in this
case in a Tesla we can generate steering commands for the car again input a bunch
of numbers that that's just images I'll put a single number that gives you the
steering of the of the car okay
What Cant We Do
so let's step back for a second and think about what can't we do with
machine learning we talked we talked about you can map numbers to numbers let's think about what we can't do this
at the core of artificial intelligence in terms of making an impact on this world is robotics so what can't we solve
in robotics and artificial intelligence with a machine learning approach and let's break down what artificial
intelligence means here's a stack starting at the very top is the environment the world you operate in
their sensors that sense that world there is feature extraction and learning from that data and there's some
reasoning planning and effectors are the ways you manipulate the world what can't
we learn in this way so we've had a lot of success as Jonathan talked about in
the history of AI with formal tasks playing games solving puzzles recently
we're having a lot of breakthroughs with medical diagnosis we're still we're
still struggling but are very excited about in the robotic space with more
mundane tasks of walking of basic perception of natural language written
and spoken and then there is the human tasks which are perhaps completely out
of reach of this pipeline at the moment is cognition imagination suggests a
subjective experience so high-level reasoning not just common sense or high level human level reasoning so let's fly
The Pipeline
through this pipeline they're sensors cameras lidar audio
there is communication that flies to the air or wired or wireless or wired I am
you measuring the movement of things so that's the way you think about it that's
the way assuming beings and as any kind of system that you design you measure the world you don't just get an API to
the world you need to somehow measure aspects of this world so that's how you
get the data so that's how you convert the world into data you can play with and once you have the data this is the
representation side you have to convert that raw data of raw pixels of raw audio raw lidar data you have to convert that
into data that's useful for the intelligence system for the learning
system to to use to discriminate between one thing and another for vision that's
The Machine Learning
finding edges corners object parts and entire objects there's the machine learning that I'll
talk about that I've talked about there's different kinds of mapping of the representation that you've learned
to an actual outputs there is once you have this so you have this idea of and
this goes to maybe a little bit of Simon's question is reasoning this is something that's out of reach or machine
learning at the moment this is going to your question then we can we can build a
world-class machine learning system for taking an image and classifying that it's a duck I wonder if this will work
wake you up so we could take this is
well studied exceptionally well studied problem could take audio sample of a doc
and tell that it's a duck in fact what species of bird it's
incredible how much research there is in bird species classification and you can look at video and we could tell that we
can do extra recognition that it's just swimming but we can't do with learning
now is reason that if it looks like a duck it swims like a duck and quacks
like a duck is very likely to be a duck this is the reasoning problem this is
the task that I personally am obsessed with and that I hope that machine
learning can close and then there is the planning action and the effectors
so this is another place where machine
learning has not had many strides there's mechanical issues here that incredibly difficult the degrees of
freedom with all the actuators involved with all the just just the ability to
localize every party yourself in this dynamic space where things are
constantly changing when there's degrees of uncertainty when there's noise just that basic problem is exceptionally
difficult
The Open Questions
let me just pose this question we talked about how machine what machine learning can do with the cats and the duck we
could do that given a representation it could predict what's in the image but one of the open questions is and deep
learning has been able to do the feature extraction the representation learning this is the big breakthrough that
everybody's excited about but can also reason these are the open questions in a
reason can it do the planning in action and as human beings do can it close the
loop entirely from sensors to effectors so learn not only the brain but the way
you sense the world and the way you affect the world
Pong
it the so the question was about the pong game thank you talk to it a little
longer it it doesn't get punished when it doesn't detect the ball this is the beautiful thing it gets punished only at
the very end of the game for losing the game and gets her water for winning the game so it knows nothing about that ball
and it learns about that ball that's something you really sit and think about
has like how do as human beings imagine if you're playing with a physical ball how do you learn what a ball is you you
get hurt by it you like squeeze and you throw it you feel the dynamics of it the physics of it and nobody tells you about
what a ball is you're just using the raw sensor input we take you for granted
and maybe this is what I can end on is this is what's something Jonathan
brought up is we take the simplicity of this task for granted because we've been
we've had eyes we broadly speaking as
living species on planet Earth there's eyes have been evolved for 540 million
years so we have 540 million years of data we've been walking for close to
that bipedal mammals we have been thinking only very recently so a hundred
thousand years versus a hundred million years and that's why we can't some of
The Mars Paradox
these problems that we're trying to solve you can't take for granted how actually difficult they are so for
example this is the marvex paradox the Jonathan brought up is that the easy problems are hard the things would think
are easy actually really hard this is state-of-the-art robot on the right playing soccer and that was a
state-of-the-art human on the left playing soccer and I'll give it a second the question
Neural Networks vs Natural Selection
was you know there's a fundamental difference between the way with train your networks and the way we've trained biological neural networks for evolution
by discarding through natural selection a bunch of the the the the neural
networks that didn't work so well that's so first of all the process of evolution is I think not well understood
meaning sorry the raw huh says he careful here the role of evolution in
the evolution of our cognition of our intelligence I don't know if that's so
this is an open question so maybe clarify this point his neural networks artificial neural networks are
fixed for the most part in size this is exactly right it's like a single human being that gets to learn we don't have
mechanisms of of modifying or revolving
those neural networks yet although you could think of researchers as doing
exactly that there you have grad students working on different neural networks and the ones that don't do a good job don't get
promoted and get a good you know there is a natural selection there but other than that it's a it's an open question
stay tuned and keep your head up because the future I believe is really promising
and the slides will be made available for sure
I think a lot of the explorations of what it means to build an intelligent
machine has been in sci-fi movies we're now beginning to actually make it a reality this is Space Odyssey to keep
with that theme in the previous lecture go ahead this is as opposed to the
dreamlike monolith view when the
astronaut is gazing out into the open sky at the stars we're going to look at the practice of AI today and how we go
if you're familiar with the movie when this new technology appeared before our eyes in we're full of excitement how we
transfer that into actual practical impact on our lives to quickly review
what we talked about last time we I presented the technology and asked the question of whether this technology
merely serves a special purpose to answer specific tasks that can be formalized or whether it can be through
through the process of transferring the knowledge learned on one domain be generalizable to where an intelligent
system that's trained in a small domain can be used to achieve general intelligent tasks like we do as human
beings the this is kind of a stack of artificial intelligence of going from
all the way up into the top of the environment the world the sensors sets the data the the
intelligence system the way it perceives this world then once you have this you convert the world into some numbers you
able to extract some representation of that world and this is where machine learning starts to come into play and
then there's the part where I rate I will raise it again today is can machine learning be doing the following steps to
that we can do very well as human beings is the reasoning step you know you can tell the difference in a cat and a dog but can you now start to reason about
what it means to be alive what it means to be a cat with living creature and what it means to be this kind of
physical object or this kind of physical object and take what's called common sense things we take for granted start
to construct models of the world through reasoning Descartes I think therefore I
am we want our neural networks to come up with that on their own and once
you do that action you'll go right back into the world you start acting in that
world so the question is can machine learning can this be learned from data or does do experts need to encode the
knowledge of reasoning the knowledge of actions the set of actions that's kind of the question open questions I raise
it continues throughout the talk today and so as we start to think about how
artificial intelligence especially machine learning as it realizes itself through robotics gets to impact the
world we start thinking about what are the easy problems what are the hard problems and it seems to us that vision
and movement walking is easy because we've been doing it for millions of years hundreds of millions of years and
thinking it's hard reasoning is hard I propose to you that it's perhaps because
we've only been doing it for a short time and so so think we're quite special because we're able to think so we have
to kind of question of what it's easy and what is hard because when we start to develop some of these systems and
what you start to realize that all these problems are equally hard so the problem of walking that we take for granted the
actuation and the physical the ability to recognize where you are in the
physical space the sense the world around you to deal deal with the
uncertainty of the perception problem and then so all of these robots by the
way this is for the most recent DARPA challenge which MIT was also part of and
so what what are these robots doing they they don't have any they only have
sparse communication with human beings on the periphery so most of the stuff
they have to do autonomously like get inside a car this is an MIT robot unfortunately that they have to get in
the car and the hardest tasks they have to get out of the car that's walking so
this kind of raises to you the very real aspect here you want to build
applications that actually work in the real world and that's the first challenge an opportunity here
than many of the technologies we talked about currently crumble under the the
reality of our world when we transfer
them from a small data set in the lab to the real world for the computer vision
is perhaps one of the best illustrations of this computer vision is the task as we talked about of interpreting images
and so when you there's been a lot of great accomplishments on interpreting images cats versus dogs now when you try
to create a system like the Tesla vehicle that I've often that we work
with and I always talk about is it's a
vision based robot right as radar for basic obstacle avoidance but most of the
understanding of the world comes from a single monocular camera now they've expanded the number of cameras but for
the most time there's been a hundred thousand vehicles driving on the roads today with a single essentially a single
webcam so when you start to do that you have to perform all of these extraction
of texture color optical flow so the the movement through time temporal dynamics of the images you have to construct
these patterns construct the understanding of objects and entities and how they interact and from that you
have to act in this world and that's all based on this computer vision system so it's no longer cats versus dogs it's
it's a huge detection of pedestrians or the wrong classification the wrong
detection is the difference between life and death so let's look at cats those
were things a little more comfortable so computer vision and I would like to illustrate to you why this is such a
hard task which we've talked about we've been doing it for 500 million years so we think it's easy computer vision is
actually incredible so all you're getting with your human eyes is you're getting essentially pixels in there's
light coming into your eyes and all you're getting is the reflection from the different surfaces in here of light
and there's perception they're sensors inside your eyes can burning that into numbers it's really
very similar to this numbers in this in the case of what we use with computers
RGB images or the individual pixels that are numbers from 0 to 255 so 256
possible numbers and there's just a bunch of them and that's all we get we get a collection of numbers where
they're spatially connected ones that are close together are part of the same object so cat-cat pixels are all
connected together that's the only thing we have to help us but the rest of it is just numbers intensity in hours and we
have to use those numbers to classify what's in the image and if you really
think about it this is a really difficult task all you get is these numbers how the heck are you supposed to
form a model of the world with which you can detect pedestrians with a with
really 99.99999% accuracy because these pedestrians are these cars are cyclists
in the car context or any kind of applications you're looking at even if your job is in the factory floor to
detect the the defective gummy bears they're flying past that like a hundred miles an hour
your task is you don't want that bad gummy bear to get by that your product and the the brand will be damaged
however serious are not serious your application is what you have to be you
have to have a computer vision system that deals with all of these aspects
viewpoint variation scale variation no matter the size of the object is still
the same object then no matter the viewpoint from which area you look at
that object is still the same object the lighting that moves with lighting consistent here because we're indoors
but when you're outdoors or you're moving the scene is moving the lighting
the complexity of the lighting variations is incredible from the illumination to just the movement of the
different objects in the scene I think
about you and this particular one it's
Twilight and the light is changing I think you know almost every time I Drive
there's one or two things that I see there really that I'm drawing like 200 million years in order
to be able to figure out it's not it's a guy who's open his car door and I can't see him but I can just see the light
doesn't look quite right on that side of the road and I'm yeah somehow I know I might in my mind it's a person but it
seems like a almost impossible problem for the machines to get right I will
argue that that the pure perception task is too hard that you come to the table
as human beings with all this huge amount of knowledge that you're not
actually interpreting all the complex lighting variations that you're seeing you actually know enough about the world
enough about your commute home enough about the way the kinds of things you
would see in this world about Boston about the way pedestrians move there's a certain light of day you bring all that
to the table that makes the perception task doable and that's one of the big missing pieces in the technology as I'll
talk about that's the open problem of machine learning it's how to bring all that knowledge
first of all build that knowledge and then bring that knowledge to the table as opposed to starting from scratch
every time and so Katz the promise gets
okay so the to me occlusion for most of the computer vision community this is
one of the biggest challenges and it really highlights how far we are from
being able to reason about this world occlusions are when what what an
inclusion is is when the objects you're trying to detect something about classify the object detect object the
object is blocked partially by another object in front of them this is
something you think it's trivial perhaps you don't even really think about it because we we reason a three-dimensional
way but the occlusion aspect is is makes makes perception incredibly difficult so
we have to design is think about this so this image is converted into numbers and we for the task of detecting is there a
cat in this image yes or no you have to be able to reason about this image with
object in the scene most of us are able to very easily detect if there's a cat in this image we're able to detect that
there is a cat in this image now think about this there's a single eye and there's an ear so you have to think
about what is it part of our brain that allows us to understand to suppose that
with some high degree of accuracy that there's a cat here in this picture I mean the degree of occlusion here is
immense and so I promise so this is for most of
you some of you will think this is in fact a monkey eating a banana but I
would venture to say that most of us are able to tell it's nevertheless a cat you
watch this for hours and so let me give you another this is kind of a paper
that's often cited our set of papers to illustrate how difficult computer vision
is how thin the line that we're walking
with all of these impressive results that we've been able to show recently in the machine learning community in this
case for deep neural networks are easily fooled paper the seminal paper at this
point shows that when you apply network trained on imagenet so basically on
detecting cats versus dogs or different categories in inside images if you're you can find an arbitrary number of
images that look like noise up in the top row where the algorithm used to
classify those images in the image net of cat versus dog is able to confidently
say with 99.6% accuracy or above that it's seeing a robin or a cheetah or an
armadillo or a panda you know in that noise so it's confidently saying given this
noise that that's obviously a robin so you have to realize that the kind of
this is patterns the kind of processes it's using to understand what's
containing the image is purely a collection of patterns that it has been able to extract from
other images that has been human annotated by humans and that perhaps is
very limiting to trying to create a system that's able to operate in the
real world this is a very sort of this is very clean illustration of that
concept and the same you can confidently predict and those images below where
there are strong patterns it's not even noise strong patterns that have nothing to do with the entities being detected
again confidently that same algorithm is able to see a penguin a starfish a baseball in the guitar in the in that
noise a more serious for people designing robots like myself in the on
the sensor side you can flip that and say I can take a image and I can distort
it with some very little amount of noise and if that if that noise is applied to
the image I can completely change the confident prediction about what's in that image so to explain what's being
shown so on the left and the column on the left and again here what's the the
same kind of neural network is able to predict accurately confidently that
there is a dog in that image but if we apply just a little bit of noise to that image to produce that image
imperceptible to our human eyes the difference between those two the same algorithm is is saying that there is
confidently in an ostrich in that image so another thing to really think about that noise can have such a significant
impact on the prediction of these algorithms this is really really quite
honestly out of all the things I'll say today and I'm aware of one of the biggest challenges of machine learning
being applied in the real world is robustness how much noise can you add
into the system before everything falls apart so how do you validate sensors so
say a car company has to produce a vehicle and it has sensors in that vehicle how do you know
that that those sensors will not start generating slight noise due to interference of various kinds and
because of that noise instead of seeing a pedestrian you will see nothing or the
opposite you'll see pedestrians everywhere so of course the most dangerous is when it will not see an
object and collide with it in the case of cars there's also spoofing which a lot of people as always with security people
are really concerned about and perhaps people here are really concerned about this issue I think this is a really
important issue but because you can apply noise and convince the system that you're seeing an ostrich when there is
in fact no ostrich you can do the same thing in a in an attacking way so you
can attack the sensors of a car and make it believe like with lidar spoofing so spoof lidar radar or ultrasonic sensors
to believe that you're seeing pedestrians when they're not there and the opposite to hide pedestrians make
pedestrians invisible to the sensor when they're in fact there so whenever you
have Indulgence systems operating in this world they become susceptible to
the fact that everything so much of the work is done in software and based on sensors so at any point in the chain if
there's a failure you have to be able to detect that failure and right now we have no mechanisms for automatically
detecting that failure so on the data side so one challenge is that we're
constantly dealing with is that we are
the algorithms in machine learning algorithms that we're using our need
labeled data and we have very little labeled data labeled data again is when
you have pairs of input data and the ground truth the the true label
annotation class that that image belongs to or concept and the it doesn't have to
be an image it could be any source of data it's a really costly process to do so because it's so costly we
rely every breakthrough we've had so far relies on that label data and because of
its cost we don't have much of it so all the problems that come from data can either be solved by having a lot more of
this data which I believe is most people believe it's too challenging it's too
challenging to have human beings annotate huge amounts of data or we have to develop algorithms that are able to
do something with the unlabeled data its the unsupervised semi-supervised sparsely supervised reinforcement
learning as we talked about last time I mention again here so one way you
understand something about data when you don't have labels is you reason about it
all you're given is a few facts when you're a baby your parents give you a few facts and you go into this world
with those facts and you grow your knowledge graph your knowledge base your understanding of the world from those
few facts we don't have a good method of doing that an automated unrestricted way
the inefficiency of our learners the machine learning algorithms I've talked about the neural networks need a lot of
examples of every single concept that they're given in order to learn anything about them thousands tens of thousands
of cats are needed to understand what the spatial patterns at every level the
representation of a cat the visual representation would cap we don't we can't do anything with a single example
there's a few approaches but nothing quite robust yet and we haven't come up
with a way this is also possible to make annotation this labeling process somehow
be very cheap so leveraging this is something being called human computation that term has fallen out of favor a
little bit one of my big passions is human computation is using something about our behavior something about what
we do in this world online or in the real world to annotate data automatically so for example as you
drive which is what we do everybody has to draw and we can collect data about you
driving in order to train self-driving vehicles to to to drive and that's a
free annotation so here are the annotated data sets we have the
supervised learning data sets there's many but these are ones some of the more famous ones from the very from
the toy data sets of M NIST - the large broad arbitrary categories
of images data sets and there which is what image net is and there's in
healthcare there's an audio there's an video there's are you know there's a huge number of data sets now but each
one of them is usually in the scale of hundreds of thousands millions tens of
millions not billions or trillions which is what we need to create systems that operate in the in the real world and
again these are the kinds of machine learning algorithms we have there's five
listed here the teachers on the left is what is what is the input to the system
that requires to Train it from the supervised learning at the very top is what we have all of our successes and
everything else is where the promise lies the semi-supervised the reinforcement or the fully unsupervised
learning where the input from the human is very minimal and another way to think about this so every whenever you think
about machine learning today whenever somebody talks about machine learning what they're talking about is systems that memorize that memorize
patterns and so this is one of the big criticisms of the current machine
learning approaches where all they're doing is you're providing there only as good as the human annotated data that
they're provided we don't have mechanisms for actually understanding you can pause and think about this in
order to create an intelligent system it shouldn't just memorize it should understand the representations inside
that data in order to operate in that world and that's the open question one
of them and one of the challenges and opportunities for machine learning researchers today is to extend machine
learning memorization to understanding this is that duck the reasoning if you get
information from the perception systems that it looks like a duck from the audio processing that it quacks like a duck
and then from video classification that it the activity recognition that it swims like a duck
the reasoning step is how to connect those facts to then say that it is in
fact a duck okay so that's on the algorithm side and the data side now
this is one of the reasons compute computational power computational hardware that is at the core of the
success of machine learning so our algorithms have been the same since the
60s since the 80s 90s depending on how you're counting the big breakthroughs
came and compute so there's Moore's law most of you know the way our the CPU
side of our computers works for a single CPU is that it's for the most part
executing a single action at a time in a sequence so sequential very different
from our brain which is a massively parallel eyes system so because it's
sequential the clock speed matters because that's how fast essentially those instructions are able to be
executed and so we're we're leveling off physics is stopping us from continuing
Moore's Law so Intel AMD are aggressively pushing this Moore's law
forward but and there's some promise that it will actually continue for
another ten or fifteen years then
there's another form of parallelism massive parallelism is the GPU and this is this is essential for neural networks
this is essential to the success recent success of neural networks is the ability to utilize these inherently
parallel architectures of graphics processing units GPUs the same thing
used for video games this is the this is the reason Nvidia stock doing extremely well is is GPUs so it's
parallelism of basic computational processes that make machine learning work on the GPU one of the limitations
of GPUs one of the challenges is in bringing them to in scaling and bringing
them into real-world applications this power usage its power consumption and so
there is a lot of specialized chips specialized just from the neural network
architectures coming out from Google with their tensor processing unit from IBM Intel and so on it's unclear how far
this goes so this is sort of the direction of trying to design an electronic brain so it has the
efficiency our human brain is exceptionally efficient at running the neural networks in our heads and the
orders of magnitude more efficient than our computers are and this is trying to design systems they're able to grow
towards that efficiency why do you care about efficiency for several reasons one
of course as I'm sure will talk about throughout this class is about the thing
in our smart phones battery usage and this is the big one community I think I
think it could be attributed to the big breakthroughs in machine learning recently in the last decade is the you
know compute as important algorithm development is important but it's the
community of nerds global this is global artificial intelligence and I will show
in several ways why global is essential here is is tens of hundreds of thousands
millions of programmers Mechanical Engineers building robots building
intelligent systems building machine learning algorithms the exciting nature of the growth of the community perhaps
is the key for the future to unlock in the power of machine learning so this is
just one example github is a repository for code and this is showing on the y-axis at the bottom is 2008 one github
first open Institute going up to 2012 quick near exponential growth of the number of
users participating and the number of repositories so these are standalone unique projects that are being hosted on
github so this is one example I'll show you about this competition that we're recently running and then I'll challenge
people here to participate in this competition if you dare so this is a
chance for you to build a neural network in your browser so you can do this on
your phone later tonight of course on your phone
you can specify various parameters of the neural network specify different numbers of layers and the depth the
depth of the network the number of neurons in network the type of layers and it's pretty it's pretty self-explanatory it's super easy in
terms of just tweaking little things and remember machine learning to a large part is an art at this point it's a more
perhaps than even you know more than a well understood theoretically bounded science which is one of the challenges
but it's also an opportunity deep traffic is a chance so we've all been stuck in traffic
there you go Americans spend 8 billion hours stuck in traffic every year that's our pitch for this competition so
deep neural networks can help and so you have a neural network that drives that little car with an MIT logo red one on
this highway and tries to weave in and out of traffic to get to his destination and trying to achieve a speed of 80
miles an hour which is the speed limit which is a physical speed limit of the car of course the actual speed limit of
the road is 65 miles an hour but we don't care about that we just want to get to work as quickly as possible at home so what the basic structure of this
game is and I want to explain this game a little bit and then tell you how incredibly popular it's gotten and how
incredibly powerful the networks that
people built from all over the world the community has built of this over a single month is incredible and this
happens for thousands of projects out there now another challenging
opportunity ok so you may have seen this this is kind of ethics most engineers most I don't like I love
the love philosophy but this kind of construction of ethics that's often presented here is one that is not
usually concerned to engineering so what is this question you know when you have a car you have a bunch of pedestrians do
you hit the larger group of pedestrians or the smaller group of pedestrians do you avoid the group of pedestrians but
put yourself into danger these kinds of ethical questions of an intelligent system it's a very interesting question
it's it's one that we can debate and there's really no good answer quite honestly but it's a problem that both
humans and machines struggle with and so it's not interesting on the engineering side we're interested with problems that
we can solve on the engineering side so the kind of problem that I am obsessed with and very interested in is the
real-world problem of controlling a vehicle through this space so there's it happens in in a few seconds here so this
is a Manhattan New York intersection right this is pedestrians walking perfectly
legally I think they have a green light of course there's a lot of jaywalking too as well well this car just slide
it's not part of the point but yes exactly there's an ambulance and so there's another car that starts making a
left turn in a little bit I may have missed it hopefully not so yeah and then there's another car after that too that
just illustrates when you design an algorithm that's supposed to move through the space like watch this car
the aggression it shows now this isn't a trivial example for those that try to build robots this is this is the real
question is how do you design a system that's able so you have to think you
have to put reward functions objective functions utility functions under which it performs the planning so a car like
that has several thousand candidate trajectories you can take that
intersection you can take a trajectory where it speeds up to 60 miles an hour it doesn't stop and just swerves and
hits everything okay that's a bad trajectory right then there is a trajectory which most companies take
which is most a Google self-driving car and every company that's is concerned about PR is whenever there's any kind of
obstacle any kind of risk that's it all reasonable that you can maybe even touch an
obstacle then you're not going to take that trajectory so what that means is you're going to navigate to this intersection at 10 miles an hour and you
let people abuse you by walking in front of you because they know you're not going to stop and so in the middle there
is hundreds thousands of trajectories that are ethically questionable in the
sense that you're putting other human beings at risk in order to safely and successfully navigate to an intersection
and the design of those objective functions is is the kind of question you
have to ask for intelligent systems fork for cars is there's no grandma and a few
children you have to choose who gets to die very very difficult problems of
course but the problem of when I'm very interested in in streets of Boston streets of New York is how to gently
nudge yourself through a crowd of pedestrians in the way we all actually do when we drive in New York in order to
be able to safely navigate these environments and these questions come up in healthcare these questions come up in Factory in robust in in armed and
humanoid robots that operate with other human beings and that's one of the big
challenges another sort of funny illustration that folks that openly I
use often to illustrate well let me just pause for a second the the gamified version of this there's a game called
coast runners and you're you're racing against other boats along this track and your job is
there's your score here at the bottom-left number of laps your time and you're trying to get to the destination
as quickly as possible while also collecting funky little things like there's these green these green little
things along the way okay so what they've done is the bill Denton system
the one the general-purpose one that we talked about last time that learns oops that learns how to navigate successfully
through the space so you're trying to maximize the reward and what this boat
learns to do is instead of finishing the race it learns to find a loop
it can keep going around and around collecting those green dots and it
learns the fact that they regenerate with time so learns to maximize this score by going around and round now
these are the kinds of things this is the big challenge of our award functions of designing systems of designing what
you want your system to achieve is not only is it difficult to the ethical
questions are difficult but just avoiding the pitfalls of local optima of
vet figuring out something really good that happens in the short-term the greedy what it is that those psychology
experiments of the kid eats the marshmallow and can't wait for you know can't delayed gratification this kind of
the idea of delayed gratification in the case of designing intelligent system was a huge actual serious problem and this
is a good illustration of that so we
flew through a few concepts here is there any is there any questions about
some of the compute and the algorithm side we talked about today yes so the question was yeah used you
highlighted some of the limitations of machine computer vision algorithms machine learning algorithms but you
haven't highlighted some of the limitations of human beings and if you put those in a column and you compare those it's our machines doing better
overall or is there any kind of way to compare those I mean that there's actually interesting work on image net
so image net is this categorization task of where you have to classify images and
you can ask the question when I present you images of cats and dogs where our machine is better than humans and when
when are they not so you can compare when machines do better what are the fail points and what are the fail points
for humans and there's a lot of interesting visual perception questions there I think overall it's certainly
true that machines fail differently than human beings but in order to make an
artificial intelligence system that's usable and could make you a lot of money
and people would want to use it has to be better for that particular task in
every single way in order in order for you to want to use a system
has to be it has to be superior to human performance and usually far superior to human performance so so it's on the
philosophical level it's an interesting thing to compare what are we good at what are not but if you're using Amazon
echo your voice recognition or any kind of natural language chatbots or a car
you're not gonna be well this car is not so good with pedestrians but I appreciate the fact that you can stay in
the lane fortunately you have a very high standard for every single thing that you're good at and it has to be
superior to that I I think maybe maybe that's unfair to the robots I'm more of
the nerd that makes the technology happen but it's certainly on the self-driving car aspect policy is
probably the biggest challenge and I don't think there's good answers there some of those ethical questions that
come up well it's it's it feels like so we work a lot with Tesla in Drive so I'm driving a Tesla round every day and
we're playing around with it and studying human behavior inside Tesla's and it seems like there's so much hunger
amongst the media to jump on something and it feels like a very shaky PR
terrain a very shaky policy terrain we're all walking because we have no idea how how we coexist with intelligent
systems and so and and then of course government is nervous because how to regulate the shaky terrain and
everybody's nervous and excited so I'm not sure there's no same kind of
question to Jason a moment thanks a lot legs for another great session [Applause]

----------

-----

--21--

-----
Date: 2017.12.13
Link: [# Sertac Karaman (MIT) on Motion Planning in a Complex World - MIT Self-Driving Cars](https://www.youtube.com/watch?v=0fLSf3NO0-s)
Transcription:

first we have shirts and we'll give those all tomorrow and Friday if you're
here for the shirts if you here for the knowledge today our speaker is cert at
Carmen he is a professor here at MIT in the aero-astro department he builds and
studies autonomous vehicles that move on land and in the air that includes ones
that have 18 wheels and two wheels and everything in between robots that move
fast and aggressively and robots that move slowly and safely he takes both the formal optimization
based approach and the data-driven deep learning approach to robotics he's a
mentor to me and many other researchers here at MIT and Beyond and while he is
one of the leading experts in the world and building autonomous vehicles for the
nerds out there he still programs he programs on a Kinesis keyboard uses
Emacs which is how you know he's legit so please thanks please give a warm
welcome to Suresh thank ya thanks a lot
thanks a lot like sight I really had the pleasure to work with Lex for some time and it seems like this
class is him and the TAS have put together some amazing class I'm really happy to be here thank you so much for
Presentation
joining he gave me this title past present future of motion planning or something hopefully that's not quite
exactly what you were expecting so I took a whole bunch of slides from different talks and put them together and I am hoping to just kind of go
through all you know as much as I can and to tell you some of the interesting things I think in a domain that's
happening and touch upon motion planning at some point may be a starting point would be to tell you a little bit about
my background it is exactly a decade probably today that I shook John
Boehner's hand who you've met before I shook John Boehner's hand as a graduate student and joined the dark urban
challenge team it's been exactly a decade off of it we worked through it
with a number of people some of them are in the audience I can count some and the at the time that we
were doing these kind of things back in the day it was an academic project you can look at the DARPA urban challenge teams and you'll recognize they're all
University teams at least all the finishers and it came from an academic project - the thing that's going to
change the world in ten years so I hope to give you a bit of a history and then some some thoughts on that as well okay
Background
let me start with my background so I started graduate school with this we built these beasts that I'm going to
talk to you about a little bit I wonder if John there talked at all but I'll give you some details
this was our entry to the DARPA urban challenge I was a Land Rover lr3 that we made autonomous that navigated through
that course and it was one of the six finishers a number of my friends you know went out and they did their own
careers with a number of others we stayed here at MIT we built a number of other autonomous vehicles let me show
you one thing that we have done that I was kind of doing that I was the motion
planning lead for was this autonomous forklift it was a forklift that you could literally take a megaphone and
speak to you could say forklift go to X Y Z and it would go to that location here is trying to go to receiving which
happens to be an area where trucks pull up with pallets on it so that you can kind of pick this pallets up and and you
can put a mouse back so it's going to go there it has a front camera it looks through that camera it beams that camera
image to a handheld tablet device made by Nokia back in the day there was a company called Nokia they would make
these phones and handheld devices so you could see what it's seeing you would circle so you didn't have tapping back
then but you kept these pan gestures you could circle something and the thing would scan it and take a look at it you
could so you know we don't let me just
kind of go through this because it's kind of a bit slow so it'll scan through the pallet it'll pick it up but one thing I would like to show you guys is
that once that's done you can you can also talk to a tablet the tablet would
recognize your voice and then it would command the robot to do that kind of thing this was before autonomous cars
before iPhone before Alexa before Siri and things like that
so I spent like a couple years kind of doing this type of project that really shaped up my PhD thesis and later when I
started as a faculty I also worked on a number of things so let me show you one we built like autonomous golf carts and
in Singapore's and US National University Singapore campuses to go
there and do mobility on demand and so on the one thing that I ended up doing there was throughout these projects I
focus mainly on motion planning that you are expecting the one algorithm that I was working on was called rapidly
exploring random tree the idea is quite simple so you're starting in the middle of off so this is the area that you're
looking at there's that orange dot that you're starting from you want to go to the magenta goal region there's this red
obstacles you want to find a path that starts from the initial condition goes to the goal that's the very basic motion
planning problem turns out this problem is computationally pretty challenging especially as the number of dimensions
of this province is two-dimensional but if you increase the number of dimensions you can prove that any complete
algorithm meaning any algum that we towards a solution lamina exists and returns fail or otherwise will scale
exponential it's computation time so at some point you're going to run out of memory or time to do these things the
album that I was working on was called rapidly exploring random 3 the idea is simple you just land on a bunch of
samples every time you put like a random sample you connect it to it the nearest node in a tree of trajectories that
you're building and in this way you sort of rapidly explore the state space to find a whole bunch of paths some of
these paths may reach the ball so those that's the path that you pick so it's going to run in a second as you can see it's just sampling the
environment trying to build this set of trajectories that don't collide the obstacles if your trajectory Kleist but
an obstacle you just kind of delete it and you move on with other samples and then you would build this kind of a tree ok it's an algorithm that's kind of
pretty widely used and and it goes well beyond these kind of simple cases for example in our urban challenge kind of
entry we were using this algorithm so here you're seeing the algorithm in action so we're trying to park at a
location during what DARPA call the enqueue event so you can see a whole
bunch of cars that our vehicle is seeing generating this map read our obstacles black is a drivable
region it's going to try to park into it and then it's going to unpark you're seeing something hairy here so that's a
set of trajectories that are generated by the robot by the RT algorithm so it's trying to unpark now go there so as you
can see that trajectories are going back and then going towards that obstacles it's generating trajectory is picking
the best one so we've used the solder and throughout the race it worked okay so you can see the performance as it's
running so this is a media that video that's made about 30 times faster kind of showing you how the thing works when
Forklift
we switch to the forklift kind of algorithm forklift platform I started
working on this and the one thing that we realized is that you know the the forklift tries to go here to park in
front our truck and it finds this trajectory at some point it discovers there's an obstacle here and it finds
this looping trajectory and and it never gets out of that loop you would think that it's trying to minimize the path
length so you would think that it would be easier to come up with something that just kind of turns left and aligns but
it turns out that once you have that loop even if you add more samples to it you're stuck with that loop and so you
would never improve this type of trajectory so back in the day professor
said teller who passed away unfortunately a couple of years ago but he really pushed me he was telling me this doesn't work and every time it just
makes this loop right in front of the army generals who are the sponsor and it just looks ridiculous you need to fix
this kind of thing and try and find the fix for it we realized that the algorithm actually has some fundamental
flaws in it so specifically we were able to kind of write down a formal proof that the rrt
algorithm actually fails to converge the optimal solutions is this kind of something interesting so you would think that if you add more samples you will
get better and better trajectories but it turns out that the first few trajectories that you found it just
constrains you so it closes the the space that you want to search and you're
stuck with bad trajectories and this almost always happens sometimes you're lucky your bad trajectory is kind of
good enough but most of the time it's pretty bad we were able to come up with another album that we called our arty
star which just does a little bit more work but guarantees asymptotic
optimality meaning it will always converge to optimum solutions and the difference computational difference between the two
is very little if you were to run them side by side our artists our tree would look like this what it's doing is it's
it's just looking at the pads locally and it's just kind of correcting I'm locally just a little bit and that little bit correction is enough to
converge to below the optimal trajectory so that turned out to be my doctoral thesis back in 2011 and we applied to a
number of things let me show you one simulation scenario imagine a race car coming into like a turn we also turn
very quickly generates these trajectories so the right thing to do is to kind of slow down a little bit start
skinning hit one end of the road now start speeding up and go as fast as possible so that you hit the other end
the road and you complete the turn these kind of things would come out just naturally from the algorithm okay you
don't have to program you have to do these kind of things but you just run the algorithm and these are that this is the best rejected finds it would be it'd
be impossible to get something like this from an IRT we applied at a number of
other robots as well I don't know like PR to type robots or this autonomous
forklifts and got good results out of it so that kind of maybe gives you a bit of an idea of my background meaning like my
Autonomous Vehicles
graduate school experience a little bit and the PhD let me kind of tell you a bit quickly what my research group does
so I always say sort of so we do a lot of things in a fortunate and unfortunate
way so it's hard to find the focus sometimes admittedly but I usually tell people that we work on autonomous
vehicles the problem is quite interesting both at the vehicle level meaning how are you going to build these autonomous vehicles individually and
also interesting other systems that when you think about it most of the autonomous vehicle is most valuable if you put them into a system that they can
work let me give you some examples so a system the autonomous vehicles would be for example this Kibo system scenario
you know nowadays you buy something from Amazon the way it's you'd buy two books the way it's packed is that books are
brought by robots to a picker and the picker just puts them into the same box and sends it to you so this is done by
500 autonomous vehicles for example there would be a good example of a system another one is that there
ports are on Dyer in the world you know that are working just completely with autonomous vehicles and cranes if you
project a little bit forward you can think maybe you know you can have drawn delivery systems and and they maybe
don't have enough battery so they have to relay packages to one another so you need to build a system or some vehicles or if you have I don't know like
autonomous cars maybe it's best to use them in like an uber like scenario so
you can autonomous taxis that they can work together and such so let me tell you a bit more on the vehicle level
Vehicle Level Challenges
problems and the system level problems some of the crazy things that we try to do on the vehicle level we're interested
in all aspects about perception and planning usually challenges are sort of either complexity or either
computational complexity so you it's very hard just computationally so you really need to know or it's just the
system becomes very complex so we need to figure that out we're for example recently motivated by
really fast and agile kind of vehicles how we can build that like one thing that we were motivated for example is
sort of like imagine there's a drone flying and you want to you want to catch
it in the fly I wonder if this is gonna play so you know turns out that
Netherlands police is some people fly UAVs around and you somehow want to take it down it's not like you can shoot at it so
people train Eagles and things like that so we thought it would be great to actually build these types of robots that we try to in our group so you can
once you start to do these kind of things you wonder like how much I can push the boundaries of very very agile
vehicles and systems so here you're going to see a falcon diving for a prey
you're going to see a goose right at the last like a split-second so if you look at the scene from a 20 Hertz camera this
is what you would see so they are definitely much faster they do very complicated you know planning and
maneuvering to be able to do these kind of things so you know in the research group we look at a number of different perception problems where you're
High Performance Computing
multi-agency have ultra high rate cameras like for example we have drones with 200 Hertz cameras on and so you're
trying to understand the person that you're tracking its dynamics its intentions on the control level you're
trying to pull off really complicated maneuvers like the one that you've seen the race car now you want to do it in real time at like a kilohertz probably
so how can you do these types of things we use a lot high-performance computing so for example the drones that we have
actually have GPUs on them they fly GPUs they fly like teraflop computers to be able to do these kind of things we also
use them offline like the deep learning computers that you would use normally you have access to things like DG x1 and
so on that we use that to compute controllers for example here's an
Controllers
example of I don't know like one GPU drawn just kind of passing through a window this is from a long time ago but
these are the controllers that we would compute on supercomputers and we would deploy and on the perception side for
example we're looking at things where like you can use visual Arama tree you can just have a camera and just look
through the world from the camera and try to understand your own position so we have certain algorithms to pick the
features just right so that you can do these things with just like 10 features or something like that so they're just computationally very
efficient on the system's aspects of things and when you put them together yeah so this is maybe kind of yeah so
Computing Controllers
the question was what do you mean by sort of computing the controllers would you want to find the best constants so
controllers are actually pretty complicated objects so you have a drone it has suppose it has 16 there's
actually 12 degrees of freedom but suppose there's six degrees of freedom it's a six dimensional space six
dimensional space is very very large suppose you discretize every dimension
with 200 points so six dimensional position and orientation 200 points 200
to the six would be thousands of trillions if you were to write one byte
for every point in the States are you looking at the state space where every point in the state space what's the action that I'm going to do if I end up
at that position and orientation what action should I do if you use one byte to write it in the memory it would make
2.5 petabytes of this controller it's pretty large when you think about it you
don't really need it would be very surprising if that menu were really to be able to describe it like an
information theoretic terms to be able describe it it'd be very surprising if it requires thousands of trillions of
parameters I mean how complicated is it really so millions maybe but trillions seriously so what we do is to be able to
compute these things we take very simple controllers like for example zero don't do anything we compress them like isn't
data compression and then we work on the compressed versions and then that compressed version grows at to a level
that comes down to something like two megabytes that's probably essentially what you
would need rather than three terabytes for example we use kind of you know
singular value decomposition type techniques to do compression you may have done the same thing using images
for example if you compress an image JPEG you save an order-of-magnitude no
surprise right if you compress video you save to three orders of magnitude because video is three-dimensional as
you increase the dimensions there's more to compress so when you compress this way this saves ten orders of magnitude
which honestly is no surprise when you think about a delivery so those are the control like the planning and control
items that we use these viral supercomputers stole so we compute them in I don't know five minutes that gives
you a lookup table that's two megabytes you put in so that you can quickly execute it then look up tables
essentially do you want to kilo Hertz control you won't be able to compute a trajectory of technique okay that
question came in and that's the whole talk in terms of present of motion planning and I can show you some other
Agility
stuff and there's a lot to do especially in terms of agility on the systems domain as well like I don't know I
pulled up this is not the kind of stuff that only stuff that we do but I pulled up the most interesting thing I think
maybe the most crazy thing off of my hard disk imagine you have a whole bunch of vehicles coming to an intersection suppose they're fully autonomous how
would you make it so that they would pass through the intersection as fast as possible okay so if you were to really
utilize algorithms that will do that here is what I would look like so you would have vehicles coming in and you
could it looks like so you probably don't want to sit in this vehicle just sort of like just to understand the
fundamental limit sort of situation just to understand how far you can push these things you can see
looks like they're getting very lucky but really what's happening is that they're just speeding and slowing down just so little so that they could avoid
one another so you can actually sit down and do some math and try to understand you know given the dynamics like your
acceleration deceleration limits how fast you can push these things maybe it doesn't immediately apply to
self-driving cars but certainly you can use it in their houses and things like that which would actually improve operations quite a bit I wonder if any
of you have seen kiba systems where houses you look at it most of the robots are stopped they're just sitting there
yes so the question is is there anyone sort of working on robustness aspects of distributed control so that's a good
point it's it's very right we have looked at things like from the theoretical perspective it turns out
that like even in this case there's something like a critical density of
these things so below the critical density things are very simple you're going to be robust you're going to be
able to find Pat's and you're going to be able to execute above the critical density things are very hard it's very
fairchild like if something fails just kind of the whole system will crash into one another and this is no surprise
either like this is kind of the physics of many you know just like you see it everywhere I mean it's the same thing as
I don't know you heat this thing there's the critical temperature above it it looks different below it it looks like a
liquid you can use the same kind of thinking or theoretical arguments to
come up with these types of things and I know that a lot of people work on specific controllers for vehicle level
to guarantee robustness and so on probably those are the kind of things that one needs to do before implementing
these types of algorithms sort of like in the current existing like multi
vehicle setups like Kiva systems or ports and things like that we are far
away from this kind of thing the main problem some of it is control
like we don't understand the control aspects but we also don't trust our sensors and things like that so that's another big problem
so probably the more of the research is only not research for implementations on the sensor side I'd say okay so yeah so
Other projects
we have been doing a number of other projects currently as well on autonomous vehicles if you're interested in any one
of them let me know I'm not gonna show you videos but let me just kind of tell you with one slide and a few pictures
this was several slides but I felt really bad so um so we have an autonomous tricycle that may sound funny
but it's actually pretty hard to test with autonomous vehicles so we currently have five of these and
we're hoping to build 30 so that we can put them in and they're currently in a little robotic enclosed area and Taiwan
and they're just driving around collecting data so that we can for example you can pay them into deep learning algorithms we also have in a B
eyes warehouses we have these we have one of these robots it's a very house robot and supposed to be kind of like
you know I'm sure you know what we think robotics like they make this robot on it's supposed to be very easy you can
interact with so imagine a warehouse robot that way you can just talk to it you can tell it's tough to do when it
can do that you can show it you can hop on it you can do it yourself type of thing I am also a epi together with sort
of I'm working with Daniella ruse on mi t--'s effort with Stanford and Toyota to
build safer vehicles and finally I'm still API on the MIT Singapore
partnership right now from golf carts we've moved into doing these electric vehicles and and we're working on
basically integrating a lot of electric vehicles together to make them kind of work nicer we've also kind of not
looking into an autonomous kind of wheelchair that's also in that project that I didn't show him so my group works
on like a number of other projects in this domain admittedly my group is a bit more on the theory side as well so maybe
like half the group is a bit your theory oriented the other half is more experimental I usually say we have quite
a spectrum in the group so we would have mathematicians like I would have people who don't have any engineering degrees
like for example we have one post type who is a mathematician by training is a post doctoral scholar here we have
undergrad to undergraduates to graduate students whose undergraduate degrees are from mathematics on the other hand we
have sort of mechanical engineers and so on who would actually build these things throughout the group and we were funded
by a number of people throughout so okay um there was supposed to be like a quick
DARPA Urban Challenge
summary and entrance into what I was going to talk about so let me kind of tell you maybe our DARPA urban challenge
effort so that I can tell you a little bit more about how we implemented his motion planning algorithms if time
allows I could talk more broadly about motion planning algorithms but I don't think we'll get a chance to okay so I'm
going to start with this effort the darpur every challenge I'm sure many of you have heard people usually believe
that it kind of just kick-started of all these autonomous vehicles type what
answer that's been going on let me introduce to you a little bit so this is was DARPA did things called DARPA Grand
Challenge one and two I'll tell you in a second what they are but this is the third one essentially the idea is that
you would take a street-legal vehicle you would instrument it with sensors and computers and you would enter this race
to drive 60 miles in under six hours in an urban traffic right there's other vehicles driving around as well so the I
proposed is back in 2006 stated that race in November 2007 the it was pretty
hard you know you would have to do a lot of different things like u-turn skate pointers you'd have to be careful with
stop signs and so on it's pretty complicated but if you win it they would give you two million dollars there's good incentive 89 teams entered the race
we usually say it's a mighty spur Series entry but MIT is non serious entry was I
guess the team that later turned into cruise automation which GM ended up
buying for a billion dollars so this is the serious one of our entries they just want there to have fun
I think and then later they continued their interest into autonomous cars and and built cruise automation did a great
job we went after we were not directly connected to it that team our team had
Team
mainly MIT faculty postdocs and students so we had eight full-time graduate students kind of roughly I was one of
them you can see me right here I looked different back then and we had a lot of
support from Draper laboratory mainly on the sort of system integration vehicle integration and support some of them are
in the audience and we also had some vehicle engineering support from Olin
College we had a first version of the vehicle where cables were coming out and then Olin College came in and they
packaged it nicely we built a vehicle it looked like this we took a Land Rover lr3 line
Vehicle
one of the sponsors but also it was nice that the vehicle was pretty big we put
an EMC driver wire system to it so this is kind of a driver wire system for people who are disabled like for example
if you can't use your legs they would give you like a little joystick type device so that you can actuate you know
gas and brake so it came very handy we used it to make our vehicle driver wire we needed to put a lot of sensors on it
so I'm going to say as I wish this wasn't recorded but hey so I think our
situation was the following there was a lot of other teams out there and they were very experienced they had done the
other other Grand Challenges before and so on we were not as experienced I would
say that our team was talented but not experienced and we had a lot of sponsors so we had a lot of money so our strategy
turned into if it fits on the vehicle let's put it on the vehicle and we'll figure out a way to use it if we don't
use it it's dead weight we'll just kind of carry it so with that mindset we ended up with five cameras sixteen
radars twelve planar laser scanners one 3d laser scanner and one GPS a new unit
this was a lot of sensors they generated a lot of data you had to process it so
we had to buy a 40 CPU 40 gigs of ram quanta computer that normally at that
time would run on like a Google server type thing it was a server rack 10 computers essentially that we had to put
in so yeah we used to joke that this was like the fastest mobile computer on
campus or something like both in terms of speed and compute power now this requires a lot of energy so we put on an
internal amount of generator now if this generates a lot of heat so we put an air conditioner on top you can kind of see
it here so that became our vehicle one thing to note though is that we just had the number of sensors was or a number of
computers was large but but the sensor suit was very similar to the other people who have finished one important
sensor was this 3d laser scanner that I'm going to show you in a second so this is the thing that sits on top of
Laser Scanner
the vehicle looks like that Kentucky Fried Chicken type of bucket and essentially what it has is that probably
a lot of people here are familiar but it has for lasers that measure range and those
sixty-four lasers are stacked up on a vertical plane and that plane will turn and 15 Hertz so it will give you a 3d
point cloud if you drive with it in Harvard Square here is what the raw data will look like this is colored by height
you're just looking at raw data and you can you know easily pick up I don't know bus here another building may be a
person a bunch of others so that gives you a great data already like you could work with this right so be taught so
other teams thought this sensor is made by a company called Melodyne it came
pretty much just in time for the urban challenge my guess is that if you didn't
have this 3d point cloud it would be pretty hard to complete that challenge there was only one team that didn't have
it and complete it and they had a 2d laser scanner that was kind of turning like they essentially build their own Melodyne okay so we had also this sort
of 12 planar laser scanners you would need these kind of things to cover the blind spots of the vehicle the thing is
on top so you're not seeing kind of area nearby we had five from the push rooms looking down and seven on the skirts so
this is kind of what it would look like so you're seeing the curves here and you know a bunch of other things and the vehicles are when the vehicles are very
close to you can still see them we had 16 radars radars are great they can see
very far like laser scanners would see 70 meters radars would see twice as much the problem is that they have a very
narrow field of view so we needed 16 of them to cover 27 degrees around the vehicle 207 degrees around the vehicle
270 degrees so you know you can park somewhere and you can see this is meters per second you can see a whole bunch of
other vehicles kind of coming through helps quite a bit and finally we had five cameras in this configuration we
Cameras
were using cameras to actually look at lane markings I think actually you are the only finishing team that was using
cameras for any purpose of any kind the other vehicles were just kind of working with the laser scanner and we were
mainly working with laser scanner but we were picking up lane markings with this and we bought this GPS em unit there was
an expensive thing but it would give you your position you the algorithmic stack it gets pretty
Algorithmic Stack
complicated I think by the end of the race we would probably have like the
active code that was running could be order hundreds of thousands of lines of
C code so maybe like two hundred thousand good I remember the forklift and there was about half a million lines of code I
think this was a bit less we head around like a hundred processes that are running sending messages to one another
on that forty core system that you've seen so that would generate a huge
software diagram so I simplified it for you it turned into this you have some
sensors you get that data you process it through perception algorithms you generate a map of the environment close
to the robot and you have this three-tier stack you have a navigator much like your Google Maps it would
compute a map to get to your next goal which may be kilometres away and it would also give you the right the next
Waypoint that you should hit that would hopefully be within your grid map and there's a motion planner that looks at
the map sees all the obstacles and everything sees the goal point and finds that path to get to the goal point using
the RT and then once that trajectory is computer it was passed to a controller that actually steers the vehicle that
Motion Planner
way so I've already shown you how the motion planner works it just kind of computes these things so here's the goal
point our car finds the path to get there and you can run these things
together to get like a good behavior it doesn't always go well let me show you
Simulation
what doesn't work in the sky rakia
yes so we have like um honestly so so here are a couple of things so we had one thing is that we had a pretty good
simulation system going for motion planning and things like that it helped a lot like on the day of the on the sort
of like that was the day before the race my 24/7 job was to keep simulating our
algorithms like I had two computers kind of start simulation here start look at it if one fails log it and and send it
out so simulation really helped we had done some testing but I don't think we
actually I think the race itself was the farthest that we had driven without any human intervention like before then we
hadn't done that much I think this was like 60 miles if I remember this correctly we had done like a 20 mile stretch or something like that but we
hadn't done as many so admittedly we didn't have too much on the testing from
going the only reason why was because it's just we didn't have time to do this
kind of thing we so I mean we started maybe a year before that we put together
some of the infrastructure like this message sending and things like that but
the vehicle itself to test it in reality the vehicle I think the race was in November we probably got this vehicle I
mean here another vehicle before but we got this one clean I think it was April and then we put the sensors on or
something like that so really it was just the summer time that we had to test and admittedly we couldn't test much and
Draper laboratory helped out a lot with the testing if we didn't have them you probably wouldn't test any so we're
probably just kind of failed outright or something in this kind of thing testing is very important it'll be very
important for future as well simulation will be very important simulation has come a long way actually like nowadays you can I mean you guys
are working with simulator as you can see but there's a lot of other things that people are going to put out in the
next year or two and and you know like we can nowadays ran there things that you can show it to people and it's very
hard to like people don't cigarette surrendering uh always wasn't
back then I think that would be probably the right thing to do right now but back then we had this one platform that you
know you could just run the whole software stack but if you start up a simulator it would actually simulate all
the sensor data and everything if you don't start a simulator then the processes will be waiting for the data
to come in so you could put it on a real vehicle or something so back then we thought that would be the best thing to do the question was was your simulated
environment and your development environment separate or integrate they were very integrated right now I think
you would do things differently yeah there's kind of a lot to talk about
so I thought that it would be just kind of great to give you guys some ideas given the the courses on autonomous
vehicles so here's an example of a case
that we got into so what's happening here is we arrive at an intersection and
there's another car it's Cornell's car and they're just sitting right in the middle of the intersection and they
don't seem to be moving I think they've been sitting there for a few minutes before we even arrived so DARPA decided
that they should let us go and we're probably going to take over and we'll do great and it's going to be an important
moment in robotics history that for the first time you know a robot takes or another robot while the other robot is
stuck and it's going to be great so they decide to go forward with this so here's how we're seeing things from inside our
car our car is right here wants to go there or our T generates trajectories there's an object here that's the car
that we're seeing we're not seeing all of it but we're seeing in a fraction of it so we're going to play it a little
bit so you know like we were actually able to turn around it so I think I need
to stop it somewhere but now let's look at here so we seen the whole car the new
goal point is further away regenerating this trajectories looks great it turns that this car is just somehow stuck for
some reason so we wrote a paper together with them it's not I'm clear to them either but my understanding is that they
think that the obstacle is on top of the car and the way the algorithm is written is it just kind of generates a
trajectory and asks if the trajectory is collision-free or not right the collision checker doesn't say this part
of the trajectory is in collision it's just every time it passes a trajectory because the route is in collision it
just says you know there's nothing that they have another little piece where it just updates the map every time there's
no information from the sensors if there's no new information there's no need to update so they ended up getting
stuck on this obstacle and they're not refreshing their map because nothing is moving up until we move right next to
them they refresh again and they say oh I'm actually not sitting around obstacle that was an error so next time the path
comes going forward it says this is a great path go forward with it that happens right when we're passing so
if you look at this blob right now as I play it the blob starts to move so they
are going in a direction that we are going a quick thing will happen so if our car if our car at some point
realizes that there's no paths a collision is imminent and there's nothing to do about it it generates
shows that wide circle around it and that basically means that we are headed
to a crash there's nothing we can do about it we're just going to slam the brakes and hope not to bad things happen
so it starts to do that I think at this time this camera is more fun to look at
you can kind of take a look at it and sort of see what happened and so this kind of like collision happens we
collide with the car DARPA what they did is that they actually pulled the Cornell
DARPA
car back they started us we finished they finished as well so both of the teams finished well you can see some of
the things that are a little bit hard for example if you yourself were deriving our intersection that there's a car that's sitting there you probably
would stop your car take out go and ask if there's anything wrong even if you don't do that suppose you're not very
decent of a human being you don't decide not to do that you would still steer away from the car you probably wouldn't
get as close to this car as we do so there are some problems that are at the inference level that we do without even
thinking and it's actually kind of hard things for these types of cars to do
especially if you're going fast you're in a complicated environment you're not expecting things and you might collide
into things we do different kinds of inference that we can't do a name but you know you look at the way a person
walks on the sidewalk and you can say well this person is kind of dangerous or maybe we will walk into the street or
not you know you make that decision and it's actually pretty complicated thing for a robot to do okay so you know this
Race Results
is kind of like the results of the race I'm not gonna go too much into it basically the idea is that 89 people
started six finished we were one of the finishers CMU came first so they got the 2 million dollar check I believe
Stanford came second they got a 1 million dollar check Virginia Tech came third they got half a million we came
fourth we didn't get anybody but you know we got a lot of experience it was great to be a part of it I think one
Google Color
note is that the Google car that you may have heard a lot was essentially sort of like a spinoff from this race so if you
look at the Google color you will see that the sensing package is very similar it's very laser scanner oriented has a
couple of radars on it that it could utilize and is working somewhat with the cameras but not so much essentially
Google engineered this thing that we built or all the other teams built independently they engineered it for ten
years and that's the kind of thing that they utilize nowadays there's also like this whole Tesla brand of camera based
cars or deep learning and so on that's coming in and just very recently back ten years ago you know we knew about
people learning and so on but it just it just didn't work the moment somebody figured out doing it on a GPU it started
working pretty well okay so there's a lot I can tell you about path planning
Challenges
but I think here is kind of maybe what I should do if you if you do not mind
rather than I'm telling you about our RTS and making this into a lecture that I'm not sure if you're going to like it
let me talk maybe a little bit more about south driving vehicles and I think that's something that you might enjoy
better
so the question is sort of building it from scratch what was the biggest challenge so I'm going to say admittedly
I was a junior student back time so my challenge was to get these controllers and some parts of the rrt working and I
had simulation systems and things like that and life was good for me I would think that I mean we ended up building
pretty complicated hardware so that was one of the challenges and that probably
all in college Draper you know they did all of that that was great the other challenge that we had is that
nowadays there's like maybe you guys use it like robot operating system and so on that infrastructure software we had none
of that so we ended up building our own I don't know if anybody uses but there is this thing called lightweight
communications and marshalling LCN so that ended up being built for this and it just kind of got spun out there was
another big challenge that we actually faced so LCM nowadays is utilized throughout the industry like for example
for autonomous cars we'll use it Toyota will use it no Donna mean uses it
so it ended up coming out of this this challenge and it was probably like the
first you know I would say the first six seven months was devoted to it and and
for necessity I mean we just we wanted to do other things but we just couldn't because you needed something like this there was another big challenge I would
say testing was a big challenge things like that
pretty collaborative as far as where I was because probably papers published
paper with other team things like that and I've seen like my aren't people
founded becoming much more isolated like
yeah wrong I guess it's good and bad it's kinda hard to assess so competition
is always good so the the the question was that you know back in the day we
were really collaborative like it's very interesting that we actually wrote a paper with Cornell about our collision
just to teach the whole community why these kind of things happen but nowadays like everybody is just kind of doing
their own thing and there's no kind of going out so there's there's a quick question is a quick answer for that and
there's a kind of broader answer so the quick answer is that yeah I mean it became important there's a lot of you
know sort of people invested a lot of money and they are expecting returns and things like that and that affects the
environment that definitely drove it I think we're still you know trying to work on it in academia and trying to
publish papers but a lot of people are you know worried about competing with these huge companies and things like
that which I think it's not a big war because there's a lot to do still sobani when you look at the industry
there's little competition but that for some reason the broader answer is that that became a norm so back 50 years ago
you would look at the top company of the day this is like starting from a century ago like with bowel for example they
would form labs and they will publish in there to science and things like that will be very open and novel day is the
big companies of the day they kind of rather prefer secretive labs and things like that so that I think Microsoft was
the last big company of the day to do that nowadays googles and apples and
things like that they don't do that anymore there's a bit of that as well good or bad but it became that way and sometimes
competition is good honestly it's a good thing that people feel like you don't
know what the others are doing and you want to compete so that makes you better
and better even though the others maybe I don't know ok any other questions yeah
maybe a vision only a challenge or or something that races
I'm not sure I don't think it's purely an industry problem because it's it's still kind of it's it's quite
complicated honestly so there may be things that people can do but i i i am
wondering if DARPA would be interested in doing a challenge so let's set DARPA aside differently and research otherwise
like when you think about DARPA dark ways a defense agency and when they talk
about the challenge they had honestly defense problems in mind so for example they didn't allow you to go around and
drive in the area with your sensors the idea was that they would give you a map of the environment 24 hours in advance
and then five minutes before they would give you a mission like hit this waypoint hit that way point and so on so
that's a military setting they were really it really the whole thing started with the US Congress mandate to get you
know one third of combat vehicles autonomous by 2015 which didn't happen but it was a war military setting so
DARPA is usually sort of that minded and they did the DARPA Robotics
so the idea is to build a quadcopter that can fly here 20 meters a second
like 40 miles an hour in indoor environments type of thing I think they'll do that but there may be other
things like there may be other you know people kind of coming in pushing the boundary of research like something for
example just with cameras would be very interesting and I think we are just in it may be a couple years away from doing
that very well and probably neat learning would be a lot of it ok so I
don't know much time and I don't want to hold you here but sort of you know let
me tell you a few things about autonomous cars in general and let's see if we can you know in like 10 minutes we
can fit something interesting transportations is a very interesting thing it actually defines how you live
Transportation
quite a bit so if you look at for example the kind of cities that you know you know you may be living in today they
look like this and they are produced thanks to one invention that was the affordable car which was about as
you go if you look at it you know throughout the last century like in 1950s cars were big and and you would
Transportation in 1950s
find you know that everywhere these kind of subways were being constructed for the first time the reason was cities were dirty they
were deemed diseased prone so now he had the car you could move way into a better living lifestyle and it would improve it
and that was the 20th century invention that you had it also changed the cities
quite a bit I mean like for example this is Boston's sort of central artery that was built in you know 50s that around
that time to kind of service the colors coming in and out of the city the cars
Suburban Sprawl
kind of generated this kind of thing you know in some places at the extreme like if you go to places like Los Angeles in
the United States you will see the suburban sprawl it's very different in other places so places that didn't have
the time to expand that didn't have the resources to expand or just didn't have the place to expand it caused many
problems like here's the suburbia and in Mexico City you can see the dirt that it generates in the distance even if you're
Rich Countries
rich it doesn't really matter you know even in in rich countries this quick expansion it just doesn't work and it
creates if anything just ugly environments and in some places it
Ugly environments
creates like you need to be dense and you need to be big and so you have the cars but you just have to build you know
big buildings that you cannot even serve with cars so you generates these type of things where you know like there's a I think
it's probably my just I was just gonna say only let the congestion and pollution in the rest of the world but it generated these kind of things where
China traffic jam
I don't know if you heard there was a traffic jam in China it lasted like nine days and it was a hundred miles long so
it generated this kind of thing is just a quick introduction of the affordable car sort of what it did to the
environment in the cities there so pollution is one problem and so on but
Pollution
if you look through it it's actually pollution and energy consumption wise a lot of it comes from the cars especially
inside the cities an interesting point is that if you look through the cars the
cars are actually pretty inefficient the way they sort of sit currently
if you look true for example BMWs over the years you would see that they get heavier and they get faster this is very
correlated if you get faster you have to become heavier because you have to pass crash tests and things like that so you
know you're you know just to be faster so in order to pasture has two crash
tests you build structure and things like that and that makes the vehicle heavier ultimately so like a BMW that
you would buy in the 70s would weigh something like you know twenty five hundred pounds nowadays it's like you
know like four thousand pounds roughly so it would you know if you look at the
average passenger weight that it's carrying it's about 25 times the weight of the passengers and the size as well
Average Passenger Weight
you know it's about ten times the size of the passengers that it carries in terms of parking spots if you look
Parking Spots
through the cities there are places in the you know usually what we have in the United States is that for every car we
have two parking spots so roughly that's the number in some places parking spots take up like half to sea
so for example in a way on average it's about one third so you might ask the
City Environment
question like this is the kind of thing this is the kind of environment that city is created and do you really want
to live in this type of environment and it's to kind of give you the idea I mean if you if you walk out a lot of the
infrastructure a lot of the things that you see are made by cars like for instance and it really kind of
interferes with your thinking as well so for instance we never walk on the street nowadays like nobody jaywalks
streets are for cars my cars go on the streets and we go on the sidewalk it wasn't like that a hundred years ago you
could walk on the streets however you wanted cars came in and they took it over and so they changed the urban
landscape quite a bit the point is that it seems like there's a there's an
Opportunities
opportunity today to actually kind of use 21st century technologies this could
be robotics but a number of other things like online services new business models and things like that and so on maybe
high-performance computer whatever and to kind of service the needs of people in the cities
I'm not sure I don't think that the kind of the service aspects of it goes away so you know you will need to prop my
guess what would happen is that I think people could be more mobile so I think they want to be more mobile but they're
just not if it was very accessible very easy I think they would be so there be
increase in being mobile but at the same time that's a you know resources are
spent on it you need to pay for it somehow so you would still generate
economic activity off of that in fact you would probably generate more economic activity for example if the
moment you change people's behavior this the way that you generate like a new economic activity so if there's a way
for example transportation is more available more affordable and it changes their behavior it makes you more mobile
like for example you're fine with having a class here and then 20 minutes later
having a class at Harvard nobody would do that nowadays but if it was that easy to get there you would probably do it
and so that would make you more mobile and that's the way ultimately would generate more economic activity rather
than buying cars we the service is still there you need to pay for it somehow any
other questions yes what the point is that you know you can use these type of technologies to do for example like
either maybe like mobility on the man you know whenever you need to be mobile you can be mobile or deliver things and so on and I think that let me just kind
Mobility
of I was gonna tell you a bit the
history but I think that I'm just going to pass it in and tell you a few things so autonomous vehicles are sort of one
thing that you can utilize and you can actually do these types of things I think you can make this even better like
Integration
for example you can integrate a few things into it one thing you can integrate sharing like so you can make
Internet user type scenario you can use autonomy as well and finally like
electrification especially if you're going a little bit slower so you don't have to pass - crash crash tests and
things like that you could really reduce the cost of transportation like to the point where you could imagine things
like like you could go for anywhere to anywhere else for $0.99 in Boston with like five-minute wait time
if you want to share your ride it could be 50 cents if you want to admit to like
one stop a lot of us do one stop you know you can take a subway and then take a bus one stop makes your transportation
much cheaper if you were to take an airplane suppose if you wanted to take one stop you could pay 30 cents and you
could go anywhere to anywhere else in Boston I think there's a good opportunity to kind of you know push for
things and utilize technology to bring the cost of transportation to a point or
availability of transportation to a point to like really just change a lot of things it's not very easy the way I
Speed vs Complexity
usually look at the technological landscape is that you can imagine sort of speed versus complexity speed is the
speed of the vehicle that's being involved it's involved in this and and maybe complexity is the complexity of
the environment that you're dealing with like you can have high speed low complexity environments like highways
Complex Environments
they're actually easy to work with we might actually conquer them like in the next I don't know three years or
something like that another thing would be like for example parks or university campus or something much slower but much
more complex like people walking around and and things like that fully autonomous driving is probably pretty
far but there is some opportunity to do some interesting things elsewhere very quickly the one of the problems is that
this is not just an technology problem to be honest as you have seen earlier
there's a lot of enrollment in like for example architecture like how do you actually utilize the city the best and
so on but one of the biggest problems ends up being this law and insurance and
regulations aspects there are good or bad things like for example sometimes you allow by the law to be able to do
certain things but then is it really like is it a safety hazard is it of an
ethical the kind of a lot of people to just kind of test stuff around so it's a bit of a question like whether or not
this is the kind of thing I think sort of going forward if I could say one thing to you guys is that this is this
Is it ethical
is just not like a just a problem in sort of technology but it's also like a problem
and technology sort of society policy architecture law insurance and business
as well like you may need new business models and so on so I personally think that the it's it's
right out there but I think that we still need just a bit of more like a better thinking to do to be able to
attack this problem and to really kind of enable it so that you can kind of do
good and interesting things with it I might I could close with a couple things one thing is that I was going to talk a
Sertacs new company Optimist
little bit more but maybe I'll just kind of pass with one slide I am a part of a
new company I've got a few companies outside so this is the latest thing that we've been working on it's called optimist right it's working
on autonomous vehicles it's currently in stock no it just raised like a little more than five million dollars in seed
round to kind of start to see operations I am joined the founding team includes a
number of sort of friends Ryan chin for example I don't know if you guys have heard of the MIT city car the Spalding
car that was his doctoral thesis he's been an MIT PI for a while he joined
Optimus Albert Wong is a friend of mine who we worked together in the urban challenge he was later a sort of a chief
architect Software Architect at rethink robotics then the lead perception engineer at Google X were project wing and then he
joined Optimus ramiro Romania is a sort of a designer so he's a layup fellow
from the Harvard Graduate School of Design as his fellowship they would invite eight mid-career you know best
mid-career designers so he has that kind of a background he also built Kito's subway system raised two billion dollars
for it Jaynee Larios Berlin is also a joint MBA and an urban planning master's
from MIT she was the managing director of university campus operations of Zipcar
so we kind of started this kind of thing and thinking about these types of problems if you're interested send me an
email would be happy to talk to you more about it I also will tell you one more thing I am advising a team
Formula SAE
it's trying to do Formula SAE autonomously they're doing it for the first time this year they're actually
using a lot of deep learning type algorithm so we're not I was telling them I'm not really sure if we're deep
learning you're gonna write a little wave in it because people might come with all the heuristics and things like that but I think you know you may VIN it
at people's hearts just say that you're on the algorithm is deep learning or something like that so they're doing
FSA
that if you're interested please send an email to autonomous dash F Formula SAE FSA that I might tell you and they're
working on a number of things you're more than welcome to join them thank you so much that's all I have it's exactly
one hour yeah I'm here yep so maybe a
few questions if anybody has questions so that a lot of this classes of bought deep learning and in terms of autonomous
vehicles deep learning is mostly focused on the vision sensor or cameras so how
far away away from a car that safely navigates the streets of Boston without
lidar and without any mapping so purely on the sensors using the sensors and
perception so it's a bit of a guess game to be honest I it wasn't some what the
Computer Vision
slides that I kind of passed through but I am a big believer in computer vision
and I I do not think it's too far away it just ends up being a bit of a guest
game but it's not like I don't know how many of you have worked with cameras but
deep learning is one approach you can also use geometric approaches like you can use a single camera and the motion
of the car to build a 3d map of an environment these are not too far away cameras are actually pretty good sensors
the only problem with cameras is that they is just a lot of data and there's little information and if you need to
fish it out so you need computers to accompany it it seems like the computers are coming out so it's still hard to
know but I like I would be surprised if in ten years you can't build a car that just has a bunch of cameras and
navigates with cameras period they will be very surprising to me it'd be also surprising if it happens
next year like some people are saying but in between these I you know I would
think you would be able to so once you like what I would suggest is if any of you is working with cameras
I would suggest deep learning is an excellent technique so whilst you I think you're kind of using it here and
I'm sure you're being surprised as as as it gives you the kind of information that you need try out model-based
techniques as well they're also coming along pretty well so probably a solution that just integrates them as best as
possible would be viable I would guess in three to five years I'd be surprised otherwise it's optimistic
okay anybody have questions this is the question was an autonomous intersections
what role the communication plays if you wanted to do crazy things that like I've shown you you need to make sure
everything communicates but everything else that would break pretty badly if you don't do it I would actually imagine
that like one interesting things to quickly do would be to have cars communicate with each other to do some
interesting things like not just maybe intersection but lane following and things like that like there are a few
things that you may see pretty quickly with autonomous cars back curriculum in 25 years so this could be either read to
be related so communicate with other vehicles vehicle to infrastructure related I mean you could like the deep
learning and things like that you could put up a camera on our infrastructure and people could tune into it the
biggest problems are cybersecurity to be honest to deploy these things and on the autonomous vehicles and you could see
things like maybe not with the communication you could see either sharing like you know you have a button you press your timeshare or you can have
sharing with you know like for example you can use autonomy technology for
safety so that's a different type of sharing or you can find autonomous vehicles in isolated environments so
this stuff that you can do with communication I think you can quickly see Lane following and maybe at
intersections but things like that and with autonomy there are certain things that we might see but they don't involve communication at all
all right let's get start to ask one more time thank you

----------

-----

--20--

-----
Date: 2017.12.06
Link: [# Chris Gerdes (Stanford) on Technology, Policy and Vehicle Safety - MIT Self-Driving Cars](https://www.youtube.com/watch?v=LDprUza7yT4)
Transcription:

so today we have Chris Gertie's with us he's a professor at Stanford University
where he studies how to build autonomous cars that perform at or beyond human
levels both on the racetrack and on public roads so that includes a race car
that goes 120 miles an hour autonomously on the racetrack this is awesome he
spent most of 2016 as the chief innovation officer at the United States
Department of Transportation and was part of the team that developed a federal automated vehicle policy so he
deeply cares about the role that artificial intelligence plays in our society both from the technology side
and the policy perspective so he is now I guess you could say a policy wonk
world renowned engineer and I think Oh was a car guy yes
so he told me that he did a Q&A session with a group of three graders through great third graders last week and he
answered all of their heart hitting questions so I encourage you guys to continue on that thread and ask Chris
questions after his talk so please give a warm welcome to Chris
great Lex thanks for that great introduction and thanks for having me here to talk to everybody today so this
Chris Gerdes
is this is sort of my first week back in a civilian role I wrapped up at USDOT
last week so I'm gonna no longer speaking and officially representing the department although some of the slides
are very similar to things that I used to speak and represent the department so I think as of Friday this was still
fairly current but I am sort of talking in my own capacity here so I wanted to
talk about both the technology side and the policy side of automated vehicles and in particular how some of the
techniques that you're learning in this class around deep learning and neural networks really place some challenges on
regulators and policymakers attempting to ensure vehicle safety so just a bit
about some of the the cars in my background I am a car guy and I've gotten a chance to work on a lot of cool
ones I actually have been working in automated vehicles since 1992 in the Lincoln Town Cars in the upper corner
are part of an automated highway project I worked on as a PhD student at Berkeley I then went to freight lidar heavy
trucks in daimler-benz and worked with suspensions on heavy trucks before coming to Stanford and doing things like
building p1 in the upper right corner there that's an entirely student built
electric steer by wire drive by wire vehicle we've also instrumented vintage racecars
electrified a DeLorean which I'll show a little bit later and worked as Lex
mentioned with Shelley which is our self-driving Audi TT which is an automated race car in addition to the
Stanford work I was a co-founder of peloton technology which is a truck platooning firm looking at bringing
platooning technology so vehicle to vehicle communication which allows for shorter following distance out on the
highway so these are some of the things i've had a chance to work with to give you a little bit of a sense this is
shelley going around the racetrack at Thunderhill she can actually go up to about 120 miles an hour or so on that
track it's really just limited by the length of the straight it's kind of fun to watch from the outside a little
disconcerting occasionally as you see there's nobody in the car although from inside it actually looks all
pretty chill so Shelly we've been working with her for a while out on the
track she's able to get performance now which exceeds the capability of anybody on the
development team I'll even many of us are amateur racers in fact actually most of my PhD students
have their novice racing license we make sure that they get that license before going out on the track and testing so
Shelly could be in anybody in the research group she actually can beat the president of the track david Vaadin now
and we've had the opportunity to work recently with Junior Hildebrandt the IndyCar driver who finished six this
last year in the Indy 500 he's faster but but he's actually only about a
second or so faster on a minute and 25 second lap so we're approaching his
performance and he's actually helping us get there now the interesting thing about this is that we've approached this
problem really from one of physics force equals mass times acceleration so the car is really out there calculating what
it needs to do to break down into the next corner how much grip that it thinks it has and so forth as it's going around the track
it's not actually a learning approach at its core although we've added on top a
number of algorithms for learning because it turns out that the difference between the cars performance and the
human performance really getting that last little bit of capability out of the tires
humans drive instinctively in a way the best of humans at any rate drive instinctively in a way which is
constantly pushing to the limits of the cars capability and so if you sort of prejudge what those limits are you're
not going to be quite as fast and so that's one of the things we've actually been working with learning algorithms on
is to try to figure out well how much friction do I have in this particular corner and how is that changing as the
tires warm up and as a track warms up from the course of the morning till the afternoon these are the things that we
need to be fast on the racetrack but they're also the things that you need to take into account to be safe in the real
world because what we're trying to do with this project is understand how the car can drive at the maximum capability
of the limits of the friction between the tire and the road now racecar drivers do that to be fast as they say
in racing if you want to finish first you have to finish so it's important
that they actually be fast but also accident free so we're trying to learn the same things so that on the road when
you may have unknown conditions ahead of you the car can make the safest maneuver that's using all the friction in between
the tire in the road to avoid ultimately any accident that the car would be physically capable of avoiding that's
our goal with that so we've had a lot of fun with Shelley we've gotten to drive the car up Pikes Peak in the Bonneville
Salt Flats actually Shelley appeared in an Audi commercial with Zach Quinto and
Leonard Nimoy and so at the end of the commercial they both look at each other and declare it fascinating so if you're
as big of a science fiction fan as I am you realize that once your work has been declared fascinating by two Spock's
there's nowhere to go so I had to take a stint and try something different in
government and so I spent the last year as the first chief innovation officer at the US Department of Transportation
which I think honestly was the coolest gig in the federal government because I really didn't have any assigned
day-to-day responsibilities but I got to kind of dive in and help with all manner of really cool projects including the
development of the first federal automated vehicle policy so it's a really great opportunity to sort of see
things from a different perspective and so what I wanted to do was you know kind of coming into this from an engineer give you a perspective of what is it
like from somebody looking at the regulatory side on vehicle safety and how are they thinking about the technologies you're developing and where
does that actually leave some opportunities for engineers to make some big contributions to society so let's
What is vehicle safety
start with with what vehicle safety is like today so today we have a system of
federal motor vehicle safety standards so these are rules they're minimum performance requirements and each of
them must have associated with it an objective test so you can tell does the vehicle meet this requirement or does it
not meet this requirement now interestingly there is no federal agency that is testing vehicles before they are
sold we rely in this country on a system of manufacturers self certification so the
government puts these rules out there and manufacturers go we got this we can meet this and then they sell
certify and put the vehicles out on the market the National Highway Traffic Safety Administration can then purchase
vehicles and test them and make sure that they comply but we rely on manufacturers self-certification this is
a different system than in most of the rest of the world which actually has pre market certification where before you
can sell it the government agency has to say yes we've checked it and it meets all the requirements Aviation in this
country for instance has that aircraft require certification before they can be sold cars do not now where did that
system come from so a little quick history lesson in 1965 Ralph Nader released a book entitled unsafe at any
speed and this is often thought of as a book about the Corvair it's it's not the
Corvair featured prominently in there as an example of a design that Nader considered to be unsafe what was very
interesting about this this book was that he was actually advocating for things like airbags and anti-lock brakes
back in 1965 these technologies didn't come along until much later his argument
was that the auto industry had failed it wasn't a failure of engineering but it
was a failure of imagination and if you're interested in vehicle safety I would really recommend you read this book because it's fascinating they have
quotes from people in the 1960s basically saying that we believe that any collision more than about forty or
forty-five miles an hour is not survivable therefore there's no reason for seatbelts there's no reason for
collapsible steering wheels in fact there's a quote from somebody who made great advances in Road Safety saying I
can't conceive of what help a seatbelt would give you beyond like firmly bracing yourself with your hands those
of you who have studied physics know that's kind of patently ridiculous but there was a common feeling that there
was no sense of doing anything about vehicle crash worthiness because once you got above a certain speed it was
inherently unsurvivable and I think it's interesting to look at that today because if we were to be in a collision
I think if any of us were to be in a collision in around about 40 miles an hour in a in a modern automobile we'd
probably expect to walk away you know we wouldn't really be thinking about our survival and so what this did is it led
to a lot of public outcry and ultimately the National traffic and Motor Vehicle Safety Act in 1966 which established
nitzan established this set of federal motor vehicle safety standards now the process to get a new standard
made which is a rulemaking process in government is very time-consuming optimistically about the minimum time it
can possibly take is two years realistically it's more like seven and
so if you think about going through this process that's really problematic I mean
think about what we were talking about with automated vehicles two years ago or seven years ago I think about trying to
start seven years ago and make laws they're gonna determine how those vehicles operate on the road today it's
crazy right there's really no way to do that and the other thing is is that if you think about it our system evolved
from really this sense of failure of imagination that the government needs to say hey industry do this stop slacking
off these are the requirements get there but I think it's hard to argue today with all the advances in automation that
there is any failure of imagination on the part of industry people are coming up with all sorts of ideas and concepts
for new transportation and automation tech companies startup companies large OEMs there's all sorts of concepts being
tested out on the road it's hard to argue that there's still any lack of imagination now the question is are
things like this legal it's an interesting question right can I actually legally do this well from the
federal level there's an interesting report that came out about ten months ago from the folks across the street at
Federal motor vehicle safety standards
Volpe who did scan and said well what are the things that might prevent you based on the current federal motor
vehicle safety standards from putting an automated vehicle out on the road and the answer was honestly not much if you
have a vehicle if you start and you automate a vehicle that is currently meeting all the standards because there
are no standards that relate specifically to automation you can certify your vehicle as meeting the
federal motor vehicle safety standards therefore there's nothing at the federal level that prevents in general an
automated vehicle from being put on the road so it makes sense so if there isn't a safety standard
that you have to meet then you can put a vehicle out on the road that meets all the existing ones and does something new
and there's no federal barrier to that now there are a couple of exceptions there were a few points in there that
referenced a driver and in fact Nitsa gave a an interpretation of the rule
which is one of the things that they can do is to say well we're going to give an interpretation it's not making a new rule but basically interpreting the ones
that we have and they said that actually these references to the driver could in fact refer to the AI system and so that
actually is now a policy statement from from the department that many of the
references to driver in the federal motor vehicle safety standards can be replaced with your self-driving aai
system and the rules applied accordingly so in fact there's very little that prevents you from putting a vehicle out
on the road if it meets the current standards so if it's a modern production car automated federal motor vehicle
safety standards don't stop that now a lot of the designs that I showed though things that wouldn't have a steering wheel or other things are actually not
compliant because there are requirements that you have a steering wheel that you have pedals again these are best
practices that evolved in the days of course when people were not thinking of cars that could drive themselves and so
these things would require an exemption by Nitsa a process of saying that okay
this vehicle is allowed on the road even though it doesn't meet the current standards because it meets some equivalent and studying that equivalent
can be a bit of a challenge okay so the question then is well alright if the federal government is responsible and
that's by the traffic safety act is responsible for safety on the roads but it can't prevent people from putting
anything out what do you do right one approach is to say well let's get some federal motor vehicle safety standards
out there but as we already said that's probably about a seven year process and if you were to start setting in best
practices now what would that look like so we've got this challenge we want to encourage this technology to come out
Setting in best practices
onto the roads and be tested because that's the way you're gonna learn to get
the real-world data to get the real-world experience at the same time the federal government is responsible for
safety on the nation's roads it can recall things that don't work so if you do put your automated system out on the
highway and it's deemed to present an unreasonable risk to safety even if you're an aftermarket
manufacturer the government can tell you to take that off the road but the question is how can you do better how
can you be proactive to try to have a discussion here so we know standards are
maybe not the best way of doing that because they're too slow we'd like to make sure the public is protected but
this technology gets tested and so the approach taken to sort of provide some encouragement for this innovation while
at the same time looking at safety was the federal automated vehicle policy which rolled out in September so this
Federal Automated Vehicle Policy
was an attempt to really say okay let's put out a different framework from the
federal motor vehicle safety standards let's actually put out a system of voluntary guidance so what Anisa is
doing is to ask manufacturers to voluntarily follow certain guidance and
submit to the agency a letter that they have followed a certain safety assessment now the interesting thing is
is that the way that this is set up is not to tell manufacturers how to do something but really to say these are
the things that we want you to address and we want you to come to us to explain how you've addressed them with the idea
that from this best practices will emerge we'll be able to figure out in the future what really is the best way
of ensuring some of these safety items so this rolled out in September we've
got the BMI t car here on the side so you see you've got the Massachusetts
license plate so thanks to Brian for for bringing that if you do put gaudy stickers on your card then you get
closer to the center so that's something to consider for for for future future reference but this was was rolled out in
Washington Washington DC by the secretary and consists largely of of
Safety Assessment
multiple parts but I think the most relevant to vehicle design is this 15 point safety assessment so these are the
15 points that that are assessed and I'd like to kind of talk about a few of these in some more detail and it starts
with this concept of an operational design domain and minimal risk or fallback conditions and what
that means is instead of trying to put a taxonomy on here and say well your
automation system could be an adaptive cruise control that works on the highway or it could be fully self-driving or it
Operational Design Domain
might be something that operates a low-speed shuttle the guidance asked the manufacturers to define this and the
definition is known as operational design domain so in other words you tell us where your system is supposed to work
is it supposed to work on the highway is it supposed to work in restricted areas
can it work in all-weather or is this sort of something that operates only in
daylight hours in the sunshine in this area of South Florida all of those are fine but the it's incumbent upon the
manufacturer developer to define the operational design domain and then once you've defined where the system operates
you need to define how you make sure that it is only operating in those conditions how do you make sure the
system stays there and what's your fallback in case it doesn't and that fallback can be different
obviously if this is a car which is normally human driven as you see here from the volvo drive me experiment it
might be reasonable to say we're gonna ask the human driver to retake control whereas clearly if you're going to
enable blind passengers or you are going to have a vehicle that has no steering
wheel you need a different fallback system and so within the the guidance it
really allows manufacturers to have a lot of different concepts of what they want their automation to be so long as
they can define where it works what the fallback is in the event that it doesn't work and how you have educated the
consumer about what your technology does and what it doesn't do so that people
have a good understanding of the system performance a few things if we go down you see also validation methods and
ethical considerations are our aspects that are brought up here as well and so validation methods are really
Validation Methods
interesting as it applies to AI so really the idea is that there's lots of
different ways that you might tell an automated vehicle you might go out on the test track and run it through a
series of standard maneuvers you may develop a certain number of miles of experience driving in real-world traffic
and figure out how does the vehicle behave in a limited environment there's questions about a test track obviously
because you don't have the sort of unknowns that can happen in the real-world environment but if you test
in one real-world environment you also have a question of is this transferable information so if I've driven a certain
number of miles in Mountain View California does that tell me anything about how the vehicle is likely to
behave in Cambridge Massachusetts maybe maybe not it's a little bit hard to extrapolate
sometimes and then finally there's also the idea of simulation and analysis so if I can record these situations if I
can actually create a virtual environment of the sorts of things that I see on the road maybe I can actually
run the vehicle through many many of these scenarios perturbed in some way and actually test the system much more
robustly in simulation than I can ever actually do out on the road so the guidance is actually neutral on which of
these techniques manufacturers take and allow manufacturers to approach it in different ways and I think you know
based upon conversations when you think about the way customers are companies develop this they do take all these
different approaches a company like Tesla for instance which is recording all the data streams from all their
vehicles basically is able to run ideas or technologies silently in their
vehicle they can actually test systems out get real-world data and then decide whether or not to make that system
active companies that don't have that access to data really can't use that sort of development method and may rely
much more heavily on simulation or test track experience so the guidance really doesn't have this particular blend of
this and in fact it does envision that you might have over-the-air software updates in the in the future so it is
interesting though to think about whether you have data driven approaches things like artificial neural networks
or whether you actually start to program in hard and fast rules because as you
start to think about requirements on a system how do you actually set require on a system which has learned its
behavior and you don't necessarily know what the internal workings or our algorithms look like there's another one
Ethical Considerations
that that comes up which is the ethical consideration so I'm gonna pick on MIT for a moment here so this is an area
that I actually did a lot of work on with Stanford together with with some philosophers who join joined our group
and so when people hear ethical considerations in automated vehicles it often conjures up the trolley car
problem and and so this sort of classic formulation here about the fact that you
have a self-driving car which is heading towards a group of 10 people and it can either plow in and kill those 10 people
or it can divert and kill the driver what do you do and these are classic questions in philosophy you actually
look in fact at at the trolley car problem which is I have a runaway trolley car and I need to either divert
it to another track where it will kill somebody who's wandering across that track or the five people on the trolley car are killed what do I do well in fact
it's this article points out it's like you know before they the automated vehicles can become widespread car
makers must solve an impossible ethical dilemma of algorithmic morality so if all this wasn't hard enough I mean your
understanding how tough the technology is to actually program this stuff and then you have to get the regulations
right and now we actually have to solve impossible philosophical questions well
I don't think that's actually true and I think you know it's good for engineers to work with philosophers but not to be
so literal about this this is a question that philosophers can ask but engineers
might ask a number of different questions like who's responsible for the brakes on this trolley why wasn't there
a backup system I mean why am I headed into a group of 10 people without any
capability to stop so an engineer would in fact have to answer this question but
might approach it much differently so if I look at the trolley car problem I might say ok let's see my options are
I've got a trolley car which is out of control first of all I'd like to have an emergency braking system let's make sure
that I have that well there's a chance that that could break as well so my emergency if my base breaking system
goes and my emergency braking system goes my next option would be to divert it to this sidetrack well knowing that
that's my option I should probably put up a fence with a warning sign that says do not cross runaway trolley track okay
now let's say that I've done all of that the brakes fail the big emergency brakes
fail I have to divert the trolley and somebody has ignored my sign and crossed over the fence and now he's hit by the
trolley do I feel a little differently about this whole scenario and then I did at the beginning of just trying to
decide who lived and who died the solution was made but by thinking of it as an engineer trying to reduce risk and
not by thinking of levels of morality and who deserves to live or die and so I
think this is a very important issue and the reason it's in the guidance is not to get basically have everybody solve
trolley car problems but to try to think about these larger issues and so I think
ethics is is not just about these sorts of situations which actually will be in automated vehicles I think addressed
much more by engineering principles than by trying to figure out from philosophical merits who deserves to
live and die but there's broader issues here just any time that you have concern for human safety how close do I get to
pedestrians how close do I get to bicycles how much care should I put in
to other people in the environment that's very much an ethical question and
it's an ethical question that manufacturers are actually already addressing today if you look at the
automatic emergency braking systems that most manufacturers are putting on their vehicles they will actually use a
different algorithm depending upon whether that obstacle in front of it is a vehicle or a human so they're already
detecting and making a decision that the impact of this vehicle with the human could be far worse than the impact in
this vehicle with a vehicle and so they're choosing to brake a little bit more heavily in that case that's
actually where these ethical considerations come in and the idea of the guidance is to begin to share and have a discussion openly about how
manufacturers are approaching this with the idea of getting to a best practice where not only the people in
automated vehicles but other road users feel that there's an appropriate level of care taken for their well-being
that's one of the areas where ethics is important the other area where ethics is important is that we have different
objectives as we drive down the road we have objectives for safety we'd like to get there we have objectives for
mobility we'd like you to get there probably pretty quickly and we also have the idea of legality we'd like to follow
the rules but sometimes these things come into conflict with each other so let's say you're driving down the
Double Yellow Line
road and there's a van that's parked where it has absolutely no business parking you've got a double yellow line
is it okay to cross well at least in California there's no exception to the
double yellow line representing the lane boundary for a vehicle that's parked where it has no business being parked so
according to the vehicle code you're supposed to kind of come to a stop here I don't think any of us would right in
fact actually when you're in California and you're riding through the hills and you come upon a cyclist virtually every
vehicle on the road is deviating across the double yellow line to give extra room to the cyclists that's also not
what you're supposed to do by the vehicle code you're supposed to stay on your side of the double yellow line but slow to an appropriate speed to pass
right so there's behaviors where our desire for mobility or our desire for
safety are outweighing our desire for legality this becomes a challenge if you think about how do I program the
self-driving car should it be based on the way that humans drive or should it be based on the way that the legal code
tells me to drive of course the legal code was never actually anticipating a self-driving car from a human standpoint
that double yellow line is a great shorthand that says maybe there's something coming up here where you don't want to be in this other Lane but if I
actually have a car with the sensing capability to make that determination itself this is a double yellow line
actually all that meaningful anymore these are things that have to be sorted out speed limits being another one you
Speed Limits
know if we're out on the highway it's usually a little bit flexible do we give that same flexibility to the automated
vehicle or do we create this wonderful automated vehicle roadblocks of vehicles going to the
speed limit when nobody else around them is do we allow them to accelerate a
little bit to merge into the flow of traffic do we allow vehicles to speed if they could avoid an accident is our
desire for safety greater than our desire for legality these are the sort of ethical questions then I think are
really important these are things that need to be talked through because I believe if we actually have vehicles
that follow the law nobody will want to drive with them and so we need to think about either ways of giving flexibility
to the vehicles or to the law in the sense that vehicles can drive like humans do so this brings up some really
Learning and Programming
interesting areas I think with respect to learning and programming and so the question is you know should our
automated vehicles drive like humans and exhibit the same behavior that humans do or should they drive like robots and
actually execute the way that the law tells them that they should drive
obviously fixed rules can be one solution to this behavior learned from
human drivers could be another solution to this we might have some sort of balance of different objectives that we
do more analytically in terms of how much we want to obey the double yellow line when there are other things
influencing it in the environment now what's interesting is that is you start to think about this there's limits to
any of these approaches in the extreme you know as we found with our self-driving racecar if you're not
learning from experience you're not making use of all the data you're not gonna do as well and there's
no way that you can possibly pre program an automated vehicle for every scenario it's going to encounter somehow you have
to think about interpolating somehow you have to think about learning at the same time you can say well why don't we just
measure humans well human error is actually the the cause or a factor the
primary factor in 94 percent of accidents it's either a lack of judgment or lack of perception on the part of the
human so if we're simply following humans we're actually only learning how well humans can do things and we're
leaving a lot on the table in terms of the potential of the car and so this is a really interesting discussion that I
think will continue to be both in the development side of these vehicles in the policy side what is the
right balance what do I want to learn versus what do I want a program how do I avoid leaving anything on the table here
so because it's the point where you know I've had a bunch of slides with words here I want to give people a little bit
of a sense for what you could be leaving on the table if in fact you don't adapt
Marty Marty
this is Marty marty is a DeLorean that we've been working with in my lab now DeLoreans are
really fantastic cars unless you want to accelerate brake or turn it really
didn't do any of those things terribly well there's no power steering there's an underpowered engine and and very
small brakes all of these things are fixable in fact what's nice about the DeLorean is it separates quite nicely
the whole fiberglass tub comes up you can take out the engine you can take out
the brakes you can make some modifications to the frame stiffen the suspension work with renova motors start
up in Silicon Valley to put in a new electric drivetrain and put it all back
together and when you do you come up with a car that's actually pretty darn fun and when we've programmed to drive
itself this is Adam Savage from Mythbusters going along for a drive
[Music]
what do you see is Marnie doing something at a level of precision that we're pretty sure no human driver can
meet Junior said there's no way he can do this you see it's going into a perfect drift doing a perfect doughnut
around this cone and then it launches itself through the next gate sideways
into the next cone now it's doing this you see it shoots through the gate missing those cones and then launches
into a tight circle around the next cone it's actually doing this as sort of an algorithm similar to orbital mechanics
if you think about how it's how it's actually orbiting these different points as it sets the trajectory now the limit
on this as tires as you can see as it comes around here the tires disintegrate into many chunks flying at the camera as
we do this but the the ability of the car to really continue even as the tires
heat up to execute this pretty pretty nice trajectory here you see it going through the gates again and launching
into a stable equilibrium putting pretty much the tire tracks right over where they were in the previous run and then
finally ending so this is a sort of thing that I think is possible as you
The Potential
look at these vehicles there's a huge potential out there for these things to not drive about as well as an average
human but to far exceed human performance in their abilities to use
all the capabilities of the tires to do some amazing things so maybe that's not the way that you want your your daily
drive to go although when we first posted some of this some of this video one of the commenters was like I want
this car that way I can like go into the store to buy donuts while it sits in the parking lot doing donuts wasn't a use
case that I had thought of but that's one of one of the things that we thought of this really how if you limit yourself
to only thinking about what the tires can do before they get to the saturation
of the friction in the road you're only taking to account one class of trajectories there's a lot more beyond
that that could be very advantageous in some emergency situations would it be great if the car had access to that now
that's not a way that we're going to get if we only sort of monitor day to day driving we're not going to get that
capability in our cars so one other aspect that came through in the in the
Data Sharing
policy which I think is extremely important as we think about neural networks and learning is this idea of
data sharing and there's a huge potential to accelerate the development of automated vehicles if we can share
some information about edge case scenarios in particular so if you think
about trying to train a neural network to handle some extreme situations that's really much easier if your set of
training data contains those extreme situations right so if you think about the weird things that can happen out on
the road if you had a database of those and those comprised your training set you'd have a head start in terms of
being able to get a neural net where I can begin to validate that it would work in these situations so the question is
you know is there a way for the ecosystem around self-driving cars to actually share some of this information so that different players can actually
share some information about the critical situations and be able to make sure that if you learn something that
yes you can make your cars safer but actually all the cars out on the road gets safer now clearly you need to
balance this with some other considerations there's there's the intellectual property concerns of the company there's privacy concerns of any
individuals who might be involved but it does seem to me that there's a big potential here to think about ways of
sharing certain data that can contribute to safety and this is a discussion
that's going to be ongoing and I think academia can do a lot to sort of help broker this discussion because you know
the first level people say you know data sharing I don't know companies aren't going to share we're not going to get the
information we need but most of the time people stay in the abstract as opposed to saying well what information would be
most helpful what information it's really going to give people confidence in the safety of these cars it's gonna
let regulators understand how they operate and at the same time is going to protect the amount of development effort
that companies put in there I think there is a solution here and in fact if you look at aviation there's a really
good example that already exists it's known as the Esaias system it's started with only four Airlines
that decided to share safety information with each other and this goes through mitre which is a federally funded R&D
center and it's actually now up to 40 Airlines and if companies get kicked out
of the mitre a project they really try very hard to get back in now this is anonymized data its anonymized data so
that you know companies actually get a assessment of what their safety record
is like and they can compare it to other airlines in the abstract but they can't compare it to any identifiable airline
so there's no ranking of this it's not used for any enforcement techniques and
it took people a long time to kind of build up and begin to share that but now there's a huge amount of trust and
they're sharing more and more data and looking at ways that they can perhaps actually start to code in things like
weather and time of day which had been removed for anonymization purposes and the original version of the system so I
think there's some good examples out there and this is something that's very important to think about for automated
vehicles and I think as this discussion goes forward those of you who are interested in developing these vehicles
using techniques that rely on data are going to be an important voice for the
importance of data sharing I think there's a there's a large role here to kind of make people aware that this
actually does have value in the larger ecosystem so this is something that I
was able to work on more broadly as well so I was part now is the d-o-t representative on the National Science
and Technology Committee's Subcommittee on machine learning and artificial
intelligence and this was one of the recommendations that was really pushed forward as well because AI has tended to
really make great advances with the availability of good datasets and in order to make those sort of good
advances in transportation this group is also advocating that those datasets need
to be made broadly available so this is a little bit about the vision behind the
Automated Vehicle Policy
the automated vehicle policy what the goal was to really achieve here the idea
of trying to move towards a proactive safety culture not to necessarily put in regulations prematurely and try to set
standards honestly we don't know the best way to develop automated vehicles but to allow the government to kind of get involved
in discussions with manufacturers early and be comfortable with what's going out on the roadway and actually to kind of
help the u.s. to continue to play a leading role in this obviously if vehicles are going to be banned from the
roads it would be very difficult for the country to continue to be a place where
people could could test and develop this technology and then the belief really that there can be an acceleration of the
safety benefits of this through data sharing so each car doesn't have to encounter all the weird situations
itself but in fact can learn from what other vehicles experience and the idea
is that really this is meant to be an evolving framework so it comes out as guidance it really generates
conversations it generates best practices which can eventually evolved into standards and law and there's a
huge opportunity here because the belief isn't that the National Highway Traffic Safety Administration will be doing all
of the development of these best practices but that that'll really evolve from what companies do and what all of
us at universities are able to do to sort of generate ways to solve these problems in creative manners ways to
actually keep the innovation going but ensure that we have safety so as you
start to think about all of the AI systems that you're developing and you start to flip around a little bit and think about how does a regulator gonna
get comfortable that it's not going to do something weird these are great research questions I think these are
great practical questions and these are things that will need to be worked out going forward so I you with that as a
challenge to think about to think as you take this course not only about the technology that you're learning but how
do you communicate that to other people and where are the gaps that need to be
filled because I think you'll find some great opportunities for for research startup companies and ultimately work
with policy and government there so thanks for the opportunity to talk to all of you and I want to stop there
because probably the things that you want to talk about are more interesting than the things that I wanted to talk about so I'm happy to take questions
along there
good we had a quick hand here yeah
Safety Requirements
accidents were part of our economies the excess rates are extremely low do you
think some of these safety requirements may roll back like I do I think that's a
great question and okay so the question thanks for reminding me so the question was whether in the future when you have
all vehicles automated would we be able to actually roll back things like
airbags and seatbelts and other things that we have on there what we might know is as passive safety devices in vehicles
I believe that we will in fact actually one of the things that I think is most extraordinary if you think about this
from a sustainability standpoint when you look at the average sort of mass of vehicles and average occupancy of
vehicles in the u.s. you know with single with passenger cars we're using maybe about ninety percent of the energy
to move the vehicle as opposed to moving the people inside and one of the reasons for that is crashworthiness standards
which are great because that's what's enabled us to be surviving these crashes at 40 miles an hour but if we do have
vehicles that are not going to crash or if they are going to have certain modes which might be designed with very
carefully design you know crush areas or things like this we could potentially
take a lot of that mass out particularly if these are low-speed vehicles which are designed only for the urban environment and they're not going to to
crash because they're going to drive you know somewhat conservatively or in some ways separated from pedestrians then I
think you can get a lot of the mass out and then you start to actually have transportation options which you know
from an environmental standpoint are comparable to cycling so so I think I think that's actually a really really
good goal to strive for although we either have to kind of limit the environment or think in the far future
with some of those techniques
Learning from Humans
to apply it which you guys learn
good yeah that's a great question so what are we what are we doing with Shelly is our mission really just to
drive as fast as possible and faster than a human or are we trying to learn from this something that we can apply to
other automated vehicles it really is a desire to learn from other automated you
know for the development of other automated vehicles and we've often said that at the point where you know the
difference between Shelly's performance in the human driver you know starts to be really mundane things like you know
our shift pattern or something which isn't applicable we kind of lose interest at that however you know up to
this point every insight that we've gotten from Shelly has been directly transferable and we've programmed the
car to do some emergency lane changes in situations where you don't have enough room to brake and we've actually been
demonstrating in some cases that the car can can do this much faster than a human
even an expert humans response can be so there's certain scenarios that we've done like that and I would say from the
bigger picture what's really fascinating is that we originally started out with this idea of let's find the best path
around the track and track it as close as we can but in fact when you look at human race car drivers what they're
doing is actually very different they're pushing the car to the limits and then sort of seeing what paths that opens up
to them and it flips the problem a bit on its head in a way that I think is actually very applicable for developing
safety systems out on the road but it's not a way that people have looked at it to the best of my knowledge up to this
point and so you know that's really what we're hoping is that the inspiration in trying to reproduce human performance
there leads us to better safety algorithms so long you know so far that's been the case and when that
ceases to be the case I think we are definitely much less interested
yeah so so liability is a good question so what what who is liable if I can can
Liability
sort of rephrase you know for an accident in an automated vehicle on the
one hand that's kind of an open question on the other hand we do have a court system and so whenever there are new
technologies these things are actually generally figured out in the courts and it can be different from state to state
so this is one aspect where you know potentially some discussions so that manufacturers aren't subject to
different conditions in different states would be helpful but the way that it works now is that it's it's usually not
binary we have in the US a sense of joint and several liability and so you
can actually assign different portions of responsibility to different players in the game you have had companies like
Volvo and in fact Google make statements that if there are vehicles are involved in accidents then they would expect to
be liable for it so people have often talked about needing something really
new for liability but I'm not sure that's the case we do have a court system that can ultimately figure out
who is liable with new technologies and we have some manufacturers that are starting to make some statements about
assuming product liability for that the one thing that really could be helpful as I mentioned is perhaps some
harmonization because right now insurance is something that is set state-by-state and so the rules in one
state as to who's at fault for an accident may be very different in another state
Safety
okay so what what if companies you know as they send in the safety letters are are using criteria to set safety that
that may not be broadly acceptable to the to the public whether the public would like these vehicles to have
greater safety I think you know the the nice thing about this process is first of all we would know that right so we
would have a sense that companies are developing with certain measures of
safety in mind and there could actually be a discussion as to you know whether that is setting an acceptable level it's
it's a difficult question because it's it's not clear that people really know what an acceptable level is is it does
it have to be safer than then humans drive now you know my personal feeling I would say yes and does it have to be
much much safer well that that's hard to say you know you start to then get into
the situation of we're comfortable to a certain extent with our existing legal system and with the fact that humans
could cause errors that have fatal consequences do we feel the same way about machines right you know we tend to
think the machines really should to have a higher level of perfection so we may as a society be less tolerant people
will often say well so long as the overall national figures go down that would be good but that's really not
going to matter much to the families who are impacted by an automated vehicle particularly if it's a if it's a
scenario with very very bad optics and what do I mean by that it's if you think
about the failures of mechanical systems because they're different than the failures of human beings they can often
like look really bad right if you sort of think about a vehicle that doesn't detect something and then just continues
to plow ahead you know visually that's that's really striking and that's the
sort of thing that you know we'd get replayed and be in people's consciousness and raise some fears and so you
I think that's that's an issue that's going to have to be have to be sorted out these are average being different
Policy
you know between research in other parts of the world to exchange technologies
yes so that's that's a good question what's being done really from a global standpoint to sort of share ideas to
share research and to kind of work through some of these things particularly on the policy side so most of the auto manufacturers are global
corporations and so a lot of the research in this is done in very different parts of the world so
renault-nissan for instance is doing a lot in Silicon Valley in Europe and and in Japan and I think you see a lot of
that with the different manufacturers one of the cool things that I got to do as part of my role was to go with the
Secretary of Transportation to the g7 transportation ministers meeting in Japan and address the ministers about
sort of the the u.s. policy on on automated vehicles and one of the parts
of that discussion was well the US has a very different set of rules so we have
this manufacturer self certification as opposed to pre market certification but testing for instance is something that
has to be done regardless so either it's testing that's done by a manufacturer or it's testing that's done by for instance
in you know in Germany the the the tooth and other agencies that are responsible
for for road safety and so the idea is maybe we should be sharing best
practices on testing so we have a set of standard tests and then manufacturers across the globe could test to a certain
set of standards that might be translated differently according to the policies and regulate or e environments
in different countries so that was that was part of the idea that we advanced at the g7 and it seemed to kick off really
well I never had a conscious decision on
this I actually got a call from the White House one day you know and and you know I got this message just or this
email you know I'm reaching out for the White House when you give my call you know give me a call back so of course I called back immediately and Pam Coleman
on the other end of the line it's like I love doing that she's like you know when you're calling for the White House everybody returns your call and and so
honestly you know the she said here's the situation we're looking at a lot of these areas in the Department of
Transportation that seem to hit upon your areas of expertise we want to talk about where with you in some way the holy grail
would be for you to come out and work in DC for a while and then I got a call from the Department of Transportation
and they're like well we know you wouldn't want to come out to DC for a while oh my god try me could I do you no could I do cool
stuff and could I make an impact and then you know I met with the Secretary of Transportation out in San Francisco
and you know he assured me he's like you would be surprised you would be very surprised at how much of an impact you
could have and this ended up being really true a lot of times this stuff
moves quickly and people who are involved in policymaking may or may not have a technical background in this they may have come through the campaign for
instance and then ended up in political roles yet the folks that I worked with we're really trying to get good
information and make good decisions and so I just kept getting called in for advice on all sorts of things and I
found that people actually really wanted to have that technical information and then used it so so that that's the way
it happened it seemed like it was an opportunity to take things that I've worked on as I mentioned you know automated vehicles since 1992 and then
to be part of this policy development which went really quickly it was a one-page outline when I arrived in
February and then in September it rolled out and along the way it was all sorts of editing and negotiations the White
House and other agencies fascinating fascinating process so so I kind of fell
into this but you know as Lex mentioned I think I'm emerging as a policy wonk here because it was a it was a very fun
experience you have a lot of companies
Sharing data
that have somewhat of a monopoly on a lot of data especially like Google has so much more data available yeah a lot
of the smaller startups how do you incentivize companies actually share their data good
companies to share their data when they have an awful lot in invested in them in
that in the gathering of that data and being able to process that data and I think the answer is to start small and
to try to say are there certain high value things they could again make the public comfortable make policymakers
comfortable that really aren't going to be a burden on the company you know so so one of the you know one of the things
that from the peloton standpoint that was bounced around at one points are our trucks actually use vehicle-to-vehicle
communications as part of their link well when you do that you discover that there's actually an awful lot of places
where that drops out because cell phone towers which are not supposed to be broadcasting on that frequency seem to
create an awful lot of interference there well that can be very interesting from a public policy perspective to know
you know where are you know we were sort of monitoring for incursions in that in that frequency range everywhere we go
that for instance might be very useful piece of information to share with policymakers that wouldn't be any real
proprietary issue to share from the company's perspective and so I think
that the trick is to start small and find what are the high-value data where there isn't a big issue of sharing I
mean if you go to Google and say all right Google what will it take for you to share all of the data you're
acquiring from your entire self-driving car program I guess way mo now I think that would be a very big number and so I
don't think that's the starting point I think you start with you know what is the high value data data that's of high value for the public policy sense and
really minimal hassle to the to the companies I don't know how much longer
I'm happy to stay in and and answer it answer as many questions but I know you
have a class to run how are we okay good yes
Accident data simulations
[Music] no standards for sharing that data accident data simulations good is there
any effort underway for for sharing map data some of the edge case accident data
simulation capabilities and things like that this is one of the next steps that MIT se outlined in the policy and so
there are people at admits actually working on taking some of these next steps again is sort of a pilot or
prototype mode so so that's something that's that's currently being worked on in the in the department you could
probably expect to hear more from in the not so distant future
Testing in urban and rural environments
but executing our production our to be happy
okay so the question is testing in urban and rural environments or even driving in urban and rural environments are very
different in should that the government actually come up with a standard set of data that all companies have to attest
to I think one of the reasons that the policy was designed the way it was was
to make sure we have this concept of operational design domain so in fact if
the only area that I've mapped and the only area that I want to drive is say in
a campus environment or in one quarter square mile then then the idea is that
we would like the companies to explain how they handle the eventualities in that one quarter square mile but they
should really have no reason to handle other situations right because their vehicle won't encounter that so long as
it's been designed to stay within its operational design domain so I think in the short term you know what you see is
people often looking at hyperlocal solutions or kind of the low-hanging fruit for for a lot of automation and
even if you think about offering mobility as a service if I'm gonna offer a sort of a an automated taxi I'm
probably going to do that in a limited environment to start with and so if I'm only doing this in Cambridge does it
really matter if I can drive in Mountain View or not and so you know I think the idea is to start with the definition of
the operational design domain with a data set that is appropriate for that operational design domain and then as
people's design domains start to expand nationwide then I think you know the idea of common data sets starts to be
starts to be interesting although you know there is a sense that no finite data set is really going to capture
every eventualities and so you know people will be able to develop or sort of you know design to
the test in some ways is that sufficient I think it'll make people feel better but I I personally wonder how much value
there is you know it seemed it with test track testing I could think of 20 different tests that automated vehicles
will have to pass and people will design ways to pass all 20 of those tests it may make some people more comfortable
but it doesn't make me all that much more comfortable that they'd be able to handle a real-world situation
all right let's see could you could you
Opensource cars
make an open-source car under okay so the question is could you make an open-source car under the the guidance
provided by us do t the question would
be you know this from from a practical question you're supposed to submit a safety assessment letter which is
supposed to be signed by somebody responsible for that and so an issue if
you were to open source would be you know do I use this module and who is actually signing signing off on out what
I feel comfortable signing off on something which I then allowed to be open source I you know not a lawyer but
I would think that you know I don't think there would be anything that would prevent that if you had a development
team that was doing that and people who are willing to sign off on whatever version of the software was actually
used in an open source car you know I will say that the the guidance does
apply to universities or to or to other groups that would be putting a car out
on the road and I think if you look through the 15 points they're not really meant to be overly restrictive in fact I
would argue that pretty much any group that is going to sort of put real people at risk by by putting an automated
vehicle out on the road should really have thought through these things so I don't think it's a I don't think it's a terribly high high burden to to meet I
think it would be you know it would be me double by a group it's just a question would be you know from the open source sense how do you sort of trace
who's responsible and who's signing off on that alright I think we gave those
third graders or run for their money yeah absolutely thank you so much let's give Chris a big hand
great thanks a lot [Applause]

----------

-----

--19--

-----
Date: 2017.02.18
Link: [# MIT 6.S094: Deep Learning for Human-Centered Semi-Autonomous Vehicles](https://www.youtube.com/watch?v=ByZF8_-OJNI)
Transcription:

The human side of AI, how do we turn this camera
back in on the human, we are talking about perception,
how to detect cats and dogs, pedestrians lanes,
how to steer a vehicle based on the external environment, the thing that's really fascinating and severely understudied,
is the human side, we talked about the Tesla, we have cameras in 17 Tesla's driving around Cambridge
because Tesla is one of the only vehicles allowing you
to experience in a real way, on the road, the interaction between the human
and the Machine, the thing that we don't have,
that deep learning needs on the human side of semi-autonomous vehicles and fully-autonomous vehicles
is video of drivers, that's what we're collecting, that's what my work is in, is looking at billions
of video frames, of human beings driving 60 miles an hour plus on the highway
in their semi-autonomous Tesla, what are the things that we want to know about the human?
If we were a deep learning therapist, wed try to break apart
the different things we can detect from this raw set of pixels, we can look here, from the green to red
is a different detection problem, a different computer vision detection problem green means it's less challenging,
it's feasible, even under poor lighting conditions, variable pose, noisy environment, poor resolution,
red means it's really hard no matter what you do, that's starting on the left with face detection body pose,
one of the best studied and one of the easier computer vision problems, we have huge datasets for these,
then there is micro saccades, the slight tremors of the eye that happen at a rate of a thousand times a second.
All right let's look at First, why do we even care
about the human in the car? One is trust, this trust part is a If you think about it,
to build trust the car needs to have some awareness of the biological thing
it's carrying inside, the human inside, you assume the car knows about you, because you're sitting there controlling it,
but if you think about it, almost every single car on the road today, has no sensors
with which it's perceiving you, it knows, some cars have a pressure sensor on the steering wheel
and a pressure sensor or some kind of sensor detecting that you're sitting in the seat,
that's the only thing it knows about you, that's it, so how is the car supposed to
this same car is driving 70 miles an hour, on the highway, autonomously, how is it supposed
to build trust with you if it doesn't perceive you? That's one of the critical things here,
so if I'm constantly advocating something, is that we should have a driver facing camera in every car,
despite the privacy concerns, you have a camera on your phone and you don't have as much
of a privacy concern there, but despite the privacy concerns, the safety benefits are huge, the trust benefits are huge.
Let's start with the easy one, detecting body pose, why do we care?
There is a seatbelt design, there are these dummies,
crash-test dummies, which we can use to design the passive safety systems of our cars,
and they make certain assumptions about body shapes, male, female, child, body shapes, but they also make assumptions
about the position of your body in the seat, they have the optimal position, the position they assume you take,
the reality is, in a Tesla, when the car is driving itself,
the variability, if you remember the deformable [unintelligible 00:04:44] you start doing a little bit more of that, you start to
reach back in the back seat, in your purse, your bag, for your cell phone, these kinds of things,
that's when the crashes happen, we to know how often that happens, the car needs to know that you're in that position,
that's critical for that very serious moment when the actual crash happens,
how do you do? This is deep learning class, this is deep learning to the rescue,
whenever you have these kinds of tasks, of detecting for example body poses, you're detecting points of the shoulders, points of the head,
five-ten points along the arms, the skeleton.
How do you do that? You have a CNN, convolutional neural network, that takes its input image and takes an output,
it's a regressor, it gives an XY position of whatever you're looking for, the left shoulder, right shoulder,
then you have a cascade of regressors they give you all of these points, they give you the shoulders, the arms and so on,
then you have through time on every single frame you make that prediction
and then you optimize, you can make certain assumptions about physics,
your arm can't be in this place in one frame and then the next frame be over here, it moves smoothly
through space so under those constraints you can then minimize the error--
the temporal error from frame to frame or you can just dump all the frames,
as if there are different channels like RGB is three channels, you can think of those channels as in time,
you can dump all those frames together, and that's what I call 3D convolutional neural networks,
you've dumped them all together and then you estimate the body pose in all the frames at once.
There are some data sets for sports and we're building our own I don't know who that guy is
Let's fly through this a little bit, so what's called gaze classification,
gaze is another word for glance, it's a classification problem,
here's one of the TAs for this class, Not here because he's married, he had to be home,
I know were his priorities are at, this is on camera, he should be here, [chuckles]
There's five cameras, this is why we're recording in the Tesla. This is a Tesla vehicle, in the bottom right, there's a blue icon
that lights up automatically detected if it's operating under autopilot, that means the car is currently driving itself,
there's five cameras one on the forward roadway, one on the instrument cluster, one on the center stack, steering wheel, his face,
then it's a classification problem, you dump the raw pixels into a convolutional neural network, have six classes forward roadway,
you're predicting where the person is looking, forward roadway, left, right,
center stack, instrument cluster, rearview mirror, and you give millions of frames
for every class, simple. And It does incredibly well at predicting
where the driver is looking, the process is the same for majority of the driver state problems that have to do with the face,
the face has so much information, where are you looking, emotion, drowsiness, different degrees of frustration,
I'll fly through those as well, but the process is the same, there's some pre-processing, this is in the wild data,
there's a lot of crazy light going on, there's noises, vibration from the vehicle, so first you have to
video stabilization you have to remove all that vibration, all that noise, as best as you can, there's a lot of algorithms,
non-neural network algorithms, boring but they work
for removing the noise, removing the effects of sudden light variations and vibrations of the vehicle,
there's the automated calibration, so you have to estimate the frame of the camera, the position of the camera,
and estimate the identity of the person you're looking at. The more you can specialize the network to the identity of the person
and the identity of the car the person is riding in, the better the performance for the different driver state classification.
So you personalize the network, you have a background model that works on everyorne and you specialize each individual,
this is transfer learning, you specialize each individual network to that one individual.
There is a face frontalization, fancy name for the fact that no matter where they're looking,
you want to transfer that face so the eyes, nose are the exact same position in the image, that way if you want to look at the eyes
and you want to study the subtle movement of the eyes the subtle blinking, the dynamics of the eyelid, the velocity of the eyelid,
it's always in the same place so you can really focus in remove all effects of any other motion of the head,
and then you just it's the beauty of deep learning, there is some pre-processing, because this is real-world data,
but you just dump the raw pixels in, you dump the raw pixels in and predict whatever you need.
What do you need? One is emotion, You can have I had a study where people
used a crappy and a good voice based navigation system, so the crappy one got them really frustrated,
and they self-reported it as the frustrating experience or not on scale one to 10, that gives us ground truth,
a bunch of people to used this system, they put themselves as frustrated or not,
so then we can predict, we can train a Convolutional neural network to predict is this person frustrated or not, I think we've seen a video of that,
turns out smiling is a strong indication of frustration, you can also predict drowsiness in this way,
gaze estimation in this way, cognitive load, I'll briefly look at that, the process is all the same,
you detect the face, you find the landmark points in the face, for the face alignment, face frontalization,
and then you dump the raw pixels in for classification, step five. You can use SVM's there
or you can use what everyone uses now, convolutional neural networks.
This is the one part where CNN's still struggle to compete, is the alignment problem,
this is why I talked about the Cascade regressors, is finding the landmarks on the eyebrows, the nose,
the jawline, the mouth, there are certain constraints there,
so algorithms that can utilize those constraints effectively can often perform better than end-to-end regressors
that just don't have any concept of what a face is shaped like. There are huge data sets and we're a part
of the awesome community that's building those data sets for face alignment.
This is the TA in its younger form,
this is live in the car, the real time system predicting where they're looking,
this is taking slow steps towards the exciting direction
that machine learning is headed, which is unsupervised learning, the less you have to have humans look to the data
and annotate that data, the more power these machine learning algorithms get,
currently supervised learning is what's needed, you need human beings to label a cat and label a dog,
if you can only have a human being label 1%, one tenth of a percent of a data set, only the hard cases,
so the machine can come to the human and be like, I don't know what I'm looking at in these pictures,
because of the partial light occlusions, we're not good at dealing with occlusions,
whether it's your own arm or because of light conditions, we're not good with crazy light drowning out the image,
this is what Google self-driving cars struggle with when they're trying to use their vision sensors, moving out of frame,
all kinds of occlusion They are really hard for computer vision algorithms,
and in those cases we want a machine to step in and say-- and pass that image on to the human, be like "help me out with this"
and the other corner case is, in driving for example 90 plus percent of the time all you're doing is
staring forward at the roadway the same way, that's where the Machine shines, that's where machine automated annotation shines,
because it's seen that face for hundreds of millions of frames already, in that exact position,
so it can do all the hard work of annotation for you, it's in the transition away from those positions
that it needs a little bit of help, just to make sure that this person just started looking away
from the road to the rear view, and you bring those points up, so you're-- there's a using optical flow,
putting the optical flow in the convolutional neural network, you use that to predict when something has changed
when something has changed you bring that to the machine for annotation all of this is to build a giant
Billions of frames annotated data set, our ground truth, on which you train your driver state algorithms,
in this way you can control, on the x-axis is the fraction of frames the human has to annotate,
zero percent on the Left, ten percent on the right, and then the accuracy trade-off, the more the human annotates,
the higher the accuracy, you approach 100% accuracy, but you can still do pretty good, this is for the gaze classification task,
With an 84-- 84 fold to almost towards the magnitude reduction in human annotation,
this is the future of machine learning, and hopefully one day no human annotation,
and the result is millions of images like these video frames,
same thing, driver frustration, this is what I was talking about, the frustrated driver is the one that's on the bottom,
so a lot of movement of the eyebrows and a lot of smiling, and that's true subject after the subject,
And they're Happy, the satisfied, I don't want to say happy, the satisfied driver is cold and stoic,
and that's true for subject after subject, because driving is a boring experience and you want it to stay that way
Yes, question.
Great, great question, they're not--
Absolutely, that's a great question So these cars owned by MIT, there is somebody in the back
The comment was my emotions then have nothing to do with the driving experience.
Yes, let me continue that comment, your emotions are often
You're an actor on the stage for others with your emotion, when you're alone, you might not express emotion,
you're really expressing emotion oftentimes for others, your frustration is like "What the heck"
that's for the passenger, and that's absolutely right, so one of the cool things we're doing
As I said, we now have over a billion video frames in the Tesla, We're starting to collected huge amounts of data in the Tesla,
emotion is a complex thing, in this case, we know the ground truth, how frustrated they were,
in naturalistic data, when it's just people driving around, we don't know how they're really feeling at the moment,
we're not asking to enter an app "how are you feeling right now?" but we do know certain things, we know that people sing a lot,
that has to be on paper at some point, it's awesome, people love singing,
so that doesn't happen in this kind of data, because there's somebody singing in the car, and I think the expression of frustration is also the same.
Yes. The question is or the comment is that the solo data set is probably going to be very different
from a data set that's not solo, with a passenger, that's very true, the tricky thing about driving
this is why it's a huge challenge for self-driving cars for the external facing sensors and for the internal facing sensors analyzing human behavior,
is 99.9% of driving is the same thing, it's really boring.
So finding the interesting bits is actually pretty complicated, so that has to do with emotion, that has to do with
so singing is easy to find, we can track the mouth pretty well, so when you're talking of singing we can find that,
but how do you find the subtle expressions of emotion? It's hard, when you're solo.
Cognitive load, that's a fascinating thing,
I mean, similar emotion it's a little more concrete in a sense that there's good science and ways to measure cognitive load,
cognitive workload, how occupied your mind is, mental workload is another term used,
the window to the soul, the cognitive workload soul is the eyes,
so pupil first of all the eyes move in two different ways they move in a lot of ways but two major ways is saccades,
these are these ballistic movements, they jump around whenever you look around the room, they're actually just jumping around,
when you read the eyes are jumping around, Like if all of you just follow this bottle with your eyes,
your eyes are actually going to move smoothly, a smooth pursuit. Somebody actually told me today,
that probably has to do with our hunting background as animals,
I don't know how that helps, like frogs track flies really well, so you have to like Anyway, the point is
there are smooth pursuit movements where the eyes move smoothly, and those are all indications of certain aspects of cognitive load,
and then there are these very subtle movements, which are almost imperceptible for computer vision and these are micro saccades, these are tremors of the eye,
a work from here, from Bill Freeman, magnifying those subtle movements, these are taken at 500 frames a second.
So cognitive load when the pupil, that black dot in the middle,
in case you don't know what a pupil is, in the middle of the eye, when it gets larger that's an indicative of high cognitive load,
but it also gets larger when the light is dim. So there's this complex interplay,
so we can't rely in the wild outside, in the car, or just in general outdoors,
using the pupil size, even though pupil size has been used effectively in a lab to measure cognitive load, it can't be reliably used in the car,
the same with blinks, when there's a high cognitive load, your blink rate decreases and your blink duration shortens,
I think I'm just repeating the same thing over and over, but you can imagine how we can predict cognitive load,
We extract a video of the eye. Here is the primary eye of the person the system is observing,
happens to be the same TA once again.
We take the sequence of 100-- it's 90 images, that's six seconds,
16 frames a second, 15 frames a second, we dump that into a 3D convolutional neural network,
that means it's 90 channels, it's 90 frames, grayscale,
and then the prediction is one of three classes of cognitive load,
low cognitive load, medium cognitive load and high cognitive load, there's ground truth for that, because we have people--
over 500 different people do different tasks of various cognitive load, and after some frontalization again,
where you see the eyes are traced no matter where the person looking,
the image of the face is transposed in such a way that the corner of the eyes remain always in the same position,
after the frontalization, we find the eye, active appearance models, find 39 points of the eyelids, the iris,
and four points on the pupil.
Putting all of that into a 3D CNN model, they're positioned,eye sequence on the left, 3D CNN model in the middle,
cognitive load prediction on the right. This code by the way is freely available online.
All you have to do, dump a web-cam from the video stream, CNN runs faster than real-time,
predicts cognitive load. Same process as detecting the identity of the face,
same process as detecting where the driver is looking, same process as detecting emotion
and all of those require very little hyper parameter tuning on the convolutional neural networks,
they only require huge amounts of data.
Why do we care about detecting what the drivers doing? I think Eric has mentioned this is--
On the-- Oh man, this is the comeback of the slide, [laughter]
I was criticized for this being a very cheesy slide, in the past towards full automation,
we're likely to take gradual steps towards that.
I can't, it's enough of that, this is better Especially given that This is given today,
our new president, this is a pickup truck country,
this is a manually controlled vehicle country, for quite a little while, we like control
and control being given to somebody else, to the machine, will be a gradual process,
it's a gradual process of that machine earning trust, and through that process, the machine,
like the Tesla, like the BMW, like the Mercedes, the Volvo,
that's now playing with these ideas, it's going to need to see what the human is doing,
and for that, to see what the human is doing,
we have billions of miles of forward-facing data, what we need,
is billions of miles of driver facing data as well. We're in the process of collecting that,
this is a pitch for automakers and everybody to buy cars
that have a driver facing camera. And let me close--
I said we need a lot of data but I think this class has been
through your own research you'll find that we're in the very early stages
of discovering the power of deep learning,
for example, recently, Jean [?] said
that it seems that the deeper the network, the better the results in a lot of really important cases,
even though the data is not increasing, why does the deeper network give better results?
This is a mysterious thing we don't understand, there's these hundreds of millions of parameters,
from them is emerging some kind of structure, some kind of representation of the knowledge that we're giving it.
One of my favorite examples of this emergent concept is the Conway's Game of Life.
For those of you who knows what this is, will probably criticize me for being as cheesy
as the stairway slide, but I think it's such a simple
and brilliant example of how-- Like a neuron in a neural network
is a really simple computational unit, and then incredible power emerges when you combine a lot of them
in a network, in the same way, this is called the cellular automata,
that's a weird pronunciation,
every single cells is operating under a simple rule, you can think of it as a cell living and dying,
it's filled in black when it's alive and white when it's dead, if it's alive
and it has two or three neighbors, it survives to the next time,
otherwise it dies, and if it has exactly three neighbors, and it's dead,
it comes back to life, if it has exactly three neighbors, that's a simple rule, whatever, you can just imagine, it's just simple
All is doing, is operating under this very local process, same as a neuron.
It's a or in the way we're currently training neural networks and there's this local gradient,
we're optimizing over a local gradient, the same local rules, and what happens if you run this system,
operating under really local rules, what you get on the right, it's not Again, you have to go home,
hopefully no drugs involved, but you have to open up your mind [chuckles]
and see how amazing that is, because what happens is, it's a local computational unit,
that knows very little about the world, but somehow really complex patterns emerge
and we don't understand why, in fact under different rules, incredible patterns emerge, and it feels like
it's living creatures communicating, when you just watch it, not these examples, this is the original,
they get complex and interesting, but even in these examples, these complex geometric patterns that emerge,
it's incredible, we don't understand why, same with neural networks, we don't understand why, and we need to in order to see how these networks will be able to reason.
What's next? I encourage you to read the deep learning book,
it's available online, deeplearningbook.org. As I mentioned to a few people, you should--
Well, first there's a ton of amazing papers every day coming out on archive,
I'll put these links up, but there's a lot of good collections of strong papers,
lists of papers, there is the literally awesome list, the awesome deep learning papers on GitHub,
it's calling itself awesome, but it happens to be awesome, there is a lot of blogs, it's just amazing,
that's how I recommend you learn machine learning, on blogs, and if you're interested
in the application of deep learning in the automotive space, you can come and do research in our group,
just email me. Anyway, we have three winners,
Jeffrey Hu, Michael Gump how do you-- Are you here?
How do you say your name? No, that's not my name [laughter]
My name is Purna [?]
Oh, I see. [?]
Well, anyway here-- [applause]
He achieved the stunning speed of-- So this is kind of incredible,
I didn't know what kind of speed we were going to be able to achieve, I thought 73 was unbeatable, because we played with it for a while
and we couldn't achieve 73, we design a deterministic algorithm that was able to achieve 74 I believe,
meaning like it's cheating, with the cheating algorithm that got 74, folks have come up
with algorithms that have done that had beaten 73 and then 74, so this is really incredible,
and the other two guys all three of you get a free term at the Udacity self-driving car engineering degree,
Thanks to those guys for giving that award and bringing their army of brilliant
So they have people who are obsessed about self-driving cars, and we've received
over 2,000 submissions for this competition, a lot of them from those guys, they're just brilliant,
it's really exciting to have such a big community of deep learning folks working in this field,
this is for the rest of eternity, we're going to change this up a little bit, but this is actually
the three neural networks, the three winning neural networks
running side by side, you can see the number of cars passed there, the first place is on the left,
second place, and third place, and in fact, the third place it's almost-- right now, second place is winning currently,
but that just tells you the random nature of competition,
sometimes you win, sometimes loose.
The actual evaluation process runs through a lot of iterations and takes the medium evaluation.
With that, let me thank you guys so much for Wait, we have a question
are the winning networks online? Yes.
All three guys wrote me a note about how their networks work, I did not read that note,
[chuckles] I'll postThis tells you how crazy this has been,
I'll post the winning networks online,
and I encourage you to continue competing and continue submitting networks. This will run for a while we're working on a journal paper
for this game. We're trying to find the optimal solutions.
Okay. This is the first time I've ever taught a class, and the first time obviously teaching this class,
so thank you so much for being a part of it. [Applause] Thank you to Eric,
if you didn't get a shirt please come back, please come down and get a shirt, just write your email on the note,
on the on the index note. Thank you.


----------

-----

--18-- 

-----
Date: 2017.02.01
Link: [# MIT 6.S094: Recurrent Neural Networks for Steering Through Time](https://www.youtube.com/watch?v=nFTQ7kHQWtc)
Transcription:

All right. So, we have talked about regular neural networks,
fully connected neural networks, we have talked about convolutional neural networks that work with images,
we have talked about Reinforcement, Deeper Reinforcement Learning, where we plug in a neural network
into a Reinforcement Learning Algorithm, when a system has to not only
perceive the world but also act in it, and collect a reward. And today we will talk about,
perhaps the least understood but the most exciting neural network out there,
flavor of neural networks, is Recurrent Neural Networks.
Administrative
But first, for administrative stuff, theres a website. I dont know if you heard,
cars.mit.edu, where you should create an account, if youre a registered student, thats one of the requirements.
You need to have an account if you want to get credit for this, you need to submit code
for DeepTrafficJS, and DeepTeslaJS, and for DeepTraffic,
you have to have a neural network that drives faster than 65mph. If you need help to achieve that speed
please e-mail us. We can give you some hints.
For those of you who are old school SNL fans, theres the Deep Thoughts section now,
in the profile page, where we encourage you to talk about the kinds of things that you tried in DeepTraffic
or any of the other DeepTesla or any of the work you've done
as part of this class for DeepLearning. Okay,
we have talked about the Vanilla Neural Networks on the left. The Vanilla Neural Network
Flavors of Neural Networks
is the one where it's computing is approximating a function that maps from one input
to one output. An example is mapping images to the number that is shown in the image.
For ImageNet is mapping an image to what's the object in the image. It can be anything.
In fact, Convolutional Neural Networks can operate on audio, you can give it a chunk of audio, a five second audio clip,
that still counts as one input because its fixed-size. As long as the size of the input is fixed,
that's one chunk of input and as long as you have ground truth
that maps that chunk of input to some output ground truth, thats the Vanilla Neural Network.
Whether there's a fully connected neural network or convolutional neural network.
Today well talk about the amazing, the mysterious Recurrent Neural Networks.
They compute functions from one to many, from many to one,
from many to many.
Also bidirectional. What does that mean? They take its input sequences,
time series, audio, video, whenever there's a sequence of data,
and that temporal dynamics that connects the data is more important than the spatial
content of each individual frame. So, whenever there's a lot of information being conveyed in a sequence,
in a temporal change of whatever that type of data is, that's when you want to use Recurrent Neural Networks
like speech, natural language, audio
and the power of this is that for many of them, for a Recurrent Neural Network, where they really shine,
is when the size of the input is variable, so you dont have a fixed chunk of data
that you're putting in is variable input. And the same goes for the output,
so you can give it a sequence of speech, several seconds of speech
and then the output is a single label of whether the speaker is male or female.
Thats many to one. You can also do
many to many. Translation. You can have natural language
put into the network in Spanish and the output is in English.
Machine translation. That's many to many. And that many to many doesn't have to be
mapped directly into same sized sequences. For video, the sequence size might be the same
you're labeling every single frame, you put in a five second clip
of somebody playing basketball and you can label every single frame counting the number of people in every single frame.
That's many to many when the size of the input and the size of the output is the same Yes, question?
The question was, are there are any models where there's feedback from output and input? That's exactly what Recurrent Neural Networks are.
It produces output, and it copies that output and loops it back in.
That's almost the definition of a Recurrent Neural Network. There's a loop in there that produces the output
and also takes that output as input once again.
There's also many to many where the sequences don't align. Like machine translation,
the size of the output sequence might be totally different than the input sequence. We will look on a lot of cool applications;
you can start a song, learn the audio of a particular song have the Recurrent Neural Network
to continue that song after a certain period of time. So it can learn to generate sequences
of audio, of natural language, of video. Okay.
Back to Basics: Backpropagation
I know I promised not many equations, but this is so beautifully simple
that we have to cover backpropagation. It's also the thing that, if you're a little bit lazy
and you go to the internet and start using the basic tutorials of TensorFlow, you ignore how backpropagation work.
At you peril. You kind of assume it just works. I give it some inputs, some outputs,
and it's like Lego pieces I can assemble them like you might have done with DeepTraffic A bunch of layers put in together
and then just press Train. backpropagation is the mechanism that neural networks currently--
The best mechanism we know of that is used for training. So you need to understand
the simple power of backpropagation, but also the dangers.
Summary, I put on the top of the slide, there's an input for the network that's an image,
there's a bunch of neurons, all with differentiable smooth activation functions on each neuron,
and then, as you pass through those activation functions,
take in an input, pass it through this net of differentiable compute nodes,
you produce an output. In that output you also have a ground truth,
the correct, the truth that you hope or you expect the network to produce.
And you can look at the differences between what the network actually produced and what you hoped it would produce,
and that's an error. And then you backward propagate that error, punishing or rewarding
the parameters of the network that resulted in that output
Let's start with a really simple example.
There's a function that takes its input up on top,
three variables, X, Y and Z. The function does two things: it adds X and Y
and then it multiplies that sum by Z. And then we can formulate that as a circuit,
circuit of gates, where there's a Plus gate, and a Multiplication gate.
Backpropagation: Forward Pass
Let's take some inputs, shown in blue. Let's say it's X is negative two,
Y is five and Z is negative four. And let's do a forward pass
through the circuit to produce the output. Negative two plus five equals three
q is that intermediate value, three.
This is so simple, and so important to understand that I just want to take my time for this
because everything else about neural networks just builds on these concepts
The add gate produces q, in this case, is three, and three times negative four is twelve.
That's the output. The output of the circuit of this network,
if you think of it as such, is negative twelve. The forward pass is shown in blue
the backward pass will be shown in red in a second here What we want to do, what would make us happy,
what would make f happy is for the output to be as high possible. Negative twelve, so-so, it could be better.
How do we teach it How do we adjust X, Y and Z,
to ensure it produces a higher f
makes f happier. Let's start backward,
The backward pass. We'll make the gradient on the output one,
meaning we want this to increase. We want f to increase. That's how we encode our happiness.
We want it to go up by one. In order to then propagate
Backpropagation: By Example
that fact that we want the f to go up by one, we have to look at
the gradient on each one of the gates. And what's a gradient?
It's a partial derivative
with respect to its inputs. The partial derivative of the output of the gate with respect to its inputs,
if you don't know what that means, is just
how much does the output change when I change the inputs a little bit. What is the slope of that change if I increase X
for the first function of addition, f of X, Y equals X plus Y. If I increase X by a little bit,
what happens to f? If I increase Y by a little bit, what happens to f? Taking a partial derivative of those
with respect to X and Y you just get a slope of one When you increase X,
f increases linearly. Same with Y. Multiplication is a little trickier.
When you increase X, f increases by Y.
Do the partial derivative of f with respect to X is Y, the partial derivative of f with respect to Y is X.
If you think about it, what happens is the gradients, when you change X,
the gradient of change doesn't care about X. It cares about Y.
Backpropagation: Backward Pass
It's flipped. So we can backpropagate that one, the indication of what makes X happy backward.
And that's done by computing the local gradient.
For q, the partial derivative of f with respect to q,
that intermediate value, that gradient would be negative four.
It will take the value of Z as I said it's the Multiplication gate, It'll take the value of Z
and assign it to the gradient. And the same for the partial derivative of f with respect to Z,
it will assign that to q. The value of the forward pass on the q. There's a three
and a negative four on the forward pass in blue and that's flipped. Negative four and three
on the backward pass. That's the gradient. And then we continue in the same exact process.
But wait. What makes all of this work,
is the Chain Rule. It's magical.
What it allows us to do is to compute the gradient,
the gradien of f with respect to the inputs X, Y, Z. We don't need to construct
the giant function that is the partial derivative of f with respect to X, Y and Z
analytically. We can do it step by step backpropagating the gradients. We can multiply the gradients together
Modular Magic: Chain Rule
as opposed to doing the partial derivative of f with respect to X. We have just the intermediate,
the local gradient of f with respect to q, and of q with respect to X,
and multiply them together. So, Instead of computing
gradient of that giant function X plus Y times Z,
in this case is not that giant, but it gets pretty giant with neural networks, we just go step by step.
Look at the first function, simple addition, q equals X plus Y, and the second function, multiplication,
f equals q times Z.
The gradient on X and Y, the partial derivative
of f with respect to X and Y is computed by multiplying
the gradient on the output, negative four, times the gradient on the inputs,
which as we talked about, when the operation is addition, that's just one.
It's negative four times one.
Interpreting Gradients
What does that mean? Let's interpret those numbers. You now have gradients on X, Y and Z
the partial derivatives of F with respect to X, Y, Z. That means,
for X and Y is negative four, for Z is three. That means, in order to make f happy,
we have to decrease the inputs that have a negative gradient
and increase the inputs that have a positive gradient. The negatives ones are X and Y,
the positive is Z.
Hopefully, I don't say the word Beautiful too many times in this presentation this is very simple. Beautifully simple.
Because this gradient is a local worker, it propagates for you;
it has no knowledge of the broader happiness of f.
It computes the greater between the output and the input. And it can propagate this gradient
based on, in this case f, a gradient of one but also the error.
Instead of one we can have on the output the error as the measure of happiness. And then we can propagate that error backwards.
These gates are important because we can break down almost every operation we can think of
that we work within neural networks into one or several gates like these.
The most popular are three, which is addition, multiplication and the Max operation.
For addition,
the process is you take a forward pass through the network, so we have a value on every single gate,
and then you take the backward pass. And through the backward pass you compute those gradients.
For an add gate, you equally distribute the gradients on the output to the input,
when the gradient on the output is negative four, you equally distribute it tonegative four.
And you ignore the forward pass value. That three is ignored when you backpropagate it.
On the Multiply gate, it's trickier. You switch the forward pass values,
if you look at f, that's a Multiply gate,
the forward pass values are switched and multiplied by the value of the gradient in the output.
If it's confusing, go through the slides slowly. It'll make a lot more sense.
Hopefully. One more gate. There's the Max gate, which takes the inputs
and produces as output the value that is larger.
When computing the gradient of the Max gate, it distributes the gradient
similarly to the Add gate, but to only one, to only one of the inputs;
the largest one. unlike the Add gate, pays attention to the input
the input values on the forward pass. All right.
Lots of numbers but the whole point here is, it's really simple;
Modularity Expanded: Sigmoid Activation Function
a neural network is just a simple collection of these gates. You take a forward pass,
you calculate some kind of function in the end, the gradient in the very end, and you propagate that back.
Usually, for neural networks, that's an Error function. A Loss function, Objective function,
a Cost function. All the same word. That's the Sigmoid function there
When you have three weights W zero, W one, W two and X, two inputs, X0, X1,
that's going to be the Sigmoid function. That's how you compute the output
of the neuron. But then you can decompose that neuron you can separate it all into
just a set of gates like this Addition, multiplication, there's an exponential in there and division
but all very similar. And you repeat the exact same process.
there's five inputs, there's three weights and two inputs. X zero, X one.
You take a forward pass through this circuit,
in this case again, you want it to increase so that the gradient of the output is one
and you backpropagate that gradient of one, to the inputs.
Now in neural networks, there's a bunch of parameters that you're trying through this process, modify.
And you don't get to modify the inputs You get to modify the weights along the way,
and the biases. The inputs are fixed, the outputs are fixed, the outputs that you hope
the network will produce. What you're modifying is the weights. So I get to try to adjust those weights
in the direction of the gradient.
Learning with Backpropagation
That's the task of backpropagation.  The main way that neural networks learn.
As we update the weights and the biases to decrease the loss function.
The lower the loss function the better. In this case, you have
three inputs on the top left. A simple network, three inputs.
Three weights on each of the inputs. There's a bias on the node, b and produces an output
a, and that little symbol is indicating a Sigmoid function.
And the loss is computed as Y minus A squared,
divided by two, where Y is the ground truth,
the output that you want the network to produce. And that loss function is backpropagating
in exactly the same way that we described before. The subtasks involved in this update of weights and biases
is that the forward pass computes the network output at every neuron, and finally, the output layer,
computes the error, the difference between a and b, and then
backward propagates the gradients. Instead of one on the output, it will be the error on the output and you backpropagated.
And then, once you know the gradient, you adjust the weights and the biases in the direction of the gradient.
Actually, the opposite of the direction of the gradient, because you want the loss to decrease.
And the amount by which you make that adjustment is called the Learning Rate.
The learning rate can be the same across the entire network or can be individual through every weight.
And the process of adjusting the weights and biases is just optimization.
Learning is an Optimization problem. You have an objective function, and you're trying to minimize it.
And your variables are the parameters, the weights and biases. Neural networks just happen to have
tens, hundreds of thousands, millions of those parameters.
So the function that you're trying to minimize is highly non-linear. But it boils down to something like this, you have
two weights, two plots-- or actually one weight
and as you adjust it, the cost
you adjust in such a way that minimizes the output cost.
And there's a bunch of optimization methods for doing this. this is a convex function,
You can find the local minimum. If you know about these kinds of terminologies,
the local minimum is the same as the global minimum, it's not a weirdly hilly terrain
where you can get stuck in. Your goal is to get to the bottom of this thing and if it's really complex terrain,
it will be hard to get to the bottom of it.
This general approach is gradient descent, and there's a lot of different ways to do a gradient descent.
Various ways of adding randomness into the process, so you don't get stuck into the weird
crevices of the terrain. But it's messy.
You have to be really careful. This is the part you have to be aware of, when you design a network for DeepTraffic
and nothing is happening this might be what's happening:
vanishing gradients or exploding gradients.
When the partial derivatives are small, so you take the Sigmoid function,
the most popular for a while, activation function, the derivative is zero at the tails.
When the input to the Sigmoid functions is really high or really low,
that derivative is going to be zero.
Gradient tells on how much I want to adjust the weights. The gradient might be zero,
and so you backpropagate that zero, a very low number, and it gets less and less
as you backpropagate and so the result is that
you think you don't need to adjust the weights at all. And when a large fraction of the network weights don't need to be adjusted,
they don't adjust the weights. And you are not doing any learning So the learning is slow.
Optimization is Hard: Dying ReLUS
There are some fixes to this, there are different types of functions.
There's a piece, the ReLUs function which is the most popular activation function.
But again, if the neurons are initialized poorly,
this function might not fire. it might be zero gradient
for the entire data set. Nothing that you produce as input,
you run all your thousands of images of cats, and none of them fire at all.
That's the danger here. So you have to pick
Optimization is Hard: Saddle Point
both the optimization engine, the solver that you use
and the activation functions carefully. You can't just plug and play like they're Lego's
You have to be aware of the function. SGD, Stochastic Gradient Descent,
that's the Vanilla optimization algorithm for gradient descent.
For optimizing the loss function over the gradients And what's visualized here is,
again, if you have done any numerical optimization, and non-linear optimization,
there's the famous saddle point, that's tricky for these algorithms to deal with.
What happens is, it's easy for them to oscillate, get stuck in that saddle and oscillating back and forth
as opposed to what they want to do which is go down into-- You get so happy that you found this
low point that you forget there's a much lower point. So you get stuck with the gradient.
The momentum of the gradient keeps rocking it back and forth without you going to a much greater global minimum.
And there's a lot of clever ways to solving that, the Atom optimizer is one of those.
Learning is an Optimization Problem
But in this case, as long as the gradients don't vanish
SGD, the Stochastic Gradient Descent, one of these algorithms will get you there It might take a little while, but it will get you there
Yes, question. The question was,
you're dealing with a function that is not convex, how do we ensure anything about
converging to anything that's reasonably good, the local optimum converges to--
The answer is, you can't. This isn't only a non-linear function
it's a highly non-function The power and the beauty of neural networks
is that it can represent these arbitrarily complex functions.
It's incredible. And it can learn these functions from data
But the reason people are referring to neural networks training as art
is you're trying to play with parameters that don't get stuck in these local optimal.
For stupid reasons and for clever reasons. Yes, question.
The Question continues on the same thread.
The thing is, we're dealing with functions where we don't know what the global optimal is.
That's the crocs of it. Everything we talked about,
interpreting text, interpreting video, even driving.
What's the optimal for driving? Never crashing?
It sounds easy to say that, you actually have to formulate the world under which it defines all of those things and it becomes a really
non-linear objective function for which you don't know what the optimal is.
That's why you keep trying and get impressed every time it gets better. It is essentially the process.
And you can also compare, you can compare with human-level performance. For ImageNet,
who can tell the difference between cats and dogs, and top five categories,
96% of the time accuracy, and then you get impressed when a machine can do better than that.
But you don't know what the best is.
These videos can be watched for hours, I won't play it until I explain this slide.
Let's pause to reflect on backpropagation before I go on to Recurrent Neural Networks. Yes, question.
In this practical manner, how can you tell when you're actually creating a net whether you're facing the management gradient problem
or you need to change your optimizer
or you've reached a local minimum? The question was,
how do you practically know when you hit the vanishing gradient problem?
The vanishing gradient could be--
Optimization is Hard: Vanishing Gradients
The derivative being zero on the gradient, happens when the activation is exploding,
like really high values and really low values. To really high values is easy. Your network has just gone crazy.
It produces very large values. And you can fix a lot of those things by just capping the activations.
The values being really low, resulting in a vanishing gradient, are really hard to detect
There's a lot of research in trying to figure out how to detect these things.
If you're not careful, often times you can find that,
and this isn't hard to do, we're like 40 or 50 percent of the network, of the neurons,
are dead. We will call it, for ReLU, they're dead ReLU
They're not firing at all. How do you detect that? That's part of learning
If they never fire you can detect that by running it through the entire training set. There are a lot of tricks. But that's the problem.
You try to learn and then you look at the loss function and it's not
converging to anything reasonable. They are going all over the place, or just converging very slowly.
And that's an indication that something is wrong That something could be the loss function is bad, that something could be you already found the optimal,
or that something could be the vanishing gradient. And again, that's why it's an art.
Certainly, at least some fraction of the neurons needs to be firing.
Otherwise, initialization is really poorly done. Okay, to reflect on the
Reflections on Backpropagation
simplicity of backpropagation and the power of it,
this kind of step of backpropagating the loss function to the gradients locally,
is the way neural networks learn. It's really the only way
that we have effectively been able to to train a neural network
network to learn a function. To adjusting the weights and biases, the huge number of weights and biases, the parameters
It's just through this optimization. It's backpropagating the error, where you have the supervised ground truth.
the question is whether this process, of fitting,
adjusting the parameters of a highly non-linear function to minimize a single objective,
is the way you achieve intelligence.
Human-level intelligence. That's something to think about. You have to think about, for driving purposes,
what is the limitation of this approach? What's not happening? The neural network designed, the architecture
is not being adjusted. any of the edges, the layers, nothing is being evolved
There are other optimization approaches that I think are more
interesting and inspiring than effective. For example, this is
using soft cubes to-- This is falling out of the field
of evolutionary robotics. Where you evolve
the dynamics of a robot using genetic algorithms and that's
These robots have been taught to, in simulation, obviously,
to walk and to swim. That one is swimming.
The nice thing here is that dynamics that highly non- linear space as well,
that controls the dynamics of this weird shaped robot with a lot of degrees of freedom,
it's the same kind of thing as the neural network. In fact, people have applied generic algorithms,
ant colony optimization, all kinds of sort of nature inspire algorithms for automatizing the weights and the biases
but they don't seem to currently work that well. It's a cool idea to be using
nature-type evolutionary algorithms to evolve something that's already nature inspired which is neural networks.
But, something to think about the backpropagation, while really simple
it's kind of dumb and the question is whether general intelligence reasoning can be achieved with this process.
All right, Recurrent Neural Networks, on the left there's an input X
Unrolling a Recurrent Neural Network
with weights on the input, U, there's a hidden state, hidden layer S,
with weights on
the edge connecting the hidden states to each other and then more weights, V, the on the output O.
It's a really simple network, there's inputs, there's hidden states,
the memory of this network and there's outputs.
But the fact that there's this loop where the hidden states are connected to each other
means that as opposed to producing a single input, the network takes arbitrary numbers of inputs,
it just keeps taking X, one at a time and produces a sequence of Xs
through time. Depending on
the duration of the sequence you're interested in, you can think of this network in its unrolled state.
You can unroll this neural network where the inputs are in the bottom, Xt-1, Xt, Xt+1,
and same with the outputs, Ot-1, Ot, Ot+1,
and it becomes like a regular neural network, unrolled some arbitrary number of times.
RNN Observations
The parameters, again, there's weights, there's biases, similar to CNNs,
convolutional neural networks and just like convolutional neural networks make certain spatial consistency assumptions,
the recurrent neural network assume temporal consistency amongst the parameters,
shares the parameters. That W, that U, that V, is the same for every single time step.
You're learning the same parameter, no matter the duration of the sequence
and that allows you to look at arbitrary long sequences
without having an explosion of parameters. 
This process is the same exact process that's repeated base on the different variants that we talk about before,
in terms of inputs and outputs, one to many, many to one, many to many.
Backpropagation Through Time (BPTT)
The backpropagation process is exactly the same as for regular neural networks.
It's a fancy name of backpropagation through time, BPTT,
but it's just backpropagation through an unrolled
recurrent neural network, where the errors are on the computed on the outputs,
the gradients are computed, backpropagated
and computed on the inputs, again, suffering for the same exact problem
of vanishing gradients. The problem is that the depth of these networks can be arbitrary long
if at any point the gradients hits a lower number, zero,
becomes, that neural becomes saturated. That gradient, let's call it saturated,
that gradient gets-- drives all the earlier layer to zero,
so is easy to run to a problem where you're really ignoring the majority of the sequence.
This is just another Python weight, sudo-called weight to look at it.
Is you have the same w, remember you're sharing the weights and all the parameters from time to time,
so if the weights are such WHH,
if the weights are such that they produce [unintelligible]
they have a negative value that results in the gradient that goes to zero,
that propagates through the rest. That's the sudo-call for backpropagation,
pass to the RNN, that WHH propagates back.
Gradients Can Explode or Vanish Geometric Interpretation
You get this things with exploding and vanishing gradients
for example, error surfaces for a single hidden unit RNN, this is visualizing the gradient,
the value of the weight, the value of the bias and the error, the error could be really flat or could explode,
both are going to lead to you not making--
either making steps that are too gradual or too big. It's the geometric interpretation.
RNN Variants: Bidirectional RNNS
Okay. What other variants that we look at, a little bit? are they [unintelligible 00:41:13]? It doesn't have to be only one way,
it can be bi-directional, that could be edges going forward and edges going back
What that's needed for is things like filling in missing, whatever the data is,
filling in missing elements of that data, whether that's images, or words, or audio.
Generally, as always is the case in neural network, the deeper it goes, the better.
That deep referring to the number of layers in a single temporal instance.
On the right of the slide we're stacking
node in the temporal domain. Each of those layers has its own set of weights,
its own set of biases. These things are awesome but they need a lot of data
Long-Term Dependency
when you add extra layers in this way. The problem is, while recurrent neural network,
in theory, is supposed to be able to learn any kind of sequence, the reality is they're not really good at remembering
what happened a while ago, the long-term dependency. Here's a silly example,
let's think of a story about Bob, Bob is eating an apple.
The apple part is generated by the recurrent neural network.
Your recurrent neural networks can learn to generate "apple" because it's seen in a lot of sentences, with "Bob" and "eating"
and it can generate the word apple. For a longer sentence, like
"Bob likes apples, he's hungry and decided to have a snack, so now he's eating an apple",
you have to maintain the state that we're talking about Bob and we're talking about apples,
through several discreet semantic
sentences. That kind of long-term memory is not--
because of different effects, but vanishing gradients,
it's difficult to propagate the important stuff that happened a while ago in order to maintain that context
in generating "apple", or classifying some concept that happened way down the line. 
When people talk about recurrent neural networks
Long Short Term Memory (LSTM) Networks
these days, they're talking about LSTMs, long-short-term memory networks
so all the impressive results results on time series and audio and video and all that, that requires LSTMs.
Again, vanilla RNNs are on top of the slide,
each cell is simple, there are some hidden units, there's an input, and there's an output.
Here, we used TANH as activation function,
it's just another popular Sigmoid type activation function.
LSTMs are more complicated, or they look more complicated but
in some ways, they're more intuitive for us to understand. There's a bunch of gates in each cell,
we'll go through those. In yellow are different neural network layers,
Sigmoid and TANH, are just different types of activation functions.
TANH is an activation function that squishes the input into the range of negative one to one.
Sigmoid function squishes it between zero and one and that serve different purposes.
There's some pointwise operations, addition, multiplication,
and there's connections, data being passed from layer to layer,
shown by the arrows. There's concatenation and there's a copy operation on the output
We copy, the output of each cell it's copied to the next cell and to the output.
LSTM: Gates Regulate
Let me try to make it, clarified,
LSTM: Pick What to Forget and What To Remember
clarify a little bit. There's this conveyer belt
going through inside of each individual cell and they all have, there's really three steps in the conveyer belt.
The first is, there is a Sigmoid function
that's responsible for deciding
what to forget and what to ignore, it's responsible for
taking in the input, the new input, x(t), taking in the state of the previous,
the output of the previous cell, previous time step and deciding "do I want to keep that in my memory or not?"
and "do I want to integrate the new input into my memory or not?" This allows you to
selective about the information which you learn. For example, there's that sentence "Bob and Alice are having lunch,
Bob likes apples, Alice like oranges, she is eating an orange".
Bob and Alice are having lunch, Bob likes apples, right now, if you had said you have a hidden state,
keeping track of the gender of the person we're talking about
you might say that there's both genders on the first sentence, there's male in the second sentence,
female in the third sentence, and that way when you have to generate a sentence about who's eating what,
you'll know- you keep the gender information in order to make an accurate generation of text
corresponding to the proper person. You have to forget certain things,
like forget that Bob existed at that moment, you have to forget Bob likes apples
but you have to remember that Alice likes oranges so you have to selectively remember and forget certain things
that's LSTM in a nutshell. You decided what to forget, decided what to remember
and decided what to output in that cell.
LSTM Conveyer Belt
Zoom in a little bit, because this is pretty cool There's a state running through the cell,
this conveyer belt, previous state like the gender
that we're currently talking about, that's the state that you're keeping track of
and that's running through the cell. Then there's three Sigmoid layers
outputting one, a number between the zero and one,
one when you want that information to go through and zero when you don't want it to go through,
the conveyer belt that maintains the state.
First, Sigmoid function is, we decided what to forget and what to ignore,
that's the first one, we take the input from the previous time step, the input to the network
on the current time step and decided, do I want to forget or do I want to ignore those?
Then we decided which part of the state to update,
what part of our memory do we have to update with this information and what values to insert in that update.
Third step is, we perform the actual update and perform the actual forgetting,
that's why you have the Sigmoid function, you just multiply it,
when is zero is forgetting, when is one that information passes through.
Finally, we produce an output from the cell,
if its translation is producing an output in the English language
where the input was in Spanish language and then that same output
it's copied to the next cell.
Application: Machine Translation
What can we get done with this kind of approach? We can look at machine translation.
I guess what I'm trying to-- question. what is your representation of this state?
Is it like a floating point or is it like a vector or what is it, exactly?
The state is the activation
multiplied by the weight, it's the output of the Sigmoid or the TANH activations.
There's a bunch of neurons and they're firing a number between negative one or one, or between zero and one,
that whole's a state. It just that calling it a state it's sort of simplifying, but the point is that there's
a bunch of numbers been constantly modified by the weights and the biases,
those numbers hold the state and the modification of those numbers
is controlled by the weights and then once all of that is done,
the resulting output of the recurrent neural network it's compared to the desired output
and the errors are backpropagated to the weights.
Hopefully, that makes sense.  So, machine translation is one popular application
all of it is the same, all of these networks that I've talked about,
they're really similar constructs. You have some inputs,
whatever language that is again, German maybe, I think everything is German,
and the output. The inputs are in one language, a set of characters
composed a word in one language, there's a state being propagated and once that sentence is over,
you start, as opposed to collecting inputs, start producing outputs and you can output in the English language.
Application: Handwriting Generation from Text
There's a ton of great work on machine translations. It's what Google is supposedly using for their translator,
same thing. I've show this previously but now you all know how it works,
same exact thing, LSTMs generating handwritten characters,
handwriting in arbitrary styles, controlling the drawing,
where the input is text and the output is handwriting. Is again, the same kind of
network with some depths here, the input is the text,
the output is the control of the writing. Character-level text generation,
Application: Character-Level Text Generation
this is the thing that taught us about life, the meaning of life,
literary recognition and the tradition of ancient human reproduction. That's again, the same process,
input one character at the time, what we see there is the encoding of the characters on the input layer,
there's a hidden state, hidden layer that is keeping track of those activations,
the outputs of the activation functions and every single
time it's outputting its best prediction
of the next character that follows. Now, on a lot of these applications you want to ignore the output
until the input sentence is over and then you start listening to the output,
but the point is that it just keeps generating text, whether is given an input or not, so you producing input
is just adding, steering the recurrent neural network. You can answer questions
Application: Image Question Answering
about an image, the input you get there, you could almost arbitrary stack things together,
you take an image as your input, bottom left there, put it in your convolutional neural network,
and take the question. There's something call word embeddings,
it's to broaden the representative meaning of the words. "How many books?" is the question.
You want to take the word embeddings and the image and produce your best estimate of the answer.
For question of "what color is the cat?" it could be gray or black, it's the different LSTM flavors
producing that answer. Same with counting chairs you can give an image of a chair
and as the question "how many chairs are there?" And it can produce an answer of "three".
I should say this is really hard, arbitrary question asks an arbitrary image,
you are both interpreting-- you are doing natural languages processing and you're doing computer vision, all in one network.
Application: Image Caption Generation
Same thing with the image capture generation, you can detect
the different objects in the scene, generate those words, stitch them together in syntactically correct sentences
and rearrange the sentences. All of those are LSTMs, the second and the third step,
the first is computer vision detecting the objects, segmenting the image and detecting the objects,
that way you can generate a caption that says "a man is sitting in a chair with a dog in his lap".
Application: Video Description Generation
Again, LSTMs for video. Caption generation for video,
the input, and every frame it's an image that goes into the LSTM,
the input is an image and the output is a set of characters.
First, you load in the video, in this case the output is on top, you encode
the video into a representation inside the network and then you start generating words
about that video. First comes the input, the encoding stage, then the decoding stage.
Take in the video, say a man is taking, talking, whatever
and because the input and the output are arbitrary, there also has to be indicators of the beginnings and
the ends of a sentence, in this case, end of sentences. You want to know when you stop
Application: Modeling Attention Steering
in order to generate syntactically correct sentences. that indicates the end of a sentence. You want also to be able to generate a period
You can also, again, recurrent neural networks, LSTMs here, controlling
the steering of a sliding window on an image
that is used to classify what is contained in that image. Here, a CNN being steered by a recurrent neural network
in order to convert this imagen into the number that's associated with a house number,
Application: Drawing with Selective Attention Writing
it's called visual attention. That visual attention can be used to steer for the perception side
and it can be used to steer a network for the generation. On the right, we can generate an image as--
So the output of the network-- it's a LSTM where the output on every time step
is visual, and this way you can draw numbers.
Application: Adding Audio to Silent Film
Here, I mention this before,
is taking in as input silent video, sequence of images
and producing audio. This is
an LSTM that has convolutional layers for every single frame,
takes images as input and produces
a spectrogram, audio as output.
The training set is a person hitting an object with a drumstick and your task is to generate, given a silent video,
generate the sound that the drumstick will make when in contact with that object.
Application: Medical Diagnosis
Medical diagnosis, that's actually-- I've listed some places where it has been really successful
and pretty cool, but it's also beginning to be applied in places where
can actually really help
civilization, in medical applications. For medical diagnosis
there's the highly spars and
variable lengths sequence of information in the form of,
for example, patient electronic health records. So, Every time you visit a doctor, there's a test being done, that information is there
and you can look it as a sequence over a period of time and then given that data, that's the input,
the output is the diagnosis, a medical diagnosis,
in this case, we can look at predicting diabetes, scoliosis, asthma and so on,
with pretty good accuracy. There's something that
all of us wish we could do, is stock market prediction.
You can input, for example, well first of all, you can input the raw stock data,
[unintelligible 01:00:30] books and so on, financial data, but you can also look at news articles from all over the web
and take those as input as shown here, on the X axis is time, articles from different days,
LSTM, once again, and produce an output of your prediction,
binary prediction, whether the stock would go up or down. Nobody has been able to really successfully do this
but there is a bunch of results and trying to perform above random
which is how you make money, significantly above random
on the prediction of it's going up or down? So you could buy or sell and especially
when there is-- in the cases when there was crashes it's easier to predict,
so you can predict an encroaching crash. These are shown in the table, the error rates from different stocks,
automotive stocks. You can also generate audio,
is the exact same process as it generates language, you generate audio. Here's trained on
a single speaker, a few hours epics
of them speaking and you just learn, that's raw audio of the speaker
and it's learning slowly to generate [audio]
Obviously, they were reading numbers.
this is incredible, this is trained on a compress spectrogram of the audio, raw audio
and is producing something that over just a few epics is producing something that sounds like words,
it could do this lecture for me, I wish.
This is amazing, this is raw input, raw output,
all again, LSTMs, and there's a lot of work in voice recognition,
audio recognition. You're mapping--
let me turn it up. You are mapping any kind of audio to a classification,
you can take the audio of the road
and that's the spectrogram on the bottom there, being shown you could detect whether the road is wet
is wet or the road is dry.
you could do the same thing for recognizing the gender of the speaker
or recognizing many to many map of the actual words being spoken,
speech recognition. This is about driving, so let's see where recurrent neural| networks apply in driving.
We talked about the NVIDIA approach, the thing that actually powers DeepTeslaJS,
it is a simple convolutional neural network, there's five convolutional layers
in their approach, three fully connected layers, you can add as many layers as you want in DeepTesla,
that's a quarter of million parameters to optimize
all you are taking is a single image, no temporal information, single image and producing the steering angle, that's the approach,
that's the DeepTesla way,
taking a single imagen image and learning a regression of the steering angle.
One of the prizes for the competition is the Udacity, self-driving
car engineer nanodegree for free, this thing is awesome,
I encourage everyone to check it out, but they did a competition
that's very similar to ours, but a very large group of obsessed people,
they were very clever, they went beyond convolutional neural networks of predicting steering,
taking a sequence of images and predicting steering, what they did is, the winners,
at least the first and I'll talk about the second place winner tomorrow,
on 3D convolutional neural networks, the first and the third place winners used RNNs,
used LSTMs, recurrent neural networks and map a sequence of images
to a sequence of steering angles. For anyone, statistically speaking,
anybody here who is not a computer vision person, most likely what'd you want to use, for whatever application
you're interested in, is RNNs, the world is full of time series data,
very few of us are working on data that is no time series data,
in fact, whenever it's just snapshots, you're really just reducing the problem to
the size that you can handle but most data in the world is time series data.
This is the approach you end up using if you want to apply it in your own research,
RNNs is the way to go.
Again, what are they doing? How do you put images
into a recurrent neural network? it's the same thing, you take,
you have to convert an image into numbers in some kind of way, a powerful way of doing that is convolutional neural networks,
so you can take either 3D convolutional neural networks
or 2D convolutional neural networks once it takes time into consideration and whatnot,
process that image to extract a representation of that image and that becomes the input to the LSTM
and the output at every single cell, at every single timestep, is a predicted steering angle,
the speed of the vehicle and the torque that's what the first place winner did, they didn't just do the steering angle,
also did the speed and torque and the sequence length that they were using
for training and for testing, for the input and the output, is a sequence length of 10 
did they used supervised learning or did they used reinforcement learning? The question was, did they used supervised learning?
Yes, they were given the same thing as in DeepTesla, a sequence of frames where the have a sequence of
steering angles, speed and torque, I think there's other information too available,
there's no reinforcement learning here. Question.  Do you have a sense of how much information
is being passed, how many LSTM gates are there in this problem?
The question was, how many LSTM gates are in this problem?
This network,
it's true that this diagrams kind of hide the number of parameters here, but it's arbitrary
just like convolutional neural networks are arbitrary, the size of the input is arbitrary,
the size of Sigmoid function, TANH is arbitrary, so you can make it as large as you want, as deep as you want
and the deeper and larger, the better.  What these folks actually used--
the way these competitions work and I encourage you, if you're interested in machine learning
to participate in Kaggle, I don't know how to pronounce it, competitions
where basically everyone is doing the same thing, you're using LSTMs or if it's one- on-one mapping,
using convolutional neural network fully connecting networks with some clever pre-processing
and the whole job is that takes months and you probably, if you're a researcher, that's what you'd be doing your own research,
playing with parameters, playing with pre-processing of the data, playing with the different parameter that controls the size of the network
the learning rate, I've mentioned, this type of optimizer, all these kinds of things, that's what you're playing with,
using your own human intuition and you're using your--
whatever probing you can do in monitoring the performansce of the network through time.
Yes?
The question was, you said that there's a
memory of tenth in this LCM, and I thought RNNs are supposed to be arbitrary.
It has to do with the training, how the network is trained.
It's trained with sequences of 10. The structure is still the same, you only have one cell that's looping onto each other.
But the question is, in what chunks, what is the size of the sequence
that we should do in the training and then the testing. It can be arbitrary length.
It's just usually better to be consistent and have a fixed length.
You're not stacking 10 cells together. It's just a single cell still.
The third-place winner, Team Chauffeur,
used something called transfer learning and it's something I don't think I mentioned
but it's kind of implied, the amazing power of neural networks.
First, you need a lot of data to do anything. That's the cost, that's the limitation in neural networks.
But what you could do is, there's
neural networks that have been trained on very large data sets. ImageNet,
Vdg Net, AlexNet, ResNet, all these networks are trained on a huge amount of data.
Those networks are trained to tell the differences between a cat and dog Specific optical recognition
of single images. How do I then take that network and apply it to my problem,
say of driving or length detection, or medical diagnosis, or cancer or not?
The beauty of neural networks,
the promise of transfer learning, is that you can just take that network, chop off the final layer,
the fully connected layer that maps from all those cool high-dimensional features that you have learned about visual space,
and as opposed to predicting cat vs. dog, you teach it to predict cancer or no cancer.
You teach it to predict lane or no lane, truck or no truck.
As long as the visual space under which that network operates is similar or the data like if it's audio or whatever
if it's similar, if the features are useful then you learn, in studying the problem of cat vs dog deeply,
you have learned actually how to see the world. As you're going to apply that visual knowledge,
you can transfer that learning to another domain. That's the beautiful power of neural networks
it's that they're transferable. What they did here is--
I didn't spend enough time looking through the code I'm not sure which of the giant nework they took
but they took a giant convolutional neural network, they chopped off
the end layer, which produced 3000 features, and they took those 3000 features
to every single image frame, and that's the Xt. They gave that as the input to LSTM.
And the sequence length, in that case, was 50. This process is pretty
similar across domains. That's the beauty of it. The art of neural networks is in the--
Well that's a good sign [chuckles], I guess I should warp it up--
The art of the neural networks is in the proper parameter tuning.  That's the tricky part, and that's the part you can't be taught.
That's experience, sadly enough. That's why they talk about
Stochastic Gradient Descent SGD, That's what Geoffrey Hinton
refers to as Stochastic Graduate Student Descent,
meaning you just keep hiring graduate students to play with the hyperparameters until the problem is solved
[laughter].
I have about 100+ slides on driver state, which is the thing that I'm most passionate about,
and I think will save the best for last. I'll talk about that tomorrow. We have a guest speaker
from the White House, will talk about the future of Artificial Intelligence from the perspective of policy,
and what I would like you to do first off you registered students is submit the two tutorial assignments,
and pick up can we just set the boxes right here or something?
Just stop by and pick up a shirt. And give us a card on the way.
Thanks guys. [Applause]

----------

-----

--17--

-----
Date: 2017.01.25
Link: [# MIT 6.S094: Convolutional Neural Networks for End-to-End Learning of the Driving Task](https://www.youtube.com/watch?v=U1toUkZw6VI)
Transcription:

Alright, welcome back everyone. Sound okay? Alright.
So today we will- We talked a little bit about neural networks, started to talk about neural networks yesterday.
Today we'll continue to talk about neural networks that work with images, convolutional neural networks,
and see how those types of networks can help us drive a car. If we have time we'll cover a simple illustrative case study
Illustrative Case Study: Traffic Light Detection
of detecting traffic lights. The problem of detecting green, yellow, red. If we can't teach our neural networks to do that, we're in trouble,
but it's a good, clear, illustrative case study of a three-class classification problem. Okay, next there's
Deep Tesla: End-to-End Learning from Human and Autopilot Driving
DeepTesla here looped over and over in a very short GIF. This is actually running live in a website right now.
We'll show it towards the end of the lecture, this once again just like DeepTraffic is a neural network that learns to
steer a vehicle based on the video of the forward road way. And once again, doing all of that in the browser using javascript.
So you'll be able to train your own very network to drive using real world data.
I'll explain how. We will also have a tutorial and code. Briefly described today at the end of the lecture,
if there's time how to do the same thing in TensorFlow. So if you want to build a network that's bigger, deeper and you want to utilize GPUs to train that network,
you want to not do it in your browser, you want to do it offline using TensorFlow
and having a powerful GPU on your computer and we'll explain how to do that. Computer vision.
Computer Vision is Machine Learning
So we talked about vanilla machine learning where there's no- Where the size, yesterday,
where the size of the input is small for the most part. The number of neurons, in the case the neural networks, is on the order of 10, 100, 1,000.
When you think of images, images are a collection of pixels, one of the most iconic images from computer vision
on the bottom left there is Lenna. I encourage you to Google it and figure out the story behind that image. It's quite shocking when I found out recently.
So once again, computer vision is, these days, dominated by data driven approaches by machine learning
where all of the same methods that are used on other types of data are used on images where the input is just
a collection of pixels and pixels are numbers from 0 to 255 discrete values.
So we can think exactly what we've talked about previously, you could think of images in the same exact way. It's just numbers
and so we can do the same kind of thing. We could do supervised learning where you have an input image
and output label. The input image here is a picture of a woman; the label might be "woman".
On supervised learning, same thing. We'll look at that briefly as well as clustering images into categories.
Again semi-supervised and reinforcement learning. In fact, the Atari games that talked about yesterday.
do some pre-processing on the images. They're doing computer vision; they're using convolutional neural networks as we'll discuss today
and the pipeline for supervised learning is again the same: there's raw data in the form of images,
there's labels on those images. We perform a machine learning algorithm, performs feature extraction,
it trains given the inputs and outputs on the images and the labels of those images, constructs the model
and then test that model. And we get a metric and accuracy. Accuracy is the term that's used to often describe how well the model performs.
The percentage.
Images are Numbers
I apologise for the constant presence of cats throughout this course. I assure you this course is about driving, not cats.
but images are numbers. So for us we take it for granted.
We're really good at looking and converting visual perception as human beings, converting visual perception, into semantics.
We see this image and we know it's a cat but a computer only sees numbers: RGB values for a color image.
There's three values for every single pixel from 0 to 255.
And so given that image, we can think of two problems: one is regression and the other is classification. Regression is when given an image
we want to produce a real value of output put back. So if we have an image of the four roadway,
we want to produce a value for the steering wheel angle and if you have an algorithm that's really smart,
It can take any image of the forward roadway and produce the perfectly correct steering angle that drives the car safely across the United States.
We'll talk about how to do that and where that fails. Classification is when the input again is an image
and the output is a class label, a discrete class label. Underneath it though often is still a regression problem
and once produced is a probability that this particular image belongs to a particular category.
And we use a threshold to chop off the outputs associated with low probabilities
and take the labels associated with high probabilities and convert it into a discrete classification.
Computer Vision is Hard
I mentioned this yesterday but it bears saying again, computer vision is hard.
We, once again, take it for granted. As human beings, we're really good at dealing with all these problems.
There's viewpoint variation: the object looks wholly different in terms of the numbers behind the images
in terms of the pixels when viewed from a different angle. Viewpoint variation: objects when you're standing far away from them or up close are totally different size.
We're good at detecting that there are different size. It's still the same object as human beings but that's still a really hard problem because those sizes can vary drastically.
We talked about occlusions and deformations with cats; well understood problem. There's background clutter.
You have to separate the object of interest from the background and given the three dimensional structure of our world.
There's a lot of stuff often going on in the background: the clutter, their inter-class variation.
That's often greater than inter-class variation; meaning objects of the same type often have more variation
than the objects that you're trying to separate them from. There is the hard one for driving: illumination.
Light is the way we perceive things; the reflection of light off the surface
and the source of that light changes the way that object appears and we have to be robust to all of that.
Image Classification Pipeline
So the image classification pipeline is the same as I mentioned. There are categories,
It's the classification problems for those categories of cat, dog, mug, hat.
You have a bunch of examples, image examples of each of those categories and so the input is just those images paired with the category.
And you train to map, to estimate a function that maps from the images to the categories.
Famous Computer Vision Datasets
For all of that you need data; a lot of it. There is, unfortunately, a growing number of data sets but there are still relatively small.
We get excited. There are millions of images but they're not billions or trillions of images and these are,
the data sets that you will see if you read academic literature most often. Mnist, the one that's been beaten to death.
And then we use as well in this course the data set of handwritten digits where the categories are 0 to 9.
ImageNet, one of the largest image data sets; fully labeled image data sets in the world has images with a hierarchy of categories from Word Net.
And what you see there is a labeling of what image is associated with which words are present in the data set.
CIFAR-10 and CIFAR-100 are tiny images that are used to prove in a very efficient and quick way
offhand that your algorithm that you're trying to publish on, or trying to impress the world with, works well.
It's small, it's a small data set: CIFAR-10 means there's 10 categories.
And places is a data set of natural scenes: woods, nature, city, and so on.
Let's Build an Image Classifier for CIFAR-10
So let's look at CIFAR-10 as a data set of 10 categories: airplane, automobile, bird, cat, and so on.
They're shown there with sample images as the rose. And so let's build a classifier that's able to take images from one of these 10 categories and tell us what
is shown in the image. So how do we do that? Once again, all the algorithm sees is numbers.
So we have to try to have at the very core, we have to have an operator for comparing two images.
So given an image and I want to save it as a cat or dog. I want to compare it to images of cats and compare it to images of dogs and see which one matches better.
So there has to be a comparative operator. Okay so one way to do that is take the absolute difference between the two images
pixel by pixel, take the difference between each individual pixel shown on the bottom of the slide for a 4x4 image. And then we sum that pixel-wise
pixel-wise absolute difference into a single number. So if the image is totally different pixel-wise,
that will be a high number. If it's the same image, the number will be 0. Oh, it's the absolute value too of the difference.
And that's called L1 distance. It doesn't matter. When we speak of distance, we usually mean L2 distance.
And so, if we try to- So we can build the classifier that just uses this operator to compare it to every single image in the data set
and say I'm going to pick the, I'm going to pick the category
that's the closest using this comparative operator. I'm going to find- I have a picture of a cat
and I'm going to look through the dataset and find the image that's the closest to this picture
and say that is the category that this picture belongs to. So if we just flip the coin and randomly pick which category an image belongs to get that accuracy,
would be on average 10%. It's random. The accuracy with which our brilliant image difference algorithm that just goes through the data set
and finds the closest one is 38% which is pretty good, it's way above 10%.
K-Nearest Neighbors: Generalizing the Image-Diff Classifier
So you can think about this operation of look into the base and finding the closest image as what's called K-Nearest Neighbors
or K in that case. Meaning you find the one closest neighbor to this image that you're asking questions about
and accept the label from that image. You could do the same thing increasing K.
Increasing K to 2 means you take the two nearest neighbors. You find the two closest in terms of pixel-wise image difference through this particular query image
and find which categories did those belong to. What's shown up top on the left is the data set we're working with: red, green, blue.
What's shown in the middle is the one nearest neighbor classifier, meaning
this is how you segment the entire space of different things that you can compare.
And if a point falls into any of these regions, it will be immediately associated with the nearest neighbor algorithm to belong to that image, to that region.
With the five nearest neighbors, there's immediately an issue. The issue is that there is white regions.
There's tie breakers where your five closest neighbors are from various categories. So it's unclear where you belong to.
So this is a good example of parameter tuning. You have one parameter: K.
And your task as a teacher of machine learning, you have to teach this algorithm how to do your learning for you,
is to figure out that parameter. That's called "parameter tuning" or "hyper-parameter tuning" as it's called in neural networks.
And so on the bottom right of the slide on the x-axis is K. As we increase it from 0 to 100 and
the y-axis is classification accuracy. It turns out that the best K for this data set is 7, 7 years neighbors.
With that we get a performance of 30% human level performance
and I should say that the way we get that number as we do with a lot of the machine learning pipeline
process is you separate the data into the parts of days that you use for training
and another part they use for testing. You're not allowed to touch the testing part. That's cheating.
You construct your model of the world on the training data set and you use what's called cross validation
where you take a small part of the training data shown "fold five" there in yellow to leave that part out from
the training and then use it as part of the hyper-parameter tuning. As you train, figure out with that yellow part fold five
how well you're doing and then you choose a different fold and see how well you're doing
And keep playing with parameters never touching the test part. And when you're ready, you run the algorithm on a test data to see how well you really do. How will it really generalizes. Yes, question.
(INAUDIBLE QUESTION) So, the question was: "is there a good way to-
Is any good intuition behind what a good K is?" There are general rules for different data sets
but usually you just have to run through it. Grid search, brute force. Yes, question.
(INAUDIBLE QUESTION) (CHUCKLING) Good question. Yes. (INAUDIBLE QUESTION)
Yes, the question was: "is each pixel 1 number or 3 numbers?" For majority of computer vision throughout its history used grayscale images so it's 1 number but RGB
is 3 numbers and there's sometimes a depth value too, so it's 4 numbers. So it's-
If you have a stereo vision camera that gives you the depth information of the pixels, that's a fourth and then if you stack two images together there could be 6. In general,
everything we work with will be 3 numbers for a pixel.
Yes, so the question: "as to the absolute value is just one number?" Exactly right. So in that case, those are grayscale images.
So it's not RGB images. So, you know, this algorithm is pretty good if we use the best.
We optimize the hyper-parameters of this algorithm, choose K of 7,
seems to work well for this particular CIFAR-10 data set. Okay, we get 30% accuracy.
It's impressive, higher than 10%. Human beings perform at about 94, slightly above 94%
accuracy for CIFAR-10. So given an image and it's a tiny image. I should clarify it, it's like a little icon.
Given that image, human beings are able to determine accurately one of the 10 categories with 94% accuracy.
And the currently state-of-the-art convolutional neural networks is ninety five, it's 95.4% accuracy and, believe it or not, it's a heated battle
but the most important, the critical fact here, is it's recently surpassed humans.
And certainly surpass the k-nearest neighbors algorithm. So,how does this work? Let's briefly look back.
Reminder: Weighing the Evidence
It all still boils down to this little guy: the neuron, that sums the weights of its inputs, adds a bias, produces an output based on an activation, a smooth activation function.
Yes, question. (INAUDIBLE QUESTION)
Reminder Classify and Image of a Number
The question was: "do you take a picture of Cassie, you know it's a cat, but that's not encoded anywhere, like you have to write that down somewhere.
So you have to write as a caption: "This is my cat." And then the unfortunate thing, given the internet and how woody it is, you can't trust the captions on images.
because maybe you're just being clever and it's not a cat all, it's a dog dressed as a cat. Yes, question.
(INAUDIBLE QUESTION) Sorry. Seen as do better than what?
Yes, so the question was: "do convolutional neural networks generally do better than nearest neighbors? There's very few problems on which neural networks don't do better, yes ,they almost always do better
except when you have almost no data. So you need data. And convolutional neural networks isn't some special magical thing.
It's just neural networks with some cheating up front that I'll explain, some tricks to try to reduce the size
and make it capable to deal with images. So again. Yes, the input is, in this case that we looked at classifying an image of a number,
as opposed to doing some fancy convolutional tricks. We just take the the entire 28x28
pixel image that's 784 pixels as the input.
That's 784 neurons in the input, 15 neurons on the hidden layer and 10 neurons in the output.
Now everything we'll talk about has the same exact structure. Nothing fancy.
Reminder: "Learning" is Optimization of a Function
There is a forward pass through the network where you take an input image and produce an output classification
and there's a backward pass through the network for Back Propagation where you adjust the weights
when your prediction doesn't match the Ground Truth output.
And learning just boils down to optimization; it's just optimizing a smooth function.
Differentiable function; that's defined as the lost function.
That's usually as simple as a squared error between the true output
and the one you actually got. So what's the difference? What are convolutional neural networks?
Convolutional neural networks take inputs that have some spatial consistency, have some meaning to the spatial-
Has some spatial meaning in them like images. There's other things, you can think of the dimension of time.
And you can input audio signal into a convolutional neural network. And so the input is, usually for every single layer,
that's a convolutional layer, the input is a 3D volume and the output is a 3D volume.
I'm simplifying because you can call it 4D too but it's 3D. There's height, width and depth.
So that's an image. The height and the width is the width and the height of the image. And then the depth for grayscale image is 1; for an RGB image is 3;
for a ten-frame video of greyscale images the depth is 10.
It's just a volume, a three-dimensional matrix of numbers. And everything-
The only thing that a convolutional layer does is take a 3D volume's input, produce a 3D volume as output
and has some smooth function. Operating on the inputs, on the sum of the inputs,
that may or may not be a parameter that you tune, that you try to optimize. That's it.
Convolutional Neural Networks: Layers
So Lego pieces that you stack together in the same way as we talked about before.
So what are the types of layers that a convolutional neural networks have? There's inputs. So for example a color image of 32x32 will be a volume of 32x32x3.
A convolutional layer takes advantage of the spatial relationships of the input neurons and a convolutional layer,
Dealing with Images: Local Connectivity
it's the same exact neuron as for fully connected network, the regular we talked about before.
But it has a narrower receptive field, it's more focused, the inputs to a neuron on the convolutional layer
come from a specific region from the previous layer. And the parameters on each filter, you can think of this as a filter, because you slide it across the entire image.
And those parameters are shared. So supposed you've taken the- If you think about two layers,
as opposed to connecting every single pixel in the first layer to every single neuron in the following layer.
You only connect the neurons in the input layer that are close to each other, to the output layer, and then you
enforce the weights to be tied together spatially.
And what that results in is a filter every single layer on the output,
you can think of as a filter, they get excited for example for an edge
and when it sees this particular kind of edge in the image, it will get excited. And it'll get excited in the top left of the image, on the top right, bottom left, bottom right.
The assumption there is that a powerful feature for detecting a cat
is just as important no matter where in the image it is. And this allows you to cut away a huge number of connections between neurons but it still boils down on the right,
as a neuron that sums a collection of inputs and applies weights to them. The spatial arrangement of the output volume relative to the input volume
ConvNets: Spatial Arrangement of Output Volume
is controlled by three things. The number of filters. So for every single "filter"
you get an extra layer on the output. So if the input,
let's talk about the very first layer, the input is 32x32x3. It's in RGB
image of 32x32. If the number of filters is 10,
then the resulting depth the resulting number of stacked channels in the output will be 10. Stride is given.
is the step size of the filter that you slide along the image. Often times as just 1 or 3
and that directly reduces the size, the spatial size the width and the height, of the output image.
and then there is a convenient thing that it's often done is padding. The image on the outside zeros.
So that the input and the output have the same height and width. So this is a visualization of convolution.
I encourage you to, kind of maybe offline, think about what's happening. It's similar to the way human vision works, crudely so, if there's any experts in the audience.
So the input here on the left is a collection of numbers: 0, 1, 2. And a filter
or there are two filters shown as W1- W0 and W1. Those filters shown in red, are the different weights applied in those filters.
And each of the filters have a certain depth; just like the input a depth of 3.
So there are three of them in each column and so,
so you slide death filter along the image keeping the weights the same.
this is the sharing of the weights and so your first filter you pick the weights, this is an optimization problem. you pick the weights in such a way
that it fires, it gets excited, for useful features and doesn't fire for not useful features.
And then there's a second filter that fires for useful features and not. And produces a signal on the output depending on a positive number, meaning there's a strong feature in that region,
and negative number if there isn't but the filter is the same. This allows for a drastic reduction in the parameters and so you can deal with
inputs. There are a thousand by thousand pixel image, for example, or video. There's a really powerful concept there.
ConvNets: Pooling
The spatial sharing of weights. That means there's a spatial invariance to the features you're detecting. It allows you to learn from arbitrary images
so you don't have to be concerned about pre-processing the images in some clever way,
you just give the raw image. There is another operation: pooling.
It's a way to reduce the size of the layers by, for example in this case,
it's max pooling for taking a collection of outputs and choose x1
and summarizing those collection of pixels such that the output of the pooling operation is much smaller than the input.
Because the justification there is that you don't need a high resolution.
Localization of which pixel is important in the image or according to, you know,
you don't need to know exactly which pixel is associated with the cat ear or a cat face.
As long as you, kind of, know it's around that part and that reduces a lot of complexity in the operations. Yes, question.
The question was: "when is too much pooling, when do you stop pooling?"
So pooling is a very crude operation that doesn't have any, one thing you need to know, is it doesn't have any
parameters that are learnable. So you can't learn anything clever about pooling. You're just picking, in this case
max pool, so you're picking the largest number. So you're reducing the resolution, you're losing a lot of information.
There's an argument that you're not, you know, losing that much information as long as you're not pooling the entire
image into a single value but you're gaining training efficiency, you're gaining the memory size, reducing the size of the network.
So, it's definitely a thing that people debate and it's a parameter that you play with to see what works for you.
Computer Vision: Object Recognition / Classification
Okay, so how does this thing look like as a whole, a convolutional neural network, the input is an image
there's usually a convolutional layer, there is a pooling operation, another convolutional layer, another pooling operation and so on.
At the very end, if the task is classification you have a stack of convolutional layers and pooling layers.
There are several fully connected layers. So, you go from those spatial convolutional operations to fully connecting every single neuron in a layer to the
following layer. And you do this so that by the end, you have a collection of neurons each one is associated with a particular class.
So in what we looked at yesterday is the input, is an image of a number 0 through 9.
The output here would be 10 neurons. So you blow down that image with a collection of convolutional layers,
with 1 or 2 or 3 fully connected layers at the end that all lead to 10 neurons
and each of those neuron's job is to get fired up
when it sees a particular number and for the other ones to produce a low probability. And so this kind of process
is how you have the 95 percentile accuracy on the CIFAR-10 problem.
This here is ImageNet data set that I mentioned. It's how you take this image of a leopard, of a container ship,
and produce a probability that that is a container ship or a leopard.
Also shown there are the outputs of the other nearest neurons in terms of their confidence.
Computer Vision: Segmentation
Now you can use the same exact operation by chopping off the fully connected layer at the end
and as opposed to mapping from image to a prediction of what's contained in the image, you map from the image to another image.
And you can train that image to be one that gets excited
spatially, meaning it gives you a high, close to one value, for areas of the image that contain the object of interest
and then a low number for areas of the image that are unlikely to contain that image.
And so from this you can go on the left, an original image of a woman on a horse, to a segmented image of knowing where the woman is and where the horse is
and where the background is. The same process can be done for detecting the object.
So you can segment the scene into a bunch of interesting objects, candidates for interesting objects and then go through those candidates one by one
and perform the same kind of classification as in the previous step where it's just an input as an image and the output as a classification.
And through this process of hopping around an image, you can figure out exactly where is the best way to segment the cow
out of the image. That's called object detection. Okay, so
How Can Convolutional Neural Networks Help Us Drive?
how can these magical convolutional neural networks help us in driving? This is a video of the forward road way from a
data set that we'll look at, that we've collected from a Tesla. But first let me look at driving.
Driving: The Numbers
Briefly, the general driving task from the human perspective. On average an American driver in the United States
drives 10,000 miles a year. A little more for rural, a little less for urban. There is about 30,000
fatal crashes and >32,000 sometimes as high as 38,000 fatalities a year.
This includes car occupants, pedestrians, bicyclists and motorcycle riders.
This may be a surprising fact but in a class on self-driving cars we should remember that.
So ignore the 59.9%, that's other. The most popular cars in the United States are pickup trucks: Ford F-1 Series,
Chevy Silverado, Ram. It's an important point that we're still married to our,
Human at the Center of Automation
to wanting to be in control and so one of the interesting cars that we look at
and the car that is the days that we provide to the class is collected from is a Tesla.
It's the one that comes at the intersection of the Ford F-150 and the cute, little Google self-driving car on the right. It's fast, it allows you to have a feeling of control
but it can also drive itself for hundreds of miles on the highway, if need be.
It allows you to press a button and the car takes over. It's a fascinating trade-off, of transferring control from the human to the car.
It's a transfer of trust and it's a chance for us to study the psychology of human beings as they relate to machines at >60 miles an hour.
Distracted Humans
In case you're not aware a little summary of human beings, where distracted things: would like to text, use the smartphone,
watch videos, groom, talk to passengers, eat, drink, texting.
169 billion texts were sent in the US every single month in 2014.
On average, 5 seconds our eyes spent off the road while texting - 5 seconds.
4 D's of Being Human: Drunk, Drugged, Distracted, Drowsy
That's the opportunity for automation to step in. More than that, there's what NHTSA refers to as the 4 D's: drunk, drugged,
distracted and drowsy. Each one of those opportunity is for automation to step in. Drunk driving stands to benefit significantly
In Context: Traffic Fatalities
from automation, perhaps. So the miles, let's look at the miles. The data. There's 3 trillion (3 million million)
3 million million miles driven every year and TESLA autopilot, our case study for this class,
and as human beings is driven on full auto-pilot mode.
So it's driving by itself 300 million miles as of December 2016
and the fatalities for human control vehicles is 1:90,000,000.
It's about >30,000 fatalities a year and currently under TESLA auto-pilot there's one fatality.
There's a lot of ways you could tear that statistic apart but it's one to think about. Already, perhaps automation
results in safer driving. The thing is, we don't understand automation,
because we don't have the data: we don't have the data on the forward roadway video, we don't have the data on the driver
and we just don't have that many cars on the road today that drive themselves. So we need a lot of data.
We'll provide some of it to you in the class and as part of our research at MIT were collecting huge amounts of it,
of cars driving themselves, and collecting that data is how we get to understanding.
So talking about the data and what we'll be doing training our algorithms on, here is a Tesla Model S, Model X
we've instrumented 17 of them, have collected over 5,000 hours and 70,000 miles.
And I'll talk about the cameras that we've put in them. We're collecting video of the forward road way.
This is a highlight of a trip from Boston to Florida of one of the people driving a Tesla. What's also shown in blue is the
amount of time that autopilot was engaged: currently 0 minutes and then it grows and grows.
For prolonged periods of time, so hundreds of miles, people engage autopilot. Out of 1.3 billion
miles driven a Tesla, 300,000,000 are on autopilot. You do the math whatever that is, 25%.
So we are collecting data of the forward roadway, of the driver. We have 2 cameras on the driver.
What we're providing with the class is epics of time of the forward roadway, for privacy considerations. Cameras used
Camera and Lens Selection
to record are your regular Webcam, the work horse of the computer vision community. The C920,
and we have some special lenses on top of it. Now what's special about these webcams? Nothing that costs $70 can be that good, right?
What's special about them is that they do onboard compression and allow you to collect huge amounts of data
and use reasonably sized storage capacity to store that data and train your algorithms on.
Semi-Autonomous Vehicle Components
So what on the self-driving side do we have to work with? How do we build a self-driving car?
There is these sensors: radar, lidar, vision,
audio - all looking outside helping you detect the objects in the external environment to localize yourself and so on.
And there's the sensors facing inside: visible light camera, audio again,
and infrared camera to help detect peoples. So we can decompose the self-driving car task into 4 steps: localization, answering where am I; scene understanding,
Self-Driving Car Tasks
using the texture of the information of the scene around, to interpret the identity of the different objects in the
scene and the semantic meaning of those objects, of their movement.
There's movement planning - once you figured all that out, found all the pedestrians, found all the other cars,
how do I navigate through this maze, a clutter of objects in a safe and legal way. And there's driver state,
how do I detect using video or other information.
The video of the driver detect information about their emotional state or their distraction level. Yes, question.
(INAUDIBLE QUESTION) Yes, that's the real-time figure from lidar. Lidars are sensors that provides you the 3D point cloud
of the external scene. So lidar is the technology used by
most folks working with self-driving cars to give you a strong Ground Truth of the objects. It's probably the best sensor we have
for getting 3D information, the least noisy 3D information about the external environment. Question.
The Data
So autopilot is always changing. One of the most amazing things about this vehicle is that the updates to autopilot come in
the form of software. So the amount of time it's available to changes has become more conservative with time.
But in this, this one of the earlier versions, and it shows, the second line in yellow, shows how often autopilot was available but not turned on.
So the total driving time was 10 hours, autopilot was available 7 hours and was engaged an hour.
This particular person is a responsible driver because what you see or
is a more cautious driver. What you see is it's raining, autopilot is still available
but- (INAUDIBLE QUESTION) the comment was that you shouldn't trust that one fatality number as an indication of safety because the drivers
elect to only engage the system when it's safe to do so. It's a totally open,
there's a lot bigger arguments for that number than just that one, the question is whether that's a bad
thing so maybe we can trust human beings to engage, you know,
despite the poorly filmed YouTube videos, despite the hype in the media, you're still a human being.
riding 60 miles an hour in a metal box with your life on the line. You won't engage the system unless you know it's completely safe unless you've built up a relationship with it.
It's not all the stuff you see where a person gets in the back of a Tesla and start sleeping or is playing chess,
or whatever. That's all for YouTube, the reality is when it's just you in the car
it's still your life on the line and so you're going to do the responsible thing unless perhaps you're a teenager and so on but that never changes no matter what you're in.
Question. (INAUDIBLE QUESTION) The question was: "what do you need to see
or sense about the external environment to be able to successfully drive? Do you need lane markings? Do you need other-
what are the landmarks based on which you do the localization and navigation?" And that depends on the sensors. So with the Google self-driving car in sunny California,
it depends on lidar in a high-resolution way, map the environment in order to be able to localize itself
based on lidar. And lidar, now I don't know the details of exactly where lidar fails,
but it's not good with rain, it's not good with snow, it's not good when the environment is changing. So what snow does is it changes the visual, the appearance, the reflective texture
of the surfaces around. Us human beings are still able to figure stuff out but a car that's relying heavily on lidar won't be able to localize itself using the landmarks
it previously has detected because they look different now with the snow. Computer vision can help us with lanes
or following a car. The two landmarks that we used in a lane is following the car in front of you
or staying between two lanes. That's the nice thing about our roadways it's they're designed for human eyes.
So you can use computer vision for lanes and for cars in front to follow them. And there is radar.
That's a crude but a reliable source of distance information that allows you to not collide with metal objects.
So all that together depending on what you want to rely on more gives you a lot of information.
The question is when its messy complexity of real life occurs,
how reliable it would be in the urban environment and so on. So localization- How can deep learning help?
So first, just a quick summary of visual odometry. It's using a monocular or stereo input of video images
to determine your orientation in the world. The orientation, in this case, of a vehicle in the frame of the world
and all you have to work with is a video of the forward roadway
and with stereo you get a little extra information of how far away different objects are.
SLAM: Simultaneous Localization and Mapping
And so this is where one of our speakers on Friday will talk about his expertise (SLAM) Simultaneous Localization and Mapping.
This is a very well-studied and understood problem of detecting unique features in the external scene
and localizing yourself based on the trajectory of those unique features.
When the number of features is high enough it becomes an optimization problem.
You know this particular lane moved a little bit from frame to frame you can track that information.
And fuse everything together in order to be able to estimate your trajectory through the three dimensional space.
You also have other sensors to help you out. You have GPS which is pretty accurate, not perfect but pretty accurate.
It's another signal to help you localize yourself. You also have IMU. Accelerometer
tells you your acceleration, from the gyroscope, the accelerometer, you have the six degree of freedom
of movement information about how the moving object, the car, is navigating through space.
Visual Odometry in Parts
So you can do that using the old school way of optimization.
Given a unique set of features, like sift features, and that step involves with stereo input understorting and
and rectifying the images. You have two images, from the two images compute the depth map but for every single pixel
computing the best estimate of the depth of that pixel, the three dimensional position, relative to the camera then you
compute, that's where you compute the disparity map, that's what that's called, from which you get the distance
then you detect unique, interesting features in the scene. Sift is a popular one.
It's a popular algorithm for detecting unique features and you, over time, track those features.
And that tracking is what allows you through the vision alone to get information about your trajectory
through three-dimensional space. You estimate that trajectory. There's a lot of assumptions, assumptions that bodies are rigid.
So you have to figure out if a large object passes right in front of you, you have to figure out what that was.
You have to figure out mobile objects in the scene. And those are the stationary.
End-to-End Visual Odometry
Or you can cheat or we'll talk about and do it using neural networks end-to-end.
Now what does end-to-end mean? And this will come up a bunch of times throughout this class and today. End-to-end means,
and I refer to it as cheating because it takes away a lot of the hard work of panageneric features. You take the raw input
of whatever sensors. In this case, it's taking stereo input from a stereo vision cameras so two images, a sequence of two
images coming from a stereo vision camera, and the output is a estimate of your trajectory through space.
So it's supposed to be doing the hard work of SLAM, of detecting unique features, of localizing yourself, of tracking those
features and figuring out where your trajectory is. You simply train the network.
With some Ground Truth, you have form a more accurate sensor like lidar,
and you train it on a set of inputs, the stereo vision inputs, and outputs is the trajectory through space. You have a separate convolutional neural networks for the velocity
and for the orientation. And this works pretty well. Unfortunately, not quite well
and John Leonard will talk about that. SLAM is one of the places were deep learning is not being able to outperform the
previous approaches. Where deep learning really helps is the scene understanding part. It's interpreting the objects in the scene.
It's detecting the various parts of the scene, segmenting them
and with optical flow determining their movement. So previous approaches for detecting objects
Object Detection
like the traffic signal, the classification of detection that we have the TensorFlow tutorial for
or to use car-like features or other types of features that are hard-engineered from the images.
Now we can use convolutional neural networks to replace the extraction of those features.
Full Driving Scene Segmentation
And there's TensorFlow implementation of SegNet which is taking the exact same neural network that I talked about. It's the same thing, the beauty is you just apply
similar types of networks to different problems and depending on the complexity of the problem, can get quite amazing performance. In this case, we convolutionize
network, meaning the output is an image, input is an image, a single monocular image. The output is a
segmented image where the colors indicate your best pixel-by-pixel estimate of what object is in that part.
This is not using any spatial information, it's not using any temporal information.
So it's processing every single frame separately and it's able to separate the road from the trees,
from the pedestrians, other cars, and so on.
This is intended to lie on top of a radar / lidar type of technology that's giving you the three dimensional
or stereo vision three-dimensional information about the scene. You're, sort of, painting that scene with the identity of
the objects that are in it, your best estimate of it. This is something I'll talk about tomorrow is recurring neural networks
Road Texture and Condition from Audio
and we can use recurring neural networks that work with temporal data to process video
and also process audio. In this case, we can process what's shown on the bottom is
a spectrogram of audio for a wet road and a dry road. You can look at that spectrogram as an image
and process it in a temporal way using recurring neural networks. Just slide it across and keep feeding it to a network.
And it does incredibly well on the simple tasks, certainly of dry road versus wet road. This is important,
a subtle, but very important task and there's many like it to know the road, the texture, the quality.,
the characteristics of the road, wetness being a critical one. When it's not raining but the road is still wet, that information is very important.
Okay, so for movement planning. The same kind of approach. On the right is work from one of our other speakers
Previous approaches: optimization-based control  Where deep learning can help: reinforcement leaming
Sertec Karaman. The same approach we're using to solve traffic through friendly competition
is the same that we can use for what Chris Gerdes does with his race cars for planning trajectories in high speed movement
along complex curve.
So we can solve that problem using optimization, solve the control problem using optimization,
or we can use it with reinforcement learning by running tens of millions, hundreds of millions of times through that simulation of taking that curve
and learning which trajectory doesn't both optimizes the speed at which you take the turn
and the safety of the vehicle. Exactly the same thing that you're using for traffic.
And for driver state, this is what will talk about next week. It's all the fun face stuff: eyes, face, emotion.
This is with video of the driver, video of the driver's body, video the driver's face. On the left is one of the TAs
in his younger days. Still looks the same. There he is. So in that particular case,
you're doing one of the easier problems which is one of detecting where the head and the eyes are positioned.
The head and eye pose. You know it determine what's called he gaze of the driver, where the driver's looking, glance. And so,
we'll talk about these problems. From the left to the right: on the left in green are the easier problems; on the red
are the harder from the computer vision aspect. So on the left is body pose, head pose. The larger the object the easier it is the detect
and the orientation of it is easier to detect. And then there is pupil diameter. Detecting the pupil, the characteristics, the position, the size of the pupil.
And there's micro saccade, things that happen at one millisecond frequency, the tremors of the eye. All important information to determine the state of the driver.
Some are possible computer vision, some are not. This is something that we'll talk about, I think, on Thursday.
Is the detection of where the driver's looking. So, this is a bunch of the cameras that we have in the Tesla. This is
This is Dan driving a Tesla and detecting exactly where of one of six regions
We've converted into a classification problem of left, right, rear view mirror instrument cluster center stack
or forward roadway. So we have to determine out of those six categories which direction is the driver looking at. This is important for driving. We don't care exactly the X, Y, Z
position of where the driver is looking at. We care that they're looking at the road or not. Are they looking at their cell phone in their lap or are they looking at the forward roadway?
And we'll be able to answer that pretty effectively using convolutional neural networks.
You can also look at emotion using CNNs to extract,
again converting emotion, the complex world of emotion, into a binary problem of frustrated versus satisfied.
This is the video of drivers interacting with a voice navigation system. If you've ever used one,
you know that may be a source of frustration from folks. And so this is self reported, this is one of the hard, you know, driver
emotion if you're in what's called "Effective Computing." It's the field of studying emotion from the computational side.
If you work in that field, you know that the annotation side of emotion is really challenging one.
So getting the Ground Truth of, well okay since this guy's smiling
so can I label that as happy or he's frowning because that mean he's sad. Most effective computing folks do just that.
In this case we self report ask people how frustrated they're were in a scale of 1 to 10.
Dan up top reported a "1" for not frustrated, he's satisfied with the interaction,
and the other driver reported as a "9" he was very frustrated with the interaction. And what you notice
is there is a very cold, stoic look on Dan's face which is an indication of happiness.
And in the case of frustration, the driver is smiling.
So this is a sort of a good reminder that we can't trust our own human instincts.
It's an engineering feature. Engineering the ground truth. We have to trust the data, trust the Ground Truth
that we believe is the closest reflection of the actual semantics of what's going on in the scene.
Okay, so end-to-end driving. Getting to the the project and the tutorial.
So if driving is like a conversation and, thank you for someone to clarifying, that this is the Arch of Triumph
in Paris in this video. If driving is like a natural language conversation, then we can think of end-to-end driving as skipping the entire Turing Test
components and treating it as an end-to-end natural language generation.
So what we do is we take as input the external sensors
and output, the control of the vehicle. And the magic happens in the middle.
We replace that entire step with a neural network.
TAs told me to not include this image because it's the cheesiest we've ever seen. I apologize. Thank you, thank you.
I regret nothing. So this is to show our path to self-driving cars
but it still explain a point that we have a large data set of Ground Truth.
If we were to formulate the driving task to simply taking external images
and producing steering commands, acceleration of braking commands, then we have a lot of Ground Truth.
We have a large number of drivers on the road every day driving
and, therefore, collecting our Ground Truth for us because they're an interested party in producing the steering commands
that keep them alive and, therefore, if we were to record that data it becomes Ground Truth.
So if it's possible to learn this, what we can do is we can collect data for the manually controlled vehicles
and use that data to train an algorithm to control a self-driving vehicle.
Okay, so one of the first folks who did this is Nvidia where they actually train in an external image, the image of the forward roadway.
and a neural network, a convolutional network, a simple vanilla convolutional neural network I'll briefly outline:
take an image in, produce a steering command out and they're able to successfully, to some degree, learn to navigate basic turns, curves and even stop
or make sharp turns at a keener section. So this this now work is simple.
There is input on the bottom, output up top. The input is a 66x200 pixel image, RGB.
Shown on the left is the raw input and then you crop it a little bit and resize it down
66x200. That's what we have in the code as well in the two versions of the code we'll provide to you.
Both that runs in the browser and in TensorFlow. It has a few layers. A few convolutional layers, a few fully connected layers.
And an output. This is a regression network. It's producing not a classification of cat versus dog, it's producing a steering command.
How do I turn the steering wheel? That's it. The rest is magic and we train it on a human input.
What we have here is a project, is an implementation of the system in ConvNetJS that runs in your browser.
This is the tutorial to follow and the project to take on. So unlike the DeepTraffic game, this is reality.
This is a real input from real vehicles. So you can go to this link. Demo went wonderfully yesterday so let's see, maybe two for two.
There's the tutorial and then the actual game, the actual simulation is on DeepTesla.JS, I apologize.
Everyone is going there now, aren't they? Does it work on a phone? It does, great.
Again similar structure up top is the visualization of the lost function as the network is learning
and it's always training.
Next is the input for the layout of the network, there's the specification of the input 200x66.
There's a convolutional layer. There's a pooling layer and the output is a regression layer. A single neuron.
This is a tiny version, DeepTiny, right? It's a tiny version of
the Nvidia architecture and then you can visualize the operation of this network on real video.
The actual wheel value that produced by the driver, by the autopilot system,
is in blue and the output of the network is in white.
And what's indicated by green is the cropping of the image that is then resized to produce the 66x200
input to the network. So once again, amazingly, this is running in your browser, training on real world video.
So you can get in your car today input it and maybe teach a neural network to drive like you.
We have the code in ConvNetJS and TensorFlow to do that and the tutorial. Well, let me briefly describe some of the work here.
So the input to the network as a single image. This is for DeepTesla.JS, single image and the output is a steering wheel value between -20 and 20.
That's in degrees. We record,
like I said, thousands of hours but we provide publicly 10 video clips of highway driving from a Tesla.
Half are driven by autopilot, half are driven by human. The wheel values extracted from a perfectly synchronized
CAN, we are collecting all of the messages from CAN, which contains steering wheel value and that's synchronized to the video. We crop, extract the window. The green one I mentioned.
And then provide that as input to the network. So this is a slight difference from DeepTraffic with the red car weaving through traffic because there is the messy
reality of real world lighting conditions. And your task for the most part, in this simple steering task, is to stay inside the lane,
inside the lane markings. In an end-to-end way, learn to do just that. So ConvNetJS
is a javascript implementation of CNNs, of convolutional neural networks. It supports really arbitrary networks.
I mean all neural networks are simple but because it runs in javascript it's not utilizing GPU.
The larger the network the more it's going to be weighed down computationally.
Now unlike DeepTraffic, this isn't a competition but if you are a student registered for the course you still do have to submit the code, you still have to submit your own
car as part of the class. Question. So the question was the amount of data that's needed.
Is there a general rules of thumb for the amount of data needed for a particular task in driving for example?
It's a good question. You generally have to, like I said, neural networks are good memorizers so you have to just have every case represented in the
training said that you're interested in. As much as possible, so that means, in general if you want a picture, if you want to
classify the difference between cats and dogs, you want to have at least a thousand cats and a thousand dogs
and they do really well. The problem with driving is twofold: one, is that most of the time driving looks the same.
And the stuff you really care about is when driving looks different. It's all the edge cases. So we're not good with neural networks is generalizing from the common case to the edge cases, to the outliers.
So avoiding a crash just because you can stand the highway for thousands of hours successfully doesn't mean you can avoid a crash with
somebody runs in front of you on the road and the other part with driving is the accuracy you have to achieve is really high. So for cat versus dog,
No, life doesn't depend on your error. On your ability to steer a car inside of the lane.
You better be very close to 100% accurate. There's a box for designing the network.
There's a visualization of the metrics measuring the performance of the network as it trains.
There is a visualization, layer visualization, of what features the network is extracting at every convolutional layer
and every fully connected layer. There is ability to restart the training.
Visualize the network performing on real video. There is the input layer, the convolutional layers.
The video visualization, an interesting tidbit on the bottom right is a barcode that Will has ingeniously designed.
How do I clearly explain why this is so cool? It's a way to through video synchronized multiple streams of data together,
so it's very easy for those who have worked with multi-modal data where there are several streams of data
for them to become unsynchronized especially when a big component of training a neural network is shuffling the data.
So you have to shuffle the data in clever ways so you're not overfitting any one little aspect of the video and yet maintain the data perfectly synchronized.
So what he did instead doing the hard work of connecting the steering wheel and in the video is actually putting the steering on top of the video as a barcode.
The final result is you can watch the network operate and over time it learns more and more to steer correctly.
I'll fly through this a little bit in the interest of time just kind of summarize some of the things that you can play with in terms of tutorials and let you guys go. This is the same kind of process end-to-end driving with
So we have code available on GetHub. You just put up on my GetHub and the DeepTesla.
That takes in a single video or an arbitrary number of videos trains on them
and produces a visualization that compares the steering wheel, the actual steering wheel and the predicted steering wheel.
The steering wheel, when it agrees with the human driver or the autopilot system lighting up as green
and when it disagrees, lighting up as red. Hopefully not too often.
Again, this is some of the details of how that's exactly done in TensorFlow.
This is vanilla convolution neural networks. Specifying a bunch of layers, convolutional layers, a fully connected layer, train the model, so you iterate over the batches of images.
Run the model over a test set of images and get this result. We have a tutorial
on iPython Notebook into the tutorial up on this. This is perhaps the best way to get started with convolutional neural networks in terms of our class. It's looking at the
simplest image classification problem, of traffic light classification. So we have these images of traffic lights.
We did the hard work of detecting them for you. So now you have to figure out, you have to build the convolutional network that gets
figures out the concept of color and gets excited when it sees red, yellow or green. If anyone has questions,
I'll welcome those. You can stay after class if you have any concerns with Docker,
with TensorFlow, with how to win DeepTraffic. Just stay after class or come by Friday, 5 to 7. See you guys tomorrow.

----------

-----

--16--

-----
Date: 2017.01.22
Link: [# MIT 6.S094: Deep Reinforcement Learning for Motion Planning](https://www.youtube.com/watch?v=QDzM8r3WgBw)
Transcription:

All right. Hello everybody. Welcome back. Glad you came back. Today,
we will unveil the first tutorial. The first project is DeepTraffic, code named "DeepTraffic,"
where your task is to solve the traffic problem using Deep Reinforcement Learning.
And I'll talk about what's involved in designing a network there. How you submit your own network and how you participate in the competition.
As I said the winner gets a very special prize, to be announced later.
What is machine learning? Several types. There's supervised learning, as I mentioned yesterday that's what it meant,
Types of machine learning
usually when you discuss about, you talk about, machine learning and talk about its successes.
Supervised learning requires a data set where you know the Ground Truth.
You know the inputs and the outputs. And you provide that to the machine learning algorithm
in order to learn the mapping between the inputs and the outputs in such a way that you can generalize to further examples in the future.
On supervised learning, it's the other side, when you know absolutely nothing about the outputs.
About the truth of the data that you're working with. All you get is data and you have to find underlying structure,
underlying representation of the data that's meaningful for you to accomplish certain tasks, whatever that is.
There is semi-supervised data, or only parts, usually a very small amount
is labeled as Ground Truth of available for just a small fraction of it
If you think of images that are out there on the Internet
and then you think about ImageNet, a data set where every image is labeled, the size of that ImageNet data set
is a tiny subset of all the images available online.
But that's the task we're dealing with as human beings, as people interested in doing machine learning
is how to expand the size of that,
of the part of our data that we know something confidently about.
And reinforcement learning sit somewhere in between. It's semi supervised learning where
there's an agent that has to exist in the world. And that agent know the inputs that the world provides
but knows very little about that world except through occasional time delayed rewards.
This is what it's like to be human. This is what life is about.
You don't know what's good and bad, you got to have to just live it and, every once in a while,
you find out that all that stuff you did last week was pretty bad idea. That's reinforcement learning.
That's semi-supervised, in the sense that only a small subset of the data
comes with some ground truth, some certainty, you have to, then extract knowledge from.
So first at the core of anything that works currently
in terms of, in the practical sense, there has to be some Ground Truth. There has to be some truth
that we can hold on to as we try to generalize. And that supervised learning.
Even as in Reinforcement Learning, the only thing we can count on is that truth that comes in the form of a reward.
So the standard supervised learning pipeline is you have some raw data,
the inputs. you have Ground Truth, the labels,
the outputs and matches to the inputs. You know of ground truth. Then you run any kind of algorithm, whether it's a neural network
or another pre-processing algorithm that extracts the features from that data set.
You can think of a picture of a face, that algorithm could extract
the nose, the eyes, the corners of the eyes, the pupil or even lower level features in that image.
After that we insert those features
into a model. A machine learning model. We train that model.
Then we, whatever that algorithm is as it passes through that training process, we then evaluate.
After we've seen this one particular example, how much better are we at other tasks?
And as we repeat this loop, the model learns to perform better and better
at generalizing from the raw data to the labels that we have. And finally, you get to release that model into the wild
to actually do prediction on data as never seen before that you don't know about.
And the task there is to predict the labels.
Perceptron: Weighing the Evidence
Okay. So neural networks is what this class is about.
It's one of the machine learning algorithms that has proven to be very successful.
And the computational building block of a neural network is a neuron.
A perceptron is a type of neuron. It's the original old school neuron
where the output is binary, a zero or one. It's not real valued.
And the process that a perceptron goes through is it has multiple inputs and a single output.
Each of the inputs have weights on them. Shown here on the left is 0.7, 0.6, 1.4.
Those weights are applied to the inputs. And a perceptron, the inputs are 1s or 0s -
binary. When those weights are applied
and then summed together a bias on each neuron
is then added on top and a threshold,
there's a test, whether that summed value plus the bias is below or above a threshold. If it's above a threshold, produces a 1;
below a threshold produces a 0. Simple. So one of the only things we understand about neural networks confidently,
we can prove a lot of things about this neuron.
Perceptron: Implement a NAND Gate
For example, what we know is that a neuron can approximate a NAND gate.
A NAND gate is a logical operation,
a logical function, that takes as input, has two inputs A and B,
here on the on the diagram in the left. And the table shows what that function is
when the inputs are 0s, 01, in any order, the output is a 1.
Otherwise, it's a 0. The cool thing about a NAND gate is that it's a universal gate
that you can build up any computer you have where you have your phone in your pocket today
can be built out of just NAND gates. So it's functionally complete.
You could build any logical function out of them. You stack them together in arbitrary ways.
The problem with NAND gates and computers. is they're built from the bottom up.
You have to design these circuits of NAND gates. So the cool thing here is the perceptron,
we can learn. This magical NAND gate, we can learn its function.
So let's go through how we can do that. How a perceptron can perform the NAND operation.
There's the four examples. If you put the weights of -2 on each of the inputs
and a bias of three on the neuron, snd if we perform that same operation
of summing the weights times the inputs. plus the bias, in the top left we get
when the inputs are 0s and there's sum to the bias, we get a 3.
That's a positive number which means the output of a perceptron will be a 1.
On the top right, when the input is a 0 and a 1, that sum is still a positive number, again produces a 1.
And so on. When the inputs are both 1s, then the output is a -1. Less than zero.
So while this is simple, it's really important to think about.
Perceptron NAND Gate
It's a sort of one basic computational truth you can hold on to
as we talk about some of the magical things neural networks can do
because if you compare a circuit of NAND gates
and a circuit of neurons the difference, while a circuit of neurons
which is what we think of as a neural network, can perform the same thing as a circuit of NAND gates.
What it can also do is it can learn; It can learn the arbitrary logical functions
that has arbitrary circuit of NAND gates can represent but it doesn't require the human designer.
We can evolve, if you will.
The Process of Learning Small Change in Weights  Small Change in Output
So one of the key aspects here, one of the key drawbacks of perceptron,
is it's not very smooth in it's output. As we change the weights on the inputs
and we change the bias, and we tweak it a little bit, it's very likely that when you get-
It it's very easy to make the neuron- I'll put a 0 instead of a 1, or 1 instead of a 0.
So when we start stacking many of these together, it's hard to control the output of the thing as a whole.
Now the essential step that makes the neural network work,
that a circuit perceptrons doesn't, Is if the output is made smooth,
it's made continuous with an activation function.
And so instead of using a step function like a perceptron does shown there on the left,
we use any kind of smooth function. Sigmoid, where the output can change gradually as you change the weights and the bias.
And this is a basic but critical step
and so learning is generally the process of adjusting those weights gradually
and seeing how it has an effect on the rest of the network. You just keep tweaking weights here and there
and seeing how much closer you get to the Ground Truth.
And if you get farther away, you just adjust the weights in the opposite direction.
That's neural networks in a nutshell.
Combining Neurons into Layers
What we'll mostly talk about today is feed forward neural network.
On the left, going from inputs to outputs. With no loops, there is also
these amazing things called recurrent neural networks. They're amazing because they have memory.
They have a memory of state; they remember the temporal dynamics of the data they went through.
But the painful thing is that they're really hard to train.
Today will talk about feed for neural networks. So let's look at this example,
Task: Classify and Image of a Number
an example of stacking a few of these neurons together. Let's think of the task,
the basic task now famous, using a classification of numbers.
You have an image of a number in red number and your task is given that image to say what number is in that image.
Now, what is an image? An image is a collection of pixels; in this case 28 X 28 pixels.
That's a total of 784 numbers; those numbers are from 0 to 255.
And on the left of the network, the size of that input, despite the diagram, is 784 neurons.
That's the input. Then comes the hidden layer.
It's called the hidden layer because it has no interaction with the input or the output.
It is simply a block used at the core of the computational power of neural networks,
is the hidden layer. It's tasked with forming a representation of the data
in such a way that it maps from the inputs to the outputs. In this case, there is fifteen neurons in the hidden layer.
There is ten values on the output.
corresponding to each of the numbers. There are several ways you can build this kind of network
and this is what the magic of neural networks as you can do in a lot of ways. You only really need 4 outputs to represent values 0 through 9.
But in practice, it seems that having 10 outputs works better. And how do these work?
Whenever the input is a 5, the output neuron in charge of the five gets really excited.
And I'll put a value that's close to 1, from 0 to 1, close to 1.
And then the other 1s, I'll put a value, hopefully, that is close to 0.
And when they don't, we adjust the weights in such a way that they get closer to zero
and closer to one depending on whether this is the correct neuron associated with a picture.
We'll talk about the details of this training process more tomorrow when it's more relevant
but what we've discussed just now is the forward pass through the network.
It's the pass when you take the inputs, apply the weights, sum them together, add the bias, produce the output,
and check which of the outputs produces the highest confidence of the number
then once those probabilities for each of the numbers is is provided,
we determine the gradient that's used
to punish or reward the weights that resulted in either the correct or the incorrect decision.
And that's called Back Propagation. We step backwards through the network applying those punishments or rewards
Because of the smoothness of the activation functions, that is a mathematically efficient operation.
That's where the GPU step in. So far examples of numbers the Ground Truth for number 6
looks like the following in the slides. Y of X equals to 10 dimensional vector
where only one of them the sixth values a 1, the rest are zero.
That's the Ground Truth that comes with the image. The lost function here, the basic lost function, is the squared error.
Y of X is the Ground Truth and A is the output of the neural network
resulting from the forward pass. So when you input that number of a 6 and outputs, whatever it outputs
that's "a", a 10 dimensional vector. And it's summed over the inputs to produce the squared error.
That's our lost function. The lost function, the objective function. That's was used to determine
how much to reward or punish the Back Propagated weights throughout the network.
And the basic operation of optimizing that loss function, of minimizing that loss function,
is done with various variants of gradient descent.
It's hopefully a somewhat smooth function but it's a highly non-linear function.
This is why we can't prove much about neural networks, is it's a highly, high dimensional, highly non-linear function that's hopefully smooth enough
where the gradient descent can find its way to a least a good solution.
And there has to be some stochastic element there that
that jumps around to ensure that it doesn't get stuck in a local minimum of this very complex function.
Philosophical Motivation for Reinforcement Learning
Okay, that's supervised learning: there's inputs, there's outputs. Ground Truth.
That's our comfort zone, we're pretty confident we know what's going on. All you have to do is just, you have this data set you train and,
you train a network on that data set and you can evaluate it. You can write a paper and try to beat a previous paper. It's great.
The problem is when you then use that neural network to create an intelligent system that you put out there in the world,
and now that system is no longer is working with your data set. It has to exist in this world that's
maybe very different from the Ground Truth. So the take away from supervised learning
is that a neural network's a great memorization but in the sort of philosophical way they might not be great at generalizing,
at reasoning beyond the specific flavor of data set that they were trained on.
The hope for reinforcement learning is that we can extend the knowledge we gain in a supervised way
to the huge world outside where we don't have
the Ground Truth of how to act, how good a certain state is,
or how barristers say it is, this is a kind of brute force reasoning. And I'll talk about, kind of what I mean there, but it feels like
Agent and Environment
it's closer to reasoning as opposed to memorization. That's a good way to think of supervised learning - is memorization.
You're just studying for an exam. And as many of you know, that doesn't mean you're going to be successful in life just because you get an A.
And so, a reinforcement learning agent or just any agent;
a human being or any machine existing in this world
can operate in the following way from the perspective of the agent. You can execute an action;
it can receive an observation resulting from that action
in a form of a new state and it can receive a reward or punishment.
You can break down our existence in this way, simplistic view,
but it's a convenient one on the computational side and from the environment side,
the environment receives the action amidst the observation. So your action changes the world, therefore, that world has to change
and then tell you about it and give you a reward or punishment for it.
So, again one of the most fascinating things
I'll try to convey while this is fascinating a little bit later on,
is the work of deep mind on Atari.
This is Atari Breakout a game were a paddle has to move around.
That's the world it's existing in, the agent is the paddle and there's a bouncing ball
and you're trying to move, your actions are right: move right, move left.
You are trying to move in such a way that the ball doesn't get past you.
And so, here is a human level performance of that agent. And so what does this paddle have to do?
That's to operate in this environment; that's to act: move left, move right.
Each action changes the state of the world. It may seem obvious but
moving right changes visually the state of the world. In fact what we're watching now on the slides
is the world changing before your eyes for this little guy.
And it get rewards or punishments. Rewards it gets in the form of points,
they're racking up points in the top left of the video.
And then when the ball gets past the paddle, it gets punished by "dying".
And that's the number of lives there's left. Going from 5 to 4 to 3, down to 0.
And so the goal is to select at any one moment the action that maximizes future reward.
Without any knowledge of what a reward is in the greater sense of the word,
all you have is an instantaneous reward or punishment, instantaneous response of the world to your actions
Markov Decision Process
and this can be model as a mark of decision process. Mark of decision process is a mathematically convenient construct.
It has no memory, all you get is you have a state that you're currently in.
You perform an action, you get a reward. And you find yourself in a new state. And that repeats over and over.
You start from state 0, you go to state 1. You once again repeat an action, get a reward for the next state.
OK that's the formulation that we're operating in. When you're in a certain state, you have no memory of what happened two states ago.
Major Components of an Rl Agent
Everything is operating on the instantaneous.
Instantaneously. And so what are the major components of a reinforcement learning agent?
There's a policy. The function broadly defined an agent's behavior.
That means that includes the knowledge of how, for any given state,
what is an action that I will take with some probability.
Value function is how good each state and action are in any particular state.
And there's a model. Now this is a subtle thing that is
actually the biggest problem with everything you'll see today, is the model as how we represent the environment.
And we'll see today some amazing things that neural networks can achieve on a relatively simplistic model of the world
and the question whether that model can extend to the real world where human lives are at stake in the case of driving.
Robot in a Room
So let's look at the simplistic world. A robot in a room.
You start at the bottom left, Your goal is to get to the top right.
Your possible actions are going up, down, left and right.
Now this world can be deterministic which means when you go up, you actually go up.
Or it could be non-deterministic as human life is is
because when you go up, sometimes you go right. So in this case if you choose to go up, you move up 80% of the time.
You move left 10% of the time and you move right 10% of the time. And when you get to the top right you get a reward of +1
and you get to the second block from that, for two you get -1. You get punished.
And every time you take a step you get a slight punishment, a -0.04.
Is this a solution?
Okay. So the question is, if you start at the bottom left, is this a good solution?
Is this a good policy by which you exist in the world?
And it is if the world is deterministic. If whenever you choose to go up, you go.
Whenever you choose to go right, you go right.
But if the actions are stochastic, that's not the case.
Optimal policy
In what I described previously with point eight up and probability of .1 going left and right.
This is the optimal policy. Now if we punish every single step with a -2 as opposed to a -0.04.
Reward for each step-2
So every time you take a step,it hurts. You're going to try to get through a positive block as quickly as possible
and that's what this policy says. I'll walk through a negative one if I have to as long as I stop getting a -2.
Reward for each step: +0.01
Now if the reward for each step is a -.1, you might choose to go around that -1 block,
slight detour to avoid the pain.
And then you might take an even longer detour as the reward for each step goes up or the punishment goes down, I guess.
And then if there is an actual positive reward for every step you take
you'll avoid going to the finish line. You'll just wander the world.
We saw that with the Coast Racer yesterday,
the boat that chose not to finish the race because it was having too much fun getting points in the middle.
Value Function
So let's look at the world that this agent is operating in as a value function.
Now value function depends on a reward, the reward that comes from the future
and that reward is discounted because the world is stochasted, we can't expect the reward to come along to us in the way that
we hope it does based on the policy, based on the way we choose to act.
And so there's a gamma there that over time, as the award is farther and farther into the future discounts that reward.
Diminishes the impact of that future reward in your evaluation of the current state.
And so your goal is to develop a strategy that maximizes the discounted future reward.
The sum, the discounted sum, and reinforcement learning
Q Learning
there is a lot of approaches for coming up with a good policy,
a near optimal, an optimal policy. There's a lot of fun math there.
You could try to construct a model that optimizes some estimate of this world.
You can try in the Monte Carlo way through just simulate that world and see how it unrolls.
And as it unrolls you try to compute the optimal policy. Or what we'll talk about today is Q learning.
It's an off policy approach, where the policy is estimated as we go along.
The policy is represented as a Q-Function.
The Q-Function shown there on the left is,
I apologize for the equations, I lied. There'll be some equations.
The input to the Q-Function is a state at time t, "st".
An action they choose to take and that state "at".
and your goal is in that state to choose an action which maximizes the reward in the next step.
And what Q-Learning does, and I'll describe the process, is it's able to approximate through experience the optimal Q-Function,
the optimal function that tells you how to act in any state of the world.
You just have to live it. You have to simulate this world.
You have to move about it. You have to explore in order to see every possible state,
try every different action, get rewarded, get punished,
and figure out what is the optimal thing to do. That's done using this Bellman equation.
On the left, the output, is the new state. The estimate, the Q-Function estimate of the new state, for new action.
And this is the update rule at the core of Q Learning. You take the estimate, the old estimate, and add
based on the learning rate alpha from 0 to 1,
they update the evaluation of that state based on your new reward that you received at that time.
So you've arrived in this certain state as "t". You tried to do an action
and then you got a certain reward and you update your estimate of that state and action pair based on this rule.
When the learning rate is 0, you don't learn when alpha is 0.
You never change your world view based on the new incoming evidence.
When alpha is 1, every time change your world evaluation based on the new evidence.
Exploration vs Exploitation
And that's the key ingredient to Reinforcement Learning. First you explore, then you exploit.
First, you explore in a non-greedy way and then you get greedy. You figure out what's good for you and you keep doing it.
So if you wanted to learn an Atari game, First you try every single action, every state, you screw up,
get punished, get rewarded and, eventually, you figure out what's actually the right thing to do and you just keep doing it.
And that's how you win against the greatest human players in the world
in a game of "Go" for example, as we'll talk about. And the way you do that is you have an "Epsilon Greedy Policy"
that over time with a probability of 1 - Epsilon,
you perform an optimal Greedy action. With a probability of Epsilon, you perform a random action.
Random action being explore. And so, as epsilon goes down from 1 to 0 you explore less and less.
Q-Learning: Value Iteration
So the algorithm here is really simple. On the bottom of the slide there is the algorithm version,
the pseudo code version of the equation. The Bellman equation update.
You initialize your estimate of state action pairs arbitrarily,
a random number. This is an important point. When you start playing or living or doing whatever you're doing
in whatever you're doing with Reinforcement Learning or driving, you have no preconceived notion of what's good and bad, it's random.
Or however you choose to initialize it. And the fact that it learns anything is amazing.
I want you to remember that. That's one of the amazing things about Q-Learning at all
and then the Deep neural network version of Q-Learning.
The algorithm repeats the following step. You step into the world, observe an initial state, you select an action "a"
so that action, if you're exploring, will be a random action; if you're greedily pursuing the best, actually you can,
it will be the action that maximizes the Q-Function. You observe a reward after you take the action,
and a new state that you find yourself in. And then you update your estimate of the previous day you are in
having taken that action using that Bellman Equation Update.
And repeat this over and over. And so there on the bottom of the slide is a summary of life.
Q-Learning: Representation Matters
Yes. (CHUCKLING) Q-Function? Yes, yes. Yeah, it's a single- The question was
is the Q-Function a single value? And yes, it's just a single continuous value.
So the question was: "how do you model the world?"
So the way you model, so let's start, is very simplistic world of Atari paddle.
You think you model it as a paddle that can move left and right and there's some blocks and you model the physics of the ball.
That requires a lot of expert knowledge in that particular game. So you sit there hand crafting this model.
That's hard to do even for a simplistic game. The other model you could take
is looking at this world in the way the humans do visually.
So take the model in as a set of pixels. Just the model is all the pixels of the world.
You know nothing about paddles or balls or physics or colors and points, they're just pixels coming in.
That seems like a ridiculous model of the world but it seems to work for Atari. It seems to work for human beings.
When you're born, you see there's light coming into your eyes
and you don't have any, as far as we know,
you don't come with an instruction when you're born. You know there's people in the world
then there is good guys and bad guys, and there's this is how you walk.
No, all you get is light, sound and the other sensors.
And you get to learn about every single thing you think of as
the way you model the world is a learned representation and we will talk about how a neural network does that.
It learns to represent the world but if we have to hand model the world,
it's an impossible task. That's the question and if we have to hand model the world,
then that world better be a simplistic one. Yeah.
That's a great question. And so the question was: "what is the robustness of this model
if the way you represent the world is at all, even slightly different, from the way you thought that world is.
That's not that well studied as far as I'm aware. I mean, it's already amazing that you keep constructing,
if you have a certain import of the world, If you have a certain model of the world that you can learn anything is already amazing.
The question is, and it's an important one, is we'll talk a little bit about it,
not about the world model but the reward function. If the reward function is slightly different.
the real reward function of life or driving or of coast runner
is different than what you expected it to be. What's the negative there?
Yes, it could be huge. (CHUCKLING)
There's another question or no? Oh, no. Yes. Sorry, can you ask that again?
Yes, you can change it over. So the question was: "do you change alpha value over time?" You certainly should change alpha value over time, yes.
So the question was: "what is the complex interplay of the Epsilon Function with the Q-Learning Update?"
That's 100% fine-tuned to the particular learning problem.
So you certainly wanted-
The more complex, the larger the number of states in the world
and the larger the number of actions, the longer you have to wait
before you decrease the Epsilon to 0 but you have to play with it. And it's one of the parameters you have to play with, unfortunately,
and there's quite a few of them which is why you can't just drop a Reinforcement Learning agent into the world.
Oh, the effect in that sense? No, no. It's just a coin flip. And if that Epsilon is 0.5,
half the time you're going to take a random action. So there's no specific- It's not like you'll take the best action
and then with some probability take the second best, and so on. I mean you can certainly do that but in the simple formulation that works if you just take a random action
because you don't wanted to have a preconceived notion of what's a good action to try when you're exploring.
The wjhole point is you try crazy stuff, if it's a simulation.
So, good question. So representation matters.
This is the question about how we represent the world. So we can think of this world of break up, for example,
of this Atari game as a paddle the moves left and right.
and the exact position of the different things you can hit to construct this complex model,
this expert driven model that has to fine tune it to this particular problem.
But in practice the more complex this model gets,
the worse that Bellman Equation Update, that value that's trying to construct a Q-Function
for every single combination of state and actions becomes too difficult because that function is too sparse and huge
so if you think of looking at this world in a general way,
in the way human beings would is a collection of pixels visually. If you just take in a pixel,
this game as a collection of 84 by 84 pixels, an image, an RGB image,
And then you look at not just the current image, but look at the temporal trajectory of those images
so like if there's a ball moving you want to know about that movement. So you look at 4 images; so, current image and 3 images back
and say, they're gray scale with 256 gray levels that size of the Q-Table
that the Q value function has to learn is
whatever that number is, but it's certainly larger than the number of atoms in the universe. That's a large number.
So you have to run the simulation long enough to touch at least a few times the most of the states in that Q-Table.
So as Elon Musk says you may need to run,
you know, we live in a simulation, and you may have to run a universe just to compute the Q-Function in this case.
Philosophical Motivation for Deep Reinforcement Learning
So that's where deep learning steps in as instead of modeling the world as a Q-Table
you estimate, you try to learn that function.
And so, the takeaway from supervised learning, if you remember, that it's good at memorizing or good at memorizing data.
The hope for reinforcement learning With a Q-Learning is that we can extend
the occasional rewards we get to generalize over the operation,
the actions you take in that world leading up to the rewards. And the hope for deep learning is that we can move this
Reinforcement learning system into a world that doesn't need to be, they can be defined arbitrarily.
It can include all the pixels of an Atari game, can include all the pixels sense by a drone, a robot or car
but still needs a formalized definition of that world which is much easier to do when you're able to take in sensors like an image
So Deep Q-Learning, deep version.
So instead of learning a Q-Table, a Q-Function, we try in estimating that Q-Prime.
We try to learn it using machine learning. It tries to learn some parameters, this huge complex function.
We try to learn it and the way we do that as we have a neural network
the same kind that showed that learned the numbers to map from an image to a classification of that image into a number.
The same kind of network is used to take in a state, an action and produce a Q-Value.
Now here's the amazing thing: that without knowing anything in the beginning,
Deep Q-Network: Atari
as I said, with a Q-Table it's initialized randomly. The Q-Function. this deep network, knows nothing in the beginning.
All it knows is, in the simulated world, their words you get
for a particular game, so you have to play time and time again and see the rewards you get for every single iteration of the game.
But in the beginning it knows nothing. And it's able to learn to play better than human beings.
This is a deep mind paper playing Atary with deep reinforcement learning from 2013.
There's one other key things that got everybody excited about the role of deep learning in artificial intelligence
is that using a convolutional neural work, which I'll talk about tomorrow,
but it's a vanilla network, like any other like I talk about earlier today, just a regular network
That takes the raw pixels, as I said, and estimates that Q-Function from the raw pixels as able to play on many of those games
better than a human being. And the lost function that I mentioned previously,
Deep Q-Network Training
so, again, very vanilla lost function,
very simple objective function. The first one you'll probably implement. We have a tutorial on TensorFlow.
Squared Error. So we take this Bellman Equation where the estimate is Q-
The Q-Function Estimate of state and action is the maximum reward you get for taking any of the actions
that take you to any of the future states. And you try to take that action, observe the result of that action,
and if the target is different that your learn target,
what the function is learned is the expected reward in that case, is different than what you actually got you adjust it.
You adjust the weights of the network. And this is exactly the process by which we learn
how to exist in this pixel world. So your mapping states and actions to a Q-Value,
the algorithm is as follows. This is how we train it. We're given a transition as current state action taken in that state
are the rewards you get, an S-Prime, as what the state you find yourself in.
And so we replace the basic of their rule, in the previous pseudo code,
by taking a forward pass through the network given that S-state.
We'll look at what the predicted Q-value is of that action.
We then do another forward pass through that network and see what we actually get.
And then if we're totally off, we punish,
we Back Propagate the weights in a way that. next time we'll make less of that mistake. And you repeat this process.
This is a simulation. You're learning against yourself.
And again, the same rule applies here. exploration versus exploitation.
You start out with an Epsilon of 0 or 1, you are mostly exploring.
And then you move towards an Epsilon of 0. And with Atari Breakout. this is the deep mind paper result
Atari Breakout
is Training Epochs on the x-axis, on the y-axis is the average action value
and the average reward per episode. I'll show why it's kind of a an amazing result but it's messy
because there's a lot of tricks involved. So it's not just putting in a bunch of pixels of a game
and getting an agent that knows how to win at that game. there's a lot of pre-processing and playing with the data required.
So which is unfortunate because the truth is messier than the hope
but one of the critical tricks needed is called experience replay.
So as opposed to letting an agent, So you're learning this big network that tries
to build a model of what's good to do in the world and what's not.
And you're learning as you go. With experience replay you're keeping a track
of all the things you did. And every once in a while, you look back into your memory and pull out some of those old experiences.
the good old times and trying on those again. As opposed to letting the agent run itself into some local optima
where it tries to learn a very subtle aspect of the game that actually in the global sense
doesn't get you farther to winning the game. Very much like life.
Deep Q-Learning Algorithm
So here's the algorithm, deep Q learning algorithm pseudo code.
We initialize the replay memory, again there's this little trick that's required.
Is keeping a track of stuff that's happened in the past, we initialize the action value function Q with random weights
and observe initial state, again same thing. Select an action with the probability Epsilon
explore, otherwise choose the best one based on the estimate provided by the neural network.
And then carry out the action, observe the reward and store that experience in the replay memory
and then sample random transition from replay memory.
So with a certain probability, you bring those old times back
to get yourself out of the local minima and then you train the Q-network
using the difference between what you actually got
and your estimate and you repeat this process over and over.
So here's what you can do after ten minutes of training on the left, so that's very little training, what you get is
a paddle that learns hardly anything and it just keeps dying.
It goes from 5 to 4 to 2 to 2 to 1, Those are the number of lives left.
Then after two hours of training in a single GPU,
it learns to win, you know, not die. Rack up points
and learns to avoid the ball from passing the paddle which is great.
That's human level performance really, better than some humans, you know, but it still dies sometimes so it's very human level.
And then after four hours it does something really amazing.
It figures out how to win the game in a very lazy way
which is drill a hole through the blocks up to the top
and get the ball stuck up there. And it does all the hard work for you. That minimizes the probability of the ball getting pas your paddle
because it's just stuck in the in the blocks up top.
So that might be something that you wouldn't even figure out to do yourself. And that's- I need to sort to pause here
to clearly explain what's happening. The input to this algorithm is just the pixels of the game.
It's the same thing that human beings take in when they take visual perception and it's able to learn
under this constrained definition of what is a reward and a punishment.
It's able to learn to get a high reward.
That's general artificial intelligence. A very small example of it but its general.
It's general purpose, it knows nothing about games and knows nothing about paddles or physics.
It's just take answer input of the game and they've did the same thing for a bunch of different games in Atari
And what's shown here in this plot on the x-axis is a bunch of different games from Atari
and on the y-axis is a percentile where 100% is about the best that human beings can do.
Meaning it's the score that human beings who get so everything about there in the middle, everything to the left of that
is far exceeding human low performance and below that is on par or worse than human performance.
So it can learn so many- Boxing, Pinball,
all of these games, and it doesn't know anything about any of the individual games, it's just taking in pixels.
It's just as if you put a human being. behind any of these games and
ask them to learn to be beat the game.
and there's been a lot of improvements in this algorithm recently. Yes, question.
No. So the question was: "do they customize the model for game, for a particular game?
And no, the point- You could, of course, but the point is it doesn't need to be customized for the game but
the important thing is that it's still only on Atari games.
Alright, so the question whether this is transferable to driving, Perhaps not.
Right, you play the game where you do. No, you don't have the- Well, yeah you play one step of the game.
So you take action in a state and then you observe that.
So you have that simulation. I mean, really that's one of the biggest problems here
is you require the simulation in order to get the Ground Truth.
So that's a great question or comment. The comment was that for a lot of these situations,
the reward function might not change at all depending on your actions. The rewards are really, most of the time, delayed
10, 20, 30 steps down the line which is why
It is amazing that this works at all. That it's learning locally.
and through that process of simulation of hundreds a thousand times runs through the game, It's able to learn what to do now such that I get a reward later.
It's if you just pause, look at the math of it. It's very simple math and look at the result, it's incredible.
So there's a lot of improvements, this one called the general reinforcement learning architecture Gorila.
The cool thing about this in the simulated world at least is that you can run deep reinforcement learning in a distributed way.
You could do both the simulation in a distributed way, you can do the learning in the distributed way,
you can generate experiences which is what this kind of diagram shows, you can, either from human beings or from simulation.
So for example, the way that Alpha Go the deep mind team
is beat the game of Go is they learn from both expert games and by playing itself.
So, you can do this in a distributed way and you could do the learning in a distributor way so you can scale.
And in this particular case, the Gorila has achieved
the better result than the DQN network and that's part of the their nature paper.
Okay, so let me now get to driving for a second here
where words of reinforcement learning,
where reinforcement learning can step in and help. So this is back to the open question they asked yesterday:
is driving closer to chess or to everyday conversation? Chess, meaning it can be formalized in a simplistic way
and if you could think about it as an obstacle avoidance problem and once the obstacle avoidance is solved,
you just navigate that constrained space you choose to move left, you choose to move right in a lane
you choose to speed up or slow down. Well, if it's a game like chess which we'll assume for today.
as opposed to for tomorrow, for today we're going to go with the one on the left
and we're going to look at DeepTraffic. Here is this game of simulation
where the goal is to achieve the highest average speed you can
on this seven lane highway full of cars.
And so, as a side note for students, the requirement is they have to follow the tutorial that I'll present a link for
at the end of this presentation. And what they have to do is achieve a speed,
build a network that achieves a speed of 65 miles an hour or higher.
There is a leaderboard and you get to submit the model you come up with with a simple click of a button.
So all of this runs in the browser which is also another amazing thing.
And then you immediately or relatively so, make your way up the leaderboard.
So let's look, let's zoom in. What is this world, two-dimensional world of traffic is,
what does it look like for the intelligent system?
We descritize that world into a grid shown here on the left. That's the representation of the state.
There are seven lanes and every single lane is broken up into blocks spatially.
And if there is a car in that block, the length of a car is about 3 blocks,
3 of those grid blocks, then that grid is seen as occupied.
and then the red car is you. That's the thing that's running in the intelligent agent.
There is on the left, is the current speed of the red car, actually says MIT on top.
And then you also have a count of how many cars you passed and if your network sucks then that number is going to get to be negative.
You can also change with a drop down the simulation speed from normal on the left to fast on the right.
So, you know, the fast speads up the replay of the simulation.
The one on the left, normal, it feels a little more like real driving.
There is a drop down for different display options. The default is non, in terms of stuff you show on the road.
Then there is the learning input which is the, while that whole space is descritized,
you can choose what your car sees
and that's you could choose how far ahead it sees behind, how far to the left and right It sees.
And so by choosing the learning input, to visualize learning input, you get to see what you set that input to be.
Then there is the safety system. This is a system that protects you from yourself.
The way we've made this game is they operates under something similar
if you have some intelligence in if you're driving you have adaptive cruise control in your car.
It operates in the same way. When he gets close to the car in front, It slows down for you It operates in the same way. When he gets close to the car in front, It slows down for you
and it doesn't let you run the car to the left of you, to the right of you, off the road.
So constrains the movement capabilities of your car
in such a way that you don't hit anybody because then it would have to simulate collisions and that would just be a mess.
So, it protects you from that and so you can choose to visualize that "safety system" with a visualization box.
And then you can also choose to visualize the full map. This is the full occupancy map that you get
if you would like to provide as input to the network.
Now that input for every single grid that it's a number. It's not just a 0, 1 whether there's a car in there.
It's the maximum speed limit which is 80 miles per hour.
Don't get crazy eighty miles an hour is the speed limit. That block when it's empty is set to the 85 miles eighty miles an hour.
And when it's occupied, it's set to the number that is the speed of the car.
And then, the blocks that the red car is occupying is set to the number, to a very large number
much higher than the speed limit.
So safety system, here shown in red, are the parts of the grid that your car can't move into.
Question. What's that?
Yes. Yes. The question was: "what was the third option I just mentioned and
t's you the red car itself, you yourself, the blocks underneath that car I set to really high number.
It's a way for the algorithm to know, for the learning algorithm to know that these blocks are special.
So safety system, shows read here, if
the car can't move into those blocks. So ,in terms of when it lights up red, it means
the car can't speed up anymore in front of it and when the blocks to the left or to the right light up as red
that means you can't change lanes to the left or right. On the right of the slide, you're free to go,
free to do whatever you want. That's what that indicates is all the blocks are yellow.
Safety system says you're free to choose any of the five actions. In the five actions are move left, move right,
same place, accelerate or slow down.
And those actions are given as input. That action was produced by the what's called here, the brain.
The brain takes in the current state as input, the last reward, and produces and learns and uses that reward
to train the network through backward function there,
back propagation, and then ask the brain given the current state,
to give it the next action with the forward pass, the forward function. You don't need to know the operation of this function in particular,
this is not something you need to worry about, but you can if you want, you can customize this learning step.
There's, by the way, what I'm describing now there's just a few lines of code right there in the browser
that you can change immediately with the press of a button
changes the simulation or the design of the network. You don't need to have any special hardware,
you dont' need to do anything special. And the tutorial cleanly outlines exactly all of these steps
but it's kind of amazing that you can design a deep neural network that's part of the reinforcement learning agent.
So it's a deep Q learning agent right there in the browser.
So you can choose the lane side variable which controls how many lanes to the side you see.
So in that value zero you only look forward. When their values 1, you have one lane to the left, one valid to the right.
It's really the lane the radius of your perception system. Patches ahead is how far ahead you look;
patches behind is how far behind you look. And so for example here, the lane side equals 2 that means
it looks to the left, to the right; obviously, if to the right, is off road.
It provides a value of 0 in those blocks.
If we set the patches behind to be 10, it looks 10 patches back behind starting at the 1 patch back is starting from the front of the car.
The scoring for the evaluation of the competition
is your average speed over a predefined period of time. And so the method we do we use to collect that speed
is we we run the agent 10 runs, about 30 simulated minutes of game each.
And take the median speed of the 10 runs. That's the score.
This is done server side and so given that we've gotten some
for this code recently gotten some publicity online unfortunately.
This might be a dangerous thing to say there's no cheating possible. But because it's done server side and this is javascript
and runs in the browser, it's hopefully a sandbox. So we can't do anything tricky but we dare you to try.
You can try it locally to get an estimate, you know, and there's a button that says evaluate and it gives you a score right back
of how well you're doing with the current network.
That button is: Start Evaluation Run; you press the button.
It does a progress bar and gives you the average speed
There's a code box where you modify all the variables I mentioned
and the tutorial describe this in detail. And then once you're ready, you modify a few things
you can press apply code it restarts, it kills all the training
that you've done up to this point or resets it and start the training again.
So save often and there's a save button. So the training is done a separate thread in Web Workers
which are exciting things that allow javascript to run
amazingly on multiple CPU Cores in a parallel way.
So the simulation that scores this or, sorry, the training is done a lot faster than real time, a thousand frames a second.
That's a thousand movement steps a second. This is all in javascript.
And the next they get shipped to the main simulation from time to time as the training goes on.
So all you have to do is press run training. And it trains and the car behaves better over time.
Maybe like I should show it in the browser.
Let's see if will work well, is this going to mess up? We're good.
What can possibly go wrong?
So there's the game. When it starts, this is running live in the browser.
Artificial intelligence, ladies and gentleman in the browser. a neural network.
So currently it's not very good, it's driving at 2 miles an hour and watching everybody pass.
So what's being shown live is the lost function which is pretty poor.
So in order to train, like I said, a thousand frames a second
you just press the "Run Training" button and pretty quickly it learns based on the network you specify in the code box, how to-
and based on the input and all the things that I mentioned, training finished. It learns how to do a little better.
We, on purpose. put in a network that's not very good in there. So right now I won't, on the average, be doing that well
but it does better than standing there in place and then you could do the start Evaluation Run
to simulate the network much faster than real time, to see how well it does
This is a similar evaluation step that we take when determining where you stand on the leaderboard
at the current current average speed. In that 10 run simulation is 56.56 miles per hour.
Now, I may be logged in, maybe not. If you're logged in, you click "Submit your code."
If you're not logged in, it says: "You're not logged in. Please log in to submit your code."
And then all you have to do is log in. This is the most flawless demo of my life.
And then you press "Submit Model" again and success. Oh man.
"Thank you for your submission." And so now my submission is entered as "Lex" in the leaderboard
and my 56.56, or whatever it was. So I dare all of you to try to beat that. So too.
As as you play around with stuff if you want to save the code
you could do so by pressing the "Save Code" button. That saves the various javascript configurations
and that saves the network layout to file. And you can load from files as well. the danger it overrides the code for you.
And you press the "Submit" button to submit the model to the competition. Make sure that you train the network, we don't train it for you.
You submit a model and you have to press "Train". And he gets evaluated the time it enters a queue to get evaluated.
This is public phasing so the queue can grow pretty big and it goes to that queue, evaluates it and then depending on where you stand
you get added to the leaderboard showing the top ten entries. You can resubmit often and only the highest score counts.
Okay, we're using code- Now implementation of neural networks done in just javascript
by Andrej Karpathy from Stanford now OpenAI. ConvNet.JS is a library and what's being visualized there
is also being visualized in the game is the inputs to the network. In this case it's 135 inputs. You can also specify not just the
how far ahead behind you're seeing to the left and to the right, you can specify how far back in time you look as well.
And so what's visualize there is the input to the network 135 neurons
and then the output, a regression, similar to the kind of opo we saw with numbers where there's 10 outputs saying
if it's a 0, 1 through 9, here the output is one of the five actions:
left, right, stay in place, speed up or slow down. The ConvNet.JS settings is you can select a number of inputs
if you want to mess with this stuff, this is all stuff you don't need to mess with because we already gave you the variables of lane side and patches ahead and so on.
You can select a number of actions, the temporal window and the network size.
So the network definition here is the-
This is the input, the size of the input. Again all this is in the tutorial just to give you a little outline.
There is the first fully connected layer has 10 neurons
with relu activation functions, same kind of smooth
function that we talked about before and the regression layer for the output.
And there's a bunch of other messy options you play with if you dare.
But those aren't, the ones I mentioned before is really the important ones. Selecting the number of layers, the size of those layers,
you get to build your own very neural network that drives. And the actual learning is done with a backward propagation
and then that returns the action by doing a forward pass to the network.
In case you're interested in this kind of stuff, there is an amazingly cool code editor.
That's the Monaco Editor. It just works, it does some auto-completions
so you get to play with it makes everything very convenient in terms of coding editing.
A lot of this visualization of the game and the simulation we'll talk about tomorrow
is done in the browser using HTML5 canvas. So here is a simple specification of a blue box with canvas
and this is very efficient and easy to work with.
And the thing that a lot of us are excited about, a very subtle one, but there you can, not just run.
So with the V8 Engine javascript has become super fast. You could train neural networks in the browser that's already amazing.
And then with Web Workers as long as you have Chrome, a modern browser.
You can run multiple processes in separate threads
so you could do a lot of stuff you can do visualization separately and you can train separate threads, very cool.
Okay. so the tutorial is cars.mit,edu/deeptraffic. We won't put these links on the website for a little bit because.
We got put on the front page of Hacker News which we don't want those to leak out
especially with the claims the you can't cheat.
And while it's pretty efficient in terms of running everything on your machine, client side,
it's still. you have to pull some images here and pull some of the code. So the tutorials on cars.mit,edu/deeptraffic and the simulation is deeptrafficjs
So cars.mit,edu/deeptrafficjs I encourage you to go there play with the network submit your code.
and win the very special prize and it is pretty cool one but we're still working on it.
There is a prize I swear. All right so let's take a pause and think about what we talked about today.
So the very best of deep reinforcement learning is the most exciting accomplishment,
I think, is when the game- When I first started as a freshman, took "Intro to Artificial Intelligence"
it was said that it's a game that's impossible for machines to beat because of the combinatorial complexity they just
the sheer number of options. it's so much more complex than chess and so the most amazing accomplishment of deep reinforcement learning
to me is the design of AlphaGo when for the first time the world champion in Go was beaten
by DeepMind AlphaGo and the way they did it
and this is, I think very relevant to driving is you start by creating first in a supervised way training a policy network.
So you take expert games to construct a network first so you look you don't play against yourself.
They agent doesn't play against itself but they learn from expert games, so there is some human Ground Truth.
This Human Ground Truth represents reality, so for driving this is important We have a- Well we're starting to get a lot of data were video of drivers is being recorded.
So we can learn on that data before would then run the agents through a simulation where it learns much larger magnitudes
of data sets through simulation. And they did just that. Now as a reminder that when you let an agent drive itself.
This is probably one of the favorite videos of all time but I just recently saw a cyclist and just watch this for hours.
but it's a reminder that you can't trust your first estimates of a reward function to be those that are safe
and productive for our society when you're talking about an intelligence system that gets to operate in the real world.
This is just as clear of a reminder of that as there is. So again all the references are available online.
For these slides. we'll put up the slides. I imagine you might have, if you want to come down and talk to us for questions for the either Docker
or javascript. Question. The question was: "What is the visualization you're seeing in deep traffic?"
You're seeing a car move about. Why is it moving? It's moving based on the latest snapshot of the network you trained, so it's just visualizing; for you, just for fun.
The network you train most recently. Okay so if people have questions, stick around afterwards. Just details on Docker and [CHUCKLING]- Yes. Do you want to do it offline?

----------

-----

--15--

-----
Date: 2017.01.16
Link: [# MIT 6.S094: Introduction to Deep Learning and Self-Driving Cars](https://www.youtube.com/watch?v=1L0TKZQcUtA)

Transcription:
Intro
Alright. Hello everybody. Hopefully you can hear me well. Yes?
Yes. Great! So, welcome to Course 6.S094.
Deep Learning for Self-Driving Cars. We will introduce to you the methods of deep learning,
of deep neural networks using the guiding case study of building self-driving cars.
My name is Lex Fridman. You get to listen to me for a majority of these lectures
and I am part of an amazing team with some brilliant TAs.
Would you say brilliant? (CHUCKLES) Dan Brown.
You guys want to stand up? They're in the front row. Spencer, William Angell.
Administrative
Spencer Dodd and all the way in the back. The smartest and the tallest person I know, Benedict Jenik.
Well you see there on the left of the slide is a visualization of one of the two projects that one of the two simulations, games that we'll get to go through.
We use it as a way to teach you about deep reinforcement learning but also as a way to excite you.
By challenging you to compete against others if you wish to in a special prize yet to be announced.
Super secret prize. So you can reach me and the TA's at deepcars@MIT.edu if you have any questions about the tutorials, about the lecture, about anything at all.
The website cars.mit.edu has the lecture content. Code tutorials, again like today, the lectures slides for today are already up in PDF form.
The slides themselves, if you want to see them just e-mail me but there are over a gigabyte in size because they're very heavy in videos so I'm just posting the PDS.
And there will be lecture videos available a few days after the lectures were given.
So speaking of which there is a camera in the back. This is being videotaped and recorded but for the most part the camera is just on the speaker.
So you shouldn't have to worry. If that kind of thing worries you then you could sit on the periphery of the classroom
or maybe I suggest sunglasses and a moustache, fake mustache, would be a good idea.
There is a competition for the game that you see on the left. I'll describe exactly what's involved
in order to get credit for the course you have to design a neural network that drives the car just above the speed limit sixty five miles an hour.
But if you want to win, we need to go a little faster than that.
So who's this class is for? You may be new to programming,
new to machine learning, new to robotics, or you're an expert in those fields but want to go back to the basics.
So what you will learn is an overview of deep reinforcement learning, of convolutional neural networks,
recurring neural networks and how these methods can help improve each of the components of autonomous driving -
perception, visual perception, localization, mapping, control planning and the detection of driver state.
Okay, two projects. Code named "DeepTraffic" is the first one.
Project: Deep Traffic
There is, in this particular formulation of it, there is seven lanes. It's a top view.
It looks like a game but I assure you it's very serious. It is the agent in red,
the car in red is being controlled by a neural network and we'll explain how you can control and design the various aspects, the various parameters of this neural network
and it learns in the browser. So this, we're using ConvNet.JS
which is a library that is programmed by Andrej Karpathy in javascript.
So amazingly we live in a world where you can train in a matter of minutes
a neural network in your browser. And we'll talk about how to do that. The reason we did this
is so that there is very few requirements to get you up and started with neural networks.
So in order to complete this project for the course, you don't need any requirements except to have a Chrome browser.
And to win the competition you don't need anything except the Chrome browser.
Project: DeepTesla
The second project code name "DeepTesla" or "Tesla"
is using data from a Tesla vehicle
of the forward road way and using end-to-end learning taking the image and putting into convolutional neural networks
that directly maps "or aggressor" that maps to a steering angle.
So all it takes is a single image and it predicts a steering angle for the car.
We have data for the car itself and you get to build a neural network that tries to do better,
tries to steer better or at least as good as the car. Okay.
Let's get started with the question,
with the thing that we understand so poorly at this time because it's so shot in mystery
but it fascinates many of us. And that is the question of: "What is intelligence?"
This is from a March 1996 Time magazine.
And the question: "Can machines think?" is answered below with, "they already do."
So what if anything is special about the human mind? It's a good question for 1996,
a good question for 2016, 2017 now, and the future.
And there's two ways to ask that question. One is the special purpose version.
Can an artificial intelligence system achieve a well defined,
specifically, formally defined finite set of goals? And this little diagram
Defining Artificial Intelligence
from a book that got me into artificial intelligence as a bright-eyed high school student
they are artificial intelligence to modern approach. This is a beautifully simple diagram of a system.
It exists in an environment. It has a set of sensors that do the perception.
It takes those sensors in. It does something magical. There's a question mark there. And with a set of affectors acts in the world, manipulates objects in that world,
and so special purpose. We can,
under this formulation, as long as the environment is formally defined, well defined; as long as a set of goals are well defined.
As long as the set of actions, sensors, and the ways that the perception carries itself out as well defined.
We have good algorithms which will talk about that can optimize for those goals.
The question is, if we inch along this path,
will we get closer to the general formulation, to the general purpose version of what artificial intelligence is?
Can it achieve poorly defined, unconstrained set of goals with an unconstrained, poorly defined set of actions
and unconstrained, poorly defined utility functions rewards.
This is what human life is about. This is what we do pretty well most days.
Exist in an undefined, full of uncertainty, world.
So, okay. We can separate tasks into three different, categories, formal tasks.
This is the easiest. It doesn't seem so, it didn't seem so at the birth of artificial intelligence
but that's in fact true if you think about it. The easiest is the formal tasks, playing board games, theory improving.
All the kind of mathematical logic problems that can be formally defined.
Then there is the expert tasks. So this is where a lot of the exciting breakthroughs have been happening
where machine learning methods, data driven methods, can help aid or improve on
the performance of our human experts. This means medical diagnosis, hardware design,
scheduling, and then there is the thing that we take for granted. The trivial thing.
The thing that we do so easily every day when we wake up in the morning.
The mundane tasks of everyday speech, of written language, of visual perception,
of walking which we'll talk about in today's lecture is a fascinatingly difficult task
on object manipulation. So the question is that we're asking here,
before we talk about deep learning, before we talk about the specific methods, we really want to dig in and try to see what is it about driving,
how difficult is driving. Is it more like chess which you see on the left there
How Hard is Driving?
where we can formally define a set of lanes, a set of actions and formulate it as there's five set of actions - you can change your lane,
you can avoid obstacles. You can formally define an obstacle. You can the formally define the rules of the road.
Or is there something about natural language, something similar to everyday conversation about driving
that requires a much higher degree of reasoning, of communication,
of learning, of existing in this under-actuated space. Is it a lot more than just left lane,
right lane, speed up, slow down? So let's look at it as a chess game.
Chess Pieces: Self-Driving Car Sensors
Here's the chess pieces. What are the sensors we get to work with on an autonomous vehicle?
And we get a lot more in-depth on this especially with the guest speakers who built many of these.
There's radar. There's the Rays sensors. Radar lidar. They give you information about the obstacles in their environment.
They'll help localize the obstacles in the environment. There's the visible light camera and stereo vision that gives you texture information,
that helps you figure out not just where the obstacles are but what they are, helps to classify those,
has to understand their subtle movements.
Then there is the information about the vehicle itself, about the trajectory and the movement of the vehicle that comes from the GPS
an IMU sensors. And there is the rich state of the vehicle itself.
What is it doing? What are all the individual systems doing that comes from the canned network.
And there is one of the less studied but fascinating to us on the research side is audio.
The sounds of the road that provide the rich context
of a wet road. The sound of a road that when it stop raining but it's still wet,
the sound that it makes. The screeching tire and honking.
These are all fascinating signals as well. And the focus of the research in our group,
the thing that's really much under-investigated
is the internal facing sensors. The driver, sensing the state of the driver,
were they looking? Are they sleepy? The emotional state. Are they in the seat at all?
And the same with audio. That comes from the visual information and the audio information.
Chess Pieces: Self-Driving Car Tasks
More than that. Here are the tasks. If you were to break into modules the tasks
of what it means to build a self-driving vehicle. First, you want to know where you are.
Where am I. Localization and mapping. You want to map the external environment.
Figure out where all the different obstacles are, all the entities are,
and use that estimate of the environment to then figure out where I am,
where the robot is. Then there is scene understanding.
It's understanding not just the positional aspects of the external environment and the dynamics of it
but also what those entities are. Is it a car? Is it a pedestrian? Is it a bird?
There is movement planning. Once you have kind of figured out to the best of your abilities
your position and the position of other entities in this world, it's figuring out a trajectory through that world.
And finally, once you've figured out how to move about safely and effectively through the world
it's figuring out what the human that's on board is doing because as I will talk about
the path to a self-driving vehicle and that is, hence, our focus on Tesla
may go through semi-autonomous vehicles.
Where the vehicle must not only drive itself
but effectively hand over control from the car to the human
and back. Ok, quick history. Well, there's a lot of fun stuff from the eighty's and ninety's but
DARPA Grand Challenge II (2006)
the big breakthroughs came in the second DARPA Grand Challenge
with Stanford Stanley, when they won the competition. One of five cars that finished.
This was an incredible accomplishment in a desert race.
A fully autonomous vehicle was able to complete the race in record time.
DARPA Urban Challenge (2007)
The DARPA Urban Challenge in 2007
where the task was no longer a race to the desert
but through an urban environment and CMU's "Boss" with GM won that race
Industry Takes on the Challenge
and a lot of that work went directly into the
acceptance and large major industry players
taking on the challenge of building these vehicles. Google, now "Waymo" self-driving car.
Tesla with its "Autopilot" system and now "Autopilot 2" system.
Uber with its testing in Pittsburgh. And there's many other companies
including one of the speakers for this course of nuTonomy that are driving the wonderful streets of Boston.
How Hard is it to Pass the Turing Test?
Ok. So let's take a step back. We have, if we think about the accomplishments in the DARPA Challenge,
and if you look at the accomplishments of the Google self-driving car
which essentially boils the world down into a chess game.
It uses incredibly accurate sensors to build a three dimensional map of the world,
localize itself effectively in that world and move about that world
in a very well-defined way.
Now, what if driving... The open question is: if driving is more like a conversation,
like in natural language conversation, how hard is it to pass the Turing Test?
The Turing Test, as the popular current formulation is, can a computer be mistaken for a human being
in more than thirty percent of the time? When a human is talking behind a veil,
having a conversation with their computer or a human, can they mistake the other side of that conversation
for being a human when it's in fact a computer.
And the way you would, in a natural language, build a system that has successfully passes the Turing Test is,
the natural language processing part to enable it to communicate successfully? So, general language and interpret language,
then you represent knowledge the state of the conversation transferred over time.
And the last piece and this is the hard piece, is the automated reasoning,
is reasoning. Can we teach machine learning methods to reason?
That is something that will propagate through our discussion because as I will talk about the various methods,
the various deep learning methods, neural networks are good at learning from data
but they're not yet, there is no good mechanism for reasoning. Now reasoning could be just something
that we tell ourselves we do to feel special. Better to feel like we're better than machines.
Reasoning may be simply something as simple as learning from data.
We just need a larger network. Or there could be a totally different mechanism required
and we'll talk about the possibilities there.
Yes. (Inaudible question from one of the attendees)
No, it's very difficult to find these kind of situations in the United States. So the question was,
for this video, is it in the United States or not? I believe it's in Tokyo.
So India, as is a few European countries, are much more towards the direction
of natural language versus chess.
In the United States, generally speaking, we follow rules more concretely.
The quality of roads is better. The marking on the roads is better. So there's less requirements there.
(Inaudible question from one of the attendees)
These cars are are driving on one side?
I see. I just- Okay, you're right. It is because, yeah-
So, but it's certainly not the United States.
I spent quite a bit of googling trying to find in the United States and it is difficult.
Neuron: Biological Inspiration for Computation
So let's talk about the recent breakthroughs in machine learning
and what is at the core of those breakthroughs is neural networks
that have been around for a long time and I will talk about what has changed.
What are the cool new things and what hasn't changed and what are its possibilities.
But first a neuron, crudely,
is a computational building block of the brain. I know there's a few folks here, neuroscience folks,
this is hardly a model. It is mostly an inspiration
and so the human neuron
has inspired the artificial neuron the computational building block of a neural network,
of an artificial neural network. I have to give you some context.
These neurons, for both artificial and human brains, are interconnected.
And the human brain, there's about, I believe 10,000 outgoing connections from every neuron
on average and they're interconnected to each other,
are the largest current, as far as I'm aware, artificial neural network, has 10 billion of those connections.
Synapses. Our human brain, to the best estimate that I'm aware of,
has 10,000X that.
So one hundred to one thousand trillion synapses.
Perceptron: Forward Pass
Now what is an artificial neuron?
That is the building block of a neural network. It takes a set of inputs.
It puts a weight on each of those inputs, sums them together,
applies a bias value on each neuron
and using an activation function that takes its input,
that sum plus the bias and it squishes it together
to produce a zero to one signal.
Perceptron Algorithm
And this allows us a single neuron to take a few inputs and produces an output
a classification for example, a zero one. And then we'll talk about, simply, it can
serve as a linear classifier so it can draw a line. It can learn to draw a line between, like what you'd seen here,
between the blue dots and the yellow dots. And that's exactly what we'll do in the iPython Notebook that I'll talk about
but the basic algorithm is you initialize the weights on the inputs and you compute the output.
You perform this previous operation I talked about sum up and compute the output.
And if the output does not match the ground truth,
The expected output, the output it should produce, the weights are punished accordingly
and will talk through a little bit of the math of that.
And this process is repeated until the perceptron does not make any more mistakes.
Neural Networks are Amazing
Now here's the amazing thing about neural networks.
There are several and I'll talk about them. One on the mathematical side is the universality of neural networks
with just a single layer if you stack them together, a single hidden layer,
the inputs on the left, the outputs on the right. And in the middle there is a single hidden layer,
it can closely approximate any function. Any function.
So this is an incredible property that with a single layer any function you could think of,
that you could think of driving as a function. It takes its input,
the world outside as output to control the vehicle.
There exists a neural network out there that can drive perfectly.
It's a fascinating mathematical fact.
Special Purpose Intelligence
So we can think of this then these functions as a special purpose function, special purpose intelligence.
You can take, say as input, the number of bedrooms, the square feet,
the type of neighborhood. Those are the three inputs.
It passes that value through to the hidden layer.
And then one more step. It produces the final price estimate for the house or for the residence.
And we can teach a network to do this pretty well in a supervised way. This is supervised learning.
You provide a lot of examples where you know the number of bedrooms, the square feet,
the type of neighborhood and then you also know the final price of the house or the residence.
And then you can, as I'll talk about through a process of back propagation, teach these networks to make this prediction pretty well.
General Purpose Intelligence
Now some of the exciting breakthroughs recently have been in the general purpose intelligence.
This is is from Andrej Karpathy who is now at OpenAI.
I would like to take a moment here to try to explain how amazing this is.
This is a game of "pong". If you're not familiar with "pong", there are two paddles
and you're trying to bounce the ball back and in such a way that prevents the other guy from bouncing the ball back at you.
The artificial intelligence agent is on the right in green and up top is the score 8-1.
Now this takes about three days to train on a regular computer, this network.
What is this network doing? It's called the Policy Network.
The input is the raw pixels. There's slightly a process and also you take the difference between two frames
but it's basically the raw pixel information. That's the input.
There's a few hidden layers and the output is the single probability of moving up.
That's it. That's the whole system and what it's doing is, it learns.
You don't know at any one moment,
you don't know what the right thing to do is. Is it to move up? Is it's moved down? You only know what the right thing to do is
by the fact that eventually you win or lose the game. So this is the amazing thing here is, there's no supervised learning.
There's no universal fact about anyone stay being good or bad.
And anyone actually being good or bad in the state but if you punish or reward every single action you took,
every single action you took, for an entire game based on the result. So no matter what you did, if you won the game,
the end justifies the means. If you won the game, every action you took in every every action state pair gets rewarded.
If you lost the game, it gets punished. And this process, with only two hundred thousand games
where the system just simulates the games, it can learn to beat the computer.
This system knows nothing about "pong", nothing about games,
this is general intelligence. Except for the fact, that it's just a game "pong".
And I will talk about how this can be extended further,
why this is so promising and why we should proceed with caution.
So again, there's a set of actions you take up, down, up, down, based on the output of the network.
There's a threshold given the probability of moving up, you move up or down based on the output of the network.
And you have a set of states and every single state action pair is rewarded if there's a win
and it's punished if there's a loss.
When when you go home, think about how amazing that is
and if you don't understand why that's amazing, spend some time on it.
It's incredible. (Inaudible question from one of the attendees)
Sure, sure thing. The question was: "What is supervised learning? What is unsupervised learning? What's the difference?"
So supervised learning is, when people talk about machine learning they mean supervised learning most of the time.
Supervised learning is
learning from data, is learning from example. When you have a set of inputs and a set of outputs that you know are correct or
called Ground Truth. So you need those examples, a large amount of them,
to train any of the machine learning algorithms to learn to then generalize that to future examples.
Actually, there's a third one called Reinforcement Learning where the Ground Truth is sparse.
The information about when something is good or not,
the ground truth only happens every once in a while, at the end of the game. Not every single frame.
And unsupervised learning is when you have no information about the outputs.
They are correct or incorrect. And it is the excitement of the deep learning community is unsupervised learning,
but it has achieved no major breakthroughs at this point. I'll talk about what the future of deep learning is
and a lot of the people that are working in t he field are excited by it. But right now, any interesting accomplishment has to do with supervised learning.
(Partially inaudible question from one of the attendees)
And the wrong one is just has the [00:33:29] (Inaudible) solution like looking at the philosophy.
So basically, the reinforcement learning here is learning from somebody who has certain hopes
and how can that be guaranteed that it would generalize to somebody else?
So the question was this:
the green paddle learns to play this game successfully against this specific one brown paddle operating under specific kinds of rules.
How do we know it can generalize to other games, other things and it can't.
But the mechanism by which it learns generalizes. So as long as you let it play,
as long as you let it play in whatever world you wanted it to succeed in long enough,
it will use the same approach to learn to succeed in that world.
The problem is this works for worlds you can simulate well.
Unfortunately, one of the big challenges of neural networks
is they're not currently efficient learners. We need a lot of data to learn anything.
Human beings need one example often times and they learn very efficiently from that one example.
And again I'll talk about that as well, it's a good question. So the drawbacks of neural networks.
So if you think about the way a human being would approach this game, this game of "pong", it would only need a simple set of instructions.
You're in control of a paddle and you can move it up and down.
And your task is to bounce the ball past the other player controlled by AI.
Now the human being would immediately, they may not win the game but they would immediately understand the game
and would be able to successfully play it well enough to pretty quickly learn to beat the game.
But they would need to have a concept of control. What it means to control a paddle, need to have a concept of a paddle,
need to have a concept of moving up and down and a ball and bouncing,
they have to know, they have to have at least a loose concept of real world physics
that they can then project that real world physics on to the two dimensional world. All of these concepts are concepts that you come to the table with.
That's knowledge. And the kind of way you transfer that knowledge from your previous experience,
from childhood to now when you come to this game, that something is called reasoning.
Whatever reasoning means. And the question is whether through this same kind of process,
you can see the entire world as a game of "pong"
and reasoning is simply the ability to simulate that game in your mind
and learn very efficiently, much more efficiently, than 200,000 innovations.
The other challenge of deep neural networks and machine learning broadly is you need big data and efficient learners as I said.
And that data also need to be supervised data. You need to have Ground Truth which is very costly for annotation.
A human being looking at a particular image, for example, and labeling that as something as a cat or dog,
whatever objects is in the image, that's very costly.
And particularly for neural networks there's a lot of parameters to tune.
There's a lot of hyper-parameters. You need to figure out the network structure first.
How does this network look, how many layers? How many hidden nodes?
What type of activation function for each node? There's a lot of hyper-parameters there
and then once you've built your network, there's parameters for how you teach that network.
There's learning rate, loss function - meaning bad size - number of training iterations, gradient updates moving
and selecting even the optimizer with which you solve the various differential equations involved.
It's a topic of many research paper, certainly it's rich enough for research papers, but it's also really challenging.
It means you can't just pop the network down it will solve the problem generally.
And defining a good lost function, or in the case of "pong" or games,
a good reward function is difficult. So here's a game, this is a recent result from OpenAI,
I'm teaching a network to play the game of coast runners.
And the goal of coast runners is you're in a boat the task is to go around the track
and successfully complete a race against other people you're racing against.
Now this network is an optimal one. And what is figured out that actually in the game,
it gets a lot of points for collecting certain objects along the path. So you see it's figured out to go in a circle and collect those those green turbo things.
And what is figured out is you don't need to complete the game to earn the award.
And despite being on fire and hitting the wall and going through this whole process, it's actually achieved at least the local optima
given the reward function of maximizing the number of points.
And so it's figured out a way to earn a higher reward
while ignoring the implied bigger picture goal of finishing the race which us as humans understand much better.
This raises, for self-driving cars, ethical questions.
Besides other quick questions. (CHUCKLING) We could watch this for hours and it will do that for hours and that's the point:
Deep Learning Breakthroughs: What Changed?
It's hard to teach, it's hard to encode the formally defined utility function under which
an intelligent system needs to operate. And that's made obvious even in a simple game.
And so what is - Yup, question. (Inaudible question from one of the attendees)
So the question was: "what's an example of a local optimum that an autonomous car,
similar to the cost racer, what would be the example in the real world for an autonomous vehicle?
And it's a touchy subject. But it would certainly have to be involved
the choices we make under near crashes and crashes. The choices a car makes want to avoid.
For example, if there's a crash imminent and there's no way you can stop
to prevent the crash, do you keep the driver safe or do you keep the other people safe.
And there has to be some, even if you don't choose to acknowledge it,
even if it's only in the data and the learning that you do, there's an implied reward function there.
And we need to be aware of that reward function is because it may find something. Until you actually see it, we won't know it.
Once we see it, we realize that oh that was a bad design
and that's the scary thing. It's hard to know ahead of time what that is.
So the recent breakthroughs from deep learning came several factors.
First is the compute, Moore's Law. CPUs are getting faster, hundred times faster, every decade.
Then there's GPU use. Also the ability to train neural networks and GPUs and now ASICs
has created a lot of capabilities in terms of energy efficiency
and being able to train larger networks more efficiently.
Well, first of all in the in the 21st Century there's digitized data. There's larger data sets of digital data
and now there is that data is becoming more organized, not just vaguely available data out there on the internet,
it's actual organized data sets like Imagenet. Certainly for natural languages there's large data sets.
There is the algorithm innovations, Backprop. Back propagation, Convolutional Neural Networks, LSTMs.
All these different architectures for dealing with specific types of domains and tasks.
There is the huge one, is infrastructure. It's on the software and the hardware side.
There's Git, Ability to Share and Open Source Way software. There are pieces of software that make robotics and make machine learning easier.
ROS, TensorFlow. There is Amazon Mechanical Turk
which allows for efficient, cheap annotation of large scale data sets.
As AWS and the cloud hosting, machine learning hosting the data and the compute.
And then there's a financial backing of large companies - Google, Facebook, Amazon.
But really nothing is changed. There really has not been any significant breakthroughs.
Convolutional networks have been around since the 90s, neural networks has been around since the 60s.
There's been a few improvements but the hope is, that's in terms of methodology,
the compute has really been the work horse. The ability to do the hundred fold improvement every decade,
holds promise and the question is whether that reasoning thing I talked about,
all you need is a larger network. That is the open question.
Useful Deep Learning Terms
Some terms for deep learning. First of all deep learning, is a PR term for neural networks.
It is a term for utilising deep neural networks
for neural networks to have many layers. It is symbolic term for the newly gained capabilities that compute has brought us.
That training on GPUs have brought us. So deep learning is a subset of machine learning.
There's many other methods that are still effective. The terms that will come up in this class is, first of all, Multilayer Perceptron (MLP)
Deep neural networks (DNN), Recurrent neural networks (RNN), LSTM (Long Short-Term Memory) Networks, CNN and ConvNet (Convolutional neural networks),
Deep Belief Networks. And the operational come up is Convolutional, Pooling, Activation functions and Backpropagation.
Yes, you've got a question?
(Inaudible question from one of the attendees)
So the question was, what is the purpose of the different layers in neural network? What is the need of one configuration versus another?
So a neural network, having several layers, it's the only thing you have an understanding of, is the inputs and the outputs.
You don't have a good understanding about what these layer does. They are mysterious things, neural networks.
So I'll talk about how, with every layer, it forms a higher level.
A higher order representation of the input. So it's not like the first layer does localization,
the second layer does path planning, the third layer does navigation - how you get from here to Florida -
or maybe it does, but we don't know. So we know we're beginning to visualize neural networks for simple tasks
like for ImageNet classifying cats versus dogs. We can tell what is the thing that the first layer does, the second layer, the third layer
and we look at that. But for driving, as the input provide just the images the output the steering.
It's still unclear what you learned partially because we don't have neural networks that drive successfully yet.
(Points to a member of the class) (Inaudible question)
So the question was, does a neural network generate layers over time, like does it grow it?
That's one of the challenges, that a neural network is pre-defined. The architecture, the number of nodes, the number of layers. That's all fixed.
Unlike the human brain where the neurons die and are born all the time. A neural Network is pre-specified, that's it.
That's all you get and if you want to change that, you have to change that and then retrain everything.
So it's fixed. So what I encourage you is to proceed with caution
Neural Networks: Proceed with Caution
because there's this feeling when you first teach a network with very little effort,
how to do some amazing tasks like classify a face versus non-face,
or your face versus other faces or cats versus dogs, its an incredible feeling.
And then there's definitely this feeling that I'm an expert
but what you realize is we don't actually understand how it works.
And getting it to perform well for more generalized task, for larger scale data sets, for more useful applications,
requires a lot of hyper-parameter tuning. Figuring out how to tweak little things here and there
and still in the end, you don't understand why it work so damn well.
Deep Learning is Representation Learning
So deep learning, these deep neural network architectures is representation learning.
This is the difference between traditional machine learning methods where,
for example, for the task of having an image here is the input.
The input to the network here is on the bottom, the output up on top, and the input is a single image of a person in this case.
And so the input, specifically, is all the pixels in that image.
RGB, the different colors of the pixels in the image. And over time, what a network does is build a multiverse solutional representation of this data.
The first layer learns the concept of edges, for example.
The second layer starts to learn composition of those edges, corners, contours.
Then it starts to learn about object parts. And finally, actually provide a label for the entities that are in the input.
And this is the difference in traditional machine learning methods where the concepts like edges and corners and contours
are manually pre-specified by human beings, human experts, for that particular domain.
Representation Matters
And representation matters because figuring out a line
for the Cartesian coordinates of this particular data set where you want to design a machine learning system
that tells the difference between green triangles and blue circles is difficult.
There is no line that separates them cleanly. And if you were to ask a human being, a human expert in the field.
to try to draw that line they would probably do a Ph. D. on it and still not succeed.
But a neural network can automatically figure out to remap that input into polar coordinates
where the representation is such that it's an easily, linearly separable data set.
And so, deep learning is a subset of representation learning, is a subset of machine learning and a key subset artificial intelligence.
Deep Learning: Scalable Machine Learning
Now, because of this, because of its ability to compute an arbitrary number of features
that are at the core of the representation. So if you are trying to detect a cat in an image,
you're not specifying 215 specific features of cat ears and whiskers and so on
that a human expert will specify you allow and you'll know it discover tens of thousands of such features,
which maybe for cats you are an expert but for a lot of objects you may never be able to sufficiently provide the features
which successfully will be used for identifying the object. And so, this kind of representation learning,
one is easy in the sense that all you have to provide is inputs and outputs.
All you need to provide is a data set the care about without [00:53:39] features.
And two, because of it's ability to construct arbitrarily sized representations,
deep neural networks are hungry for data. The more data we give them,
the more they are able to learn about this particular data set.
Applications: Object Classification in Images
So let's look at some applications.
First, some cool things that deep neural networks have been able to accomplish up to this point.
Let me go through them. First, the basic one.
AlexNet is for- ImageNet is a famous data set and a competition of classification,
localization where the task is given an image, identify what are the five most likely things in that image
and what is the most likely and you have to do so correctly. So on the right, there's an image of a leopard
and you have to correctly classify that that is in fact the leopard. So they're able to do this pretty well given a specific image.
Determine that it's a leopard. And we started, what's shown here on the x-axis is years
on the y-axis is error in classification. So starting from 2012 on the left with AlexNet and today
the errors decreased from 16% and 40% before then with traditional methods
have decreased to <4%. So human level performance, if I were to give you this picture of a leopard
is a 4% of those pictures of leopards you would not say it's a leopard.
That's human level performance. So for the first time in 2015, convolutional neural networks are performed human beings.
That in itself is incredible. That is something that seemed impossible. And now is because it's done is not as impressive.
illumination Variability
But I just want to get to why this is so impressive because computer vision is hard.
Now we as human beings have evolved visual perception over millions of years, hundreds of millions of years.
So we take it for granted but computer vision is really hard, visual perception is really hard.
There's illumination variability. So it's the same object. The only way we are telling you a thing is from the shade, the reflection of light from that surface.
It could be the same object with drastically, in terms of pixels, drastically different looking shapes and we still know it's the same object.
Pose Variability and Occlusions
There is post-variability in occlusion. Probably my favorite caption for an image
for a figure in a academic paper is deformable and truncated cat.
These are pictures, you know cats are famously deformable.
They can take a lot of different shapes. (LAUGHTER) Its arbitrary poses are possible so you have to have computer vision
to know it's still the same objects, still the same class of objects, given all the variability in the pose and occlusions is a huge problem.
We still know it's an object. We still know it's a cat even when parts of it are not visible.
And sometimes large parts of it are not visible. And then there's all the inter-class variability.
Inter-class, all of these on the top two rows are cats. Many of them look drastically different.
And the top bottom two rows are dogs also look drastically different.
And yet some of the dogs look like cats, some of the cats look like dogs.
And as human beings are pretty good at telling the difference and we want computer vision to do better than that.
It's hard. So how is this done? This is done with convolutional neural networks.
Pause: Object Recognition / Classification
The input to which is a raw image. Here's an input on the left of a number three
and I'll talk about through convolutional layers
that image is processed past through convolutional layers maintain spatial information.
On the output, in this case predicts which of the images
what number is shown in the image. 0, 1, 2 through 9.
And so, these networks, everybody's using the same kind of network to determine exactly that.
Input is an image, output is a number. And in the case of probability, that is a leopard. What is that number?
Then there is segmentation built on top of these convolution neural networks where you chop off the end and convolutionise the network.
You chop off the end where the output is a heat map. So you can have, instead of a detector for a cat, you can do a cat heat map
where it's the part of the image, the output heat map gets excited,
the neurons in that output get excited in the spatially excited, in the parts of the image that contain a tabby cat.
And this kind of process can be used to segment the image into different objects, a horse.
So the original input on the left is a woman on a horse and the output is a fully segmented image of knowing where is the woman, where is the horse.
Pouse Object Detection
And this kind of process can be used for object detection which is the task of detecting an object in an image.
Now the traditional method with convolutional neural networks and in general computer vision is the sliding window approach.
We have a detector, like the leopard detector, where you slide through the image to find where in that image is the leopard.
This, the segmenting approach, the R-CNN approach, is efficiently segmenting the image
in such a way that it can propose different parts of the image that are likely to have a leopard, or in this case a cowboy,
and that drastically reduces the computational requirements of the object detection task.
And so these networks, this is currently one of the best networks for the ImageNet task of localization
is the Deep residual networks. They're deep. So VGG-19 is one of the famous ones.
You started to get above twenty layers in many cases, thirty four layers is the rise in that one.
So the lesson there is, the deeper you go the more representation power you have,
the higher accuracy but you need more data.
Other applications, colorization of images. So this again, input is a single image and output is a single image.
So you can take a black and white video from a film, from an old film,
and recolor it. And all you need to do to train that network in the supervised way
is provide modern films and convert them to grayscale. So now you have arbitrarily sized data sets, data sets of gray scale to color.
And you're able to, with very little effort on top of it, to successfully
well, somewhat successful recolor images. Again, Google Translate does image translation in this way, image to image.
It first perceives, here in German I believe, famous German correct me if I'm wrong,
dark chocolate written in German on a box. So this can take this image, detect different letters convert them to text,
translate the text and then using the image to image mapping
map the letters, the translated letters, back onto the box and you could do this in real time on video.
So what we've talked about up to this point on the left are "vanilla" neural networks,
convolutional neural networks, that map a single input, a single output, a single image to a number, single image another image.
Then there is recurrent neural networks, the map. This is the more general formulation, they map a sequence of images
or a sequence of words or a sequence of any kind to another sequence.
And these networks are able to do incredible things with natural language,
with video, and any type of series of data. For example, you can convert text to hand written digits, with hand written text.
Here, you type in and you can do this online, type in deep learning for self-driving cars
and it will use an arbitrary handwriting style to generate the words "deep learning for self-driving cars".
This is done using recurring neural networks. We can also take Char-RNNs they're called, it's character level recurring neural networks
that train on a data set an arbitrary text data set and learn to generate text one character at a time.
So there is no preconceived syntactical semantic structure that's provided to the network.
It learns that structure. So for example, you can train it on Wikipedia articles like in this case.
And it's able to generate successfully not only text that makes some kind of grammatical sense at least
but also keep perfect syntactic structure for Wikipedia, for Markdown, editing,
for late tack editing and so on. This text as "naturalism and decision for the majority of Arab countries capitalide."
Whatever that means, "was grounded by the Irish language by John Clare," and so on. These are sentences. If you didn't know better, that might sound correct.
And it does so and you pause one character at a time so these aren't words being generated.
This is one character, you start with the beginning three letters "nat",
you generate "u" completely without knowing of the word naturalism.
This is incredible. You can do this to start a sentence and let the neural network complete that sentence.
So for example if you start the sentence with "life is" or "life is about" actually,
it will complete it with a lot of fun things. "The weather." "Life is about kids."
"Life is about the true love of Mr Mom", "is about the truth now."
And this is from [01:05:59], the last two, if you start with "the meaning of life," it can complete that with
"the meaning of life is literary recognition" may be true for some of us here.
Publish or perish. And "the meaning of life is the tradition of ancient human reproduction."
(LAUGHTER) Also true for some of us here. I'm sure.
Okay, so what else can you do? You can, this has been very exciting recently is image capture recognition. No, generation, I'm sorry.
Image capture generation is important for large data sets of images.
What we want to be able to determine what's going on inside those images. Specially for search, if you want to find a man sitting in a college with a dog,
you type it into Google and it's able to find that. So here shown in black text a man sitting on a couch with a dog is generated by the system.
A man sitting in a chair with a dog in his lap is generated by a human observer.
And again these annotations are done by detecting the different obstacles, the different objects in the scene.
So segmenting the scene detecting on the right there's a woman, a crowd, a cat, a camera, holding, purple.
All of these words are being detected then a syntactically correct sentence is generated,
a lot of them, and then you order which sentence is the most likely. And in this way you can generate very accurate labeling of the images,
captions for the images. And you can do the same kind of process for image question answering.
You can ask how many for quantity, how many chairs are there?
You can ask about location, where are the ripe bananas?
You can ask about the type of object. What is the object in the chair? It's a pillow.
And these are, again, using the recurring neural networks.
You could do the same thing with video captions generation,
video captions description generation. So looking at a sequence of images as opposed to just a single image.
What is the action going on in this situation? This is the difficult task. There's a lot of work in it, in this area.
On the left is correct descriptions of a man is do stunts on his bike or a herd a zebra are walking in the field and on the right,
there's a small bus running into a building. You know it's talking about relevant entities but just doing an incorrect description.
A man is cutting a piece of a pair of a paper.
So the words are correct. Perhaps, but so you're close, but mostly are.
One of the interesting things
you can do with a recurring neural networks is if you think about the way we look at images, human beings look at images,
is we only have a small phobia with which we focus in a scene.
So right now you're periphery is very distorted. The only thing, if you're looking at the slides, you're looking at me
that's the only thing that's in focus. Majority of everything else is out of focus.
So we can use the same kind of concept to try to teach a neural network to steer around the image. Both for perception and generation of those images.
This is important first on the general artificial intelligence point of it being just fascinating that we can selectively steer our attention
but also it's important for things like drones. They have to fly at high speeds in an environment
where three hundred plus frames a second, you have to make decisions. So you can't possibly localize yourself or perceive the world around yourself successfully
if you have to interpret the entire scene. So we can do is you can steer, for example here shown, is reading a house number
by steering around an image. You can do the same task for reading and for writing.
So reading numbers here, and this data set on the left, is reading numbers. We can also selectively steer a network around an image to generate that image
starting with a blurred image first and then getting more and more higher resolution
as the steering goes on. Work here at MIT is able to map video to audio.
So head stuff for the drumstick silent video and able to generate the sound
that would drumstick hitting that particular object makes. So you can get texture information from that impact.
So here is the video of a human soccer player playing soccer
and a state-of-the-art machine playing soccer.
And, well let me give it some time, to build up.
(LAUGHTER) Okay. So soccer, we take this for granted, but walking is hard.
Object manipulation is hard. Soccer is harder than chess for us to do much harder.
On your phone now, you can have a chess engine that beats the best players in the world.
And you have to internalize that because the question is, this is a painful video, the question is: where does driving fall?
Is it closer to chess or is it closer soccer? For those incredible, brilliant engineers that worked on the most recent DARPA challenge
this would be a very painful video to watch, I apologize.
This is a video from the DARPA Challenge (LAUGHTER) of robots struggling
with basic object manipulation and walking tasks.
So it's mostly a fully autonomous navigation task. (LAUGHTER)
Maybe I'll just let this play for a few moments to let it internalize how difficult this task is,
of balancing, of planning in an underactuated way. We don't have full control of everything.
When there is a delta between your perception of what you think the world is and what reality is.
So there, a robot was trying to turn an object that wasn't there.
And this is an MIT entry that actually successfully, I believe, gotten points for this because it got into that area
(LAUGHTER) but as a lot of the teams talked about the hardest part,
So one of the things the robot had to do is get into a car and drive it and get out of the car.
And there's a few other manipulation task like walking on unsteady ground, it had to drill a hole through a wall.
All these tasks and what a lot of teams said is the hardest part, the hardest task of all of them,
is getting out of the car. So it's not getting into the car, it's this very task you saw now is the robot getting out of the car.
These are things we take for granted. So in our evaluation of what is difficult about driving,
we have to remember that some of those things we may take for granted
in the same kind of way that we take walking for granted, this is more of X paradox.
Will Hans Moravec from CMU, let me just quickly read that quote: "Encoded in the large highly evolved sensory motor portions of the human brain
is billions of years of experience about the nature of the world and how to survive in it."
So this is data. This is big data. Billions of years and abstract thought which is reasoning.
The stuff we think is intelligence is perhaps less than one hundred thousand years of data old.
We haven't yet mastered it and so, I'm sorry I'm asserting my own statements in the middle of a quote,
but it's been very recent that we've learned how to think.
And so we respected perhaps more than the things we take for granted like walking, the visual perception and so on but those may be strictly a matter of data,
data and training time and network size.
So walking is hard. The question is how hard is driving?
And that's an important question because the margin of error is small.
One, there's 1 fatality per 100 million miles.
That's the number of people that die in car crashes every year, 1 fatality per 100 million miles.
That's a point 0.000001% margin of error.
That's through all the time you spend on the road, that is the error you get.
More impressed with ImageNet being able to classify a leopard, a cat or a dog
at above human level performance but this is the margin of error we get with driving.
And we have to be able to deal with snow, with heavy rain, with big open parking lots,
with parking garages, any pedestrians that behaves irresponsibly as rarely as that happens
or just some predictably, again especially in Boston, reflections.
The ones especially some things you don't think about: the lighting variations that blind the cameras.
(Inaudible question from one of the attendees)
The question was if that number changes, if you look at just crashes, the fatalities per crash.
So one of the big things is that cars have gotten really good at crashing and not hurting anybody.
So the number of crashes is much, much larger than the number of fatalities which is a great thing, we've built safer cars.
But still, you know even one fatality is too many.
So this is one that Google self-driving car team
is quite open about their performance since hitting public road,
this is from a report that shows the number of times the driver disengaged
the car gives up control, that it asked the driver to take control back
or the driver takes control back by force. Meaning that they're unhappy with the decision that the car was making
or it was putting the car or other pedestrians or other cars in unsafe situations. And so, if you see over time there's been a total
from 2014 to 2015 there's been a total of 341 times on beautiful San Francisco roads
and I say that seriously because the weather conditions are great there, 341 times that the driver had to elect to control back.
So it's a work in progress. And let me give you something to think about here.
This, with neural networks is a big open question.
The question of robustness. So this is an amazing paper, I encourage people to read it.
There's a couple of papers around this topic. Deep neural networks are easily fooled.
So here are 8 images where, if given to a neural network as input,
a convolutional neural network as input, the network with higher than 99.6% confidence says
that the image, for example the top left, as a robin. Next to is a cheetah, then an armadillo, a panda, an electric guitar,
a baseball, a starfish, a king penguin. All of these things are obviously not in the images.
So the networks can be fooled with noise. More importantly, practically for the real world, adding just a little bit of distortion,
a little bit of noise distortion to the image, can force the network to produce a totally wrong prediction.
So here's an example, there's 3 columns, correct image classification, the slight addition of distortion
and the resulting prediction of an ostrich for all three images on the left
and a prediction of an ostrich for all three images on the right.
This ability to fool networks easily brings up an important point.
And that point is that there has been a lot of excitement
about neural networks throughout their history. There's been a lot of excitement about artificial intelligence throughout its history
and not coupling that excitement, not granting that excitement, in the reality
the real challenges around that has resulted in in crashes, in A.I. winters when funding dried out
and people became hopeless in terms of the possibilities of artificial intelligence.
So here is the 1958 New York Times article that said the Navy revealed the embryo of an electronic computer today.
This is when the first perceptron that I talked about was implemented in hardware by Frank Rosenblatt.
It took 400 pixel image input and it provided a single output.
Weights were encoded in the hardware potentiometers and waves were updated with electric motors.
Now New York Times wrote, the Navy revealed the embryo vanilla electronic computer today
that expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.
Dr. Frank Rosenblatt, a research psychologist at the Cornell Aeronautical Laboratory in Buffalo,
said perceptrons might be fired to the planets as mechanical space explorers.
This might seem ridiculous but this is the general opinion of the time.
And as we know now, perceptrons cannot even separate a non-linear function.
They're just linear classifiers. And so this led to 2 major A.I. winters in the 70s, in the late 80s and early 90s.
The Lighthill Report, in 1973 by the UK government, said there are no part of the field
of discoveries made so far produced the major impact that was promised. So if the hype builds beyond the capabilities of our research,
reports like this will come and they have the possibility of creating another A.I. winter.
So I want to pare the optimism, some of the cool things we'll talk about in this class,
with the reality of the challenges ahead of us.
The focus of the research community, this is some of the key players in deep learning,
what are the things that are next for deep learning, the five year vision?
We want to run on smaller, cheaper mobile devices. We want to explore more in the space of unsupervised learning as I mentioned
and reinforcement learning. We want to do things that explore the space of videos more,
the recurring neural networks, like being able to summarize videos or generate short videos.
One of the big efforts, especially in the companies we do in large data,
is multi-modal learning. Learning from multiple data sets with multiple sources of data.
And lastly, making money from these technologies. There's a lot of this despite the excitement.
There has been an inability for the most part to make serious money
from some of the more interesting parts of deep learning.
And while I got made fun of by the TAs for including this slide
because it's shown in so many sort of business type lectures, but it is true that we're at the peak of a hype cycle
and we have to make sure be given the large amount of hype and excited there is,
we proceed with caution.
One example of that, let me mention, is we already talked about spoofing the cameras.
Spoofing the cameras with a little bit of noise. So if you think about it, self-driving vehicles operate with a set of sensors
and they rely on those sensors to convey to accurately capture that information. And what happens, not only when the world itself produces noisy visual information,
but what if somebody actually tries to spoof that data. One of the fascinating things have been recently done is spoofing of LIDAR.
So these LIDAR is a range sense that gives a 3D-point cloud of the objects in the external environment.
And you're able to successfully do a replay attack where you have the car
see people in other cars around it when there's actually nothing around it.
In the same way that you can spoof a camera to see things that are not there.
A neural network. So let me run through some of the libraries that we'll work with
and they're out there that you my work with if you proceed with deep learning.
TensorFlow, that is the most popular one these days. It's heavily backed and developed by Google.
It's primarily a python interface and is very good at operating on multiple GPUs.
There's Keras and also TF Learn and TF Slim which are libraries that operate on top of TensorFlow
that make it slightly easier, slightly more user friendly interfaces, to get up and running.
Torch, if you're interested to get in at the lower level
tweaking of the different parameters of neural networks creating your own architectures. Torch is excellent for that with it's own Lua interface.
Lua's a programming language and heavily backed by Facebook.
There is the old school "theano" which is what I started on a lot of people early on, in deep learning started on, as one of the first libraries that supported
ahead came with GPU support. It definitely encourages lower level tinkering, has a python interface.
And many of these, if not all, rely on Nvidia's library
for doing some of the low level computations involved with training these neural networks on Nvidia GPUs.
"mxnet" heavily supported by Amazon and they have officially recently announced
that they're going to be, their AWS, is going to be all in on the mxnet.
Neon, recently bought by Intel, started out as a manufacturer of neural network chips
which is really exciting and it performs exceptionally well.
I hear good things. Caffe, started in Berkeley, also was very popular in Google before Tensorlow came out.
It's primarily designed for computer vision with ConvNet's but has now expanded to all of the domains.
There is CNTK, used to be known and now called the Microsoft Cognitive Toolkit. Nobody calls it that still I'm aware of.
It says multi GPU support, has its own brain script custom language
as well as other interfaces. And we'll get to play around in this class is, amazingly, deep learning in the browser, right.
Our favorite is ConvNetJS, what you use, built by Andrej Karpathy from Stanford now OpenAI.
It's good for explaining the basic concept of neural networks. It's fun to play around with. All you need is a browser and some very few requirements.
It can't leverage GPUs, unfortunately. But for a lot of things that we're doing, you don't need GPUs.
You'd be able to train a network with very little and relatively efficiently without the [01:30:15] GPUs.
It has full support for CNNs, RNNs and even deeper reinforcement learning.
Keras.js, which seems incredible, we try to use for this class.
It has GPU support so it runs in the browser with GPU support with Open GL or however it works magically
but we're able to accomplish a lot of things we need without the use of GPUs.
It's incredible to live in a day and age when it literally, as I'll show on the tutorials,
it takes just a few minutes to get started with building your own neural network that classifies images and a lot of these libraries are friendly in that way.
So all the references mentioned in this presentation are available at this link and the slides are available there as well.
So I think in the interest of time, let me wrap up. Thank you so much for coming in today and tomorrow I'll explain the deep reinforcement learning game
and the actual competition and how you can win. Thanks very much guys.

----------

-----
--14--

-----
Date:
Link: [# Foundations and Challenges of Deep Learning (Yoshua Bengio)](https://www.youtube.com/watch?v=11rsu_WwZTc)
Transcription:

Thank You Sammy so I'll tell you about
some very high-level stuff today no new
algorithm some of you already know about the book that Ian Goodfellow erinkoval
and I have written and it's now in presale by MIT press I think you can
find it on Amazon or something and paper
the actual shipping is going to be in December hopefully for nibs
so we've already heard that story at
least well from several people here at least from Andrew I think but it's good
to ponder a little bit some of these ingredients that seem to be important
for deep learning to succeed but in general for machine learning to succeed
to learn really complicated tasks of the kind we want to reach human level performance so if a machine is going to
be intelligent it's going to need to acquire a lot of information about the
world and the big success of machine learning for AI has been to show that we
can provide that information through data through examples but but really
think about it you know that that machine will need to know a huge amount of information about the world around us
this is not how we're doing it now because we're not able to train such big models but it will come one day and so
we'll need models that are much bigger than the ones we currently have of course that means machine learning
algorithms that can represent complicated functions that's you know one good thing about neural nets but
there are many other machine learning approaches that allow you in principle to represent very flexible forms like
nonparametric methods classical nonparametric methods or svms but
they're going to be missing 0.4 and potentially 0.5 depending on the methods
point 3 of course you you need enough computing power to train and use these big models and
point-five just says that it's not enough to be able to train the model you
have to be able to use it in a reasonably efficient way from a computational perspective this is not
always the case with some probabilistic models where inference in other words answering questions having the computer
do something can be intractable and then you need to do some approximations which could be efficient or or not now the
point I really want to talk about is the fourth one how do we defeat the curse of
dimensionality in other words if you don't assume much about the world it's
actually impossible to learn about it and and so I'm going to tell you a bit
about the assumptions that are behind a lot of deep learning algorithms which
make it possible to work as well as we are seeing in practice in the last few years something wrong Microsoft bug okay
so how do we bypass the curse of dimensionality the curse of dimensionality is about the
exponentially large number of configurations of the space variables that we want to model the number of
values that all of the variables that we observe can take is going to be
exponentially large in general because there's a compositional nature if if each pixel can take two values and you
got a million pixels then you got two to one million number of possible images so
the only way to beat an exponential is to use another exponential so we need to
make our models compositional we need to build our models in such a way that they
can represent functions that look very complicated but yet these models need to
have a reasonably small number of parameters reasonably small in the sense compared to the number of configurations
of the variables the number of parameters should be small and we can
achieve that by by composing little pieces together composing layers together it can put composing units on
the same layer together and that's essentially what's happening with deep learning so you actually have two kinds
of compositions there's the the compositions happening on the same layer this is the idea of distributed
representations which I'm going to try to explain a bit more this is what you get when you learn embeddings for
forwards or for images representations in general and then there's the idea of having multiple levels of representation
that's the notion of depth and there there is another kind of composition
takes place whereas the the first one is a kind of parallel composition I'm you know I can choose
the values of my different units separately and then they together represent an exponentially large number
of possible configurations in the second case there's a sequential composition where I take the output of one level and
and I combine them in new ways to build features for the next level and so on and so on right so so the reason deep
learning is working is because the world around us is better modeled by making
these assumptions it's not necessarily true that deep learning is going to work for any machine learning problem in fact
if if we consider the set of all possible distributions that we would like to work from deep learning is no
better than any other and that's this is basically what the no free lunch theorem is saying it's because we are incredibly
lucky that we live in this world which can be described by using composition that these algorithms are working so
well this is important to really understand this
so before I go a bit more into distributed representations let me say a
few words about non distributor presentations so if you're thinking about things like clustering engrams for
language modeling classical nearest neighbors SVM's with Gaussian kernels
classical nonparametric models with local kernels and decision trees all
these things the way these algorithms really work is actually pretty
straightforward if you you know cut the crap and hide the math and try to
understand what is going on they they look at the data in in data space and
they break that space into regions and they're going to use different free
parameters for each of those regions to figure out what the right answer should be the right answer it doesn't have to be supervised learning even an S
provides I think there's a right answer it might be the density or something like that okay and you might think that that's the
only way of solving a problem you know we consider all of the cases and we have an answer for each of the cases and we
can maybe interpolate between those cases that we've seen the problem with
this is somebody comes up with a new example which isn't in between two of
the examples we've seen something that a la requires us to extrapolate something that's you know non-trivial
generalization and and these algorithms just fail they don't they don't really have a recipe for saying something
meaningful away from the training examples there's another interesting
thing to note here which I would like to you to keep in mind before I show the next slide which is in red here which is
we can do a kind of simple counting to relate the number of parameters a number
of free parameters that can be learning and the number of regions in the data
space that we can distinguish so here we basically have linear relationship
between these two things right so for each region I'm going to need at least something like some kind of Center for
the region and maybe if I need to output something I'll lean an extra set of parameters to tell
me what the answer should be in that area so the number of parameters grows linearly with the number of regions that
I I'm going to be able to distinguish the good news is I can have any kind of
function right so I can break up the space in any way I want and then for each of those regions I can have any kind of output that I need so for
decision trees the regions would be you know splitting across axes and so on and for this is more like four nearest
neighbor or something like that now another bug I don't think I will
send this hope works this time oh I have
a another option sorry about this okay
so so here's the the point of view of
the suit representations for solving the same general machine learning problem we have a data space and we want to break
it down but we're going to break it down in a way that's not general we're going
to break it down in a way that makes assumptions about the data but it's
going to be compositional and it's going to allow us to you know be exponentially more efficient so how are we going to do
this so in the picture on the right what you see is a way to break the input
space by the intersection of half-planes and this is the kind of thing you would have with a what happens at the first
layer of a neural net so here imagine the input is 2-dimensional so I can plot it here and I have three binary hidden
units c1 c2 c3 so because they're binary you can think of them as little binary
classifiers and because it's only a one layer net you can think of what they're
doing is a linear classification and so those colored hyperplanes here are the decision surfaces for each of
them now these three bits there can take they can take eight values right
corresponding to you know whether each of them is on or off and and those
different configurations of those bits correspond to actually seven regions
here because there's one of the eight regions which does is not feasible so so
now you see that we're defining a number of regions which is corresponding to all of the possible intersections of the
corresponding half-planes and and now we can play the game of how many regions do
we get for how many parameters and what we see is that as if we played the game
of growing the number of dimensions features and also of inputs we can get
an exponentially large number of regions which are all of these intersections right there's an exponential number of these intersections corresponding to
different binary configurations yet the number of parameters grows linearly with the number of units so it looks like
we're able to express a function then on top of that I could imagine you have a linear classifier right that's that's
the one hidden layer new on that so so the number of parameters grows just linearly with the number of features but
the number of regions that the network can really provide a different answer to grows exponentially so this is very cool
and the reason it's very cool is that it allows those neural nets to generalize
because while we're learning about each of those features we can generalize to
regions we've never seen because we've learned enough about each of those features separately I'm going to give
you an example of this in a couple of slide actually it's let's do it first so
so think about those features so the input is an image of a person and think
of those features as things like I have a detector that says that the person wears glasses
and I have another unit that's detecting that the person is a female or male and
I have another unit that texts that the person is a child or not and you can imagine you know hundreds or thousands
of these things of course so so the good news is you could imagine learning about
each of these feature detectors these little classifiers separately in fact
you could do better than that you could share you know intermediate layers between the input and those features but
but let's you know take even the worst case and imagine we were to train those separately which is the case in the
linear model that I show before we have a separate set of parameters for each of
these detectors so if I have n features each of them say needs order of K
parameters then I need order of NK parameters and I need order of NK
examples and one thing you should know from you know which machine learning theory is that if you have order of P
parameters you need order of P examples to do a reasonable job of jaw's age of journalizing you can you can get around
that by regularizing and effectively having less degrees of freedom but but you know to keep things simple you need
about the same number of examples or maybe a hundred times more or ten times more as the number of really free
parameters so so now the relationship between the number of regions that I can
represent and the number of examples I need is quite nice because the number of
regions is going to be to to the number of features of these binary features so
you know a person could wear glasses or not be a female or a male or child or not and I could have a hundred of these things and I could probably recognize
reasonably well all of these two to the 100 configurations of people even though
I've obviously not seen all of those to do 100 configurations why is it that I'm able to do that I'm able to do that
because the the models can learn about each these binary features kind of independently in the sense that I don't
need to see every possible configuration of the other features to know about
wearing glasses like I can learn about wearing glasses even though I've never
seen somebody who was a female and a child and chubby and had you know yellow
shoes and and and I have seen enough examples of people wearing glasses I can
learn about wearing glasses in general I don't need to see all of the configurations of the other features to
learn about one feature okay and so so this is really what what you know why
this thing works is because we're making assumptions about the data that those
features are meaningful by themselves and you don't need to actually have data
for each of the regions the exponential number of regions in order to learn the
proper way of detecting or lore of discovering these these these intermediate features let me add
something here there were some experiments recently actually showing that this kind of thing is really
happening because the features I was talking about
not only I'm assuming that they exist but the the optimization methods or
training procedures discover them they can learn them and this is an experiment
that's been done in 2012 all Tour Alba's lab at MIT where they trained a usual
confidence to recognize places so the outputs of the net are just the types of
places like is this a beach scene or an office scene or street scene and so on but but then the the thing they've done
is they ask people to analyze the the hidden units to try to figure out what each hidden unit was doing and they found that there's a large proportion of
units that humans can find a pretty obvious interpretation for what those units like so so they see a bunch of
units which you know like people are different kinds of people or animals or buildings or
seedings or tables lighting and so on so it's like if indeed the those neural
nets are discovering semantic features they are semantic because actually people give them names as the
intermediate features you know in order to reach the final goal of here transpiring scenes and the reason
they're generalizing is because now you can combine those features in an exponentially large number of ways right you could have a scene that has a table
different kind of lighting some people you know maybe a pet and and you can say
something meaningful about the combinations of these things because the network is able to learn all of these
features without having to see all of the possible configurations of them so I
don't know if my explanation makes sense to you but now is the chance to ask me a question all clear usually it's not yeah
with decision trees right to some extent
so if the question is can't we do the same thing with a set of decision trees yeah in fact this is one of the reasons
why forests work better or bagged trees work better than single trees forests or
actually or Bank trees are like one layer one level deeper than a single trees but but they still don't have as
much of a sort of distributed aspect as neural nets so they be and and usually
they're not trained jointly I mean boosted trees are you know to some
extent in a greedy way but yeah any other question yeah cases where what non-conditional
non computer vision non compositional I
don't understand the question I mean I don't sound what you mean what do you mean non compositional yeah it's
everywhere around us I don't think I don't think that there are examples of neural nets that really work well where
the data doesn't have some kind of compositional structure in it but if you come up with an example I'd like to hear
about it okie s yes
to think about this issue in graphical model terms is is if it can be done but
you have to think about not feature detection like I've been doing here but
about generating an image or something like that right then it's easier to
think about it so so the same kinds of things happen if you think about how I could generate an image if you think
about underlying factors like which objects where they are what's their identity what's their size these are all
independent factors which you compose together in in funny ways if you were to
do a graphics engine you can see exactly what those ways are and it's much much
easier to represent that joint of distribution using this compositional structure then if you're trying to work
directly in the pixel space which is normally what you would do with a classical nonparametric method and it
wouldn't work but if you look at our best D generative models now for images for example like ganz or V AES they're
really you know we're not there yet but they're amazingly better than anything
that people could dream up just a few years ago in in machine learning okay
let me move on because of other things to talk about so this is all kind of
hand wavy but some people have done some math around these ideas and and so for
example there's one result from two years ago I clear where we study the
single layer case and we consider a network with rectifiers rellis and we
find that the the network of course computes a piecewise linear function and
so one way to quantify the richness of
the function that it can compute I was talking about regions here but well you can do the same thing here you can count how many pieces does does this
network have in its input to output function and and it turns out that is it
six potential in in the number of inputs
well it's a number of units to the power number of inputs so that's for a sort of
district representation there's this an exponential kicking in we also studied the the depth aspect so what you need to
know about depth is that there's a lot
of earlier theory that says that a single layer is sufficient to represent any function however that theory doesn't
specify how many units you get you might need and in fact you might need an especially large number of units so what
several results show is that there are functions that can be represented very
efficiently with few units so few parameters if you allow the network to
be deep enough so out of all the functions again it's a luckiness thing
right out of all the functions that exists there's a very very small fraction which happen to be very easy to
represent with a deep network and if you try to represent these these functions
with a shallow network you're screwed you're going to need an exponential number of parameters and so you're gonna
need an exponential number of examples to learn these things but again we're
incredibly lucky that the function we want to learn have this property but in
the sense it's not surprising I mean we use this kind of compositionality and depth everywhere we when we write a computer program we just don't have like
a single main we have you know functions and call functions and and we were able
to show similar things as what I was telling you about for the single layer case that as you increase depth for
these deep relu networks the number of pieces in the piecewise linear function
grows exponentially with the depth so so it's it's already exponentially large
with a single-layer but it gets exponentially even more with a deeper
net okay so so this this was a topic of representation of functions why why deep
learn deep architectures can can be very powerful if we're lucky and we seem to
be looking the other another topic I
want to mention that's kind of very much in the foundations is how is it that
we're able to train these neural nets in the first place in the 90s many people
decided to not do any more research on your nuts because there were 30 Korra's
ult's showing that there are really an exponentially large number of local minima in the training objective in of a
neural net so in other words the function we want to learn has many of
these holes and if we start at a random place well what's the chance we're going
to find the best one the the one that corresponds to a good cost and that was
one of the motivations for people who flocked into a very large area of
research in machine learning in the 90s and 2000's based on algorithms that
require on the convex optimization to Train because of course if we can do context optimization we eliminate this
problem if if the objective function is convex in the parameters then we know there's a single global minimum right so
let me show you a picture here you get a sense of if you look on the right hand top this is if you draw a random
function in 1d or 2d or 3d like here this is a kind of a random smooth
function in 2d you see that is going to have many ups and downs this is a local
minimum and but but the good news is
that in high dimension it's a totally different story so what are the dimensions here we're talking about the
parameters of the model and the vertical axis is the cost we're trying to
minimize and what happens in high dimension is that instead of having a huge number of
local minima on our way when we're trying to optimize what we encounter instead is a huge number of saddle
points so saddle point is like the thing on the bottom right in in 2d so you have
two parameters and y-axis is the cost you want to minimize and so what you see in a saddle point is yeah you have
dimensions or directions where the the objective function draws a a minimum so
there's like a curve that it curves up and in other directions it curves down
so we are you know saddle point has both a minimum in some direction and a maximum in other directions so this is
this is interesting because even though it's a these these points like saddle
points and many more are places where you could get stuck in principle if you're exactly at the subtle point you don't move but if you move a little bit
away from it you will go down the saddle right so what what our work in the other
paper other work from NYU tremonica and collaborators of Yann
Locker showed is that actually in very
high dimension not only you know it's it's the issue is more saddle points
than local minima but but the local minima are good so let me try to explain
what I mean by this so let me show you
actually first an experiment from from the NYU guys so they did an experiment
where they gradually change the size of the neural net and they they look at
what looks like local minima but they could be you know saddle points that are the lowest that they could obtain by
training and what you're looking at is a distribution of errors they get from
different initialization of their training and so what happens is that when the network is small like the
pink here on the right there's a widespread distribution of cost that you can get depending on where you you you
start and they're pretty high and if you increase the size of the network it's like all of the local minima that you
find concentrate around a particular costs so you don't get any of these bad
local minima that you would get with a small Network they're all kind of pretty good and if you increase even more the
size of network this is like a single hidden layer network you know not very complicated this phenomenon increases even more in
other words they all kind of converge to the same kind of costs so let me try to explain what's going on so if we go back
to the picture of the saddle point but instead of being in 2d imagine you are in a million D and in fact you know
people have billion D networks these days I'm sure andrew has even bigger
ones I'm not sure but so what happens in
this very high dimensional space of parameters is that if if things are not
really you know really bad for you so if you imagine a little bit of randomness
in the way the problem is set up and there it seems to be the case in order
to have a true local minimum you need to have the curvature going up like this in
all the you know billion directions so
if there is a certain probability of this event happening that all know that this particular directions is curving up
and this one is grabbing up the probability that all of them curve up becomes exponentially small so we we
tested that experimentally what you see in the bottom left is a curve that shows
the training error as a function of what's called the index of the critical
point which is just the fraction of the directions which are
curving down right so so 0% would mean
it's a local minimum a hundred percent would be it's a local maximum and
anything in between is a saddle point so what we find is that as training
progresses we're going close to a bunch of saddle points and these and none of
them are local minima otherwise we would be stuck and and in fact we never
encounter local minima until we reach the lowest possible cost that we were
able to get in addition there is a theory suggesting that so the the local
the low the the local minima will actually be close in cost to the global
minimum they will be above and they will concentrate in a little band above the global minimum but that band of local
minima will be close to the global minimum and and the larger 2-dimension the more this is going to be true so as
you go to go back to my analogy right at some point of course you will get local minima even though it's unlikely when
you're in the middle when you get close to the bottom well you can't go lower so you know it has to rise up in all the
directions but it's yeah so that's kind of good news I think in spite of this I
don't think that the optimization problem of neural nets is solved there are still many cases where we find
ourselves to be stuck and we still don't understand what the landscape looks like this set of beautiful experiments by in
Goodfellow that help us visualize a bit what's going on but I think one of the open problems of optimization for neural
nets is we know what does the landscape actually look like it's hard to visualize of course because it's very
high dimensional but for example we don't know what those saddle points
really look like when we actually measure the gradient near those when
we're approaching those saddle points is it's not close to zero so we never go to actually flat places this may be too due to the fact that
we're using SGD and it's kind of hovering above things there might be conditioning issues or even if you are
at a cell nearer saddle point you might be stuck even though it's not a local women because in many directions
it's still going up maybe you know 95% of the directions and and the other
directions are hard to reach because simply there's a lot more curvature in some directions and other directions and
that's you know the traditional ill conditioning problem we don't know exactly you know what what's making it
hard to try in some some networks usually continents are pretty easy to train but when you go into things like
machine translation or even worse reasoning tasks like with things like you know Turing machines and things like
that it gets really really hard to train these things and people have to use all kinds of tricks like curriculum learning which are essentially optimization
tricks to make the optimization easier so I don't want to tell you that all the
optimization problem of neural nets is easy it's done we don't need to worry about it but it's much easier and less
of a concern than what people thought in
the 90s ok so so was she learning I mean
deep learning is moving out of pattern recognition and into more complicated tasks for example including reasoning
and and and combining deep learning with reinforcement learning planning and things like that
you've heard about attention that's one of the tools that is really really
useful for many of these tasks we've sort of come up with attention
mechanisms as not a way to focus on what's going on in the outside will like
we usually think of attention like attention in the visual space but internal attention right in the space of
representations that have been built so that's what we do here in machine translation and it's been extremely
successful as quark said so I'm not going to show you any of these pictures
blah blah another so I'm getting more now into the domain of challenges a
challenge that I've been working on since I was a baby researcher as a PhD student is long-term dependencies and
recurrent Nets and although we've made a lot of progress this is still something
that we haven't completely cracked and it's connected to the optimization problem that I told you before but it's
a very particular kind of optimization problem so some of the ideas that we've
used to try to make the propagation of information and gradients easier include
using skip connections over time include using multiple time scales there's some
recent work in this direction from from my lab and other groups and even the
attention mechanism itself you can think of a way to help dealing with with long
term dependencies so the way to see this is to think of the place on which we're
putting attention as part of the state right so so imagine really you have a
recurrent net and it has two kinds of state it has the usual recurrent net
state but it has the content of the memory you know Kwok told you about memory nets and neural Cheng machines
and the full state really includes all of these things and and now we are able
to read or write from that memory I mean the little recurrent net is able to do that so what happens is that there are
memory elements which don't change or time maybe they're being written once
and and so the information that has been stored there it can stay for as much
time as you know they're not going to be overwritten so so that means that if you
consider the gradients back propagated through those cells they can go pretty much unhampered and there's no vanishing
gradient problem so this is something that to be that that view of the problem
of long-term dependence sieze with memory i think is could be very useful all right
in the last part of my presentation I want to tell you about what I think is the biggest challenge ahead of us which
is unsupervised learning any question about attention and memory before I move
on to and provides learning ok so why do
we care about unsupervised learning it's not working well actually it's working a
lot better than it was but it's still not something you find in industrial products at least not in an obvious way
there are less obvious ways where unsupervised learning is actually already extremely successful so for
example when you train word embeddings with word to Veck or any other model and you use that to pre train like we did
our machine translation systems or other kinds of NLP tasks you're you're exploiting as provides learning even
when you train a language model that you're going to stick in some other thing or pre train something with that
you're also doing unsupervised learning but I think the potential of and the
importance of ents provides learning is is usually underrated so why do we care
first of all the idea of ins provides learning is that we can train we can we can learn something from large
quantities of unlabeled data that humans have not curated and we have lots of
that humans are very good at learning
from unlabeled data I have an example
that I used often that is makes it very very clear that for example children can
learn all kinds of things about the world even though no one no no no adult
ever tells them anything about it until much later when is too late
physics so you know a two or three year old understands physics you know if she
has a ball she knows what's gonna happen when she drops the ball she knows you know how liquids behave she knows all
kinds of things about objects and an ordinary Newtonian physics even though
she doesn't have explicit equations and a way to destroy them with words but she can predict what's going to happen next
right and the parents don't tell the children you know force equals mass
times acceleration right so this is
purely unsupervised and it's very powerful we don't even have that right now we don't have computers that can
understand the kinds of physics that children can understand so it looks like
it's a skill that humans have and that's very important for humans to make sense
of the world around us but we haven't really yet succeeded to put in machines
let me tell you other reasons that are connected to this why unsupervised
learning to be useful when you do supervised learning essentially the way you train your system as you you you you
focus on a particular task those here's the inputs and here's the the input variables and here's an output variable
that I would like you to predict given the input your learning P of Y given X but if you're doing as provides learning
essentially you're learning about all the possible questions that could be asked about the data of your observe so
it's not that you know there's X 1 X 2 X 3 and Y everything is an X and you can
predict any of the X given any of the other X right if I give you a picture and I had a part of it you can guess
what's missing if I hide if I hide the you know the caption you can generate
the caption given the image if I hide hide the image and I give you the caption you can you can you know guess
what the image would be or draw it or figure out you know from examples which one is the most appropriate so you can
answer any questions about the data when you have captured the Joint Distribution between them essentially so that's that
could be useful another practical thing that ins
provides learning has been used in fact this is how the whole deep learning thing started is that it could be used
as a regular Iser because in addition to telling our model
that we want to predict Y given X we're saying find representations of X that
both predict Y and somehow capture something about the distribution of X
know the leading factors the explanatory factors of X and this again is making an
assumption about the data so we can use that as a regular Iser if the assumption is valid that the essentially the
assumption is that the factor Y that we're trying to predict is one of the
factors that explain X and that by doing this provides learning to discover factors that explain X we're going to
pick Y among the other factors and so it's going to be much easier now to do
supervised learning of course this is also the reason why transfer learning
works because there are underlying factors that explain the inputs for a
bunch of tasks and maybe a different subset of factors explained are relevant for one task and another subset of
factors is relevant for another task but if these factors overlap then there's a potential for synergy you know by doing
multi task learning so the reason multi task learning is working is because unsupervised learning is working is
because there are representations and factors that explain the data that can
be useful for our supervised learning tasks of interest that also could be
used for domain adaptation for the same reason um the other thing that people
don't talk about as much about unsupervised learning and I think it was
part of the initial success that we had with stacking auto-encoders and rbms is that you can actually make the
optimization problem of training deep nets easier because if you're gonna
you know for the most part if you're gonna train a bunch of RBMS or a bunch
of voto encoders and I'm not saying this is the right way of doing it but you know it captures some of the spirit of
what ins provides learning does a lot of the learning can be done locally you're trying to extract some information you're trying to discover some
dependencies that's that's a local thing once you have a slightly better representation we can again tweak it to extract better more independence or
something of that so so there's a sense in which the optimization problem might be easier if you have a very deep net
another reason why we should care about unsupervised learning even if our ultimate goal is to do supervised
learning is because sometimes the output variables are complicated
they are compositional they have a Joint Distribution so in machine translation which we talked about the output is a
sentence the sentence is a set of as a couple of words that have a complicated Joint Distribution given the input in
the other language and so it turns out that many of the things we discover by exploring unsupervised learning which is
essentially about capturing joint distributions can be often used to deal
with these structured output problems where you you have many outputs that form a you know compositional
complicated distribution there's another reason why unsupervised learning I think
is going to be really necessary for AI model-based reinforcement learning so I
think I have another slide just for this
let's think about self-driving cars is very popular topic these days how did I
learn that I shouldn't do some things with the wheel that will kill myself
right when I'm driving because I haven't experienced these states where I get
killed and I simply haven't done it like a thousand times to get learn how to avoid it
so supervised learning where we're our rather you know traditional
reinforcement learning like and policy learning kind of thing or
actor critic or things like that won't work because I need to generalize about
situations that I'm never going to encounter because otherwise if I did I would die so these are like dangerous
states that I need to generalize about these states but I you know can't have
enough data for them and and I'm sure there are lots of machine learning applications where we would be in that
situation I remember a couple of decades ago I you know I've got some data from
nuclear plant and so you know they wanted to predict that you know when it's gonna blow up to avoid it so I said
how many how many yeah it's at zero
right so you see sometimes it's hard to do supervised learning because the data
you would like to have you can't have it's it's it's data that you know situations that are very rare or you
know so how can we possibly solve this problem well the only solution I can see
is that we learn enough about the world that we can predict how things would
unfold right when I'm driving you know I have a kind of mental model of physics
and how cars behave that I can figure out you know if I turned right at this point I'm going to end up on the wall
and it's going to be very bad for me and I don't need to actually experience that to know that it's bad I can make a
mental simulation of what would happen so I need a kind of generative model of
how the world would unfold if I do such and such actions and unsupervised
learning is sort of the ideal thing to do that but of course it's going to be hard because we're going to have to
train models that capture a lot of aspects of the world in order to be able
to learn to generalize properly in those situations even though they don't see
any data of it so that's that's one reason why I think
reinforcement learning needs to be worked on more so I have a little thing
here I think people who have been doing deep learning can collaborate with
people who are doing reinforcement learning and not just by providing a black box that they can use in their
usual algorithms I think there are things that we do in supervised deep
learning that orange provides deep learning that can be useful in sort of
rethinking our enforcement learning so so one example also so well one thing I
really like to think about is credit assignment in other words how do
different machine learning algorithms figure out what the hidden units are supposed to do what the intermediate computations or the intermediate actions
should be this is what credit assignment is about and that prop is the best
recipe we currently have for doing credit assignment it tells the you know parameters of some intermediary should
change so that the costs much much later you know hundred steps later if it's a recurrent net should be reduced so we
could probably use some inspiration from backrub and how it's used to improve
reinforcement learning and one such cue is how when we do supervised backprop
say we don't predict the expected loss
that we're going to have and then try to minimize it where the expectation would
be over the different realizations of the correct class that's not what we do but this is what people do in RL they
they will learn a critic or a cue function which is the expected learning
the expected value of the future reward or the future loss in our case that might be you know minus log probability
of the correct answer given the input and then they will backdrop
through this or use it to estimate the gradient on the actions instead when we
when we do supervised learning we're going to do credit assignment where we
use the particular observations of the correct class that actually happened for this X right we have X we have Y and we
use the Y to figure out what how to change our prediction or action so it
looks like this is something that should be done for our L and in fact we we have
a paper on something like this for a sequence prediction this is this is the
kind of work which is at the intersection of dealing with structured outputs reinforcement learning and
service learning so I think there's a lot of potential benefit of changing the
frame of thinking that people in the RL have had for many decades people in RL I mean not thinking about the world in
with the same eyes as people doing your net they've been thinking about the world in terms of discrete states that
could be enumerated and proving theorems about these algorithms that depend on
essentially you know collecting enough data to fill all the possible configurations of the state and their
you know the corresponding effects on the reward when you start thinking in
terms of neural nets and deep learning the way to approach problems is very very different okay let me continue
about as provides learning and why this is so important if you look at the kinds
of mistakes that our current machine learning algorithms make you find that
our our neural nets are just cheating they're using the wrong cues to try to
produce the answers and sometimes it works sometimes it doesn't work so how can we make our our models be you know
smarter make less mistakes well
the only solution is to make sure that those models really understand how the
world works at least at the level of humans to get human level accuracy human level performance it may be not
necessary to do this for a particular problem you're trying to solve so maybe we can you know get away with doing
speech recognition without really understanding of the meaning of the words probably that's going to be okay
but for other tasks especially those involving language I think having models
that actually understand how the world tix is going to be very very important
to so how could we have machines that understand how the world works well one
of the ideas that I've been talking a lot about in the last decade is that of
disentangling factors of variation this is related to a very old idea in pattern
recognition computer vision called invariance the idea of invariance was that we would like to compute or design
initially design and now learn features say of the image that are invariant to
the things we don't care about maybe we want to do object recognition so we don't care about position or orientation
so we would like to have features that are translation invariant rotation invariant scaling invariant whatever so
this is what invariance is about but when you're in the business of doing ends provides learning of trying to figure out how the world works it's not
good enough to do two extracting variant features what we actually want to do is to extract all of the factors that
explain the data so if we're doing speech recognition we want not only to extract the phonemes but we also want to
figure out you know what kind of voice is that maybe who is it what kind of
recording conditions or what kind of microphone is it in a car is it outside all that information which you're trying
to get rid of normally you actually want to learn about so that you'll be able to
generalize even to new tasks for example maybe the next day I'm not going to ask you to recognize phonemes but recognize who's speaking more generally if we're
able to disentangle these that explained how the data varies everything becomes easy especially if
those factors now can be generated in an independent way and to generate the data
we we can for example we can learn to
answer a question that only depends on one or two factors and basically we eliminate all the other ones because we've separated them so a lot of things
become much easier so that's one notion right we can design tangle factors
there's another notion which is the notion of multiple levels of abstraction which is of course at the heart of what
we're trying to do with deep learning and the idea is that we can have
representations of the world representation of the data as you know
description that involves factors are features and we can do that at multiple
levels and there are more abstract levels so if I'm looking at a document
you know there's the level of the pixels the level of the strokes the level of the characters the level of the words
and maybe the level of the meaning of individual words and we actually have you know systems that will recognize
from a scanned document all of these levels when we go higher up we're not
sure what the right levels are but clearly there must be representations of the meaning not just of single words but
of you know sequences of words and the whole paragraph what's the story and why is it important to represent things in
that way because higher levels of abstraction are representations from
which it is much easier to do things to answer questions so the the more
semantic levels mean basically we can very easily act on the information when it's represented that way if you think
about the level of words it's much easier to check whether a particular word is in the document if I have the words extracted then if I have to do it
from the pixels and if I have to answer a complicated question about you know the intention of the person working at
level of words is not high enough it's not abstract enough I need to work at a more abstract level which in which maybe the
same notion could be represented with many different types of words where many
different sentences could express the same meaning and I want to be able to capture that meaning so the last slide I
have is something that I've been working on in the last couple of years which is
trying to which is connected to ants provides learning but more generally to
the relationship between how we can build intelligent machines and and the
intelligence of humans or animals and as you may know this was one of the key
motivations for doing neural nets in the first place the intuition is this that
we are hoping that there are a few simple key principles that explain you
know what allows us to be intelligent and that if we can discover these principles of course we can also build
machines that are intelligent that's why the neural nets were you know inspired
by things we know from the brain in the first place we don't know this is true
but if it is then you know it's it's it's great and I mean this would make it
much easier to understand how brains work as well as building AI so in in
trying to bridge this gap because right now our best neural nets are very very different from what's going on in brains
as far as you know we can tell by talking to neuro scientists in
particular backprop although it's it's kicking
Assam from a machine learning point of view it's not clear at all how something like this would be implemented in brains
so I've been trying to explore that and and also trying to see how we could
generalize those credit assignment principles that would come out in order
to also do once provide learning so we've we've made a little bit of
progress a couple of years ago I came up with an idea called target prop which is
a way of generalizing back prop 2 propagating targets for each layer of
course this idea has a long history more
recently we've been looking at ways to
implement gradient estimation in deep recurrent networks that perform some
computation that turn out to end up with
parameter updates corresponding to gradient descent in the prediction error that looked like something that
neuroscientists have been observing and and don't completely understand called SCDP spike timing-dependent plasticity
so I don't really have time to go into this but I think this whole area of
reconnecting neuroscience with machine learning and neural nets is something
that has been kind of forgotten by the the machining community because we're all so busy you know building self-driving cars but I think over the
long term it's a it's a very exciting prospect thank you very much
yes questions yeah
to begin with great talk my question is regarding you know the lack of interlab
between the results in the study of complex networks like when they study
the brain networks right there lot of publications which that talk about the emergence of hubs and especially a lot
of publications on the degree distribution of the inter neuron Network right but then when you look at the
degree distribution of the so-called neurons in deep Nets you don't get to
see the emergence of the hub behavior so right why do you think that there's such lack of overlap between like because I
think the hop story is maybe not that important first of all I really think
that in order to understand the brain we have to understand learning in the brain and if we look at our experience in
machine learning and deep learning although the architecture does matter you know what matters even more is the
general principles that allow us to train these these things so I think the
the study of the connectivity makes sense you can't have a you know fully
connected thing and having a way to have a short number of hops to go from anywhere to anywhere is a reasonable
idea but it's it I don't think it really
explains that much that the the central question is how does the brain learn
complicated things and it does it better than then our current machines yet we we
don't know even a simple way of training brains that that at least fits the
biology reasonably yeah there are any cases with real war examples where the
cursive of dimensionality is still a problem for neural nets yeah any time it
doesn't work I mean from a generalisation point of view so Andrew told us yesterday that we
can just add more data and computing power and for some problems this may work but sometimes the amount of data
you would need is just you know too large with our current techniques and
you know we'll need also to develop you know the how did you call it the Hail
Mary all right we also need to do some research on the algorithms and the
architectures to be able to learn about
how the world is organized so that we can generalize in much more powerful ways and that is needed because the the
kind of tasks we want to solve involved in many many variables that have an explanation umber of possible values and
that's the curse of dimensionality essentially so it's facing you know pretty much all of the AI problems
around us all right the question on multi-agent reinforcement learning yeah
if you assume all cars can never predict all possible potential accidents what
about the potential for transfer learning and things like that yeah so so
I was giving an example of a single human learning how to drive we might be
able to use you know the millions of people's you know using self-driving cars correcting and some of them making
accidents to actually make some progress without actually solving the hard problems and this is probably going to
be doing for a while but and we should do it we should definitely use all the data we have currently if you look at
the amount of data were using for speech recognition or language modeling it's hugely more than what any human you know
actually sees in their lifetime so we're doing something wrong and we could do
better with less data and babies and kids you know can do it yes well there's
quite a bit of work on video these days it's mostly a computational bottleneck
yeah well keep in mind we're doing em this just a couple of years ago yeah
absolutely yeah I don't think it's a fundamental issue if if we're able to do it well on
static images the same principles will you know allow us to do sequences we're
already doing you know sequential things for example an interesting project is
speech synthesis with recurrent nets and stuff like that or convolutional nets
whatever so it's more like we're not sure how to train them well and how to
discover these explanatory factors and so on that's my cue
yeah I have a question maybe non-technical so we have seen the human error rates and then the versus the our
algorithms error rates for things that are we are used to like image recognition speech recognition right
right those are any beena experiments where we try to train humans for things that we are not used to right I'm not
trained the machine at the same time see right so this how capable are algorithms
you're asking if these experiments have been done yeah I don't know but I'm I'm
sure the humans would beat the hell out of the machines for now for this kind of thing humans are able to learn a new
task or new concepts from very few examples and we know that in order for
machines to do to do as well they they just need more sort of common sense right more general knowledge of the
world this is what allows humans to learn so quickly on on a few examples yeah you presented experimental data
very showed that lots of local minimum for these parameters or maybe saddle saddle points
have similar performance yeah are these local minima that's there locally right are these local minima separated widely
in parameter space are they close by that's a good question I could I guess a related question is once you claim the
network if there are lots of local minima does that suggest that you could compress the network and represent it
with far fewer parameters maybe so for your first question we have
some experiments dating from 2009 where we try to visualize in 2d the
trajectories of training so this is a paper first author is dumitru Aaron former PhD students with me where we we
wanted to see how different depending on where you start you know where do you end up dude different trajectories end
up in the same place or do they all go in a different place it turns out they all go in a different place and so the
number of local minima is much larger than the number of trajectories that we tried like 500 or a thousand it's so
much larger that you know no two random see initial seeds end up near each other
so it looks like there's a huge number of local minimum which is in agreement with the theory that there's an
exponential number of them but the good news is they're all kind of equivalent in terms of cost if you have a large
network I'm not sure I'm sure there are
many ways to compress these networks there's a lot of redundancy in many ways
there are there are redundancies due to
the numbering like you could flip all you know take that unit put it here to
get you and put it here and so on but I don't think you're going to gain a lot of bits from that so we've talked
about that one of the main advantages of deep learning is that they it can work with lots of data and but you were
mentioning before that we need also to capture the ability of humans of working
with a few data but the reason we're able to work with fewer data is because we have first
learned from a lot of data about you know the general knowledge of the world
right so how can we adapt neural networks to bring us to this new fuel
data paradigm we have to do a lot better add-ons provides learning and of the
kind that really discovers sort of explanations about the world that's what I think let's say thank you again so
before we stop this workshop first an announcement
you might remember yesterday Carl invited all the women here for an
informal dinner it's going to be right outside right now after we close so
before we close actually I'd like to thank all the the speaker today and yesterday I think everybody appreciated
their talk so thanks again all of you
and thanks to all the attendants I think it was a very nice weekend hope you enjoyed

----------

-----

--13--

-----
Date: 2016.09.27
Link: [# Deep Learning for Computer Vision (Andrej Karpathy, OpenAI)](https://www.youtube.com/watch?v=u6aEYuemt0M)
Transcription:


Deep Learning for Computer Vision
yeah so thank you very much for the introduction so today I'll speak about deep learning especially in the context
of computer vision so you saw in the previous talk is neural networks so you saw the neural networks are organized
into these layers fully connected layers where neurons in one layer are not connected but they're connected fully to
all the neurons in the previous layer and we saw that basically we have this layer wise structure from input until
output and there are neurons and nonlinearities etc now so far we have
not made too many assumptions about the inputs so in particular here we just assume that an input is some kind of a
vector of numbers that we plug into this neural network so that both a bug and a
feature to some extent because in most in most real-world applications we actually can make some assumptions about
the input that make learning much more efficient learning much more efficient
so in particular usually we don't just want to plug in into neural networks vectors of numbers but they actually
have some kind of a structure so we don't have vectors of numbers but these numbers are arranged in some kind of a layout like an N dimensional array of
numbers so for example spectrograms are two dimensional arrays of numbers images are three dimensional arrays of numbers videos would be four dimensional arrays
of numbers text you could treat as one dimensional array of numbers and so whenever you have this kind of local
connectivity structure in your data then you'd like to take advantage of it and convolutional neural networks allow you
to do that so before I dive into commercial neural networks and all the details of the architectures I'd like to briefly talk
about a bit of the history of how this field evolved over time so I like to start off usually with talking about
Hubble and Wiesel and the experiments that they performed in 1960s so what they were doing is trying to study the
computations that happen in the early visual cortex areas of a cat and so they
had cat and they plugged in electrodes that could record from the different neurons and then they showed the cat
different patterns of light and they were trying to debug a neurons effectively and try to show them different patterns and see what they
responded to and a lot of these experiments inspired some of the modeling that came in afterwards so in
particular one of the early models that try to take advantage of some of the results of these experiments where the
was the model called Newark cockney truant from Fukushima in 1980s and so what you saw here was these this
architecture that again is layer wise similar to what you see in the cortex where you have these simple and complex
cells where the simple cells detect small things in the visual field and then you have this local connectivity
pattern and the simple and complex cells alternate in this layered architecture throughout and so this was this looks a
bit like a comm net because you have some of its features like say the local connectivity but at the time this was not trained with backpropagation these
were specific heuristic Allah chose in' updates that and this was unsupervised
learning back then so the first time that we've actually used back propagation to train some of these networks was an experiment of a young
lagoon in the 1990s and so this is an example of one of the networks that was
developed back then in 1990s by anne lagoon as lina at five and this is what you would recognize today as a convolutional neural network so it has a
lot of the very simple computational layers and it's alternating and it's a similar kind of design to what you would
see in the Fukushima's new york cognate Ron but this was actually trained with backpropagation and to end using
supervised learning now so this happened in roughly 1990s and we're here in 2016
basically about 20 years later now computer vision has has for a long time
kind of worked here on larger images and a lot of these models back then were applied to very small kind of settings
like say recognizing digits in zip codes and things like that and they were very successful in those
domains but back at least when I entered computer vision roughly 2011 it was thought that a lot of people were aware
of these models but it was thought that they would not scale up naively into large complex images that they would be
constrained to these toy tasks for a long time or I shouldn't say toy because these were very important tasks but certainly like smaller visual
recognition problems and so in computer vision in roughly 2011 it was much more common to use a kind of these feature
based approaches at the time and they didn't work essentially that well so when I entered my PhD in 2011 working on
computer vision you would run a state of the art object detector on this image and you might get something like this
Computer Vision 2011
where cars were detected in trees and you would kind of just shrug your shoulders and say well that just happens sometimes you kind of just accept it as
a as a something that would just happen and of course this is a caricature things actually worked like relatively
decent I should say but definitely there were many mistakes that you would not see today about four years nian 2016
five years later and so a lot of computer vision kind of looked much more like this when you look into a paper of
trying that try to do image classification you would find this section in the paper on the features that they used so this is one page of
features and so they would use yeah a gist hog etc and then the second page of
features and all their hyper parameters so all kinds of different histograms and you would extract this kitchen sink of features and a third page here and so
you end up with this very large complex code base because some of these feature types are implemented in MATLAB some of
them in Python some of them in C++ and you end up with this large code base of extracting all these features caching them and then eventually plugging them
into linear classifiers to do some kind of visual recognition tasks so it was quite unwieldy but it worked to some
extent but they were definitely a room for improvement and so a lot of this change in computer vision in 2012 with
this paper from Astrid chef Sookie Ilya sutskever and Geoff Hinton so this is the first time that someone
took a convolutional neural network that is very similar to the one that you saw in from 1998 from Jana Kuhn and I'll go
into details of how they defer exactly but they took that kind of network they scaled it up they made it much bigger
and they trained it on a much bigger data set on GPUs and things basically ended up working extremely well and this
is the first time the computer vision community has really noticed these models and adopted them to work on
larger images so we saw that the performance of these models has improved
drastically here we are looking at the image net eyeless VRC visual recognition
challenge over the years and we're looking at the top 5 error so low is good and you can see that from 2010 in
the beginning these were feature based methods and then in 2012 we had this huge jump in performance and that was
due to the first kind of convolutional neural network in 2012 and then we've managed to push that over time and now
we're down to about 3.5 7% I think the results for image 2 thousand imagenet challenge 2016 are
actually due to come out today but I don't think that actually they've come out yet I have this second tab here
opened I was waiting for the result but I don't think this is a Pia tiah okay no
nothing alright well we'll get to find out very soon what happens right here so I'm very
excited to see that just to put this in context by the way because you're just looking at numbers like three point five
seven how good is that that's actually really really good so what something that I did about two years ago now now is that I try to
measure the human accuracy on this data set and so what I did for that is I developed this web interface where I
would show myself image net images from the test set and then I had this interface here where I would have all
the different classes of image net there's 1,000 of them and some example images and then basically you go down
this list and you scroll for a long time and you find what class you think that image might be and then I competed
against the ComNet at the time and this was Google net in 2000 in 2014 and so
hot dog is a very simple class you can do that quite easily but why is the accuracy not 0% it well some of the
things like hot dog seems very easy why isn't it trivial for humans to see well it turns out that some of the images in
a test set of image net are actually mislabeled but also some of the images are just very difficult to guess so in
particular if you have this Terrier there's 50 different types of terriers and it turns out to be very difficult task to find exactly which type of
Terrier that is you can spend minutes trying to find it turns out that good convolutional neural networks are
actually extremely good at this and so this is where I would lose points compared to ComNet so I estimate that
human accuracy based on this is roughly 2 to 5 percent range depending on how much time you have and how much
expertise you have and how many people you involve and how much they really want to do this which is not too much and so really we're doing extremely well
and so we're down to 3 percent and I think the error rate if I remember correctly was about 1.5 percent so if we
get below 1.5 percent I would be extremely suspicious on image net that seems wrong so to summarize basically
what we've done is before 2012 computer somewhat like this where we had these
feature extractors and then we trained a small portion at the end of the feature extractor extraction step and so we only
trained this last piece on top of these features that were fixed and we basically replaced the feature extractor in step with a single convolutional
neural network and now we trained everything completely end-to-end and this turns out to work quite nicely so I'm going to go into details of how this
works in a bit also in terms of code complexity we kind of went from a setup
that looks whoops way ahead okay we went from a setup that looks something like that and papers to
something like you know instead of extracting all these things we just say applied 20 layers with three by three
column or something like that and things work quite well this is of course an over-exaggeration but I think it's a correct first order statement to make is
that we've definitely seen that we've reduced code complexity quite a lot because these architectures are so
homogeneous compared to what we've done before so it's also remarkable that so
we had this reduction in complexity we had this amazing performance on imagenet one other thing that was quite amazing
about the results in 2012 that is also a separate thing that did not have to be the case is that the features that you
Transfer Learning
learn by training on image that turned out to be quite generic and you can apply them in different settings so in
other words this transfer learning works extremely well and of course I didn't go into details of convolutional networks
yet but we start with an image and we have a sequence of layers just like in a normal neural network and at the end we
have a classifier and when you pre train this network on image net then it turns out that the features that you learn in
the middle are actually transferable and you can use them on different data sets and that this works extremely well and
so that didn't have to be the case you might imagine that you could have a convolutional network that works extremely well on image net but when you
try to run it on some something else like birds data set or something that it might just not work well but that is not the case and that's a very interesting
finding in my opinion so people notice this back in roughly 2013 after the
first convolution networks they noticed that you can actually take many computer vision datasets and it used to be that
you would compete on all of these kind of separately and design features maybe for some of these separately and you can just shortcut all those steps that we
had designed and you can just take these pre trained features that you get from imagenet and you can just train a linear
classifier on every single data set on top of those features and you up many state-of-the-art results across many different data sets and so this was
quite a remarkable finding back then I believe so things worked very well an image net Thanks transferred very well
and the code complexity of course got much much more manageable so now all
The power is easily accessible.
this power is actually available to you with very few lines of code if you want to just use a convolutional network on
images it turns out to be only a few lines of code if you use for example Karis is one of the deep learning libraries that I'm going to go into and
I'll mention again later in the talk but basically just load a state-of-the-art complex all neural network you take an
image you load it and you compute your predictions and it tells you that this is an African elephant inside that image
and this took a couple couple hundred or a couple ten milliseconds if you have a GPU and so everything does much faster
much simpler works really well transfers really well so this was really a huge advance in computer vision and so as a
result of all these nice properties commnets today are everywhere so here's a collection of some of the some of the
ConvNets are everywhere...
things I try to find across across different applications so for example you can search google photos for
different types of categories like in this case rubik's cube you can find
house numbers very efficiently you can of course this is very relevant in self-driving cars and we're doing perception in the cars accomplishable
networks are very relevant they're medical image diagnosis recognizing Chinese characters doing all kinds of
medical segmentation tasks quite random tasks like wail recognition and more
generally many tackle challenges satellite image analysis recognizing different types of galaxies you may have
seen recently that a wave net from deepmind also very interesting paper that they generate music and they
generate speech and so this is a generative model and that's also just a comm that is doing most of the heavy lifting here so it's a convolutional
network on top of sound and other tasks like image captioning in the context of
reinforcement learning and agent in environment interactions we've also seen a lot of advances of using commnets as
the core computational building block so when you want to play Atari games or you want to play alphago or doom or
Starcraft or if you want to get robots to perform interesting manipulation tasks all of this users come that's as a
core computational block to do very impressive things not
only are we using it for a lot of different application we're also finding uses in art so so here are some examples
from deep dreams so you can basically simulate what it looks like what it feels like maybe to be on some drugs so
you can take images and you can just loosen it features these income that's or you might be familiar with neural style which allows you to take arbitrary
images and transfer arbitrary styles of different paintings like Van Gogh on top of them and this is all using
convolutional networks the last thing I'd like to note that I find also interesting is that in the process of
trying to develop better computer vision architectures and trying to basically optimize for performance on the image
net challenge we've actually ended up converging to something that potentially might function something like your visual cortex in some ways and so these
are some of the experiments that I find interesting where they've studied macaque monkeys and they record from a
subpopulation of the of the i.t cortex this is the part that does a lot of object recognition and so they record so
basically they take a monkey and they take a ComNet and they show them images and then you look at what those images are represented at the end of this
network so inside the monkey's brain or on top of your convolutional network as we look at representations of different images and then it turns out that
there's a mapping between those two spaces that actually seems to indicate to some extent that some of the things
we're doing somehow ended up converging to something that the brain could be doing as well in the visual cortex so
that's just some intro I'm now going to dive into convolutional networks and try to explain the briefly how these
networks work of course there's an entire class on this that I taught which is a convolutional networks class and so
I'm going to distill some of you know those 13 lectures into one lecture so we'll see how that goes I won't cover
everything of course okay so convolutional neural network is really just a single function it goes
from it's a function from the raw pixels of some kind of an image so we take 224 by 224 by 3 image so 3 here is for the
color channels RGB you take the raw pixels you put it through this function and you get 1000 numbers at the end in
the case of image classification if you're trying to categorize images into 1000 different classes and really
functionally all that's happening in a convolutional net work is just dot products and max operations that's everything but
they're wired up together in interesting ways so that you are basically doing visual recognition and in particular the
this function f has a lot of knobs in it so these w's here that participate in these dot products and in these
convolutions and fully connected layers and so on these WS are all parameters of this network so normally you might have
about on the order of 10 million parameters and those are basically knobs that change this function and so we'd
like to change those knobs of course so that when you put images through that function you get probabilities that are
consistent with your training data and so that gives us a lot to tune and turns out that we can do that tuning
automatically with back propagation through that search process now more concretely accomplish on your network is
made up of a sequence of layers just as in a case of normal neural networks but we have different types of layers that
we play with so we have convolutional layers here I'm using rectified linear unit relu for short as a non-linearity
so I'm making that and explicit its own layer pooling layers and fully connected
layers the core computational building block of a convolutional network though is this convolutional layer and we have
nonlinearities interspersed we are probably getting rid of things like pooling layers we might see them slightly going away over time and fully
connected layers can actually be represented there are basically equivalent to convolutional layers as well and so really it's just a sequence
of complex in the simplest case so let me explain convolutional layer because that's the core computational building
block here that does all the heavy lifting so the entire comm that is this
Convolution Layer
collection of layers and these layers don't function over vectors so they don't transform vectors as a normal
neural network but they function over volumes so a layer will take a volume a three-dimensional volume of numbers an
array in this case for example we have a 32 by 32 by 3 image so those three dimensions are the width height and I'll
refer to the third dimension as depth we have three channels that's not to be confused with the depth of a network
which is the number of layers in that network so this is just depth of a volume so this complex layer accepts a
three dimensional volume and it produces a three dimensional volume using some weights so the way it actually produces
this output volume is as follows we're going to have these filters in a convolutional layer so these filters are always small
patiently like say for example five by five filter but their depth extends always through the input depth of the
input volume so since the input volume has three channels the depth is 3 then
our filters will always match that number so we have depth of 3 in our filters as well and then we can take
those filters and we can basically convolve them with the input volume so what that amounts to is we take this
filter oh yeah so that's just the point that the channels here must match we take that filter and we slide it through
all spatial positions of the input volume and along the way as we're sliding this filter we're computing dot products so W transpose X plus B where W
are the filters and X is a small piece of the input volume and B is offset and so this is basically the convolutional
operation you're taking this filter and you're sliding it through at all spatial positions and you're computing that products so when you do this you end up
with this activation map so in this case we get a 28 by 28 activate activation
map 28 comes from the fact that there are 28 unique positions to place this 5x5 filter into this 3 32 by 32 space so
there are 28 by 28 unique positions you can place that filter in and every one of those you're going to get a single number of how well that filter alikes
that part of the input so that carves out a single activation map and now in a
compositional layer we don't just have a single filter but we're going to have an entire set of filters so here's another filter a green filter we're going to
slide it through the input volume it has its own parameters so these there are 75 numbers here that basically make up a
filter there are different 75 numbers we convolve them through get a new activation map and we continue doing
this for all the filters in that convolutional layer so for example if we had six filters in this convolutional
For example, if we had 6 5x5 filters, we'll get 6 separate activation maps
layer then we might end up with 28 by 28 activation maps six times and we stock
them along the depth dimension to arrive at the output volume of 28 by 28 by 6 and so really what we've done is we've
we represented the original image which is 32 by 32 by 3 into a kind of a new image that is 28 by 28 by 6 where this
image basically has these 6 channels that tell you how well every filter matches or likes every part of the input
so let's compare this operation to say using fully connected layer as you would in a normal neural network
so in particular we saw that we process the 32 by 32 by 3 volume into 28 by 28
by 6 volume and one question might want to ask is how many parameters would this
require if we wanted a fully connected layer of the same number of output neurons here so we wanted 28 by 28 by 6
or times 20 times when it's 12 times 28 times 6 number of neurons fully
connected how many parameters would that be turns out that that would be quite a few parameters right because every
single neuron in the output volume would be fully connected to all of the 32 by 32 by 3 numbers here so basically every
one of those - 28 by 28 by 6 now Newell's is connected to 32 by 3 2 by 3 turns out to be about 15 million
parameters and also on that order of number of multiplies so you're doing a lot of compute and you're introducing a
huge amount of parameters into your network now since we're doing convolution instead you'll notice that
think about a number of parameters that we've introduced with this example convolutional layer so we've used we had
6 filters and every one of them was a 5 by 5 by 3 filter so basically we just
have 5 by 5 by 3 filters we have 6 of them if you just multiply that out we have 450 parameters and in this I'm not
counting the biases I'm just counting the raw weights so compared to 15 million we've only introduced very few
parameters also how many multiplies have we done so computationally how many flops are we doing well we have twenty by
twenty eight by six outputs to produce and every one of these numbers is a function of a five by five by three
region in the original image so basically we have 20 by 20 by 6 and then
there's every one of them is computed by doing 5 times 5 times 3 multiplies so you end up with only on the order of 350,000 multiplies so we've reduced from
15 million to quite a few so we're doing less flops and we're using fewer parameters and really what we've done
here is we've made assumptions right so we've made the assumption that because the fully connected layer if this wasn't
fully connected layer could compute the exact same thing but it would so specific setting of those 15 million
parameters would actually produce the exact output of this convolutional layer but we've done it much more efficiently we've done that by
reducing these biases so in particular we've made assumptions we've assumed for
example that since we have these fixed filters that we're sliding across space we've assumed that if there's some interesting feature that you'd like to
detect in one part of the image like say top left then that feature will also be useful somewhere else like on the bottom right because we fix these filters and
apply them at all the spatial positions equally you might notice that this is not always something that you might want
for example if you're getting inputs that are centered face images and you're doing some kind of a face recognition isn't like that then you might expect
that you might want different filters at different spatial positions like say for I region so you might want to have some
I like filters and for math region you might want to have mouth specific features and so on and so in that case you might not want to use convolutional
layer because those features have to be shared across all spatial positions and the second assumptions that we made is
that these filters are small locally and so we don't have global connectivity we have this local connectivity but that's
okay because we end up stacking up these convolutional layers in sequence and so this the neurons at the end of the
ComNet will grow their receptive field as you stack these convolutional layers on top of each other so at the end of
the combat those neurons end up being a function of the entire image eventually so just to give an idea about what these
activation maps look like concretely here's an example of an image on the top left this is a part of a car I believe
and we have these different filters at we have 32 different small filters here and so if we were to convolve these
filters with this image we end up with these activation labs so this filter if you convolve it you get this activation
lab and so on so this one for example has some orange stuff in it so when we convolve with this image you see that
this white here is denying the fact that that filter matches that part of the image quite well and so we get these
activation maps you stack them up and then that goes into the next convolutional layer so the way this
looks then looks like then is that we processed this with some kind of a convolutional layer we get some output
we apply a rectified linear units some kind of a non-linearity as normal and then we would just repeat that operation
so we keep plugging these values into the next convolutional layer and so they
plug into each other in sequence okay and so we end up processing the image over time so that's the convolutional
layer and you'll notice that there are a few more layers so in particular the pooling layer i'll explain very briefly pooling layer is quite simple if
you've used Photoshop or something like that you've taken a large image and you've resized it you downsampled the
image well pulling layers do basically something exactly like that but they're doing it on every single channel
independently so for every one of these channels independently in a input volume will pluck out that activation lab will
down sample it and that becomes a channel in the output volume so it's really down sampling operation on these
volumes so for example one of the common ways of doing this in the context of neural networks especially is to use
MAX POOLING
maximum operation so in this case it would be common to say for example use two by two filters stride to so and do
max operation so if this is an input channel in a volume then we're basically what that amounts to is we're truncating
it into these two by two regions and we're taking a max over four numbers to produce one piece of the output okay so
this is a very cheap operation that down samples your volumes it's really a way to control the capacity of the network so you don't want too many numbers you
don't want things to be too computationally expensive it turns out that a pooling layer allows you to down sample your volumes you're going to end
up doing less computation and it turns out to not hurt the performance too much so we use them basically as a way of
controlling the capacity of these networks and the last layer that I want to briefly mention of course is the
fully connected layer which is exactly as what you're familiar with so we have these volumes throughout as we process
the image at the end you're left with this volume and now you'd like to predict some classes so we do is we just take that volume we stretch it out into
a single column and then we apply for the connected layer which is really amounts to just a matrix multiplication and then that gives us probabilities
after applying like a soft Max or something like that so let me now show you briefly a demo of
what the convolutional Network looks like so this is common nsj this is a
deep learning library for training convolutional neural networks that I've that is implemented in JavaScript I wrote this maybe two years ago at this
point so here what we're doing is we're training a convolutional network on the c 410 dataset see 410 is a data set of
50,000 images each image is 32 by 32 by 3 and there are different ten different
classes so here we are training this network in the browser and you can the loss is decreasing which means that
we're better classifying these inputs and so here's the network specification
which you can play with because this is all done in the browser so you can just change this and play with this so this
is an input image and this convolutional network I'm showing here all the intermediate activations and all the
intermediate basically activation maps that we're producing so here we have a set of filters
we're convolving them with the image and getting all these activation maps I'm also showing the gradients but I don't
want to dwell on that too much then your threshold so rel will do anything below
zero gets clamped at zero and then you pull so this is just down sampling operation and then another convolution
relu pool compre loophole etc until at the end we have a fully connected layer and then we have our soft max so that we
get probabilities out and then we apply a loss to those probabilities and back propagate and so here we see that I've
been training in this tab for the last maybe 30 seconds or one minute and we're already getting about 30 percent
accuracy on C for ten so this these are test images from C for ten and these are the outputs of this compositional
network and you can see that it learned that this is already a car or something like that so this trains pretty quickly in JavaScript so you can play with this
and continue the architecture and so on another thing I'd like to show you is this video because it gives you again
this like very intuitive visceral feeling of exactly what this is computing is there's a very good video by Jason Kaczynski from recent advance
I'm going to play this in a bit this is from the deep visualization tool box so you can download this code and you can
play with this it's this interactive convolutional network demo this is neural networks have enabled computers
to better see and understand the world they can recognize good buses and zip top left corner we showed you in this
case kappa hi Daddy so what we're seeing here is these are activation laps in some particular shown in real time as
this demo is running so these are for the calm one layer of an Alex net which we're going to go into in much more
detail but these are the different activation maps that are being produced at this point neural network called Alex
net running in cafe by interacting with the network we can see what some of the neurons
for example on this first leg the unit in the center responds strongly to light to dark edges its neighbor one neuron
over responds to edges in the opposite direction dark to light using
optimization we can synthetically produce images that light up each neuron on this layer to see what each neuron is
looking for we can scroll through every layer in the network to see what it does including convolution pooling and
normalization layers we can switch back and forth between showing the actual activations and showing images
synthesized to produce high activation but the time you get to the fifth
convolutional layer the features being computed represent abstract concepts for
example this neuron seems to respond to phases we can further investigate this neuron by showing a few different types
of information first we can artificially create optimized images using new regularization techniques that are
described in our paper these synthetic images show that this neuron fire is in response to a face and shoulders we can
also plot the images from the training set that activate this neuron the most as well as pixels from those images most
responsible for the high activations computed via the D combination technique this feature responds to multiple faces
in different locations and by looking at the D cons we can see that it would
respond more strongly if we had even darker eyes and rosy lips we can also confirm that it cares about the head and
shoulders that ignores the arms and torso we can even see that it fires to some
extent for cat faces using back prop or decom we can see that this unit depends
most strongly on a couple units in the previous layer con four and on about a dozen or so in con 3 now let's look at
another neuron on this layer so what's this unit doing from the top 9 images we
might conclude that it fires 4 different types of clothing but examining the synthetic images shows that it may be
detecting not clothing say but wrinkles in the live plot we can see that it's activated by my shirt and
smoothing out half of my shirt causes that hack of the activations to decrease
finally here's another interesting memo this one has learned to look for printed
text in a variety of sizes colors and fonts this is pretty cool because we
never asked the network to look for wrinkles or text or faces the only papers were provided were at the very
last layer so the only reason the network learned features like text and faces in the middle was to support final
decisions at that last layer for example the text detector may provide good evidence that a rectangle is in fact a
book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase which was
one of the categories we trained the net to recognize in this video we've shown
some of the features of the deep is toolbox okay so I encourage you to play with that it's really fun so I hope that
gives you an idea about exactly what's going on there at these convolutional layers we downsample them from what from time to time there's usually some fully
connected layers at the end but mostly it's just these convolutional operations stacked on top of each other so what I'd
like to do now is I'll dive into some details of how these architectures are actually put together the way I'll do
this is I'll go over all the winners of the imagenet challenges and I'll tell you about the architectures how they came about how they differ and so you'll
get a concrete idea about what these architectures look like in practice so we'll start off with the Alex net in 2012 so the Alex net just to give you an
idea about the sizes of these networks and the images that they process it took to 27 by 220 7 by 3 images and the first
Case Study: AlexNet NELA
layer of an Alex net for example was a completion layer that had 11 by 11 filters applied with a stride of four
and there are 96 of them stride of four I didn't fully explain because I wanted to save some time but intuitively it
just means that as you're sliding this filter across the input you don't have to slide in one pixel at a time but you can actually jump a few pixels at a time
so we have 11 by 11 filters with a stride a skip of four and we have 96 of
them you can try to compute for example what is the output volume if you apply this this sort of convolutional layer on
top of this volume and I didn't go into details of how you compute that but basically there are formulas for this and you can look into details in
the class but you arrive at 55 by 55 by 96 volume as output the total number of
parameters in this layer we have 96 filters every one of them is 11 by 11 by
3 because that's the input depth of these images so basically just amounts
to 11 but times 11 times 3 and then you have 96 filters so about 35,000 parameters in this very first layer then
the second layer of an Alex net is a pooling layer so we apply three by three filters at Stride of two and they do max
pooling so you can again compute the output volume size of that after applying this to that volume and you
arrive if you do some very simple arithmetic there you arrive at 27 by 27 by 96 so this is the down sampling
operation you can think about what is the number of parameters and this pooling layer and of course it's zero so
pooling layers compute a fixed function fixed down sampling operation there are no parameters involved in pulling a
layer all the parameters are in convolutional layers and the fully connected layers which are some extent equivalent to convolutional layers so
you can go ahead and just basically based on the description in the paper although is non-trivial I think based on
the description of this particular paper but you can go ahead and decipher what the volumes are throughout you can look
at the kind of patterns that emerge in terms of how you actually increase number of filters in higher
convolutional layers so we started off with 96 then we go to 256 filters then to 384 and eventually 4096 units click
on layers you'll see also normalization layers here which have since become slightly deprecated it's not very common
to use the normalization layers that were used at the time for the election architecture what's interesting to note
is how this differs from the 1998 Iyanla cool network so in particular I usually like to think about for things
that hold back progress so at least in a deep learning so the data is a
constraint compute and then I like to differentially differentiate between algorithms and infrastructure algorithms
being something that feels like research and infrastructure being something that feels like a lot of engineering has to happen and so in particular we've had
progress in all those four fronts so we see that in 1998 the data you could get ahold of maybe
would be on the order of a few thousand whereas now we have a few million so we had three orders of magnitude of increase in number of data compute GPUs
have become available and we use them to train these networks they are about say roughly 20 times faster than CPUs and
then of course CPUs we have today are much much faster than CPUs that they have back in 1998 so I don't know
exactly to what that works out to but I wouldn't be surprised if it's again on the order of three orders of magnitude of improvement again I'd like to
actually skip over algorithm and talk about infrastructure so in this case we're talking about Nvidia releasing the
cuda library that allows you to efficiently create all these matrix vector operations and apply them on arrays of numbers so that's a piece of
software that you rely on and that we take advantage of that wasn't available before and finally algorithms is kind of
an interesting one because there's been in those 20 years there's been much less improvement in an algorithms than all
these other three pieces so in particular what we've done with the 1998 network is we've made it bigger so you
have more channels you have more layers by bit and the two really new things algorithmically are dropout and
rectified linear units so dropout is a regularization technique developed by
geoff hinton and colleagues and rectified linear units are these nonlinearities that train much faster
than sigmoids and ten HS and this paper actually had a plot that showed that the
rectified linear units trained a bit faster than sigmoids and that's intuitively because of the vanishing gradient problems and when you have very
deep networks with sigmoids those gradients banish as Hugh was talking about in last lecture so what's
interesting also to note by the way is that both drop out and relu are basically like one line or two lines of
code to change so it's about two line diff total in those twenty years and both of them consist of setting things
to zero so with the relevance of things to zero when they're lower than zero and with dropout you set things to zero at
random so it's a good idea to set things to zero apparently that's what we learned so if you try to find a new cool
algorithm look for one line dips that set something to zero probably will work better and we could add you here to this
list now some of the newest things that happened some of the
comparing it again and giving you an idea about the hyper parameters that are in this architecture it was the first
use of rectified linear units we haven't seen that as much before this network using the normalization layers which are
not used anymore at least in a specific way that they use them in this paper they used heavy data
augmentation so you don't only put in you don't only pipe these images into the networks exactly as they come from
the data set but you jitter them spatially around a bit and you work them and you change the colors a bit and you just do this randomly because you're
trying to build in some invariances to these small perturbations and you're basically hallucinating additional data it was the first real use of drop out
and roughly you see standard hyper parameters like say batch sizes of roughly 128 using stochastic gradient
descent with momentum usually point nine in the momentum learning rates of 1e
negative two you reduce them in normal ways so you're reduced roughly by factor of ten whenever validation stops
improving and weight decay of just a bit five you negative four and ensemble
so you train seven independent commercial networks separately and then you just average their predictions
always gives you additional 2% improvement so this is Alex net the winner of 2012 in 2013 the winner was
the Z F net this was developed by Matthew Siler and Rob progress in 2013
and this was an improvement on top of Alex net architecture in particular one of the bigger differences here where
that the convolutional layer the first convolutional layer they went from 11 by 11 stride four to seven by seven strike
two so if slightly smaller filters and you apply them more densely and then also they notice that these
convolutional layers in the middle if you make them larger if you scale them up then you actually gain performance so
they managed to improve a tiny bit matthew Zeiler then went he became the founder of clarify and he worked on this
a bit more inside clarify and he managed to push the performance to 11% which was the winning entry at the time but we
don't actually know what gets you from 14% to 11% because Matthew never disclosed the full details of what
happened there but he did say that it was more tweaking of these hyper parameters and optimizing that a bit so
that was 2013 winner in 2014 we saw a slightly bigger to this so one of the networks that was
introduced then was a vgg net from Karen Simonian and andrew zisserman what's beautiful about vgg net and they
explored a few architectures here and the one that ended up working best was this D column which is what I'm highlighting it was beautiful about the
vgg net is that it's so simple so you might have noticed in these previous in these previous networks you have these
different filter sizes different layers and you do different amount of strides and everything kind of looks a bit hairy and you're not sure where these hyper
parameters are coming from VG's unit is extremely uniform all you do is 3x3 convolutions with stride one pad one and
you do two by two Macs Bowling's with stride two and you do this throughout completely homogeneous architecture and
you just alternate a few columns and a few pool layers and you get top top performance so they managed to reduce
the air down to 7.3% in the vdg net just with a very simple item Oh genius
architecture so it's I've also here written out this a D architecture it's just so you can
see I'm not I'm not sure how instructed this is because it's kind of dense but you can definitely see and you can look
at this outline perhaps but you can see how these volumes develop and you can see the kinds of sizes of these filters
so they're always three by three but the number of filters again grows so we started off with 64 and then we go to
128 256 512 so we're just doubling it over time I also have a few numbers here
just to give you an idea of the scale at which these networks normally operate so we have on the order 140 million
parameters this is actually quite a lot I'll show you in a bit that this can be about five or ten million parameters and
works just as well and it's about hundred megabytes for image in terms of
memory in the forward pass and then the backward pass also needs roughly on that order so that's roughly the numbers that
were we're working with here also you can note that most of the and this is true mostly in convolutional networks is
that most of the memory is in the early convolutional layers most of the parameters at least in the case where you use these giant fully connected
layers at the top would be here so the winner actually in 2014 was not the VG
net I only present it because it's such a simple architecture but the winner was actually Google net with a slightly
hairier architecture we should say so it's still a sequence of things but in this case they've put inception modules
in sequence and this is an example inception module I don't know then too much time to go into the details but you can see that it consists
basically of convolutions and different kinds of strides and so on so the Google
net is look slightly a hairier but it turns out to be more efficient in
several respects so for example it works a bit better than vgg net at least at the time it only has 5 million
parameters compared to VG nets 140 million parameters so a huge reduction and you do that by the way by just
throwing away fully connected layers so you'll notice in this breakdown I did these poly connected layers here have 100 million parameters and 16 million
parameters turns out you don't actually need that so if you take them away that actually doesn't hurt the performance too much so you can get a huge reduction
of parameters and it was it was slightly we can also compare to the original
Alex net so compared to the original Alex net we have fewer parameters a bit more compute and a much better performance so Google net was really
optimized to have a low footprint both memory wise both computation wise and both parameter wise but it looks a bit
uglier and VG net is a very beautiful homogeneous architecture but there are some inefficiencies in it okay so that's
a 2014 now in 2015 we had a slightly bigger Delta on top of the architectures
so right now these architectures if you on laocoon looked at them maybe in 1998 he would still recognize everything so everything looks very like simple you've
just played with had parameters so one of the first kind of bigger departures I would argue was in 2015 with the
introduction of residual networks and so this has worked from kamini Hey and colleagues in Microsoft Research Asia
and so they did not only win the image net challenge in 2015 but they want a whole bunch of challenges and this was
all just by applying these residual networks that were trained on image net and then fine-tuned on all these
different tasks and you basically can crush lots of different tasks whenever you get a new awesome Kombat so at this
time the performance was basically 3.5 7% from these residual networks so this is 2015 also this paper try to argue
that if you look at the number of layers it goes up and then it they made the point that with residual
networks as well see in a bit you can introduce many more layers and they and that that correlates strongly with
performance we've since found that in fact you can make these residual works quite shop quite a lot shallower
like say on the order of 20 or 30 layers and they work just as fine just as well so it's not necessarily the depth here
but I'll go into that in a bit but you get a much better performance what's interesting about this paper is this this plot here where they compare
these residual networks and I'll go into details of how they work in a bit and these what they call plane networks which is everything I've explained until
now and the problem with plane networks is that when you try to scale them up and introduce additional layers they
don't get monotonically better so if you take a 20 layer model and on this is on
C far ten experiments if you take a 20 layer model and you run it and then you take a 56 layer model you'll see that
the 56 layer model performs worse and this is not just on the test data so it's not just an overfitting issue this
is on the training data the 56 layer model performs worse on the training data than the 20 layer model even though
the 56 layer model can imitate 20 layer model by setting 36 layers to compute identities so basically it's an
optimization problem that you can't find the solution once your problem size grows that much bigger in this plane net
architecture so in the residual networks that they proposed they found that when you wire them up in a slightly different
way you monotonically get a better performance as you add more layers so more layers always strictly better and
you don't run into these optimization issues so comparing residual networks to plane networks in plane networks as I've
explained already you have this sequence of convolutional layers where every convolutional layer operates over volume
before and produces volume in residual networks we have this first convolutional layer on top of the raw
image then there's a pooling layer so at this point we've reduced to 56 by 56 by
64 the original image and then from here on they have these residual blocks with these funny skipped connections and this
turns out to be quite important so let me show you what these look like
so the original climbing paper had this architecture here shown under original so on the left you see original residual
networks design since then they had an additional paper that played with the architecture and found that there's a better arrangement of layers inside this
block that works better empirically and so the way this works so concentrate on the proposed one in the middle since
that works so well you have this pathway where you have this representation of the image X and
then instead of transforming that representation X to get a new X to plug in later we end up having this X we go
off and we do some compute on the side so that's that residual block doing some computation and then you add your result
on top of X so you have this addition operation here going to the next residual block so you have this X and
you always compute deltas to it and I think this it's not intuitive that this should work much better or why that
works much better I think it becomes a bit more intuitively clear if you actually understand the backpropagation dynamics and how backprop works and this
is why I always urge people also to implement back rub themselves to get an intuition for how it works what it's computing and so on because if you
understand back rub you'll see that addition operation is a gradient distributor so you get a gradient from
the top and this gradient will flow equally to all the children that participated in that addition so you have gradient flowing here from the
supervision so you have supervision at the very bottom here in this diagram and it kind of flows upwards and it flows
through these residual blocks and then gets added to this stream and so you end up with but this addition distributes
that gradient always identically through so what you end up with is this kind of a gradient superhighway as I like to
call it where these gradients from your supervision go directly to the original convolutional layer and then on top of
that you get these deltas from all the residual blocks so these block can come on online and can help out that original
stream of information this is also related to I think why LST MS along short-term memory networks work better
than recurrent neural networks because they also have these kind of additional addition operations in the lsdm and it
just makes the gradients flow significantly better then there were some results on top of residual networks
that I thought were quite amusing so recently for example we had this result on deep networks with stochastic depth the idea here was that the authors
of this paper noticed that you have these residual blocks that compute Delta's on top of your string and you
can basically randomly throw out layers so you have these say hundred blocks 100 residual box and you can randomly drop
them out and at test time similar to drop out you introduce all of them and they all work at the same time but you
have to scale things and it just like with dropout but basically it's kind of a unintuitive result because you can throw out layers
at random and I think it breaks the original notion of what we had of commnets of as like these these feature
transformers we compute more and more complex features over time or something like that and I think it seems much more
intuitive to think about these residual networks at least to me as some kinds of dynamical systems where you have this
original representation of the image X and then every single residual block is kind of like a vector field that because
it computes in a delta on top of your signal and so these vector fields nudge your original representation X towards a
space where you can decode the answer Y of like the class of that X and so if
you drop off some of these residual blocks at random then if you haven't applied one of these vector fields then the other vector fields that come later
can kind of make up for it and they nudge they basically nudge the they pick
up the slack and they nudge along anyways and so that's possibly why this the image I currently have in mind of
how these things work so much more like dynamical systems in fact another experiments that people are playing with
that I also find interesting is you don't have you can share these residual blocks so it starts to look more like a
recurrent neural network so these residual blocks would have shared connectivity and then you have this dynamical system really where you're
just running a single RNN a single vector field did you keep iterating over and over and then your fixed point gives you the answer so it's kind of
interesting what's happening it looks very funny ok we've had many
more interesting results that so people are playing a lot with these residual networks and improving on them in
various ways so as I mentioned already it turns out that you can make these residual networks much shallower and
make them wider so you introduce more channels and that can work just as well if not better so it's not necessarily
the depth that is giving you a lot of the performance it's you can scale down the depth and if you increase the width
that can actually work better and they're also more efficient if you do it that way there's more funny
regularization techniques here swap-out is a funny regularization technique that actually interpolates between plane nets
rez nets and dropout so that's also a funny paper with fractal nets we
actually have many more different types of nets and so people have really experimented with this a lot I'm really eager to see what the winning
we'll be in 2016 as a result of a lot of this one of the things that has really enabled this rapid experimentation in
the community is that somehow we've developed luckily this culture of sharing a lot of code among ourselves so
for example Facebook has released just as an example Facebook has released residual networks code and torch that is
really good that a lot of these papers I believe have adopted and worked on top of and that allowed them to actually really scale up their experiments and
and it explore different architectures so it's great that this has happened unfortunately a lot of these papers are
come kind of on archive and it's kind of a chaos as these are being uploaded so at this point I think this is a natural point to plug very briefly in my archive
sanity calm so this is the best website ever and what it does is it crawls
archive and it takes all the papers and it analyzes all the papers the full-text
of the papers and creates tf-idf bag-of-words features for all the papers and then you can do things like you can
search a particular paper like residual networks paper here and you can look for similar papers on archive and so this is
a sorted list of basically all the residual networks papers that are most related to that paper or you can also
create user accounts and you can create a library of papers that you like and then archive Sanofi will train a support vector machine for you and basically you
can look at what our archive papers over the last month that I would enjoy the most and that's just computed by archive
sanity and so it's like a curated feed specifically for you so I use this quite a bit and I find it in useful so I hope
that other people do as well okay so we saw convolutional neural networks I
explained how they work I explained some of the background context I've given you an idea of what they look like in practice and we went through case
studies of the winning architectures over time but so far we've only looked at image classification specifically so
we're categorizing images into some number of bins so I'd like to briefly talk about addressing other tasks in
computer vision and how you might go about doing that so the way to think about doing other
tasks in computer vision is that really what we have is you can think of this computational convolutional neural network as this block of compute that
Addressing other tasks...
has a few million parameters in it and it can do basically arbitrary functions that are very nice over images and so
takes an image gives you some kind of features and now different tasks will basically look as follows you want to
picked some kind of a thing and different tasks there will be different things and you always have a desired thing and then you want to make the
predicted thing much more closer to the desired thing and you back propagate so this is the only part usually that
changes from task to task you'll see that these comments don't change too much what changes is your last function at the very end and that's what actually
helps you really transfer a lot of these winning architectures they usually use these pre trained networks and you don't
worry too much about the details of that architecture because you're only worried about you know adding a small piece at the top or changing the last function or
substituting a new data set and so on so just to make this slightly more concrete in image classification we apply this
Image Classification thing = a vector of probabilities for different classes
compute block we get these features and then if I want to do classification I would basically predict 1,000 numbers
that give me the LOC probabilities of different classes and then I have a predicted thing a desired thing
particular class and I can back prop if I'm doing image captioning the it also looks very similar instead of predicting
just a vector of 1,000 numbers I now have for example at ten thousand ten thousand words in some kind of
vocabulary and I'd be predicting ten thousand numbers and a sequence of them and so I can use a recurrent neural
network which you will hear much more about I think in Richards lecture just after this and so I produce a sequence
of ten thousand dimensional vectors and that's just a description and they indicate the probabilities of different words to be emitted at different time
steps or for example if you want to do localization again most of the block stays unchanged but now we also want
Localization
some kind of a extent in the image so suppose we want to classify we don't only just want to classify this as an
airplane but we want to localize it with X Y width height bounding box coordinates and if we make a specific
assumption as well that there's always a single one thing in the image like a single airplane in every image then you
can just afford to just predict that so we predict these softmax scores just like before and apply the cross-entropy
loss and then we can predict X Y width height on top of that and we use alloc and l2 loss or a Hooper loss or
something like that so you just have a predicted thing a desired thing and you just back drop if you want to do
Reinforcement Learning
reinforcement learning because you want to play different games then again the setup is you just predict some different thing and it has some different
semantics so in this case we will be for example predicting eight numbers that give us the probabilities of taking
different actions for example there are eight discrete actions in Atari then we just predict eight numbers and then
we trained us with a slightly different manner because in the case of reinforcement learning you don't actually have a you don't actually know
what the correct action is to take at any point in time but you can still get a desired thing eventually because you
just run these rollouts over time and you just see what what happens and then
that helps you that helps inform exactly what the correct answer should have been or what the desired thing should have
been in any one of those rollouts in any point in time I don't want to dwell on this too much in this lecture though it's outside of the scope you'll hear
much more about reinforcement learning in the in a later lecture if you wanted to do segmentation for example then you
Segmentation
don't want to predict a single vector of numbers for a single for single image but every single pixel has its own
category that you'd like to predict so data set will actually be colored like this and you have different classes different areas and then instead of
predicting a single vector of classes you predict an entire array of 224 by 224 since that's the extent of the
original image for example times 20 if you have 20 different classes and then you basically have 2 24 by 2 24
independent soft maxes here that's one way you could pose this and then you back propagate this would here would be
slightly more difficult because you see here I have a decom players mentioned here and I didn't explain the
convolutional layers they're related to convolutional layers they do a very similar operation but kind of backwards
in some way so a compilation layer kind of does these down sampling operations as it computes a decon layer does these
kind of up sampling operations as it computes these convolutions but in fact you can implement a decomp layer using
accomplish so what you do is you decom forward pass is the cobbler backward pass and the decom backward pass is the
complex basically so they're basically an identical operation but just are you up sampling we're down sampling kind of
so you can use decomp layers or you can use hyper columns and there are different things that people do in
segmentation literature but that's just the rough idea as you're just changing to loss function at the end if you
wanted to do auto-encoders so you want to do some surprise landing or something like that well you're just trying to predict the original image so you're
trying to get the convolutional network to implement the identity transformation and the trick of course it makes it
non-trivial is that you're forcing the representation to go through this representational bottleneck of 7 by 7 by
512 so the network must find an efficient represent of the original image so that it can decode it later so that would be a auto
encoder you again have an l2 loss at the end and your backdrop or if you want to do variational auto-encoders you have to
Variational Autoencoders
introduce a repair motorisation layer and you have to append an additional small loss that makes your posterior
beer prior but it's just like an additional layer and then you have an entire generative model and you can actually like sample images as well if
you wanted to do detection things get a little more hairy perhaps a compared to
Detection
localization or something like that so one of my favorite detectors perhaps to explain as the yellow detector because it's perhaps the simplest one it doesn't
work the best but it's the simplest one to explain and has the core idea of how people do detection in computer vision
and so the way this works is we reduced the original image to a seven by seven
by 512 feature so really there are these 49 discrete locations that we have and
at every single one of these 49 locations we're going to predict in yellow we're going to predict a class so
that's shown here on the top right so every single one of these forty-nine will be some kind of a soft Max and then
additionally at every single position we're going to predict some number of bounding boxes and so there's going to
be a B number of bounding boxes say B is 10 so we're going to be predicting 50
numbers and the the 5 comes from the fact that every bounding box will have five numbers associated with it so you have to describe the XY the width and
the height and you have to also indicate some kind of a confidence of that bounding box so that's the fifth number
is some kind of a confidence measure so you basically end up predicting these bounding boxes they have positions they
have class they have confidence and then you have some true bounding boxes in the image so you know that there are certain
true boxes and they have certain class and what you do then is you match up the
desired thing with the predicted thing and whatever so say for example you had one bounding box of a cat then you would
find the closest predicted bounding box and you would mark it as a positive and you would try to make that associated
grid cell predict cat and you would nudge the prediction to be slightly more towards the cat the box and so all this
can be done with simple losses and you just back propagate that and then you have a detector or if you want to get much more fancy you you could do dense
Dense Image Captioning
image captioning so in this case this is a combination of detection and image captioning this is a paper with my equal quality in Johnson and
Feifei Lee from last year and so what we did here is image comes in and it becomes much more complex I don't maybe want to go into it as much
but the first order approximation is that instead it's basically a detection but instead of predicting fixed classes
we instead predict a sequence of words so we use a recurrent neural network there but basically I can take an image
then and you can predict you can both detect and predict and describe everything in a complex visual scene so
that's just some overview of different tasks that people care about most of them consist of just changing this top
part you put different loss function a different data set but you'll see that this computational block stays
relatively unchanged from time to time and that's why as I mentioned when you do transfer learning you just want to
kind of take these blue train networks and you mostly want to use whatever works well on imagenet because a lot of that does not change too much okay so in
the last part of the talk I'd like to just make sure we're good on time okay we're good so in the last part of the
talk I just wanted to give some hints of some practical considerations when you want to apply convolutional net works in
practice so first consideration you might have if you want to run these networks is what hardware do I use so some of the options
that I think are available to you well first of all you can just buy a machine so for example and Vidya has these
digits dev boxes that you can buy they have Titan X GPUs which are strong GPUs you can also if you're much more
ambitious you can buy dgx one which has the newest Pascal P 100 GPS unfortunately the dgx one is about a
hundred and thirty thousand dollars so this is kind of an expensive supercomputer but the digits death box I
think is more accessible and so that's one option we can go with alternatively you can look at the specs of a dev box
and those specs are there good specs and then you can buy all the components yourself and assemble it like Lego
unfortunately you that's prone to mistakes of course but you can definitely reduce the price maybe by
fracture like to it compared to the Nvidia machine but of course Nvidia
machine would just come with all the software installed all the hardware is ready and you can just do work there are a few GPU offerings in the cloud but
unfortunately it's actually not at a good place right now it's actually quite difficult to get GPUs in the cloud good
GPUs at least so Amazon AWS has these great k5 five 20s they're not very good GPUs
they're not fast they don't have too much memory it's actually kind of a problem Microsoft Azure is coming up
Azura is coming up with its own offering soon so I think they've announced it and it's in some kind of a beta stage if I
remember correctly and so those are powerful GPUs K 80s that would be available to at open the eye for example
you use Sarah scale so seer scale is much more a slightly different model you can't spin up GPUs on demand but they
allow you to rent a box in the cloud so what that amounts to is that we have these boxes somewhere in the cloud I have just the DNA I just have the URL is
sh2 it it's a it's a Titan X boxes in the machine and so you can just do work that way so these options are available
to hardware wise in terms of software there are many different frameworks of course that you could use for deep
learning so these are some of the more common ones that you might see in practice so different people have
different recommendations on this I would my personal recommendation right now to most people if you just want to apply this in practical settings 90% of
the use cases are probably addressable with things like Harris so Karis would I go to number one thing to look at Karis
is a layer over tensorflow or Theano and basically just a higher-level
API over either of those so for example I usually use Karis on top of tensorflow and it's a much more higher level
language than raw tensorflow so you can also work in raw tensorflow but you'll have to do a lot of low level
stuff if you need all that freedom then that's great because that allows you to have much more freedom in terms of how you design everything but it can be
slightly more worthy for example you have to assign every single weight you have to assign a name stuff like that and so it's just much more wordy but you
can work at that level or for most applications I think Karis would be sufficient and I've used torch for a long time I still really like torch it's
very lightweight interpretable it works just just fine so those are the options that I would currently consider at least another
practical consideration you might be wondering what architecture what architecture do I use in my problem so my answer here and I've already hinted
at this is don't be a hero don't go crazy don't design your own neural
networks and convolutional layers and don't probably don't you don't to do that probably so the algorithm is
actually very simple look at whatever is currently the latest released thing that works really well in iOS VRC you
download that pre-trained model and then you potentially add or delete some layers on top because you want to do
some other tasks so that usually requires some tinkering at the top or something like that and then you fine-tune it on your application so
actually a very straightforward process the first degree I think to most applications would be don't tinker with
it too much you're going to break it but of course you can also take two 2:31 end and then you might become much better at
at tinkering with these architectures second is how do I choose the parameters
and my answer here again would be don't be a hero look into papers look what
happens they use for the most part you'll see that all papers use the same hyper parameters they look very similar so Adam when you add them for
optimization it's always learning rate one in negative three or one integrate four so four you can also use sed
momentum it's always the similar kinds of learning rates so don't go too crazy designing this one of the things you probably want to play with the most is
the regularization so and in particular not the l2 regularization but the dropout rates is something I would
advise instead and so because you might have it smaller or a much larger data
set if you have a much smaller data set and overfitting is a concern so you want to make sure that you regular eyes properly with dropout and then you might
want to as a second degree consideration may be the learning rate you want to tune that a tiny bit but that yeah
that's usually doesn't as much of an effect so really there's like two hyper parameters and you take a pre train
network and this is 90% of the use cases I would say yeah so compared to when
computer version 2011 where you might have hundreds of high parameters so yeah
okay and in terms of distributed training so if you want to work at scale
because if you want to Train imagenet or some large scale data sets you might want to train across multiple GPUs so
just to give an idea most of these state-of-the-art networks are trained on the order of a few weeks across multiple GPUs usually four or
eight GPUs and these GPS are roughly on the order of one thousand dollars each but then you also have to house them so
of course that has a different price but you almost always want to train on multiple GPUs if possible usually you
don't end up training across machines that's much more rare I think to train across machines what's much more common is you have a single machine and it has eight
Titan exes or something like that and you do distributor training on those eight titan axis there are different
ways to distribute a training so if you're very if you're feeling fancy you can try to do some model parallelism
where you split your network across multiple GPUs I would instead advise some kind of a data parallelism
architecture so usually what you see in practice is you have a GPUs so I take my batch of 256 images or something like
that I split it and I split it equally across the GPUs I do forward pass on those GPUs and then I basically just add
up all the gradients and I propagate that through so you're just distributing this batch and you're doing mathematical you're
doing the exact same thing as if you had a giant GPU but you're just splitting up that batch across different GPUs but
you're still doing synchronous training with SGD as normal so that's what you'll see most in practice which i think is the best thing to do right now for most
normal applications and other kind of considerations that sometimes enter that you could may be worried about is that
there are these bottlenecks to be aware of some particular CPU to disk bottleneck this means that you have a giant data set it's somewhere on some
disk you want that disk to probably be an SSD because you want this loading to be quick because these GPUs process data
very quickly and that might actually be a Balan like like loading the data could be a bomb like so many applications you might want to pre process your data make
sure that it's read out contiguously and very raw form from something like an HD f5 file or some kind of other binary
format and another bottleneck to be aware of is the CPU GPU bottleneck so
the GPU is doing a lot of heavy lifting of the neural network and the CPU is loading the data and you might want to use things like prefetching threads
where the CPU while the networks are doing forward backward on the GPU your CPU is busy loading the data from the
disk and maybe doing some pre-processing and making sure that it can ship it off to the GPU at the next time step so
those are some of the practical considerations I could come up with for this lecture if you wanted to learn much more about convolutional neural networks
and a lot of what I've been talking about then I encourage you to check out CS 231 n we have lecture videos
available we have notes slides and assignments everything is up and available so you're welcome to check it
out and that's it thank you
so I guess I can take some questions yeah
hello hello hi I'm Kyle afar from Luna
I'm using a lot of convolutional nets for genomics when the problems that we see is that our genomic sequence tends
to be arbitrary length so right now we're pattern for a lot of zeros but we're curious as to what your thoughts
are on using CN NS for things of arbitrary size where we can't just down sample to 277 by 277 yep
so is this lecture genomic sequence of like a TCG like that kind of sequence exactly yeah so some of the options
would be so recurrent neural networks might be a good fit because they allow arbitrarily sized contexts another
option I would say is if you look at the wave net paper from deep mind they have audio and they're using convolutional
networks for processing it and I would basically adopt that kind of an architecture they have this clever way of doing what's called a truce or
dilated convolutions and so that allows you to capture a lot of context with few layers and so let's call dilated
convolutions and the wavelet paper has some details and there's an efficient implementation of it that you should be aware of on github and so you might be
able to just drag and drop the fast wave net code into that application and so you have much larger context but it's of
course not infinite context as you might have with a recurrent Network yeah we're definitely checking those out we also tried our n ends they're quite slow for
these things our main problems that the genes can be very short or very long but the whole sequence matters so I think
that's one of the challenges that we're looking at with this type of problem interesting yeah so those would be the
two options that I would play with basically I think those are the two demo where thank you
thanks for a great lecture so my question is that is there a clear mathematical or conceptual understanding
when people decide how many hidden layers have to be part of their architecture yeah so the answer with a
lot of this is there a mathematical understanding will likely be no because we are in very early phases of just
doing a lot of empirical and I'll guess and check kind of work and so theory is
in some some ways like lagging behind a bit I would say that was residual networks you want to have more layers
usually works better and so you can take these layers outdoor you can put them in and it's just mostly computational
consideration of how much can you fit in so our consideration is usually is you have a GPU it has maybe 16 gigs of ram
or 12 gigs of ram or something I want certain batch size and I have these considerations and that upper
bounds the amount of like layers or how big they could be and so I use the biggest thing that fits in my GPU and
that's mostly what the way you choose this and then you regularize it very strongly so if you have a very small
data set then you might end up with a pretty big Network for your data set so you might want to make sure that you are tuning those dropout rates properly and
so you're not overfitting I have
question my understanding is that the recent convolution that doesn't use polling layers right so the question is
why you know why don't they use fungal areas so you know is there still a place
for puli yeah yeah so certainly so if you saw for example the residual Network at the end there was a single pooling
layer at the very beginning but mostly they went away you're right so took her I wonder if I can find the slide I
wonder if this is a good idea to try to find the slide that's bro okay let me
just find this okay so this was the residual network architecture so you see
that they do a first comm and then there's a single pool right there but certainly the trend has been to throw
them away over time and there's a paper also it's called striving for simplicity the all convolutional neural network and the
point in that paper is look you can actually do stranded convolutions you can throw away pulling layers all together or it's just as well so pulling
layers are kind of I would say this kind of a bit of a historical vestige of they needed things to be efficient then they
need to control Bastian downsample things quite a lot and so we're kind of throwing them away over time and yeah they're not doing
anything like super useful they're doing this fixed operation and you want to learn as much as possible so maybe you
don't actually want to get rid of that information so it's always more appealing to it's probably more
appealing I would say to throw them away well you mentioned there is a sort of cognitive or brain analogy that the
brain is doing polling so yeah so I think that analogy is stretched by a lot so the brain I'm not sure every brain is
doing pulling yeah about image
compression not for justification about the usage of neural networks for image
compression do we have any examples sorry I couldn't hear the question it's of classification for images can we use
the neural networks for image compression image compression yeah I think there's actually a really exciting
work in this area so one that I'm aware of for example as a recent work from Google where they're using commercial
networks and recurrent networks to come up with variably sized codes for images so certainly a lot of these generative
models I mean they are very related to compression so definitely a lot of work in the area of them that I'm excited
about also for example super resolution networks so you saw the recent acquisition of magic pony by Twitter so
they were also doing something that basically allows you to compress you can send low resolution strings because you can up sample it on the client and so a
lot of work in that area yeah I had we should patent it after you I can't
please comment on scalability regarding number of classes so what does it take
if we go up to 10,000 or 100,000 classes hmm yes so if you have a lot of classes
then of course you can grow your softmax but that becomes inefficient at some point because you're doing a giant matrix multiply so some of the ways that
people are addressing this in practice I believe is you use of like hierarchical softmax and things like that so you you
decompose your classes into groups and then you kind of predict one group at a time and you kind of converge that way
so I'm not I see these papers but I don't I'm not an expert on exactly how this works but I do know that they are
called softmax is something that people thing especially for example in language models this is often used because you have a huge amount of words and you
still need to predict them somehow so I believe Thomas Mikhailov for example he has some papers on using hierarchical softmax in this context would you could
you talk a little bit about the convolutional functions like what what considerations you should make in
selecting the functions that are used in the convolutional filters selecting the functions that are used in the
convolutional filters so these filters are just parameters right so we train those filters they're just numbers that
we trained with backpropagation okay are you talking about the nonlinearities perhaps or yeah I'm just wondering about
when you're selecting those the features or when you're getting the when you're trying to train to just understand
different features with an image what what are those filters actually doing well I see you're talking about
understanding exactly what those filters are looking for in the engine somewhat so a lot of interesting work especially for example so Jason your Sinskey he has
this deepest toolbox and I've shown you that you can kind of debug it that way a bit there's an entire lecture to encourage you to watch in CS 231 and on
visualizing understanding accomplish all networks so people use things like a decom or guided or guided back
propagation or you back propagate to image and you try to find the stimulus that maximally activates any arbitrary
neuron so different ways of probing it and different ways have been developed and there's a lecture about it so I
would I would check that out great thanks I had a question regarding the size of fine-tuning data set for
example is there a ballpark game number if you are trying to do classification how many do you put you need for
fine-tuning it to your sample set so how many how many data points do you need to
get good performance since the question okay so so okay so this is like the most
boring answer I think because the more the better always and it's really hard to say actually the company you need so
usually one way one way to look at it as one heuristic that people sometimes follow is you look at a number of
parameters and you want the number of examples to be on the order of number of parameters that's one way people sometimes break it down right for
fine-tuning because we will have an image net model so I was hoping that most of the things
would be taken care or there and then you're just fine-tuning so you you might need a lower order I say so when you're
saying fine-tuning are you finding the whole network or you're using some of it or just the top classifier just the top classifier yeah so one another way to
look at it is you have some number of parameters and you can estimate the number of bits that you think every
parameter has and then you count the number of bits in your data so that's kind of like comparisons you would do but really uh yeah I have no good answer
so the more the better and you have to try and you have to regularize and you have to cross validate that and you have to see what performance you get over time because it's to task it and then
for me to say something stronger hi I would like to know how do you think the
Covenant will work in this rady case like is they just a simple extension of the 2d case oh do we need some extra
tweak about in 3d case so you're talking specifically about say videos or some 3d
accurate talking about the image that has the depth information oh I see so
say you have like RGB D input and things like that yeah so I'm not too familiar with people do but I do know for example that people
try to have for example one thing he can do is just treat it as a fourth Channel or maybe you want the separate ComNet on
top of the depth channel and do some fusion later so I don't know exactly what the state-of-the-art in treating that depth channel is right now so I
don't exactly how they do what I do right now oh so maybe just one more question just how do you think the 3d
object great condition a 3d object yeah recognition so what is the output that
you'd like the oppo is still the class probability but we are not treating the
2d image but the 3d representation of the art I say so do you have a mesh or point cloud yeah I see yeah so also not
not exactly my area for currently but so the problem with these meshes and so on is that there's just like a rotational
degree of freedom that I'm not sure what people do about honestly so the yeah so
I'm actually not an expert on this so I don't want to comment there are some obvious things you might want to try like you might want to plug in all the possible ways you could orient this and
then a test time averaged over them so there would be some of the obvious things to play with but I don't I'm not actually sure what the state of the art
is okay thank you so coming back to the distributed
training is it possible to do even the classification a distributed way or my questions in future can I imagine my our
cellphones do these things together for one inquiry our cellphones oh I see
you're trying to get cell phones distributed training yes it's a train it's arrived quite a fun very radical
idea so related thoughts I had recently was so I had come the jas in the browser and I was thinking of basically this
trains narrow networks and I was thinking about similar questions because you could imagine shipping this off as an ad equivalent like the people just
include this in the JavaScript and then everyone's browsers are kind of like training a small network so I think
that's a related question do you think there's like too much communication overhead or it could be actually really disturbed in an efficient way
yes so the problem with distributing it a lot is actually a stale gradients problem so when you look at some of the
papers that Google has put out about distributed training as you look at the number of workers when you do
asynchronous SGD number of workers and the the performance improvement you get it kind of like plateaus quite quickly
after like eight workers or something quite small so I'm not sure if there are ways of dealing with thousands of
workers the issue is that you have a distributed every worker has this like specific snapshot of the weights that
are currently I have to pull you pull from the master and now you have a set
of ways that you're using and do forward backward and you send an update but by the time you send an update and you down your forward backward the parameters
server has now done like lots of updates from like thousands of other things and so you're grading the scale you've
evaluated it every wrong an old location and so it's an incorrect direction now
and everything breaks so that's the challenge and I'm not sure what people are doing about this yeah I was
wondering about applications of convolutional nets to two inputs at a
time so let's say you have two pictures of jigsaw puzzles puzzles these are pieces they're trying to
figure out if they fit together or whether one object compares to the other in a specific way have you heard of any
implementation of this kind yes so you have two inputs instead of one yeah so the common way
dealing with that as you put a comment on each and then you do some kind of a fusion eventually to to merge the information right I see and same for
recurrent neural networks if you have like variable input so for example in the context the videos where you have
frames coming in yeah then yeah so some of the approaches are you have accomplished all Network on the frame and then at the top you tie it in with
the recurrent neural network so you have these you reduce the image to some kind of a lower dimensional representation
and then that get that's an input to a recurrent neural network at the top there are other ways to play with this
for example you can actually make the recurrent you can make every single neuron in the calm that recurrent that's also one funny way of doing this so
right now when a neuron computes its output it's only a function of a local neighborhood and below it but you can
also make it in addition a function of that same local neighborhood or like its own activation perhaps in the previous
time step if that makes sense so so this so this neuron is not just computing a
dot product with the with the current patch but it's also incorporating a dot product of its own and maybe it's
neighborhoods activations at the previous time step of the frame so that's kind of like a small or an update hidden inside every single neuron so
those are the things that I think people play with when I'm not familiar with what currently is working best in this area pretty awesome thank you yeah yeah
thanks for the great talk final question regarding the latency for
the models that are trained using multiple layers so especially at the prediction time you know as we add more
more layers for the forward pass it will take some time you know it'll increase in the latency right for the prediction
so what are the numbers that we have seen you know you know presently that
you know that you know if you can share that you know the prediction time or the you know the latency at the the forward
pass so you're worried for example you're some you want to run a friction very quickly would it be on an embedded
device or is this in the cloud uh you're suppose you know it's a cell phone you know you have your you know identifying
the objects or you know you're you're doing some you know image analysis or something yeah so there's definitely a
lot of work on this so one way you would approach this actually is you have this network that you've trained using floating point arithmetic 32 bits say
and so there's a lot of work on taking that Network and discretizing all
into like intz and making it much smaller and pruning connections so one of the works I'm related to this for
example is someone here at Stanford has few papers on getting rid of spurious connections and reducing the network as
much as possible and then making everything very efficient with integer arithmetic so basically you achieve this
by discretizing all the weights and all the activations and throwing away and
pruning the network so there are some tricks like that that people play that's mostly what you would do in an embedded
device and then the challenge of course is you've changed the network and now you just kind of are crossing your
fingers that it works well and so I think what's interesting for Reese from research standpoint is he'd like to do
you'd like your test time to exactly match your training time right so then you get the best performance and so the
question is how do we train with low precision arithmetic and there's a lot of work on this as well so say from your show up in Joe's lab as well and so
that's exciting directions of how you train in low precision regime do you have any numbers I mean that you can
share for the new state of the art how much time does it yes I see the papers
but I'm not sure if I remember the exact reductions it's on the order of okay I don't want to say because it's go no
thanks I don't want to try to guess this thank you all right we're out of time
let's thank Andrew
lunch is outside and will restart at 12:45

----------

-----

--12--

-----
Date:  2016.09.27
Link: [# Foundations of Unsupervised Deep Learning (Ruslan Salakhutdinov, CMU)](https://www.youtube.com/watch?v=rK6bchqeaN8)
Transcription:


Deep Unsupervised Learning
sound is good okay great so I wanted to talk to you about unsupervised learning and that's the area where there's been a
lot of research but compared to supervised learning that you've heard about today like convolutional networks
you know unsupervised learning is not very yet alright so I'm going to show you lots of lots of areas parts of the
talk are going to be a little bit more mathematical I apologize for that but I'll try to give you gist off of the
foundations the math behind these models as well as try to highlight some some of the application areas okay what's the
motivation well the motivation is that you know the space of data that we have today is is just growing right you know
if you look at the space of images you know speech if you look at social network data if you look at scientific
data I would argue that most of the data that we see today is unlabeled right so
how can we develop statistical models models that can discover interesting kind of structure in unsupervised way or
semi-supervised way and that's what I'm interested in as well as how can we sort
of apply these models across multiple different multiple different domains and one particular framework of doing that
is the framework of deep learning where you're trying to learn hierarchical representations of data and and again as
we go as I go through the talk I'm going to show you some some examples I've tried so here's here's one example you
Deep Autoencoder Model
know you can take a simple bag of words representation of an article or a newspaper you can use something that's
called an autoencoder just multiple levels you extract some latent code and
then you get some representation out of it right and this is done completely in unsupervised way you don't provide any
labels and if you look at the kind of structure that the model discovering you know it could be useful for visualization for example to see what's
what kind of structure you you see in your data this was done on the on the Reuters data set I've tried to kind of
cluster together lots of different a supervised learning techniques and I'll touch on some of them it's a little bit
you know it's it's not full set but the way that I typically think about these models is that there
is a class of what I would call non probabilistic models you know models like sparse coding auto-encoders
clustering based methods and these are all very very powerful powerful
techniques and I'll cover some of them in that talk as well and then there is sort of a space of probabilistic models
and within probably stick models you have tractable models you know things like fully observed belief networks
there's a beautiful class of models called neural auto regressive density estimators more recently we've seen some
successes of so-called pixel recurrent neural network models or and I'll show
you some examples of that there is a class of so called intractable models where you know you are looking at models
like Boltzmann machines and models like variation water encoders something that's been quite there's been a lot of
development in our community and deep learning community in that space Helmholtz machines I'll tell you a
little bit about what these models are and a whole bunch of others as well right one particular structure within
these models is that when you're building these generative models or of data you typically have to specify what
the distributions you're looking at so you have to specify what the probability of the data and generally doing some
kind of approximate maximum likelihood estimation and then more recently you know we've seen some very exciting
models coming out these are generative adversarial networks moment matching networks and
this is sort of a slightly different class of models where you don't really have to specify what the density is you
just need to be able to sample from those models and I'm going to show you some some examples of that okay so my
Talk Roadmap
talk is going to be sort of structured though I'd like to introduce you to the basic building blocks models like sparse
coding models because I think that is a very important class of models particularly for folks who are working
in in industry and looking for simpler models autoencoder is a beautiful class
of models and then the second part of the talk I'll focus more on on generative models
I'll give you an introduction on into restricted Boltzmann machines and deep both machines these are sort of model statistical models that can model
so complicated complicated data and I'll spend some time showing you some
examples some recent developments in our community specifically in the case of variation autoencoders which is I view
them as a subclass of Helmholtz machines and I'll finish off by giving you an intuition about you know a slightly
different class of models which would be these generative adversarial networks okay so let's let's jump into the first
part but before I do that let me just sort of give you a little bit of motivation and know Angie's done a great
Learning Feature Representations
job and Richard sort of alluded to that as well but the idea is you know if I'm
trying to classify a particularly image right and if I say well you know if I'm looking specific pixel representation
might be difficult for me to classify what I'm seeing right on the other hand if I can find the right representations
right the right representations for these images and then I sort of get the right features so get the right
structure from the data that it might be easier for me to you know see what's what's going on with my data right so
how do I find these representations and this is this is sort of one of
Traditional Approaches
traditional approaches that we've seen for a long time is that you know you have a data you're creating some
features and then you run in your learning algorithm and for the longest time an object recognition or an audio
classification you typically use some kind of hand design features and then you start classifying what you have and
Computer Vision Features
you know like Andre was saying in the space of vision it's been a lot of different features designs of what's the
right structure we should see in the data in the space of audio same thing is
Audio Features
happening how can you find these right representations for your for your data
and the idea behind their presentation learning in particular in deep learning
is can we actually learn these representations automatically right and more importantly can we actually learn
these representations in unsupervised way right by just seeing lots and lots of unlabeled can we achieve that and you know there's
been a lot of work done in that space but we're not there yet so I wanted to sort of lower your expectations as as I
show you some some of the results ok sparse coding this is one of the models
that I think that everybody should know what it is it was actually you know
first has its roots in 96 and it was originally developed to explain early
visual processing in the brain sort of I think of it as an edge detector and the objective here is the following well if
I give you set of data points x1 up to xn you'd want to learn a dictionary of basis Phi 1 up to Phi K right so that
every single data point can be written as a linear combination of the basis that's fairly simple right there is one
constraint in that you'd want your coefficients to be sparse you'd want
them to be mostly 0 right so every data point is represented as a sparse linear
combination of bases right so this is if
you apply sparse coding to natural images right and this says this was
originally it's been a lot of work developed at Stanford with n doings group so if you apply sparse coding to
you know take little patches of images and it learn these bases these dictionaries this is how they look like
and it's they look really nice in terms of you know finding the edge edge like structure so if you've given a new
example I can say well this new example can be written as a linear combination of a few of these bases right and taking
that representation it turns out that particular representation is sparse representation is quite useful as a
feature representation of your data right so it's quite useful to have it and in general helps how do we how do we
fit these models well if I give you a whole bunch of
Sparse Coding: Training
image patches but these don't necessarily have to be image patches this could be you know little speech signals or any kind of data you're
working with you'd want to learn in a dictionary basis you have to form you have to solve this optimization problem right so the first term here
you can think of it as a reconstruction error which is to say well I take a linear combination of my basis I want
them to match my data and that is the second term which is you can think of it
as a sparse penalty term which essentially says you know try to penalize my coefficients so that most of
them are zero right that way every single data point can be written as just the linear combination sparse linear
combination of the basis and it turns out there is an easy optimization for
doing that if you fix your dictionary of bases right by one up to five SK and you solve
for the activations that becomes a standard lasso problem right there's a lot of solvers for for solving that
particular problem that's a general very you know it's it's it's a su problem
which is fairly easy to to optimize and then if you fix the activations and you optimized for dictionary basis then it's
a well-known quadratic programming problem right each problem is convex so you can sort
of alternate between finding coefficients finding bases and so forth so you can optimize this function and there's been a lot of recent work in the
last ten years of doing these things online and doing it more efficiently and so forth right at test time given a new
input or a new image patch and given a set of learned basis once you have your dictionary you can then just solve a la
Sparse Coding: Testing Time
soupe problem to find the right coefficients right so in this case given a test sample or test patch you can find
well it's written by as a linear combination of subsets of the bases
right and it turns out again that that particular representation is very useful particularly if you're interested in
classifying what you see in images and this is done in completely unsupervised way right there is no class labels there
is no specific supervisory signal that's that's here so back in 2006 there was
Image Classification Evaluated on Caltech101 object category dataset
work done again at Stanford that
basically showed a very interesting result so if I give you an input like this and these are my learned bases remember
these little edges what happens is that you just control these bases you can get these different
feature Maps much like you know the future maps that we've seen in convolutional neural networks and then
you take these feature maps and you can just do a classification right and this was done on a one of the older data sets
the Caltech 101 which is sort of a data set that predates imagenet and if you
look at you know some of the competing algorithms if you do a simple logistic regression versus if you do PCA and then
do logistic regression versus finding these features using sparse coding you
can get substantial improvements right so that's again that's that's and you
see sparks coding popping up in a lot of different areas not just in deep learning but folks who are using looking
at medical imaging domain in neuroscience these are very popular models because they're easily they're
easy to fit they're easy to to deal with so what's the interpretation of the
Interpreting Sparse Coding
sparse coding well look let's look at this equation again and we can think of sparse coding as finding and over
complete representation of your data right now the encoding function we can
think of this encoding function which is well I give you an input find me the features or sparse coefficients or bases
that make up my image we can think of encoding as an implicit and very nonlinear function of X right but it's
an implicit function we don't really specify it and the decoder or the reconstruction is just a seem simple
linear function and it's and it's very explicit just take your coefficients and
then multiply it by the you know find the right basis and get back get back
the image or the data right and that sort of flows naturally into the ideas
of autoencoders right the auto encoder is the general framework where if I give you an input data let's say it's an
input image you encode it you get some representation some feature representation and then you have a
decoder given that representation you're decoding it back into the image so you can think of encoding as a
as a feed-forward bottom-up pass right much like in convolutional neural
network given the image you're doing a forward pass and then there is also feedback and generative or top-down pass
right given features you're reconstructing back back the input image in the details is what's going inside
the encoder decoder they matter a lot and obviously you need some form of constraints you need some of constraints
to avoid learning an identity right because if you don't put these constraints what you could do is just take your input copy it to your features
and then reconstruct back right and that would be a trivial solution so we need to introduce some some additional
constraints if you're dealing with binary features if you want extract
binary features for example I'm going to show you later why you'd want to do that you can pass your your encoder through a
sigmoid non-linearity much like in the neural network and then you have a have a linear decoder that we can strike back
the input and the way we optimize these little building blocks or these little blocks is we can just have an encoder
right which takes your input takes a linear combination passes it through some non-linearity the sigmoid
non-linearity or could be rectified linear units could be 10h non-linearity and then there is a decoder where you
reconstruct back your original input right so this is nothing more than a neural network with one hidden layer and
typically that hidden layer would have a small dimensionality than the input so we can think of it as a bottleneck layer
right and we can determine the network parameters you know the parameters of the encoding the parameters of the
decoder by writing down the reconstruction error and that's what the reconstruction would look like you know
given the input in code decode and make sure whatever you decoding is as close as possible to to the original to the
original input all right then we can use back propagation algorithm to to to
Train there is an interesting sort of relationship between all encoders and property and principle component
analysis many of you have probably heard about pca as a practitioner you know if you're
dealing with large data and you want to see what's going on PCA is the first thing to use right much like what you seek regression
so and the idea here is that if the parameters of encoder and decoder are shared and you actually have the hidden
layer which is a linear layer so you don't use any nonlinearities then it turns out that the space the latent
space that the model will discover is going to be the same space as the space discovered by PCA it effectively will
collapse the principal component analysis right you're doing PCA which is sort of a nice connection because it
basically says that all encoders you can think of them as nonlinear extensions of PCA all right so you can learn a little
richer features if if you are using
autoencoders okay so here's another model if you're dealing with binary input sometimes we're dealing with like
Another Autoencoder Model
Amnesty for example again your encoder and decoder could use sigmoid nonlinearities so given an input you
extract some binary features binary features you reconstruct back the binary input and that's actually you know
relates to a model called restricted both machines something that I'm going to tell you about later in the talk okay
Predictive Sparse Decomposition
there is also other classes of models where you can say well I can also introduce some sparsity much like in
sparse coding to say that you know I need to constrain my latent features on my latent space to be sparse and that
actually allows you to learn quite reasonable features nice features here's
one particular model called predict expires decomposition where you effectively you know if you look at the
first part of the equation here the decoder part that pretty much looks like a sparse coding model right but in
addition you have an encoding part that essentially says train an encoder such
that it actually approximates what my latent code should be right so
effectively you can think of this model is that is in coder there is a decoder but then you put the sparsity constraint
on your latent representation and you can optimize for for that model and
obviously the other thing that we've been doing in the last you know seven eight and ten years is well what you can
Stacked Autoencoders
do is you can actually stack these things together right so you can learn low-level features
try to learn high-level features and so forth so just building these blocks and
perhaps at the top level if you try to solve a classification problem you can do that or and this is sometimes known
as a greedy greedy lair wise learning and this is sometimes useful whenever you have lots and lots of unlabeled data
and when you have a little label data right a small sample of label data
typically these models help you find meaningful representations such that you don't need a lot of label data to solve
the particular task that you're trying to solve right and this is again you can remove the decoding part and then you
end up with a standard or convolutional architecture again your encoder and decoder could use could be convolutional
and it's it depends on what problem you tackling and typically you know you can
stack these things together and optimize for particular tasks that you're trying to solve okay here's an example of just
Deep Autoencoders
wanted to show you some examples some early examples back in 2006 this was a way of trying to build these nonlinear
autoencoders and you can sort of pre train these models using restricted Boltzmann's or auto-encoders generally
and then you know you can stitch them together into this deep autoencoder and
back propagate through reconstruction laws right one thing I want to point out is that here's one particular example
you know the top row I show you real faces the second row you're seeing faces
reconstructed from a bottleneck of of 30 dimensional real valid bottlenecks you
can think of it as just a compression mechanism given the data high dimensional data you're compressing it down to 30 dimensional code and then
from that 30 dimensional code you're reconstructing back the original data right so if you look at the first row
this is the data the second row shows you reconstructed data and the last row shows you PCA solution right one thing I
want to point out is that you know the solution here you have a much Sharpe representation which means that it's
capturing a little bit more structure in the data it's so kind of interesting to see that sometimes these models tend to
how should I say they tend to regularize your data right like for example if you see this person with glasses removes the
glasses and that genuinely has to do with the fact that there is only one person with glasses so the model just basically said that's noise get rid of
it oh it sort of gets rid of moustaches right like if you see this there's no mustache right and then again that has
to do with the fact that there's enough capacity so the model might think that that's just a noise and you know if
Information Retrieval
you're dealing with text type of data this was done using a Reuters data set
you have about eight hundred thousand stories you take bag of words representation something very simple you
can press it down to two dimensional space and then you see what that space looks like right and I always like to
joke that you know the model basically discovers that European community economic policies that just next
disasters and accidents right this is done this was back in I think they was
collecting ninety six right I think today is probably going to become closer those difference but again this is just
a way typically autoencoders a way of compression or trying to do dimensionality reduction but we'll see
later that they don't have to be okay there is another class of algorithm called semantic hashing which is to say
Semantic Hashing
well what if you take your data and compress it down to binary representation wouldn't that be nice
because if you have binary representation you can search in the binary space very efficiently right in
fact if you can can compress your data down to twenty dimension 20 dimensional
binary code to to the twenty is about four gigabytes so you can just store everything in memory and you can look at
the you know just do memory lookups without actually doing any search at all
right so this sort of representation sometimes have been used successful in computer vision where you take your
images and then you learn these binary representations you know thirty
dimensional codes two hundred dimensional codes and it turns out it's very efficient to search through large
volumes of data using binary representation so you can you know takes a fraction of a millisecond to retrieve
images from you know a set of millions and millions of just and again this is also an active
area of research right now because people are trying to figure out we have these large databases how can you search through them
efficiently and sort of learning a semantic hashing function that maps your data to the binary presentation turns
out to be quite useful okay now let's step back a little bit and say let's now
look at generative models let's look at probabilistic models and how different they are and I'm going to show you some
examples of where they applicable here's one example of a simple model trying to
Deep Generative Model
learn a distribution of these handwritten characters so we have you know we have Sanskrit we have Arabic we
have Syria like and now we can build a model that says well can you actually
generate me what a Sanskrit should look like the flickering you see at the top
these are you know a neurons you can think of them as neurons firing and what you're seeing at the bottom is you're
seeing what the model generates what it beliefs ask it should look like right so
in some sense when you think about generative models you think about models that can generate or they can sample the
distribution or they can sample the data this is a fairly simple model we have about 25,000 characters you know coming
from 50 different alphabets about around the world you have about two million parameters is one of the older models but this is you know what the model is a
scree should look like and I think that I've asked couple of people to say that is that does that really look like
Sanskrit okay great which can mean two things it can mean
that the model is red actually generalizing or the model is overfitting right meaning that is just memorizing
what the training data looks like and I'm just showing you examples from the training data we'll come back to that point as we go through the talk here's
you know you can also do conditional simulation you know given half of the image can you complete the remaining
half right and more recently there's been a lot of advances it's actually the
last couple of years for the conditional generations and it's pretty amazing what you can do in terms of in painting given
half of the image but the other half of Lima should look like this is sort of a simple example but it does show you that it's
trying to you know be consistent with what different strokes look like right
so why is it so difficult in the space of so-called undirected graphical models
of Boltzmann machines the difficulty really comes from the following fact if I show you this image which is a 28 by
28 image it's a binary image right so some pixels are on some pixels are off there are 2 to the 28 by 28 possible
images so in fact there are 2 to the 784 possible configurations right and that space is exponential so how can you
build models that figure out in the space of characters there's only a little tiny subspace in that space right
if you start genuinely generating you know 200 by 200 images you know that
space is huge in the space of real images is really really tiny right so
how do you find that space how do we generalize to new images that's that's a very difficult question in general to to
answer one class of models is so-called fully observed models right there's sort
of been a stream of learning generative models that are tractable and they have
very nice properties like you can compute the probabilities you can do can do maximum likelihood estimation here's one example where I can if I try to
model the image I can write it down as you know taking the first pixel more than the first pixel then modeling the
second pixel given the first pixel and just just writing it down in terms of conditional project of the conditional
probabilities an inch conditional probability can take a very complicated form right it could be a complicated
neural network and oh sorry so there's
been a number of successful models one of the early models called neural auto
regressive density estimator actually developed by Hugo real-valued extension
of these models and more recently we start seeing these flavors of models there were a couple of papers popped up
actually this year from deep mind where they sort of make these conditionals to
be you know sophisticated RNN so STM's or convolutional models and they can actually generate remarkable images
and so this is just a pixel CNN generating I guess elephants yeah and
actually looks pretty pretty interesting right the drawback of these models is that we yet have to see how good of
representations these models are learning so that we could use these representations for other tasks like
classifying images or find similar images and such right now let me jump
into a class of models called restricted Boltzmann so this is the class of models where we're actually trying to learn
some latent structures some latent representation these models belong to the class of so-called graphical models
and graphical models very powerful framework for representing dependency structure between random variables this
is an example where we have you can think of this particular model you have
some pixels this is stochastic binary so called visible variables you can think of pixels in your image and you have
stochastic binary hidden variables you can think of them as feature detectors so detecting certain patterns that you see in the data
much like sparse coding models she has a bipartite structure you can write down the probability the Joint Distribution
over all of these variables you sort of have pairwise term you have Union return
but it's not really important what they look like the important thing here is that if I look at this conditional probability of the data given given the
features I can actually write down explicitly what it looks like and what does that mean that basically means that
if you tell me what features you see in the image I can generate the data for you right I can generate the
corresponding input in terms of learning features so what do these models learn they stop learn something similar that
Learning Features
we've seen in sparse coding right it's and so these classes of models are very similar to each other so given a new
image I can say well this new image is made up by some combination of these learned weights of these learned bases
and the numbers here are given by the probabilities that each particular edge is present in the data in terms of how
we learn these models one thing I want to make another point I should make here
is that given an input I can actually quickly infer what teachers I'm seeing in the image so that
operation is very easy to do unlike in sparse coding models it's it's a bit more closer to Northern Quarter given
the data I can actually tell you what features are present in my in my input which is very important for things like
information retrieval or classifying images because you need to do it you need to do it fast how do we learn these
Model Learning
models let me just give you an intuition maybe a little bit of math behind how we learn these models if I give you set of
training examples and I want to learn model parameters I can maximize the log-likelihood objective right and
you've probably seen that in these tutorials max and while objective is essentially nothing more than saying I
want to make sure that the probability of observing these images is as high as possible right so finally the parameters
of the probability of observing what I'm seeing is high and that's why you're maximizing the the likelihood objective
or the log of the likelihood objection would just you know take a product into the sum you take the derivative there's
a little bit of algebra I promise you it's not it's not very difficult like you know second-year college algebra you
differentiate and you basically have this learning rule which is the difference between two terms the first
term you can think of it as looking at sufficient statistic so cost official
statistics driven by the data and the second term is the sufficient statistics driven by the model right maybe I can
parse it out what does that mean intuitively what that means is that you look at the correlations you see in the
data right and then you look at the correlations that the model is telling you it should be and you're trying to
match the two that's what the learning is trying to do right it's trying to match the correlations that you see in
the data right so the model is actually respecting the statistics that you see in the data but it turns out that the
second term is very difficult to compute and it's precisely because the space of all possible images is so high
dimensional that you need to figure out or use some kind of approximate learning algorithms to do that
all right so you have these difference between these two terms the first term is easy to compute it turns out because
of a particular structure of the model right and we can actually do it do it
explicitly the second term is the difficult difficult one to compute right so it sort of requires you know summing
over all possible configurations all possible images that that that you could possibly see so it's this term is
intractable and what a lot of different algorithms are doing and we'll see that over and over again is using so-called
Monte Carlo sampling or Markov chain Monte Carlo sampling a Monte Carlo estimation right so let me give you an
intuition what what this term is doing that's a general trick for you know approximating exponential sums right the
whole subfield in in statistics that's basically dedicated to how do we
approximate exponential sums in fact if you could do that if you could solve that problem you could solve a lot of
problems in machine learning and the idea is very simple actually the idea is
to say well you're going to be replacing the average by sampling and there's
something that's called a Gibbs sampling Markov chain Monte Carlo which is essentially does something very simple
it basically says well start with a data sample the states of the latent
variables you know sample the data sample the states of the way through then sample the data from these conditional distributions something that
you can compute explicitly right and that's a general trick you know much like in sparse coding we you know we're
optimizing for the basis when we're optimizing for the coefficients here you're inferring the coefficients then
you you know in vary what the data should look like and so forth and then you can just run a Markov chain and sort
of approximate approximate you know this exponential sum so you start with the
data you sample the states of the hidden variables you resample the data and so forth and the only problem with a lot of
these methods is that you know you need to run them up to infinity to guarantee
that you're sort of getting the right thing and so obviously you know you will never run them you know infinite you
don't have time to do that so there's a very clever algorithm that contrastive divergence algorithm that was developed
Contrastive Divergence
by Hinton back in 2002 and it was very clever it basically said well instead of
this thing up to infinity run it for one step right and so you're just running it
for one step you start with a training vector you you update the hidden units you update all the visible units again
so that's your reconstruction much like an autoencoder you reconstruct your data you update the hidden units again and
then you just update the model parameter which is just looking at you know empirically the statistics between the data and the model right very similar to
what the auto encoder is doing but slight slight differences and implementation is basically takes about
like penalize of MATLAB code I suspect is going to be you know two lines intensive flow although I don't think
terms of flow folks implemented Boltzmann machines yet that would be my request but you can extend these models
to dealing with real valued data right so whenever you're dealing with images for example and it's just a little
change to the definition of the model and your conditional probabilities hedge is going to be bunch of gaussians so
that basically means that given the features sample me the space of images and i can sample you give you you know
real real valued images the structure of the model remains the same if you train
this model on you know the these images you sort of tend to find edges something
similar again to what you'd see in sparse coding and I see in depending upon analysis model auto-encoders and
such and again you can sort of say well every single image is made up by some some linear combination of these basis
functions you can also extend these models to dealing with count data right if you're dealing with documents in this
RBMs for Word Counts
case again it's slight change to the model K here denotes your vocabulary
size and DK the knots number of words that you're seeing in your document right so if you you know it's it's a bag
of words representation and the conditional here is given by so called softmax distribution much like what
you've seen in in the previous classes when we you know the distribution of possible words right and the parameters
here double use you can think of them as you know something similar to as what the back embedding would do and so if
you apply to in again some some of datasets you know you tend to find
reasonable features right so can to find you know features about Russia about us
about computers and so forth right so much like you found these representations little edges every image
is made up by some combination of these edges in case of documents or webpages
you're saying the same thing it's just made up cyma linear combination of of these learned topics every single
document is made up by some combination of these topics right you can also look at one-step reconstruction so you can
basically say well how can I find similarity between the words so if I show you chocolate cake and further
states of hidden units and then I reconstruct back the distribution of possible words you know it tells me you
know chocolate cake cake chocolate sweet dessert cupcake food sugar and so forth right I particularly like the one about
the flour hi and then there is a Japanese sign the model sort of generates flour Japan Sakura blossom
Tokyo all right so it sort of picks up again on low-level correlations that you see in your data you can also apply
Collaborative Filtering
these kinds of models to collaborative filtering where every single observed variable you can model you know can
represent a user rating for a particular movie right so every single user would
rate a certain subset of movies and so you can represent it as the state of visible via your hidden states can
represent user preferences what they are and on the netflix data set if you look
at the latent space that the model is learning you know some of these hidden variables are capturing specific movie
genre right so for example there is this is actually one hidden union dedicated
to michael michael moore's movies rights instead of like very strong i think it's sort of you know either people like it
or hate it so there are a few hidden specifically dedicated to that but it also finds interesting things like you
know action movies and so forth right so it finds that particular structure in the data so you can model different
kinds of modality real-valued data can model count data a multinomial x' and it's very easy to
infer the states of the hidden variables so that's given just the product off of logistic functions and that's very important in a lot of different
applications given the input that can quickly tell you what topics I see in the data right one thing that I want to
Product of Experts
point out that's an important point is a lot of these models can be viewed as product models sometimes people call
them product of experts and this is because of the following the following
intuition if I write down the Joint Distribution of my hidden observed variables I can write it down in this
sort of log-linear form right but if I sum out or integrate out the states of
the hidden variables I have a bunch of product of a whole bunch of functions right so what is what does it mean what
what's the intuition here so let me show you an example suppose the model finds these specific topics right and suppose
I'm going to be telling you that the document contains topic government corruption in mafia then the word silvio
berlusconi will have very high probability right I guess does anybody know you know everybody knows who Celia
Celia Berlusconi right he's he had like you know he's in head of the government he's connected to mafia he is his very
corrupt was corrupt and I guess I should add like a bunker bunker parties here right then it will become completely
clear what I'm talking about but then you know one point I want to make here is that it's it's you know you
can think of these models as a product each hidden variable defines a distribution over possible words over
possible topics and once you take the intersection of these distributions you can be very precise about what is it
that you modeling right so that's unlike generally topic models or let Indian
education models models where you're actually using mixture like a approach
and then typically these models do perform far better than traditional mixture based models and this comes to
Local vs. Distributed Representations
the point of local versus global versus distributed representations right in in
a lot of different algorithms you know even the supervised learning algorithm such as clustering you typically have some your
partitioning the space and you're finding local prototypes right and the
number of parameters for which you have basically you know parameters for each region the number of regions typically
grow with linearly with a number of parameters but in models like factor
models PCA restricted Boltzmann machines deep models you typically have distributed representations right what's
the idea here the idea here is that if I show you the two inputs right each particular neuron can you know
differentiate between two parts of the plane given the second one you know I can partition it again given the third
hidden variable you can partition it again so you can see that every single neuron will be affecting lots of
different regions and that's the idea behind distribute representations because every single parameter is
affecting many many widgets not just the local region and so the number of regions grow roughly exponentially with
the number of parameters right so that's the differences between these these two
classes of models important to know about them now let me jump and quickly tell you a little bit of inspiration
behind what what can we build with these models right as we've seen with convolutional networks the first layer
Deep Boltzmann Machines
we typically learn some low-level features like edges or you know if you're working with the word word table
typically learn some low-level structure and the hope is that the higher-level features will start picking up some
higher-level structure as as you are building and these kinds of models can
be built in completely unsupervised way because what you're trying to do is you're trying to model the data you try to model the distribution of of the data
Model Formulation
you can write down the probability distribution for this models known as a
Boltzmann machine model you have dependencies between hidden variables so now introducing some extra you know some
extra layers and dependencies between those layers and if we look at the equation the first part of the equation
is basically the same as what we had with restricted Boltzmann and then the second and third part of the equation
essentially modeling dependencies between you know the first and the second hidden layer and the second
hidden layer third in there right there is also a very natural notion of bottom-up and top-down so if I want to see what's the
probability of a particular Union being taking value 1 it's really depend on
what's coming from below what's coming from above so there has to be some consensus in the model to say ah yes
what I'm seeing in the image and what my model believes the overall structure should be should be an agreement right
and so in this case of course in this case hidden variables become dependent even when you condition on on the data
so these kinds of models we'll see a lot is you introducing more flexibility you're introducing more structure but
then learning becomes much more difficult right you have to deal you know how do you influence in these
models right now let me give you an intuition of what how can we learn this
model what's the maximum likelihood estimator doing here well if I differentiate this model with respect to
parameter basically run into the same learning rule and it's the same one you know you see whatever you're working with undirected graphical models factor
graphs conditional random fields you might have heard about those those ones it really is just trying to look at the
statistics driven by the data correlations that you see in the data and the correlations that the model is telling you it's seeing in the data and
you just try to match the two right that's exactly what's happening in that particular equation right but the first
term is no longer factorial so you know you have to do some approximation with these models let me give you an
intuition watch what each term is doing so far as I have some data right and I get to observe these characters well
what I can do is I really want to tell the model this is real right these are
real characters so I want to put some probability mass around them and say these are real and then there is some
sort of a data point that looks like this just bunch of pixels on and off and I really want to tell my model that you
know put all the zero probability on this this is not real right and so the
first term is exactly trying to do that the first term is just trying to say put the probability max where you see the
data and the second term is effectively trying to say well look at this entire exponential space and just say no
everything else is not real it's just the real thing is what I'm seeing in my data and so you can use sort of
advanced techniques for doing that it's a class of algorithms called variational inference it's something that's called
stochastic approximation which is Monte Carlo based inference I'm not going to go into these techniques but in general
you can you can train these models so one question is how good are they all
Good Generative Model?
right because a lot of proximation that go into these models so what I'm going to do is if you have if you haven't seen
it I'm going to show you two panels on one panel you will see the real data or
another panel you'll see data simulated by the model or the fake data and you have to tell me which one is which okay
so again these are handwritten characters coming from you know alphabets around the world how many of
you think this is simulated and the other part was real honestly okay some
what about the other way around I get half and half which is great if you look
at these images a little bit more carefully you will see the difference right so you will see that this is
simulated and this is real right because if you look at the real data it's much
crisper there is more diversity when you're simulating the data it's a lot of structure in the simulated characters
but some you know sometimes a little bit files it isn't as much diversity right and I've learned that trick from from my
neuroscience friends if I show you quickly enough you won't see the difference and and you know if you're
using these models for for for classifying you know you can do proper analysis which is to say given a new
character you find further states of the latent variables hidden variables if I classify based on that how good are they
and and they are they're you know they're much better than some of the existing techniques this is another
Generative Model of 3-D Objects
example you know trying to generate 3d objects this is sort of a toys data sets and later on I'll show you some you know
bigger advances that's been happening in the last few years this was done a few years ago you know if you look at the
space of generated samples they you know they sort of you know you obviously you
can see the difference here's here's paid look at this particular image right this image looks like car with wings
don't you think right so there's sometimes it can sort of simulate things that are not
necessarily realistic and for some reason it just doesn't generate donkeys and elephants too often right but it
generates people with guns more often like if you look at here and here and here and that again has to do with the
fact that you know you exploring this exponential space of possible images and
it's sometimes it's very hard to assign the right probabilities the different parts of the space right and then
3-D Object Recognition
obviously you can do things like pattern completion so given half of the image can you complete the remaining half so
the second one shows what the completions look like and the last one is what the truth is so you can do you can do these things so where else can we
use these models these are sort of toy examples but where else let me show you one example where these models can
Data - Collection of Modalities
potentially succeed which is trying to model the space of the multi model space
which is the space of you know images and texts or you know generally if you
look at the data it's not just single source it's a collection of different modalities alright so how can we take
all of these modalities into account and this is really just the idea of you know given images and texts can you actually
find a concept that relates these two different sources of sources of data and
Challenges - 11
there are a few challenges and that's why you know models like generative models sometimes probablistic models
could be useful general one of the biggest challenge we've seen is that typically when you're working with
images and text these are very different modalities right if you think about images in pixel representation they're
very dense if you're looking at text it's typically very sparse right so it's very difficult to learn these cross
model features from low-level representation perhaps a big challenge is that a lot of
times we see data that's very noisy right sometimes it's just non-existent giving an image there is no text or if
you look at the first image you know a lot of the tags about is what kind of camera was used to to describe that
particular image which doesn't really tell us anything about the image itself right and these are this would be the
text generated by a version of a Boltzmann machine model sort of does you know samples what what
should look like and this is the the idea again is very simple if you just build a simple representation given
A Simple Multimodal Model
images and given text you just try to find what the common representation is it's very difficult to learn these cross
model features but if you actually build a hierarchical model so you start with
representation you know you can build a Gaussian model replicate itself max molecule so build up that representation
then it turns out it's much more it gives you much richer representation is
also when if a notion of bottom-up and top-down which means that you know low level or images or tags can effectively
effect low level representation of images and the other way around so information flows between images and
texts sort of gets into some stable state and this is what you know the text
Text Generated from Images
generated from images looks like some of the examples you know a lot of them a lot of them look reasonable but more
recently with the advances of cornets this is probably not that surprising
here's some examples of the model that's not quite doing the right thing right I
particularly like the second one for some reason it sort of correlates with Barack Obama and such and we've sort of
the features when we were using this model we didn't have at that time sort of image that features right now I don't
think we would be making these mistakes but generally speaking you know what we found in a lot of the data is that there aren't a lot of images of animals right
which brings us to the next problem is that if you don't see images of animals then the mall is confused because it sees a lot of Obama signs and these are
black and also white and and blue sort of signs that appearing a lot you can
also do images from text given text or tags can retrieve relevant images so you
know this is the data set itself at about million images it's a nice nice data set and you have you know very noisy tags and the question is can you
actually learn some representation from from those images one thing that I want to highlight here is you know we've
tried you know this is 25,000 labeled image somebody went and label what's going on in those images what classes we
see in those images and you get some numbers which is main average precision but what's important here is that we
found that if we actually use label data and we pre train these channels separately using a million
label data points then we can actually get some performance improvement so at least that was a little bit of happy
sign for us to say that you know unlabeled data can help in the situations where you don't have a lot of
labeled examples so here was helping us it was helping us a lot and then once
Multimodal Linguistic Regularities
you get into this sort of representations dealing with text and images this is one particular thing you
can do and I think Richard pointed out you know what happens in the space of linguistic regularities you can do the
same thing with images just kind of fun to do they they sometimes work they don't work all the time but here's one
example if I take that particularly image at the top and I say get the representation of this image subtract
the representation of day add the night and then find closes images you get these images right and then you can do
some interesting things like take these kittens and say - ball + box to get kittens in the box right if you take
this particular image and say - box + ball get kittens in the ball right except for this thing that's a duck so
you know that's you can sort of get these interesting interesting representations of course these are all
sort of fun things to look at but they don't really mean much because we're not specifically optimizing for those things
right now let me spend some time also
talking about another class of models these are known as Helmholtz machines and variational tinker's these are the
models that have been sort of popping up in our community in the last two years right so what is a Helmholtz machine a
hell-house machine was developed back in 95 and it was developed by Hinton and
Peter Dayan and Brandon Frey and Radford Neal and it has this particular
architecture you have a generative process given some latent state you just
it's a neural net with a stochastic neural network that generates the input data right and then you have so-called
approximate inference step which is to say given the data infer approximately
what the latent States should look like right and again it was
developed in 95 there's something is called waste sleep algorithm and it never worked basically people just said
it just doesn't work and you know then we started looking at restricted boltzmann shades on both machines
because they were working a little bit better and then two years ago people figure out how to make them work and so
now 10 years later I'm going to show you the trick now these models actually working pretty well the difference
between Helmholtz machines and deep Boltzmann chains is very subtle they almost look identical the big
difference between the two is that in hell most machines you have a generative process that generates the data and you
Helmholtz Machines vs. DBMS
have a separate recognition model that tries to recognize what you see in the data so you can think of this cue
function as a convolutional neural network given the data tries to figure out what the feature should look like and then there is a generative model
given the features it generates the data Boltzmann machine is sort of similar class of models but it has undirected
connection so you can think of it as generative and recognition connections are the same so it's sort of a system
that tries to know which some equilibrium state when you're running it
so it's a little bit the semantics is a little bit different between these two models so what is the variation water
encoder variational core is is a Helmholtz machine it defines a generative process in terms of sampling
Variational Autoencoders (VAE) The VAE defines a generative process in terms of ancestral sampling through a cascade of hidden stochastic layers
through cascades of stochastic layers and it's if you look at it there's just bunch of conditional probability
distributions that you're defining so you can generate the data so theta here will denote the parameters of the
variation auto-encoders you have a number of stochastic layers and sampling
from you know this conditional probability distributions hallam you know we're assuming that we can do it it's a tractable it has to be tractable
but the innovation here is that every single conditional probability can
actually be you know it can be very complicated function in keynote you know a nonlinear you can model nonlinear
relationships it can be a multi-lane only a neural network deterministic neural network right so it becomes
fairly fairly powerful he's here's an example of I have a stochastic layer you have a deterministic way have a
stochastic way and then you generate generate the data right so you can introduce these nonlinearities
into these models and this conditional probability we didn't know the one layer
neural network right now I'll show you some examples but maybe I can just give
you a little intuition behind what these equations do and a lot of these kinds of
models learning is very hard to do and there is a class of models called variational learning and what the
variational learning is trying to do is basically trying to do the phone but I want to maximize the probability of the data that I observe but I cannot do it
directly so instead what I'm going to do is I'm going to maximize the so called variational Lobot which is this term
here right and it's effectively saying well if I take the log of expectation I
can take the log and push it inside right and it turns out just logistically
working in this representation is much easier than working in this representation right if you go a little
bit through the math turns out that you can actually you know optimize this variation above but you can't really
optimize this particular likelihood objective it's a little bit surprising
for those of you who haven't seen variation of learning how it's done you know but this one little check this one
little so-called Jensen's inequality actually allows you to solve a lot of problems right and the other way to
write the lower bound is to say well there is a log likelihood function and something is called KL divergence which
is the distance between your approximating distribution Q which is your recognition model and the truth the
truth in these models would be the true posterior cording to your to your model and it's hard to optimize these kinds of
models in general you know you're trying to optimize your generative model you try to optimize the recognition model
and back in 88 back in 95 Hinton and his students basically they developed this
wake-sleep algorithm that was a bunch of different things put together but it was never quite the right algorithm because
it wasn't really optimizing anything it was just bunch of things alternating but
in 2014 there was a beautiful trick introduced by kingman Welling and there was a few other groups that came up with
the same trick called recanalization trick right so let me show you what repolarization trick does intuitively
so let's say your recognition distribution is a Gaussian right so a Gaussian I can write it as you know I
mean in the variance so this is the mean this is the variance notice that my mean depends on the layer below could be very
nonlinear function the variance also depends on the layer below so it could also be a nonlinear function but what I
can do is I can actually do the following I can express this particular Gaussian in terms of Xillia variables so
I can say well if I sample this epsilon from normal 0 1 a Gaussian distribution then I can write this particular H right
my-my-my state in a deterministic way it's just mean plus essentially standard
deviation or variance square root of the variance times this epsilon right so
this is just a simple parameterization of the gaussians right and just pulling out the mean and the variance there's no
sort of surprises here so I can write my recognition model as this Gaussian or I
can write it in terms of noise plus the deterministic part right so the recognition distribution can be
represented as a deterministic mapping and that's that's the beauty because it turns out that you can collapse these
complicated models effectively into auto-encoders right and we know how to deal with auto-encoders we can back
propagate through the entire through the entire model so we have a deterministic encoder and then the distribution of
these auxiliary variables really don't depend on parameters right so we sort of it's almost like taking a stochastic
system and separating the stochastic part and deterministic part in the terminus tic part you can do back
propagation so you can do learning and the stochastic Park you can do sampling right so just think of it as a separation between the two the two
pieces so now if I take the gradient of the variational bound or variational objective with respect to parameters
this is something that we couldn't do back in 95 and we couldn't do it in the last 10 years people tried using
reinforced algorithm or some approximations that never worked but here what we can do is we can do the
following we can say well I can write this expression because it's a Gaussian a sampling bunch of these axillary
variables and then this log I can just inject the noise in here the whole thing here
becomes deterministic and that's that's where the beauty comes in you take these gradient here and you push it inside the
expectation right so before if you take the gradient of expectation it's like
taking the gradient of averages like you compute bunch of averages in you're taking the gradient what you're doing
now with reprimands a ssin trick is you're taking the gradients and then taking the average right it turns out
that huge it reduces the variance in your training and it actually allows you to learn these models quite quite
efficiently so the mapping edge here is is completely deterministic and gradients here can be computed by back
propagation is a deterministic system and you can think of this thing inside it's just an autoencoder that that you
are that you are optimizing and obviously there are other extensions of these models that we've looked at and a
bunch of other teams looked at where you can say well maybe we can improve these models by drawing multiple samples these
are so-called K samples importance weighting bounce and so if you can make them a little bit better a little bit
more precise you can model a little bit more complicated distributions over the over the posterior but now let me sort
of step back a little bit and say why am I telling you about this what's the point there's a bunch of equations you injecting noise why do we need noise why
do we need stochastic scene systems in general right here's a motivating
example we wanted to build a model that given captions we want to generate the
image right and my student was very ambitious and basically said I want to be able to just tell give you any
sentence and and I want to be able to generate image like kind of like an artificial pain I want to paint what
what's in my what's in my caption in the most general way right so this is one
example of a Helmholtz machine where you have a generative model which is a stochastic recurrent network is just a change sequence of a variational
auto-encoders and there's a recognition model which is you can think of a deterministic system like a
convolutional system that tries to approximate what the latency states are but why do I need why do I need to the
caste city here why do I need very short encoders here and the reasons very simple suppose I
you know I give you the following tasks right I say a stop sign is flying in blue skies okay
now if you were using a deterministic system like an auto encoder you would generate one image right because it's a
deterministic system give an input I give you you know I'll give you output once you have stochastic system you
inject this noise this latent noise that allows it to actually generate a whole space of possible images right so for
example it tends to generate like this stop sign and this stop sign they look very different right and there's a car
here so maybe it's not really flying is just can't draw the pole here this one
looks like they're clouds here's this yellow school buses flying in those guys right so here we wanted to test the
system to see does it understand something about what's what's in the sentence here's a herd of elephants is
flying in blue skies now we cannot generate elephants although there are no techniques that are getting better but
you know sometimes it generates two of them right and the commercial plane flying in blue skies but this is where
we need stochastic because we want to be able to generate the whole distribution of possible outcomes not necessarily
just one particular point right here's you know we can basically do things like
you know a yellow school bus parked in the parking lot versus a red school bus parked in the parking lot versus a green school bus park in the
parking lots it's sort of the blue school bus rights it's sort of we can't quite generate blue school buses but
we've seen blue cars and we've seen blue buses so it can sort of make an association to to draw these different
things they look a little bit fuzzy but you know in terms of comparing two
different models if I give you a group of people on the beach with surfboards this is what we can generate there is
another model called webcam model which is a model based on that the cellular networks something out I'll talk last
part of this talk and there is these models convolutional deconvolution all the ocean water encoders which is again
convolutional deconvolution 'l auto-encoders just with with some noise and you can certainly see that you know
it's generally we found it's very hard to be able to generate scenes with arbitrary inputs as a text right here's
here's my favorite one a toilet it's it's open in the bathroom all right I don't know if you can see a
toilet sits here maybe but you can say toilets it's it's open in the grass field that was a little bit better at
least the colors were quite right and when we put this paper on archive one of
one of the students basically came to me and said well this is really bad because
you can always ask Google right and if you if you type that particular query into Google search it gives you that all
right which was a little bit disappointing but now if you actually
Google or if you actually put this query into Google this image comes before this
image and generally because what's happening is that people are just
clicking on that image all the time to figure out what's going on in that image so we got bumped up before before that
other means so so now I can say that according to Google this is a much better representation for that sentence
now than listen here's another here's another sort of interesting model which
is a model where you're trying to build a recurrent neural network again it's a
generative model but it's a generative model of text this was this model was trained on about 7,000 romance novels
and you take a caption model and you hook it up to the to the caption
generation system so you're basically saying the model here's an image generate me you know in the style of
romantic books what what you'd see here and you know it generates generates
something interesting we barely were able to catch the breeze on the beach and so forth she's beautiful but the
truth is I don't know what to do the Sun was just starting to fade away leaving
people scattered around the Atlantic Ocean so and there are a bunch of different things that you can do
obviously you know we're not there yet in terms of generating romantic stories but here's a one example where it's a
generative model it seems like syntactically we can actually generate you know reasonable things semantically
we're not there yet right and actually that particular work was inspired a little bit by by actually by by deuce
system that would give image it would I think of generate poems right but the points were predefined so
it was mostly the selecting the right point for the image here we actually were trying to you know generate
something something so there's still a lot of work to do in that space because you know syntactically we can get there
semantically we are nowhere near you know getting getting the right structure here is another last example that I want
to show you this was done in the case of one shot warning which is can you build
generative model of characters right that's a very defined domain very well
defined domain it's a very simple domain but it's also very hard right here's one example we've shown this example to
people and to the algorithm and we can say well can you draw me this this example and you know on one panel humans
would draw you know how they believe these this example should look like and then on the other panel we have machines
drawing it right this is it really just a generative model of based on a single
example showing you example and try to generate what it is and so question for you how many of you think this was a
machine generated and this was human generated ah what about the other way
around more MORE aha so there is a vote what this is what about this one this is
the how many do you think this machine generated is a human generated a few what about that aware it around ah great
great well the truth is I don't really know which one was genuine about this machine because that was done I should
actually ask Brandon Lake who designed the experiments for this particular model but I can tell you that I can tell
you that you know there's been a lot of studies he's done a lot of studies and it's about you know it's almost 50/50 so
in sort of this kind of small carve domain we can actually compete with people you know trying to generate these
these characters now let me step back a little bit and tell you about a
different class of models these are models known as generative adversarial
networks and they've been gaining a lot of attraction in in our community
because they seem to produce remarkable results so here's here's the idea we're not
going to be really defining explicitly the density but we need to be able to sample from the model right and the
interesting thing is that there's no variation learning there is no maximum likelihood estimation there is no Markov chain Monte Carlo there's no sampling
how do you do that how do you learn these models and it turns out that you can learn these models by playing a game
and that's a very clever strategy and the idea is the following you're going
to be setting up a game between two players you're going to have a discriminator do you think of it as a
convolutional network convolutional neural network and then you're going to have a generator gee maybe you can think
of it as as a variational tank or a Helmholtz machine or something that gives you samples from the data the
discriminated D is going to be discriminating between a sample from the data distribution and a sample from the
generator so the goal of the discriminate is to is to basically say is this a fake sample or is this a real
sample right fake sample the sample generated by the model real sample is
what you see in your data right can you tell the difference between the two right and the generator is going to be
trying to fool the discriminator by trying to generate samples that are hard
for discriminator to discriminate so my goal is a generator would be to generate really nice-looking digits so that the
discriminator won't be able to tell the difference between you know simulate it and the real right that's the key idea
and so here is intuitively what what
that looks like let's say you have you have some data so images of faces give
you an image your face and now I have a discriminated basically Tazewell if I get a real face I push it through some
function some differentiable function think of it as a convolutional neural network or another differentiable function and here I'm outputting one
right so I want to output one if it's a real sample right then you have a generator right and generator is you
have some noise so input noise think of it as a Gaussian distribution thinking about Helmholtz machines given some
noise I go through differentiable function which is your generator and January's sample this is my design my
sample might look like right and then on top of it I take this sample I put it
into my discriminator and I say for my discriminator I want to output 0 right
because my discriminator will have to say well this is fake and this is real
right that's the goal and the generator basically says well how can I get a sample such that my discriminate is
going to be confused such that the discriminator always outputs 1 here right because it believes it's a true
sample believe is it coming from the truth data right so now you have these
systems so what's what's the objective the objective is a min Max value
function it's a very intuitive objective function that has the following
structure you have a discriminator that says well this is an expectation with respect to distribution data
distribution so this is basically saying I want to classify any data points that I get from my data as being real right
so I want this output to be 1 because if it's 1 the whole thing is going to be 0
if it's less than 1 it's going to be negative and I really want to maximize it and then discriminator says well any
time I generate a sample whatever samples comes out from my generator you
know I want to classify it as being fake right that's the goal of the discriminator and then there is a
generator the generator is sort of the other you know you try to minimize this function which essentially says well
generate samples that discriminate it would classify as real so I really want to try to imagine you know change the
parameters of my generator such that this would produce 0 right so also so
the discriminant would produce 1 right so trying to full full the discriminator
right and it turns out the optimal strategy for discriminate is this this ratio which is probability of the data
divided probability of the data by plus probability of the model and in general if you succeed in building a good
generative model then probability of the data would be the same as probability of the models so discriminator will always be confused still 1/2
right and here's one particular it seems like a simple idea but it turns out that work remarkably well here's an
architecture called D convolutional generative adversarial network architecture that takes the code this is
a random code it's a Gaussian code it passes through a sequence of convolutions also a sequence of
deconvolution so given the code you sort of deconvolve it back to high dimensional image and you train it using
at the settle setting right this is your sampling you generate the image and then
there is a discriminator which is just a convolutional neural net what is trying to say is that the real is that a fake and if you train these models on
bedrooms these are called L some data sets a bunch of bedrooms this is how
samples from the model would look like which is pretty impressive in fact when I look at these samples I'm
also sort of maybe the models memorizing the data because these samples look
remarkably impressive then there was a follow-up work these are samples from
the CFR data set so here you seeing training samples and here you're seeing
samples generated from the model which is again very impressive if you look at
the structure in these samples it's quite remarkable that you can generate
you know samples that look very realistic actually this is what's done
again this was done by team team Shannon and his collaborators if you look at the
image net and you look at the training data on the image net and looking at the samples again you look live the horse
this is like there is some animal there is an airplane and so forth is like some
kind of a truck and such right so it looks you know when I look at these images and I was very impressed by the
quality of these image because Jo is very very hard to generate realistic looking images and the last thing I want
to point out this was picked up by Ian good fellow if we cherry pick some of
the examples this is what generated images look like alright so you can sort
of like see there is a little bit of interesting structure that you're seeing in these samples right and one question
still remains with these models is how can we evaluate these models properly
right is the model really learning a space of all possible images and how
images what's the coherency in those images or is the model mostly kind of like blurring things around and just
making some small changes to the data so the question that I would really want would like to answer to be you know to
get an answer to is that if I showing you a new example a new test image a new kind of a horse would the model say yes
this is this is a likely image this is this is very probable images I've seen you know similar images before or
something like that or not so that still remains an open question but again this is the class of models which steps away
from maximum likelihood estimations sort of sets it up in the game theoretic framework which is which is a really
nice set of work and in computer vision community a lot of people are shown a lot of progress and using these kinds of
models because they tend to generate much more realistic looking images so
let me just summarize to say that you know I've shown you hopefully a set of learning algorithms for deep
unsupervised models you know there's a lot of space in these models a lot of excitement in that space and just wanted
to point out that these models the deep models they improve upon current state of the art and a lot of different application domains and as I mentioned
before there's been a lot of progress in discriminative models convolutional models using recurrent neural networks
for solving you know action recognition models dealing with videos and unsupervised learning it still remains
sort of a field where we've made some progress but there's still a lot of
progress to be made and let me let me stop there so thank you
do the mics oh sorry so as a Bayesian
guy I'm pretty depressed by the fact that can can generate clearer image than the variational in total encoder so my
question is do you think there could be a energy based framework or probabilistic interpretation of why
again is so successful other than it's just a me max game I think that generally you know if you look at I sort
of go back and forth between variational - encoders because you know some of my friends at Open AI saying that they can
actually generate really nice-looking images using variational encoders I'm
looking at Peter here but you know what I think that one of the problems with
image generation today is that with variation autoencoders there is this notion of Gaussian loss function right
and what it does is it basically says well never produce you know crystal-clear images because if you're
wrong if you put at the edge in the wrong place you're going to be penalized a lot because of the l2 loss function
right what the gans are doing ganz are basically saying well I don't really
care where I put the edge as long as it looks realistic so that I can fool my classifier so what tends to happen in
practice and a lot of times if you actually look at the images generated by Ganz sometimes they have a lot of
artifacts like you know these specific things that that pop up right whereas in
variation think odors you don't see that but again the problem variational thinker is they tend to produce images that are much more diffused or you know
not as short or not as clear as what ganz is doing and there's been some work on you know trying to sharpen the images
which is using variational encoders to generate you the globally coherent scene and then you're using generative slls to
maybe sharpen it again it's it's it depends what loss function you're using
and again seem to be able to deal with that problem implicitly right because they don't really care whether
you get the edge quite right or not as long as its Falls your classifier thank
you hi thank you very much for the interesting talk I have a question about
the evaporation auto-encoder for the Marshall engine data set like the Street
View house number I noticed that many implementation they use a PCA to
pre-process the data before the trainer model why is your thought on that pea processing step why is necessary to do
that right why don't we just learn from the raw pixel I actually don't know my
experience has been that we don't really do a lot of pre-processing I mean what you can do is you can do CCA pre-processing is you can take the mean
you can take the sort of the second-order covariance structure from the data that sometimes helps sometimes
it doesn't but I don't see any particular reason why you'd want to do PCA Prasad pre-processing right I mean it's just one of just like you know
we've seen a lot in our field people just doing X Y and then later on they figure out that they don't really need x
and y right so it's maybe it was working better for their implementation for
their particular task but generally I haven't seen people doing a lot of pre-processing using PCA for training
you know variational - encoders
any more question yes there's one where
is regarding binary rbms so if you look at the literature for let us say
estimation of the partition function for easing models right you will see that the literature is lot more rich compared
to the variational inference literature for restricted Boltzmann machines especially in the binary context is
there like a cultural reason for this because specifically you know like you have for the strictly ferromagnetic case
you have a fully polynomial time approximation scheme for estimating the
log partition function right so but then I don't see usage of these F press kind
of algorithms in the RBM space so when you kind of juxtapose with the literature for the easing models
compared to binary rpms you'll find like a very stark asymmetry is there like a reason for this yeah so they think the
thing about easing models is that you know here like in ferromagnetic case or if you are you know you have certain
particular structure to the easing models you can use a lot of techniques like even if use techniques like coupling from the paths you can draw
exact samples from the models right you can compute the log partition function the polynomial convey a specific structure they think about rbms is that
generally those assumptions don't apply like you cannot learn them all which is this ferromagnetic model with RBA it's
just way all your weights are positive right that's a lot of constraints to put on on these class of models so that's
why you know and once you get outside of these assumptions then the problem
becomes np-hard right for estimating the partition function and obviously for learning these systems you need the
gradient of the log of the partition function right and that's where all the problems come in I don't think there is there is a solution for that and
unfortunately variational methods are also not working as well as you know
approximations like contrastive divergence or something based on based on sampling people have looked at better
approximations and using sort of more sophisticated techniques but it's it hasn't it hasn't really popped up yet
practically it just doesn't work as well but it's a good question no my question
is about using auto coder with a to get
semantic hash especially in text do we need to any
special representation text text representation like word to vector as
input for the for our text sequence so you know I've talked about them all
which is very simple knowledge is modeling bag of words yes right obviously you can take word to Veck and initialize the model right because it's
a way of just taking your words and projecting them into the semantic space yeah right there is been a lot of
technique recent techniques using like richard was mentioning gr use as a way you know if you want if you want to work
with sentences or if you want to embed the entire document into the semantic space if you want to make it binary you
know you can use gr use bi-directional gr use sort of get the representation of the document I think that would probably
work better than using word to back and then just adding things up and then based on that you can learn a hashing function that map's that particular
representation to the binary space right in which case you can you can do searching fairly efficiently so as an
input representation a lots of choices you can use bi-directional gr use which is the you know the the the the method
of choice right now you can use glove or you can use work the rack can be you
know some them sum up the representations of the words okay so using only back of word we use only
normal needs work neural network that is no records which were or yeah yeah that's right that's simple it's just right but again
you're doing your representation can be whatever that representation is as long as differentiable right okay so in this case you can you can sort of back
propagate through the bi-directional gr use and get rip you know learn what
there is what this game isn't a Shashank you so much okay let's thank Russ again

----------

-----

--11--

-----
Date: 2016.09.27
Link:  [# Deep Learning for Natural Language Processing (Richard Socher, Salesforce)](https://www.youtube.com/watch?v=oGk1v1jQITw)
Transcription:

thank you everybody thanks for coming back very soon after lunch I'll try to make it entertaining to avoid some post
food coma so I actually have a lot - OH - being here - Andrew and Chris and my
PhD at Stanford here it's it's really it's always fun to be back I figured
there's a going to be a broad range of capabilities in the room so I'm sorry I
will probably bore some of you for the first two-thirds of the talk because I'll go over the basics of what's NLP
when natural language processing what's deep learning and what's really at the intersection of the two and then the
last third I will talk a little bit about some exciting new research that's happening right now so let's get started
with what is natural language processing it's really a feel at the intersection of computer science AI and linguistics
and you could define a lot of goals and a lot of these statements here we could really talk and philosophize a lot about
but I'll move through them pretty quickly for me the goal of natural
language processing is for computers to process or scare quotes understand natural language in order to perform
tasks that are actually useful for people such as question answering the
caveat here is that really fully understanding and representing the meaning of language or even defining it
is quite an elusive goal so whenever I say the model understands I'm sorry I
shouldn't say that really these models don't understand in the sense that we understand language anything so whenever somebody says they
can read or represent the full meaning and its entire glory it's it's usually
not quite true really perfect language understanding is in some sense AI
complete in the sense that you need to understand all of visual inputs and thought and and a lot of other complex
things so a little more concretely as we try to tackle this overall problem of
understanding language what are sort of the different levels that we often look at it often and for many people starts
at speech and then once you have speech you might say alright now I know what phonemes are smaller parts of words I
understand words form Nets morphology or morphological analysis once I know what
the meaning of words are I might try to understand how they're put together in grammatical ways such that the sentences
are understandable or at least grammatically correct too a lot of speakers of the language once we go and
we understand the structure we actually want to get to the meaning and that's really where I think most of the interesting most of my interests lies
and semantic interpretation actually trying to get to the meaning in some useful capacity and then after that we
might say well if we understand now the meaning of the whole sentence what's how do we actually interact what's the discourse how do we have you
know spoken dialogue system and things like that where deep learning has really improved the state of the art
significantly is really in speech recognition and syntax and semantics and
the interesting thing is that we're kind of actually skipping some of these levels deep learning doesn't require
often morphological analysis to create very useful systems and in some cases
actually skips syntactic analysis entirely as well it doesn't have to know about the grammar it doesn't have to be
taught about what mound phrases are prepositional phrases it can actually get straight to some semantically useful
tasks right away and that's going to be one of the sort of advantages that we
don't have to actually be as inspired by linguistics as traditional natural language processing had to be so why is
NLP hard well there's a lot of complexity in representing and learning
and especially using linguistics situational world and visual knowledge really all of these are connected when
it gets to the meaning of language to really understand what red means can you do that without visual understanding for
instance if you have for instance this sentence here Jane hit June and then she
fell or and then she ran depending on which verb comes after she the
definition the meaning of she actually changes and this is one subtask you might look at so called an F or a
resolution or cor efference resolution in general where you try to understand who does she actually refer to and it
really depends on the meaning again somewhat scare quotes here of the verb that follows this pronoun
similarly there's a lot of ambiguity so here we have a very simple sentence for words
I made her duck now that simple sentence can actually have at least four
different meanings if you can think about it for a little bit right you made her a duck that she loves for Christmas
as for dinner you made her dock like me just now and so on there are actually four different
meanings and to know which one requires in some sense situational awareness or
knowledge to really disambiguate what what is meant here so that's sort of the
high level of NLP now where does it actually become useful in terms of applications well they actually range
from very simple things that we kind of assume or you're given now we use them all the time every day to more and more
complex and then also more in the realm of research the simple ones are things like spell checking or key word search
and finding synonyms and ophisaurus then the meaty medium sort of difficulty ones
are the extract information from websites trying to extract sort of product prices or dates and locations
people or company names are called named entity recognition you can go a little bit above that and try to classify sort
of reading levels for school text for instance or do sentiment analysis that can be helpful if you have a lot of
customer emails that come in and you want to prioritize highly the ones of customers for really really review right
now and then the really hard ones and I think in some sense the most interesting ones are machine translation trying to
actually be able to translate between all the different languages in the world question answering clearly something
that is a very exciting and useful piece of technology especially over very large
complex domains can be used to automated for automated email replies I know pretty much everybody here would love to
have some simple automated email reply system and then spoken dialogue systems
bots are very hip right now these are all sort of complex things that are still in the realm of research to do
them really well we're making huge progress with deep learning on these three but there's still nowhere near human
accuracy so let's look at the
representations I mention you know we have morphology and words and syntax and
semantics and so on we can look at one example a namely machine translation and
look at how did people try to solve this problem of machine translation well it
turns out they actually tried all these different levels with varying degrees of success you can try to have a direct
translation of words to other words the problem is that is often a very tricky mapping one the meaning of one word in
English might have three different words in German and vice versa you can have three of the same words in
English meaning all this single same word in German for instance so then people said well let's try to maybe do
some tactic transfer where we have whole phrases like to kick the bucket just means stab them in German okay not a fun
example and then semantic transfer might be well let's try to find a logical representation of the whole sentence the
actual meaning in some human understandable form and and try to just find another surface representation of
that now of course that will also get rid of a lot of the subtleties of language and so they're tricky problems
in all these kinds of representations now the question is what does deep learning do you've already saw at least
two methods standard neural networks before and convolutional neural networks
for vision and in some sense there's going to be a huge similarity here to
these methods because just like images that are essentially a long list of
numbers the vector and standard neural networks where the hidden state is also
just a vector or a list of numbers that is also going to be the main representation that we will use
throughout for characters for words for short phrases for sentences and in some
cases for entire documents they will all be vectors and with that we are sort of
finishing up the whirlwind of what's NLP of course you could give an entire lecture on all like almost every single
slide I just gave we're very a very high level but we'll continue at that speed to try to squeeze
this complex deep learning for NLP subject area into an hour and a half I think there are two most two of the most
important basic Lego blocks that you nowadays want to know in order to be able to sort of creatively play around
with more complex models and those are going to be word vectors and sequence
models namely recurrent neural networks and I kind of split this into words
sentences and multiple sentences but really you could use recurrent neural networks for shorter phrases as well as
multiple sentences but in many cases we'll see that they have some limitations as you move to longer and
longer sequences and just use the default neural network sequence models
alright so let's start with words and maybe one last blast from the past here
to represent the meaning of words we actually used to use a taxonomy like
word net that kind of defines each word in relationship to lots of other ones so
you can for instance define hyper names and is a relationships you might say the word Panda for instance in its first
meaning as a noun basically goes through this complex tags directed acyclic graph
most of it is roughly just a tree and in the end like everything it is an entity but it's actually a physical entity a
type of object it's a whole object it's a living thing it's an organism animal and so on so you basically can define a
word like this and another way at each node of this tree you actually have so called sunset so synonym sets here's an
example for the synonym set of the word good good can have a lot of different
meanings can actually be both an adjective and as well as an adverb as
well as a noun now what are the problems with this kind of discrete representation well they can be great as
a resource of your human you want to find synonyms but they're ever they're never going to be quite sufficient to
capture all the nuances that we have in language so for instance the synonyms
here for good were adapt Axford practice proficient and skillful but of course
you would use these words in slightly different contexts you would not use the
word expert in exactly this all the same context as you would use the meaning of good or the word good likewise it will
be missing a lot of new words language is this interesting living organism we change it all the time you might have
some kids they say Yolo and all of a sudden you know you need to update your dictionary likewise maybe in Silicon
Valley you might see ninja a lot and now you need to update your dictionary again and that is basically going to be a
Sisyphus job right nobody will ever be able to really capture all the meanings and and this living breathing organism
that languages so it's also very subjective some people might think ninja
should just be deleted from the dictionary and I don't want to include it I'll just think nifty or badass is
kind of a silly word and should not be included in a proper dictionary but it's being used in real language and so on it
requires human labor as soon as you change your domain you have to ask people to update it and it's also hard
to compute accurate word similarities some of these words are subtly different and it's really a continuum in which we
can measure their similarities so instead what we're going to use and what
is also the first step for deep learning will actually realize it's not quite deep learning in many cases but it is
sort of the first step to use deep learning and NLP is we will use distributional similarities so what does
that mean basically the idea is that we'll use the neighbors of a word to represent that word itself it's a pretty
old concept and here's an example for instance for the word banking we might actually represent banking in terms of
all these other words that are around it so let's do a very simple example where
we look at a window around each word and so here the window length that's just
for simplicity say it's one we represent each word only with the words one to left and one to the right of it we'll
just use the symmetric context around each word and here's a simple example
corpus so if the three sentences in my corpus of course we would always want to use
corpora with billions of words instead of just a couple but just to give you an idea of what's being captured in these
word vectors is I like people earning I like NLP and I enjoy flying and now this
is it's very simple so-called corcoran statistic you'll just simply see here I
for instance appears twice in its window size of one here the word like isn't its
window and its context and the word enjoy is once in its context and for
like you have twice to its left I and once deep and once NLP it turns out if
you just take those vectors now this could be a vector of presentation just each row could be a vector
representation for words unfortunately as soon as your vocabulary increases that vector dimensionality would change
and hence you have to retrain your whole model it's also very sparse and really
it's going to be somewhat noisy if you use that vector now another better thing
to do might be to run SVD or something simple like say dimensionality reduction on such a co-occurrence matrix and that
actually gives you a reasonable first approximation to word vectors very old method works reasonably well now what
works even better than simple PCA is actually a model introduced by Thomas McAuliffe in 2013 called word Tyvek so
instead of capturing Corcoran's counts directly out of a matrix like that you'll actually go through each window
in a large corpus and try to predict a word that's in the center of each window
and use that to predict the words around it that way you can very quickly train
you can train almost on line though few people do this and and add words to
vocabulary very quickly in this zooming fashion so now let's look a little bit
at this model where Tyvek because it's first very simple NLP model and to sort
of is very instructive we won't go into too many details but at least look at a couple of equations so again
main goal is to breeding words in a window of some length that we define em type or
parameter of every word now the objective function will essentially try to maximize here the log probability of
any of these contacts words given the Center word so we go through our entire corpus T very long sequence and at each
time step J we will basically look at all the words in the context of the
current word T and basically try to maximize here this probability of trying
to be able to predict that word that is around the current word T and theta are
all the parameters namely all the word vectors that we'd want to optimize so now how do we actually define this
probability P here the simplest way to do this and this is not the actual way
but it's the simplest and first to understand and derive this model is with
this very simple inner product here and that's why we can't quite call a deep there's not going to be many layers of
nonlinearities like we see in deep neural networks to be just a simple inner product and the higher debt in a
product is the more likely these two will be predicting one another so here
see the context is the dissenter word sorry oh is the outside word and
basically this inner product the larger it is the more likely we were going to predict this and these are both just
standard and dimensional vectors and now in order to get a real probability we'll
essentially apply softmax to all the potential inner products that you might have in your vocabulary and one thing
you will notice here is well this denominator is actually going to be a
very large sum I will want to sum here overall potential inner products for every single window that would be true
slow so now the real methods that we would use we're going to are going to
approximate the sum in a variety of clever ways now I could literally talk
to next hour and a half just about how to optimize the details of this equation but then we'll all deplete our mental
energy for the rest of the day and so I'm just going to point you to the class I taught earlier this year so yes 24d we
we have lots of different slides that go into all the details of this equation how to approximate it and then how to
optimize it it's going to be very similar to the way we optimize any other neural network we're going to use
stochastic gradient descent we're going to look at mini batches of a couple of hundred windows at a time and an update
those word vectors and we're just going to take simple gradients of each of these vectors as we go through windows
in a large corpus all right now we briefly mentioned PCA like methods and
based on senior Lu decomposition often or standard a simple PCA now we also had
this word Tyvek model there's actually one model that combines the best of both
worlds namely glove or global vectors introduced by Geoffrey Pennington in 2014 and it has a very similar idea and
you'll notice here there's some similarity you have this inner product again for different pairs but this model
will actually go over the Corcoran's matrix once you have this Corcoran's matrix it's much more efficient to try
to predict once how often two words appear next to each other rather than do it 50 times each time that that pair
appears in an actual corpus so in some sense you can be more efficiently going through all the current statistics and
you're going to basically try to minimize the this this subtraction here
and what that basically means is that each inner product will try to approximate the log probability of these
two words actually co-occurring now you have this function here which
essentially will allow us to not overly weight certain pairs that occur very
very frequently the for instance co-occurs with lots of different words and you want to basically lower the
importance of all the words that Corker with that so you can train this very
fast it scales to gigantic corpora in fact we train this on common crawl which
is a really great data set of most of the internet it's many billions of tokens and it gets also very good
performance on small corpora because it makes use very efficiently of these Corcoran
statistics and that's essentially what words well word vectors are always capturing so if in one sentence you just
want to remember every time you hear word vectors in deep learning one they're not quite deep even though we
call them sort of step one of deep learning and to it they're really just capturing Corcoran's counts how often
does a word appear in the context of other words so let's look at the some
interesting results of these glove vectors here the first thing we do is look at nearest neighbors so now that we
have these n dimensional vectors usually you say n between 50 to at most 500 good
general numbers 100 or 200 dimensional each of these each word is now represented as a single vector and so we
can look in this vector space for words that appear close by we started and
looked for the nearest neighbors of frog and well turned out
these are the nearest neighbors which was a little confusing since we're not biologists but fortunately when you
actually look up in Google what what those mean you'll see that they are actually all indeed different kinds of
frogs some appear very rarely in the corpus and others like toad or much more
frequent now one of the most exciting results that came out of word vectors
actually these word analogies so the idea here is can linearly can there be
relationships between different word vectors that simply fall out of very linear and simple addition and
subtraction so the idea here is what is meant a woman equal to king to something
else as in what is the right analogy when I try to basically fill in here the
last missing word now the way we're going to do this is very very simple cosine similarity or basically just take
let's take an example here the vector of woman we subtract the word vector we
learned of man and we add the word vector of king and the resulting vector I the art max for this
turns out to going to be Queen for a lot of these different models and that was
very surprising again we're capturing core current statistics so man might in
its context often have things like running and fighting other silly things
that men do and then you subtract those kinds of words from the context and you
add them again and in some sense it's intuitive though surprising that it works out that well for so many
different examples so here are some some other examples similar to the king and
queen example where we basically took these two hundred dimensional vectors and we projected them down to two
dimensions again with a very simple method like PCA and what we find is
actually quite interestingly even in just the two first principal components of this space we have some very
interesting sort of female male relationships so men to women is similar
to uncle and aunt brother and sister sir and madam and so on so this is an
interesting semantic relationship that falls out of essentially Corcoran's
counts in specific windows around each word and a large corpus here's another
one that's more of a syntactic relationship we actually have here superlatives like slow slower slowest is
in a similar vector relationship to short shorter and shortest or strong
stronger and strongest so this was very exciting and of course when you see an
interesting qualitative result you want to try to quantify who can do better in
trying to understand these analogies and what are the different modes and hyper parameters that modify the performance
now this is something that you will notice in pretty much every deep learning project ever which is more data
will give you better performance it's probably the single most useful thing you can do to machine learning or deep
learning system is to train it with more data and we found that too now they're different vector sizes too which is a
common hyper parameter like I said usually between 52 and so I wondered here we have 300
dimensional that essentially gave us the best performance for these different
kinds of semantics and tactic relationships now in many ways having a
single vector for words can be oversimplifying right some words have multiple meanings maybe they should have
multiple vectors sometimes the word meaning changes overtime and so on so
there's a lot of simplifying assumptions here but again our final goal for deep NLP is going to be to create useful
systems and it turns out this is a useful first step to create such systems
that mimic some human language behavior in order to create useful applications
for us all right but words word vectors are very useful but words of course never appear in isolation and what we
really want to do is understand words in their context and so this leads us to the second section here on recurrent
neural networks so we already went over the basic definition of standard neural
networks really the main difference between a standard neural network and a recurrent neural network which I'll
abbreviate as RN and now is that we will tie the weights at each time step and that will allow us to essentially
condition the neural network on all the previous words in theory and practice how we can optimize it it won't be
really all the previous words we've more like at most the last 30 words but in
theory this is what a powerful model can do so let's look at the definition of a
recurrent neural network and this is going to be a very important definition so we'll go into a little bit of details here so let's assume for now we have our
word vectors as given and we'll represent each sequence in the beginning it's just a list of these word vectors
now what we're going to do is we're computing a hidden state HT at each time
step and the way we're going to do this is with a simple neural network architecture in fact you can think of
this summation here is really just a single layer neural network if you were
to concatenate the two matrices in these two that but intuitively we basically will map
our current word vector at that time step T sometimes I use these square
brackets to denote that we're taking the word vector from that time step in there
we map that with a linear layer a simple matrix vector product and we sum up some
that matrix vector product to another matrix vector product of the previous hidden state at the previous time step
we sum those two and reapply in one case a simple sigmoid function to define this
standard neural network layer that will be HT and now at each time step we want
to predict some kind of class probability over a set of potential
events classes words and so on and we use the standard softmax classifier some other communities called logistic
regression classifier so here we have a
simple matrix WS for the softmax weights
we have basically a number of rows are going to be a number of classes that we have and the number of columns is the
same as the hidden dimension sometimes
we want to predict the next word in a sequence in order to be able to identify
the most likely sequence so for instance if I asked for a speech recognition system what is the price of wood now in
isolation if you hear wood you would probably assume it's the wo uld
auxiliary verb wood but in this particular context the price of it wouldn't make sense to have a verb
following that and so it's more like the wo D to find the price of wood so
language modeling is very useful task and it's also very instructive to use as an example for where recurrent neural
networks refine so in our case here this softmax is going to be quite a large
matrix that goes over the entire vocabulary of all the possible words that we have so each word is going to be
our class the classes for language models are the words in our vocabulary and so we can define here
this y hat T the jf1 is basically denoting here the probability that the J
word at the J index will come next after all the previous words very useful model
again for speech recognition for machine translation for just finding a prior for
language in general alright again main difference the standard
neural networks we just have the same set of W weights at all the different time steps everything else is pretty
much a standard neural network we often initialize the first h0 here just either
randomly or all zeroes and again in language modeling in particular the next
word is our class of the softmax now we can measure basically the performance of
language models with terms are called perplexity which really is here the
average log likelihood of the basically the probabilities of being able to
predict the next word so you want to really give the highest probability to the word that actually will appear next
in a long sequence and then the higher that probability is the lower your
perplexity in hence the models less perplexed to see the next word in some sense you can think of language modeling
as almost NLP complete and some silly sense that you just if you can actually
predict every single word that follows after any arbitrary sequence of words in a perfect way you would have
disambiguated a lot of things you can you can say for instance what is the answer to the following question ask the
question and then the next couple of words would be the predicted answer so there's no way we can actually ever do
perfect job in language modeling but there's certain contexts where we can give a very high probability to the
right next couple of words now this is the standard recurrent neural network
and one problem with this is that we will modify the hidden state here at every time set so even if I have words
like the and a and sentence period and things like that it will stick
frequently modify in my hidden state now that can be problematic let's say for
instance I want to train a sentiment analysis algorithm and I talk about
movies and I talk about the plot for a very long time then I say oh man this movie was really wonderful it's great to
watch and then especially the ending and you talk again for like fifty timesteps or 50 words or hundred words about the
plot now all these plot words will essentially modify my hidden states if at the end of that whole sequence I want
to classify the sentiment the word wonderful and great that I mentioned somewhere in the middle might be completely gone because I keep updating
my hidden state with all these content words to talk about the plot now the way
to improve this is by use better kinds of recurrent units and I'll introduce
here a particular kind so called gated recurrent units introduced by Cho in
some sense and we'll learn more about the LS TM tomorrow when Kwok gives his
lecture but G R user in some sense a special case of LS DMS and the main idea
is that we want to have the ability to keep certain memories around without having the current input modify modify
them at all so again this example of sentiment analysis I say something's great that should somehow be captured in
my hidden state and I don't want all the content words to talk about the plot in a movie review to modify that is
actually overall I was a great movie and then we also want to allow error messages to flow at different strengths
depending on the input so if I say great I want that to modify a lot of things in
the past so let's define a giryu fortunately since you already know the
basic Lego block of a standard neural network there's only really one or two subtleties here that are different there
are a couple of different steps that we'll need to compute at every time step so in the standard RNN
what we did was just have this one single neural network that we hope would capture all this complexity of the
sequence instead now we'll first compute a couple of gates at that time step so
the first thing will compute is the so called update gate it's just yet another neural network
layer based on the current input word vector and again the past hidden state so these look quite familiar but this
will just be an intermediate value and we'll call it the update gate then we'll also compute a reset gate is yet another
standard neural network layer again just matrix vector product summation matrix vector product some kind of
non-linearity here namely Sigma it's actually important in this case that it is a sigmoid just just basically both of
these will be vectors with numbers that are between 0 and 1 now we'll compute a
new memory content an intermediate age tilt here with yet another neural
network but then we have this little funky symbol in here basically this will
be an element-wise multiplication so basically what this will allow us to do
is if that reset gate is 0 we can essentially ignore all the previous
memory elements and only store the new word information so for instance if I
talked for a long time about the plot now I say this was an awesome movie now
you want to basically be able to ignore if your whole goal of this sequence classification model is to capture
sentiment I'm going to be able to ignore past content this is of course if this
was a 0 entirely a 0 vector now this will be more subtle this is a long vector if you know maybe a hundred or
200 dimensions so maybe some dimensions should be reset but others maybe not and then here we'll have our finally
final memory and that essentially combines these two states the previous
hidden state and this intermediate one at our current time step and what this will allow us to do is essentially also
say well maybe we want to ignore everything that's currently happening and only update the last time step we
basically copy over the previous time step in the hidden state of that and ignore the current thing again simple
example in sentiment maybe there's a lot of talk about the plot when a movie was released if you want to basically have
the ability to ignore that and just copy that in the beginning may have said it was an awesome movie so
here's an attempt at a clean illustration I have to say personally I in the end find the equations a little
more intuitive than the visualizations that we try to do but some people are are more visual here so this is in some
ways basically here we have our word vector and it goes through different layers and then some of these layers
will essentially modify other outputs of previous time steps so this is a pretty
nifty model and it's read the second most important basic Lego block that
we're going to learn about today and so just want to make sure we take a little
bit of time I'll repeat this here again if the reset gate this R value is close
to zero those kinds of hidden dimensions are basically allowed to be dropped and
if the update gates Z basically is one then we can copy information in of that
unit through many many different time steps and if you think about optimization a lot what this will also
mean is that the gradient can flow through the recurrent wheel network through multiple time steps until it
actually matters and you want to update a specific word for instance and go all the way through many different time
steps so then what this also allows us is to
actually have some units that have different update frequencies some you
might want to reset every other word other ones you might really cap like they have some long-term context and
they stay around for much longer all right this is the geo you it's the
second most important building block for today there are like I said a lot of other variants of recurrent neural
networks lots of amazing work in that space right now and tomorrow quoc will
we'll talk a lot about some more advanced methods so now that you've
understand word vectors and neural network sequence models you really have
the two most important concepts for deep NLP and that's pretty awesome so congrats we
can now in some ways really play around with those two Lego blocks plus some slight modifications of them very
creatively and build a lot of really cool models a lot of the models that I'll show you and that you can read and
see and read the latest papers that are now coming out almost every week on archive will have some kind of component
of these will use really these two components in a major way now this is
one of the few slides now with something really new because I want to keep it
exciting for the people who already knew all this stuff and took the class and everything this is tackling a important problem
which is and all these models that you'll see in pretty much most of these
papers we have in the end one final softmax here right and that softmax is
basically our default way of classifying what we can see next what kinds of classes we can predict the problem with
that is of course that that will only ever predict accurately frequently seen classes that we had at training time but
in the case of language modeling for instance where our classes are the words we may see a test time some completely
new words maybe I'm just going to introduce to you a new name srini for
instance and nobody may have like seen that word at training time but now that
I mentioned him and I will introduce him to you you should be able to predict the word trini and that person in a new
context and so the solution that we're literally going to release only next week and in a new paper is to essentially
combine the standard softmax that we can train with a pointer component and that pointer component will allow us to point
to previous contexts and then predict based on that to see that word so let's
for instance take the example you have language modeling again we may read a long article about the Fed chair Janet
Yellen and maybe the word Yellen had not appeared in training time before so we
couldn't ever predict it even though we just learned about it and now a couple of sentences later interest rates were
based and then missus and now we want to predict that next word now if that
hadn't appeared in our softmax standard training procedure at training time we would never be able to predict it what
this model will do and we're kind of calling it a pointer sentinel mixture model is it will essentially first try
to see what any of these previous words maybe be the right candidate so we can
really take into consideration the previous context of say the last hundred words and if we see that word and that
word makes sense after you know we train it of course then we might give a lot of probability mass to just that word at
this current position in our previous immediate context at test time and then
we have also the sentinel which is basically going to be the rest of the probability if we cannot refer to the
some of the words that we just saw and that one will go directly to our
standard softmax and then what we'll essentially have is a mixture model that allows us to say either we have or we
have a combination of both of essentially words that just appeared in this context and words that we saw in
our standard softmax language modeling system so I think this is a pretty
important next step because it will allow us to predict things we've never seen a training time and that's
something that's clearly a human capability that most or pretty much none of these language models had before and
so to look at how much it actually helps it'll be interesting to look at some of
the performance before so again what we're measuring here is perplexity and the lower the better because it's
essentially inverse here of the actual probability that we assigned to the
correct next word and in just 2010 so six years ago there this was some great
work early work by Thomas McAuliffe where he compared to a lot of standard
natural language processing methods syntactic neural net syntactic models
that essentially tried to predict the next word and had a perplexity of 107 and he was able to use the standard
recurrent neural networks and actually an ensemble of eight of them to really significantly push down the
perplexity especially when you combine it with standard count based methods for
language modeling so in 2010 he made great progress by pushing it down to 87
and now this is one of the great examples of how much progress is being
made in the field thanks to deep learning we're two years ago white
chicks are memba and and his collaborators were able to push that down even further to 78 with a very
large lsdm similar to a GRU like model but even more advanced quark will will
teach you the basics of LS CMS tomorrow then last year we pushed the the
performance was pushed down even further by yarn gull and then this one actually
came out just a couple of weeks ago variational recurrent highway networks pushed it down even further but this
pointer sentiment model is able to get it down to 70 so in just a short amount of time we pushed it down by more than
10 perplexity points and in two years and that is really an increased speed in
performance that we're seeing now that deep learning so if changing a lot of areas of natural language processing
alright now we have sort of our basic Lego blocks the word vectors and the GRU
sequence models and now we can talk a little bit about some of the ongoing
research that we're working on and I'll start that with maybe a controversial
question which is could we possibly reduce all NLP tasks to essentially
question answering tasks over some kind of input and in some ways that's a trivial observation that you could do
that but it actually might help us to think of models that could take any kind
of input a question about that input and try to produce an output sequence so let
me give you a couple of examples of what I mean by this so here we have the first
one is a task that we would standardly associate with answering I'll give you a couple of
facts Mary walk to the bathroom send her went to the garden Daniel went back to the garden Sandra took the milk
there where's the milk and now you might have to logically reason so I try to
find the sentence about milk maybe Sandra took the milk there and I
would have to maybe do an F for a resolution find out what does there refer to and then you try to find you
know the previous sentence that mentioned Sandra see that it's garden and then give an answer garden so this
is a simple logical reasoning question answering task and that's what most people in the QA field sort of
associated with some kinds of question answers but we can also say everybody's
happy and the question is what's the sentiment and the answer is positive all right so this is a different subfield of
NLP that tackles sentiment analysis we can go further and ask what are the
named entities of a sentence like Jane has a baby in Dresden and you want to find out that Jane is a person in
Dresden as a location this is an example of sequence tagging you can even go as
far and say you know I think the smile is incredible and the question is what's the translation into French and you get
you know Japan's kusuma del a on clay habla and dad in some ways would be
phenomenal if we're able to actually tackle all these different kinds of
tasks with the same kind of model so maybe it would be an interesting new
goal for NLP to try to develop a single joint model for general question
answering I think it would push us to think about new kinds of sequence models
and new kinds of reasoning capabilities in an interesting way now there are two major obstacles to actually achieving
the single joint model for arbitrary QA tests the first one is that we don't
even have a single model architecture that gets consistent state-of-the-art results across a variety of different
tasks so for instance for question answering and this is a data set called Bobby did face book published
last year strongly supervised memory networks get the state of the art for sentiment analysis you had tree lsdm
models developed by cashing ty here at Stanford last year and for part of
speech tagging you might have bi-directional lsdm conditional random fields one thing you do notice is all
the current state-of-the-art methods are deep learning sometimes they still connect to other traditional methods
like conditional random fields and undirected graphical models but there's always some some kind of deep learning
component in them so that is the first obstacle the second one is that really
fully joint multitask learning is very very hard usually when we do do it we
restrict it to lower layers so for instance in natural language processing all we're currently able to share in
some principled way our word vectors we take the same word vectors we trained for instance with glove or work avec and
we initialize our deep neural network sequence models with those word vectors
in computer vision and we're actually a little further ahead and you're able to
use multiple of the different layers and you initialize a lot of your CNN models
with first pre trained CNN that was pre trained on imagenet for instance now usually people evaluate
multitask learning with only two tasks they trained on for a first task and then they evaluate the model that they
initialize from the first on the second task but they often ignore how much the performance degrades on the original
task so when somebody takes an image net CNN and applies it to a new problem they rarely ever go back and say how much did
my accuracy actually decrease on the original data set and furthermore we
usually only look at tasks that are actually related and then we find out look there's some amazing transfer learning capability going on what we
don't look a look at often in the literature and in most people's work is that when the tasks aren't related to
one another they actually hurt each other and this is a so called catastrophic forgetting it's not there's
not too much work that right now now I also would like to
say that right now almost nobody uses the exact same decoder or classifier for
a variety of different kinds of outputs right we at least replace the softmax to
try to predict different kinds of problems all right so this is the second
obstacle now for now we'll only tackle the first obstacle and this is basically
what motivated us to come up with dynamic memory networks they are essentially an architecture to try to
tackle arbitrary question-answering paths when I'll talk about dynamic
memory networks is important to note here that for each of the different tasks I'll talk about it'll be a different dynamic memory network it
won't have the exact same weights will just be the same general architecture so
the high-level idea for DM ends is as follows imagine you had to read a bunch
of facts like these here they're all very simple in and of themselves but if
I now ask you a question I showed you these and I asked where Sandra you know
it'd be very hard even if you read them all of them and be kind of hard to remember and so the idea here is that
for complex questions we might actually want to allow you to have multiple glances at just at the input and just
like I promised our one of our most important basic Lego blocks will be this GRU we just introduced in the previous
section now here's this whole model in all its gory details and we'll dive into
all of that in the next couple of slides so don't worry it's it's a big model a
couple of observations so the first one is I think we're moving in deep learning now to try to use more proper software
engineering principles basically to modularize encapsulate certain
capabilities and then take those as basic Lego blocks and build more complex models on top of them a lot of times
nowadays you just have a CNN that's like one little block in a complex paper and then other things happen on top here
we'll have the gru or word vectors basically has you know one module a sub module in these
different ones here and I'm not even mentioning word vectors anymore but word vectors still play a crucial role and
each of these words is essentially represented as this word vector but we just kind of assume that it's there
okay so let's walk on a very high level through this model they're essentially four different modules there's the input
module which will be a neural network sequence model and giryu and there's a
question module an episodic memory module and an answering module and sometimes we also have these semantic
memory modules here but for now these are Ray just our word vectors and we'll ignore that for now so let's go through
this here is our corpus and our question is where is the football and this is our
input that should allow us to answer this question now if I ask this question I will essentially use the final
representation of this question to learn to pay attention to the right kinds of inputs that seem relevant for given what
I know to answer this question so whereas the football well it would make sense to basically pay attention to all
the sentences that mention football and maybe especially the last ones if the football moves around a lot so what we'll observe here is that this
last sentence will get a lot of attention so John put down the football and now what we'll basically do is that
this hidden state of this recurrent neural network model will be given as
input to another recurrent neural network because it seemed relevant to answer this current question at hand now
we'll basically agglomerate all these different facts that seem relevant at the time and is now the gru in this
final vector m and now this vector M together with the question will be used to go over the inputs again if the model
deems that doesn't have enough information yet to answer the question so if I ask you where's the football and
it's so far only found that John put down the football you don't know enough you still don't know where it is but you
now have a new fact namely John seems relevant to answer the question and that fact is now represented in this vector M
which is also just the last in the state of another Network now we'll go over the inputs
again now that we know that John and the football irrelevant will be learned to pay attention to John move to the
bedroom and John went to the hallway again those are going to get
agglomerated here in this recurrent neural network and now the model seems
thinks that it actually knows enough because it basically intrinsically
captured things about the football John found a location and so on of course we
didn't have to tell it anybody anything about their people their locations if X moves to Y and y is in the set of
locations then this happens none of that you just give it a lot of stories like that and in its hidden states it will capture these kinds of
patterns so then we have the final vector M and we'll give that to an
answer module which produces in our standard softmax way the answer all
right now let's zoom into the different modules of this overall dynamic memory
network architecture the input fortunately is just a standard GRU the way we defined it before so simple word
vectors hidden states reset gates update gates and so on the question module is
also just the GRU a separate one with its own weights and the final vector q
here is just going to be the last hidden state of that recurrent neural networks you can't model now the interesting
stuff happens in the episodic memory module which is essentially a sort of
meta gated GRU where this gate will
basically define is defined computed by the attention mechanism and will
basically say this current state sentence si here seems to matter and the
superscript T is the episode that we have so each episode basically means we're going over the input entirely one
time so it starts at g1 here and what
this basically will allow us to do is to say well if G is
zero then what we'll do is basically just copy over the past states from the
input nothing will happen and unlike before in all these GRU equations this G is just a single scalar number it will
basically say if G is zero then this sentence is completely irrelevant to my
current question at hand I can completely skip it all right and there are lots of examples like mary mary traveled to the hallway
that are just completely irrelevant to answering the current question in those cases this g will be zero and we're just
copying the previous hidden state of this recurrent neural network over otherwise we'll have a standard giryu
model so now of course the big question is how do we compute this G and this
might look a little ugly but it's quite simple basically we're going to compute two vector similarities multiplicative
an edit of one with absolute values of all the single values of the sentence
vector that we currently have and the question vector and the first the memory state of the previous pass of the input
and the first pass over the input the memory state is initialized to be just a
question and then afterwards at agglomerated relevant facts so intuitively here if the sentence
mentions John for instance and the question is or mentions football and the question is where is the football
then you'd hope that the question vector Q mentions has some units that are more active because football was mentioned
and the sentence vector mentions football so there's some units that are more active because football is mentioned and hence some of these inner
products or absolute values of subtractions are going to be large and then what we're going to do is just plug
that into a standard through standard single layer neural network and in a standard linear layer here and then we
apply a soft max to essentially weight all of these different potential sentences that we might have to compute
the final gate so this will basically a soft attention mechanism that sums to one and we'll pay most attention to the
facts that seem most relevant given what I no so far and the question then when the
end of the input has reached all these relevant facts here are summarized in another GRU that basically moves up here
and you can train a classifier also if you have the right kind of supervision
to basically train that the model knows enough to actually answer the question and stop iterating over the inputs if
you don't have that kind of supervision you can also just say I will go over the inputs a fixed number of times and that
that works reasonably well to all right there's a lot to sink in so I'll give
you a couple seconds basically we pay attention to different facts given a
certain question we iterate over the input multiple times and we agglomerate
the facts that seem relevant given the current knowledge and the question now I
don't usually talk about neuroscience I'm not a neuroscientist but there is a very interesting relationship here that
a friend of mine Sam Gershman pointed out which is that the episodic memory in general for humans is actually the
memory of autobiographical events so it's the time when we remember the first time I went to school or something like
that and essentially a collection of our past personal experiences that occurred at a particular time in a particular
place and just like our episodic memory that can be triggered with a variety of
different inputs this is also this episodic memory is also triggered with a
specific question at hand and what's also interesting is the hippocampus which is a seat of the episodic memory
in humans is actually active during transitive inference so transitive inference is you know going from A to B
to C to have some connection from A to C or in this case here with this football
for instance you first had to find facts about John into football and then finding where John was and then find the
location of John so those are examples of transitive inference and it turns out that you also need in the dmn these
multiple passes to enable the capability to do transitive inference now the final
module again is very simple G or UN softmax to produce the final answers the main difference here is that
instead of just having the current the previous hidden state 18 minus 1 as
input will also include the question at every time and we will include the
answer that was generated at the previous time step but rather than that it's our standard softmax from your
standard cross-entropy errors to minimize it and now beautiful thing of this whole model is that it's end-to-end
trainable these four different modules will actually all train based on the
cross entropy of that final softmax all these different modules communicate with vectors and we'll just have Delta
messages and back propagation to train them now there's been a lot of work in
the last two years on models like this in fact quoc will cover a lot of these
really interesting models tomorrow different types of memory structures and so on and the dynamic memory network is
in some sense one of those models one one particular model is a proper
comparison because it's there a lot of similarities namely memory networks from
jason weston those basically also have inputs and scoring and attention
response mechanisms the main difference is that they use different kinds of
basic Lego blocks for these different kinds of mechanisms for input they use
bag of words representation z' or non-linear on linear embeddings for the
attention and responses they have different kinds of iteratively to run functions the main interesting sort of
difference to the dmn is that the dmn really use this recurrent neural network
type sequence models for all of these different modules and capabilities and in some sense that helps us to have a
broader range of applications that include things like sequence tagging and so let me go over a couple of results
and experiments of this model so the first one is on this Bobbie dataset did
Facebook publish it basically has a lot
of these kinds of simple logical reasoning type questions in fact all these like where's the
Paul those were examples from the Facebook Bobby data set and it also includes things like yes/no questions
simple counting negation some indefinite knowledge where the answer might be may
be basic coreference where you have to realize what does she
who does she refer to or he reasoning over time if this happened before that
and so on and basically this dynamic memory network I think is currently the
state of the art on this data set of the simple simple logical reasoning now the
problem with this data set is that it's a synthetic data set and so it had only
a certain set of generating like human general human defined generative
functions that created certain patterns and in that sense it's only necessary
and not a sufficient condition of solving it with sometimes a hundred percent accuracy to real question
answering so there's still a lot of complexity the main interesting bit to
point out here is that there are different numbers of training examples for each of these different subtasks and
so you have basically a thousand examples of simple negation for instance and it's always a similar kind of
pattern and hence you're able to classify it very well now real language you will never have that many examples
for each type of pattern you want to learn and so it's still general question answering is still an open problem and
non-trivial now what's cool is this same architecture of allowing the model to go
over inputs multiple times also got state of the art and sentiment analysis very different kind of task and we
actually analyzed whether it's really helpful to have multiple passes over the input and it turns out it is so there's
certain things like reasoning over three facts or Counting where you really have to have this dynamic this episodic
memory module and it goes over the input maybe five times for sentiment it
actually turns out it hurts after going over the input more than two times and
that's actually one of the things we're now working on is can we find model that does the same thing for every
single input with the same weights to try to learn this different tasks we can
actually look at a couple of fun examples of this model and what happens
with tough sentiment sentences generally to be honest sentiment you can probably
get to like seventy five percent accuracy with some very simple models that just basically find like great
words like great and wonderful and awesome and you'll get to something that's roughly right here some of the
examples that those are the kinds of examples that you now need to get right to retry to push the state-of-the-art
further in sentiment analysis so here the sentences in its ragged cheap and
unassuming way the movie works so this sentence is incorrect even if you allow
the dmn but I have this whole architecture but only allow one pass over the input once you have two passes
over the input it actually learns to pay attention not just to these very strong
adjectives but in the end actually to the movie working so here these fields
are essentially the gating function G that we defined that pays attention to specific words and the darker it is the
larger that gate is and the more open it is amor that word effects the hidden
state in the episodic memory module so it goes over the input the first time
pays attention to cheap and unassuming and way and a little bit of works too
but the second time it basically figured out it agglomerate it's sort of the facts of that sentence and then learn to
pay attention more to specific words that seem more important just one more
example here my response to the film is best described as lukewarm so in general
sentiment analysis when you look at unique an scores like the word best is
basically some of the most one of the most positive words you could possibly use in a sentence and the first time the
model passes over the sentence that also pays most attention took this incredibly positive word maybe
best but then this site once it agglomerate at the context actually realizes well best actually here is not
used in its adjective way but it's actually an adverb that best describes
something and what it describes is actually lukewarm and hence it's actually a negative sentence so those
are the kinds of examples that you need to get to now to appreciate improvements in sentiment analysis where we basically
also went from on this particular data set these are all neural network type
models that started 82 until then that same data set existed for around 8 years
and none of the standard NLP models had reached above 80% accuracy and now we're
basically in the high high 80s and and those are the kinds of improvements that that you see across a variety of
different NLP tasks now that deep learning has come and deep learning
techniques are being used in NLP and now the last task in NLP that this model
turn out are also working for Ivy Wallen as part of speech tagging now part of speech tagging is less exciting of a task it's more of an intermediate task
but it's still fascinating to see that after this data set has been around for
over 20 years you can still improve the state of the art was the same kind of architecture
that also did well and fuzzy reasoning of sentiment and discrete logical reasoning for for question answering now
we had a new person joined a group Zhiming and he he thought well that's
cool but he was more of a computer vision researcher and so he thought well could
I use this create question-answering module now to do visual question-answering so combine sort of
some stat was going on in the group and NLP and apply it to a computer vision and he did not have to know all of the
different aspects of the code all he had to do was change the input module from
one that gives you hidden states at each word over a long sequence of you know
words and sentences to an input module that would give him vector years four sequences of regions in an
image and he literally did not touch some of the other parts of the code I
did have to look carefully at this input module aware again here our basic Lego
block that Andre introduced really well of our convolutional neural network and
then each the convolutional networks will essentially give us 14 by 14 many
vectors one for each and it's one of its top states one representing each region
of an image and then what we'll do is basically take those vectors and now replace the word vectors we used to have
with CNN vectors and then plug them into GRU now again the GRU we know as our
basic Lego block we already defined it one addition here is that it'll actually be a bi-directional GRU will go once
from left to right in this snake-like fashion and another one goes from right
to left backwards now both of these will basically have hidden state and you can
just concatenate the hidden states of both of these to compute the final hidden state at each for each block of
the image and that model to actually achieve state-of-the-art results this
data set has been only released last year so everybody now works on deep learning techniques to try to solve it
and I was at first a little skeptical it was just too good to be true that this
model we developed for NLP would work so well so we really dug in to looking at
the attention so what I showed you here these G values again that we computed
with this equation now instead of paying attention to words it paid attention to
different regions in the image and we started basically analyzing going
through a bunch of those on the Deaf set and analyzing what is it actually paying attention to again it's being trained
only with the image the question and the final answer that's what you get a
training time you do not get this sort of latent representation of where you
should actually pay it attention to in the image in order to answer that question correctly so when
the question was what is the main color on the bus and learned to actually pay attention here to that bus mic well okay
maybe that's not that impressive it's just the main object in the center of the image and you know what it types the
type of trees are in the background well maybe it just you know connects tree with anything that's green and pays
attention to that so I was neat but you know not not super impressive yet so is
this in the wild kind of more interesting and actually pays attention to a man-made structure in the background and correctly answer's no
then this one is kind of interesting who is on both photos the answers girl now
to be honest I don't think the model actually knows that there are two people tries to match them and so on it just
finds the main person or main object in in this in the scene the main object is
a little baby girl so it says girl this one's also relatively trivial what time
of day was this picture taken the answers night because it's very dark picture at least in the sky now this one
is getting a little more interesting what is the boy holding the answer a surfboard and it actually does pay
attention to both of the arms and then what's just below that arm so that's a little more interesting kind of
attention visualization and then for a while we're also worried well what if in
the data set it just learns really well from language alone yes it pays attention to things but maybe it'll just
say things that it often sees in the text so if I asked you what or what color are the bananas you don't really
have to look at an image in 95% of the cases you're right just saying yellow without seeing an image so it was really
this one I was kind of excited about because it actually paid attention to the bananas in the middle and then did
say green and kind of overruled the prior that it would get from from
language alone what's the pattern on the cat's fur on its tail pays attention
mostly to the tail and says stripes now this one here was interesting and fit
the player hit the ball the answer yes though I have to say that we later had a journalist want to do his own
question he he asked John marker from New York Times and we just put together
this demo and the night before and he's like well I want to ask my own question and I am like okay and he asked is the
girl wearing a hat and you know it wasn't made for production so it's kind of slow and the system was cranking it
like well you know like trying to come up with excuses it's kind of black background and the plaque hat and it
might be kind of hard to see and unfortunately I got it right and said yes and then after the interview I said
well maybe let's look and see if like what I imma just asked it myself less
stressful situation a bunch of questions on my own and these are all the questions like the first eight questions
that I could come up with and somewhat to my surprise it actually got them all right so what is the girl holding a
tennis racket what's she playing playing tennis or what's she doing I was to go wearing shorts what is the color of the
ground brown then I was like well okay let's try to break it by asking just like what's the color of like the sound
of this the smallest object the ball actually got that right to because her skirt white also kind of interesting
like when you asked him all what she's wearing shorts but in you asked about the skirt and it still sort of is you
know sort of capturing that you might call this different things what and then this one was interesting
what did the girl just hit tennis ball and then as like well what if I asked is
the girl about to hit the tennis ball and said yes and then did the girl just hit the tennis ball and it said yes
again so then I finally found a way to break it so it doesn't have enough the Corcoran statistics to understand and
again spare quote understand sort of which angles does the arm have to be in order to assume that the ball was just
adores about it but what it basically does show us is that once it saw a lot
of examples on a specific domain it really can capture quite a lot of different things now see if we can get
the demo up I have to be a VPN to make it work but so here's here's one
one example the best way to hope for any chance of enjoying this film is by lowering your expectations again one of
those kinds of sentences that you have to now get correct in order to get
improved performance on sentiment and actually correctly says that this is
this is negative now we can also actually ask that question in Chinese
this is one of the beautiful things off of the dmn and in general really of most
deep learning techniques we don't have to be experts in a domain or even in a language to create a very very accurate
model for for that language or that domain there's no more future
engineering I'm not going to make a fool of myself trying to read that one out loud but that's an interesting example
you can also this is the what parts of speech are there you can have other things like you know named entities and
other sequence problems I can also ask what are the men wearing on the head
answers helmets and then maybe a slightly more interesting question why are the men wearing helmets and the
answer is safety so especially we're close to the circle of death here at Stanford where a lot of bikes crash and
it's a good answer all right with that I'll leave a couple of minutes for for
questions so basically the summary is word vectors and recurrent neural networks are super useful building
blocks once you really appreciate and understand those two building blocks you're kind of ready to have some fun
and build more complex models really in the end this dmn is a way to combine that in just a variety of new ways to a
larger more complex model and that's also where the state I think of deep learning is for natural language
processing we've tackled a lot of these smaller sub-problems intermediate tasks and now we can work on more interesting
complex problems like dialogue and question answering machine translation and things like that all right
thank you
I mean all right cool yeah a quick
question in the dynamic memory Network you have the the RN and you also
mentioned that if you have better assumption of the input right so you
used to work on the tray LST M right so if you change they are in into a tree
structure would that help it's a good question I I actually loved researchers
at in my whole PhD about tree structures and somewhat surprising in the last
couple of weeks to actually some new results on SNL I understand for natural
language inference data said where tree structures are again the state of the art and I have to say that I think the
the dynamic memory Network by having this ability in the episodic memory to
keep track of different sub phrases and pay attention to those and then combine them over multiple passes I think you
can kind of get away with not having a tree structures so yes you might have a slight improvement representing
sentences as trees in your input module but I think they're only going to be slight and I think the episodic memory
module that has this capability to go over the input multiple times pay attention to certain sub phrases will capture a lot of the kinds of
complexities that you might want to capture in tree structures so I don't my short answer is I don't think you necessarily need it have you tried it we
have not no thanks hi a question is about question
answering say if we want to apply questions into some specific domains
that health healthcare but we don't really have the data we don't have questions appears and what sure we'll do
are there any general principles here it's a great question what do you do if
you want to question answering on a complex domain you don't have the data I think and this feels maybe like a
cop-out but I think it's very true both in practice and in theory create the data like if you cannot possibly create
more than a thousand examples of anything then maybe automating that process is not that important so clearly you
should be able to create some data and in many cases that is the best use of your time is just to sit down or ask the
domain expert to create a lot of questions and then have people find the answers and then measure how they
actually get to those answers try to have them in a constrained environment and so on I think most companies for
instance when you try to do automated email replies which is in some ways a little bit similar to question answering
well there's a nice nice nice domain because everybody had already emailed
there were already answered before so you can use sort of past behavior now if you had a search engine where people
asked a lot of questions then you can also use that too in bootstrap and see where did they actually fail and then
take all those really tough queries where they failed have some humans sit there and collect the data so that's
that's the simplest answer now the other answer is let's work together for the Mexican like many years on research for
smaller training data set sizes and complex reasoning the the fact of the
matter for that line of research will still be if you if a system has never seen a certain type of reasoning I'll be
hard for the systems to pick up that type of reasoning I think we're going to
get with these kinds of architectures to the space where at least if it has seen this type of reasoning a specific type
of transitive reasoning or temporal reasoning or sort of cause and effect type reasoning at least like a couple
hundred times then you should be able to train a system with these kinds of models to do it are these QA systems
currently robust to false input our questions for the woman playing tennis if you asked what's the man holding
would it replied there is no man it would not and largely because at
training time you never try to mess with it like that I'm pretty sure if you added a lot of training examples where
you had those it would probably eventually pick it up those would be important for like real-world implementations and so real-world
implementations of this in security are actually kind of tricky I think whenever you train a system we know we
can for instance both steal certain classifiers by using them a lot we know we can fool them into
classifying certain images for instance as others we have folks in the audience who worked on that exact line of work so
I would be careful using it in security environments right now yeah I have a
question oh wow up there yeah I have a question actually
uh there was a slide where you had the input module and and there were a bunch of sentences so what those sentences
themselves are n ends because you know sequence is basically made up of those individual words in sake love you know
representation so what those you know also when are n ends that word you know stitch together or so the answer there
is a little complex because we have two two papers with the dmn and the answer is different for each the simplest in
the simplest form of that there it is actually a single gru that goes from the first word through all the sentences as if there
are one gigantic sequence and but it has access to each sentence period at the end to pay a special attention to the
end of sentences and so yes in the simplest form it is just a giryu that
goes over all the words this is a normal process to basically just concatenate all the sentences into one gigantic you
know so the answer there and this is kind of why I split the the talk into
three different ones from like words single sentences and in multiple sentences I think if you just had a
single gru that goes over everything and now you try to reason over that entire sequence it would not work very well
your read to have an additional structure such as an intention mechanism or a pointer mechanism that has the
ability to pay attention to specific parts of your input to do that very accurately but yeah in general that's
fine as long as you have this additional mechanism thank you thank you great question so in the recurrent neural Nets
you're using sigmoids in visual recognition I guess are
rectified linear units for the more popular non-linearity that's right so rail users are great now when you look
at the GRU equations here and you have these reset gates and so these reset gates here
you want them to essentially be be between zero and one so that it can either ignore this input entirely or you
have it normally be part of the computation of H tilt so in some cases you really do want to have Sigma lights
there but other ones for instance some like simpler things where you actually
don't have that much recurrence such as going from one member state to another in the second iteration of this model
actually rail used were we're good mom good like activation functions to did
you guys try to after training this network try to take these weights for
the images and do object detection again so these weights would be augmented with
the text victors did you try to use that is a very cool idea that we did not
explore no there you go you got to do it
fast yeah feel this feel is moving fast you just let the cat out of the box so so
those attention models are pretty powerful when you have an opportunity data and then you can learn you know to
make make yourself with data but even
though those are some of the tasks are pretty gets a trivial to human but it's hard for model tuner so what do you
think of a casinos right now even right now we have not a non G base on the web right no inequity pedia we not we know a
lot about you know common sense but how what do you think about you cover those knowledge base into those models I
actually love that line of research too and that was kind of what we start out with this semantic memory module in the
simplest form is just word vectors I think in one next iteration would activity to have knowledge bases also
influence the reasoning there's very little work on combining text and
knowledge bases to do overall complex question answering that requires reasoning thing is a phenomenally
interesting area of research so where any night hints or any starting point
about it so there are some photos there are some papers that reasoning over knowledge bases alone so
we had a paper on recursive no tensor networks that basically takes a triplet a word vector for an entity might be in
freebase might be in word net a relation a vector for a relationship and a vector
for another entity and then basically pipe them into a neural network and say yes no are these two entities actually
in that relationship and you can have a variety of different architectures I think semi work done on that as well
wait that's a different brother different Benjy oh I think over there all right and it's true that's true yeah
if antoine board right that's right that's right so so i think you can also
reason over knowledge graphs and you could then try to combine that with reasoning over fuzzy text it has been a
boat it all has been done i think nobody has yet really combined it in a principled way great question yeah one last question
a whole question so so what the model answer my questions correctly so how do
i check the model actually understand understood my question and the woods which are logic was a models logic
behind that it's a good question in some ways it's a common question for for
neural network interpretability so income division at the sometimes we can at least the visualizes the features
right so how about the right and so i think the best thing that we could do right now is to show these attention
scores where you know for sentiment we're like oh how did it come up the sentiment oh it paid attention to the
movie working and likewise for question answering we can see like which facts at
which sentences that actually pay attention to in order to answer that overall question so that is I think the
best answer that we could come up with right now but how yeah there's certain other complexities that there's still an
area of open resources thank you all right thank you everybody
so thank you Richard we'll take another coffee break for 30 minutes so please come back at 2:45 but for a presentation
by sherry more

----------

-----

--10--     

-----
Date: 2016.09.27
Link: [# Deep Learning for Speech Recognition (Adam Coates, Baidu)](https://www.youtube.com/watch?v=g-sndkf7mCs)

Notes:

**Advantages:**

1. Deep learning significantly improves speech recognition accuracy, enabling more complex and natural user interactions.
2. Speech recognition technologies powered by deep learning are surpassing traditional methods, offering faster and more reliable transcription.
3. The ability to operate hands-free through voice commands enhances safety in scenarios like driving.

**Drawbacks:**

1. Dependence on large amounts of data and high computational power for training deep learning models.
2. Challenges in dealing with accents, background noise, and the nuances of spoken language.

**Tips and Advice:**

1. Utilize a combination of deep learning models and traditional language models to improve accuracy and handle linguistic nuances.
2. Consider the application context (e.g., noisy environment vs. quiet room) when collecting and preparing training data.
3. Implement strategies like "sorta grad" and batch normalization to improve training efficiency and model performance.
4. Pay attention to the model's structure to ensure it is suitable for real-time, online applications and does not overly delay responses.

**Lecture Content:**

- The lecture covered the integration of deep learning in speech recognition, focusing on the transition from traditional methods to deep learning approaches.
- Key components of speech recognition systems, such as acoustic models and language models, were discussed, along with their evolution due to deep learning.

**Main Challenges:**

1. Building a speech recognition system that is accurate across various languages and dialects.
2. Overcoming the limitations imposed by the need for large datasets and computational resources.

**The Importance and Usefulness of the Topic:**

- Speech recognition technology is crucial for developing interactive and accessible applications, enhancing user experience across various platforms and devices.

**Accomplishments:**

- Significant improvements in speech recognition accuracy, making it competitive with human transcription in certain languages.
- Development of robust models that can handle real-world speech, including noisy environments.

**Summary of the Content:** The lecture provided an overview of how deep learning has revolutionized speech recognition, highlighting the advancements over traditional methods. It discussed the components of speech recognition systems, challenges in the field, and strategies for improving model training and performance. The lecture also emphasized the importance of speech recognition in creating interactive and accessible technology.

**Interesting Quotes or Insightful Sentences:**

1. "Deep learning has been playing an increasingly large role in speech recognition."
2. "Speech recognition is at a place right now where it's becoming good enough to enable really exciting applications."
3. "The goal of building a speech pipeline is if you just give me a raw audio wave...I want to somehow build a speech recognizer that can do this very simple task of printing out 'hello world' when I actually say 'hello world'."

Transcription:

so I want to tell you guys about speech recognition and deep learning I think deep learning has been playing
an increasingly large role in speech recognition and one of the things I think is most exciting about this field
is that speech recognitions at a place right now where it's becoming good enough to enable really exciting
applications that end up the hands of users so for example if we want to
caption video content and make it accessible to to everyone it used to be that we would sort of try to do this but
you still need a human to get really good captioning for something like a lecture but it's possible that we can do
a lot of this with higher quality in the future with deep learning we can do things like hands-free interfaces in
cars make it safer to use technology while we're on the go keep people's eyes on the road of course and make mobile
devices home devices much easier much more efficient and enjoyable to use but
another actually sort of fun recent study that that some folks if I do
participated in along with Stanford and UW is to show that for even something straight forward that we sort of take
for granted as an application of speech which is just texting someone with voice
or writing a piece of text the study show you can actually go three times faster with voice recognition systems
that are available today so it's not just like a little bit faster now even with the errors that a speech
recognition system can make it's actually a lot faster and the reason I wanted to highlight this result which is
pretty recent is that the speech engine that was used for this study is actually powered by a lot of the deep learning
methods and I'm going to tell you about so hopefully when you walk away today you have an appreciation or an understanding of the sort of high-level
ideas that make a result like this possible so there are a whole bunch of
different components that make up a complete speech application so for
example there's speech transcription so if I just talk I want to come up with words that represent you know whatever I
just said there's also other tasks though like word spotting or triggering so for example if my phone is sitting
over there and I want to say hey phone go do something for me actually has to be listening continuously for me to say that word and
likewise there are things like speaker identification or verification so that if I want to authenticate myself or I
want to be able to tell apart different users in a room I've got to be able to recognize your voice even though I don't
know what you're saying so these are different tasks I'm not going to cover all of them today instead
I'm going to just focus on the bread and butter of speech recognition we're going to focus on building a speech engine
that can accurately transcribe audio into words so that's our main goal this
is a very basic goal of artificial intelligence right historically people
are very very good at listening to someone talk just like you guys are listening to me right now and you can
very quickly turn words turn audio into words and into meaning on your own
almost effortlessly and for machines this has historically been incredibly
hard so you think of this is like one of those sort of consummate AI tasks so the
goal of building a speech pipeline is if you just give me a raw audio wave like you recorded on your laptop or your cell
phone I want to somehow build a speech recognizer that can do this very simple task of printing out hello world when I
actually say hello world so before I dig into the deep learning part I want to
step back a little bit and spend maybe ten minutes talking about how a
traditional speech recognition pipeline is working for two reasons if you're out
in the wild you're doing an internship you're trying to build a speech recognition system with a lot of the
tools that are out there you're going to bump into a lot of systems that are built on technologies that look like
this so I want you to understand a little bit of the vocabulary and how those things are put together and also
this will sort of give you a story for what deep learning is doing in speech
recognition today that is kind of special and that I think paves the way
for for much bigger results in the future so traditional systems break the
problem of converting an audio wave of taking audio and break and turning it into a
transcription into a bunch of different pieces so I'm going to start out with my
raw audio and I'm just going to represent that by X and then usually we
have to decide on some kind of feature representation we have to convert this into some other form that's easier to
deal with than a raw audio wave and in a traditional speech system I often have
something called an acoustic model and the job of the acoustic model is to learn the relationship between these
features that represent my audio and the words that someone is trying to say and
then I'll often have a language model which encapsulate Sall of my knowledge
about what kinds of words what spellings and what combinations of words are most likely in the language that I'm trying
to transcribe and once you have all of these pieces so these might be these
different models might be driven by machine learning themselves what you would need to build in a traditional system is something called a decoder and
the job of a decoder which itself might involve some modeling efforts and
machine learning algorithms is to find the sequence of words W that maximizes
this probability the probability of the particular sequence W given your audio that's straightforward but that's
equivalent to maximizing the product of the contributions from your acoustic model and from your language model so a
traditional speech system is broken down into these pieces and a lot of the effort and getting that system to work
is is in developing this sort of portion that combines them all so it turns out
that if you want to just directly transcribe audio you can't just go
straight to characters and the reason is and it's especially apparent in English that the way something is spelled in
characters doesn't always correspond well to the way that it sounds so if if
I give you the word night for example without context you don't really know whether I'm talking about like a knight
in armor or whether I'm talking like knight like in like an evening and so a way to get around this to
abstract this problem away from a traditional system is to replace this with a sort of intermediate
representation instead of trying to predict characters I'll just try to predict something called phonemes so as
an example if I want to represent the word hello what I might try to do is
break it down into these units of sound so the first one is like the that H
sound in hello and then an a sound which is actually only one possible
pronunciation of an e and then an L and an O sound and that would be my string
that I try to come up with using all of my different speech components so this
in one sense makes the modeling problem easier my acoustic model and so on can be simpler because I don't have to worry
about spelling but it does have this problem that I have to think about where these things come from
so these phonemes are intuitively they're the perceptual e distinct units
of sound that we can use to distinguish words and they're very approximate this
might be our imagination that these things actually exist it's not clear how fundamental this is but they're sort of
standardized there are a bunch of different conventions for how to define these and if you're and if you end up
working on a system that uses phonemes one popular data set is called timet and
so this actually has a corpus of audio frames with examples of each of these phonemes so once you have this phoneme
representation unfortunately it adds even more complexity to this traditional
pipeline because now my acoustic model doesn't associate this audio feature
with words it actually associates them with another kind of transcription with the transcription into phonemes and so I
have to introduce yet another component into my pipeline that tries to understand how do I convert the
transcriptions in phonemes into actual Spelling's and so I need some kind of dick or a lexicon to tell me all of that so
this is a way of taking our knowledge about a language and baking it into this engineered pipeline and then once you've
got all that again all of your work now goes into this decoder that has a slightly more complicated task in order
to infer the most likely word transcription given the audio so this is
a tried and true pipeline it's been around for a long time you'll see a whole bunch of these systems out there
and we're still using a lot of the vocabulary from these systems but
traditionally the big advantage is that it's very tweakable if you want to go add a new pronunciation for a word
you've never heard before you can just drop it right in that's great but it's also really hard to get working
well if you start from scratch with this system and you have no experience in speech recognition it's actually quite
confusing and hard to debug it's very difficult to know which of these various models is the one that's behind your
error and especially once we start dealing with things like accents heavy noise different kinds of ambiguity that
makes the problem even harder to engineer around because trying to think ourselves about how do i tweaked my
pronunciation model for example to account for someone's accent that I haven't heard that's a very hard
engineering judgment for us to make so there are all kinds of design decisions
that go into this pipeline like choosing the future representation for example so
the first place that deep learning has started to make an impact in speech
recognition starting a few years ago is to just take one of the core machine
learning components of the system and replace it with a deep learning algorithm so I mentioned back in this
previous pipeline that we had this little model here whose job is to learn the relationship between a sequence of
phonemes and the audio that we're hearing so this is called the acoustic model and there are lots of different
methods for training this thing so take your favorite machine learning algorithm you can probably find someone who is
trained in acoustic model with that algorithm whether it's a Gaussian mixture model or a bunch of decision
trees and random forests anything for estimating these kinds of densities there's a lot of work and trying to make
better acoustic models so some work by George Dahl and co-authors took what was
a state of the art deep learning system back in 2011 which is a deep belief
network with some pre training strategies and dropped it into a state of the art pipeline in place of this
acoustic model and the results are actually pretty striking because even though we had neural networks and these
pipelines for a while what ended up happening is that when you replace the
Gaussian mixture model in hmm system that already existed with this deep
belief network as an acoustic model you actually got something between like a ten and twenty percent relative
improvement in accuracy which is a huge jump this is highly noticeable to a
person and if you compare this to the amount of progress that had been made in preceding years this is a giant leap for
a single paper to make compared to a progress we've been able to make previously so this is in some sense the
first generation of deep learning for speech recognition which is I take one of these components and I swap it out
for for my favorite deep learning algorithm so the picture looks sort of
like this so with these traditional speech recognition pipelines the problem that
we would always run into is that if you gave me a lot more data he gave me a much bigger computer so that I could
train a huge model that actually didn't help me because all the problems I had were in the construction of this
pipeline and so eventually if you gave me more data in a bigger computer the
performance of our speech recognition system would just kind of peter out it would just reach a ceiling that was very
hard to get over and so we just start coming up with lots of different strategies we start specializing for
each application we try to specialize for each user and try to make things a little bit
better around the edges and what these deep learning acoustic models did was in some sense moved that barrier a little
ways it made it possible for us to take a bit more data much faster computers
that let us try a whole lot of models and move that ceiling up quite a ways so
the question that many in the research community including folks if I do have been trying to answer is can we go to a
next-generation version of this insight can we for instance build a speech
engine that is powered by deep learning all the way from the audio input to the
transcription itself can we replace as much of that traditional system with deep learning as possible so that over
time is you give researchers more data and bigger computers and the ability to
try more models their speech recognition performance just keeps going up and we can potentially solve speech for
everybody so the goal of this tutorial is not to to get you up here which
requires a whole bunch of things that I'll tell you about near the end but what we want to try to do is give you
enough to get a point on this curve and then once you're on the curve the the
idea is that what remains is now a problem of scale it's about data and about getting bigger computers and
coming up with ways to build bigger models so that's my objective so that when you walk away from here you have a
picture of what you would need to build to get this point and then after that
it's hopefully all about scale so thanks to Vinay Rao who's been helping put this
tutorial together there is going to be some starter code live for the basic
pipeline the deep learning part of the pipeline that we're talking about so there are some open source implementations of things like CTC but
we wanted to make sure that there's a system out there that's pretty representative of the acoustic models that I'm going to be talking about in
the first half of the presentation here so this will be enough that you can get
a simple pipeline going with something called max Dakota which I'll tell you about later and the
idea is that this is sort of a scale model of the acoustic models that I do and other places are powering real
production speech engines so this will get you that point on the curve okay
so here's what we're going to talk about the first part I'm just going to introduce a few preliminaries talk about
pre-processing so we still have a little bit of pre-processing around but it's not really fundamental I think it's
probably going to go away in the long run we'll talk about what is probably the most mature piece of sequence
learning technologies for deep learning right now so it turns out that one of the fundamental problems of doing speech
recognition is how do I build a neural network that can map this audio signal to a transcription that can have a quite
variable length and so CTC is one highly mature method for doing this and I think
you're actually going to hear about maybe some some other solutions later today then I'll say a little bit about
training and just what that looks like oops and then finally say a bit about
decoding and language models which is sort of an addendum to the current acoustic models that we can build that
make them perform a lot better and then once you have this that's a picture of what you need to to get this point on
the curve and then I'll talk a little bit about what's remaining how do you scale up from this little scale model up
to the full thing what does what does that actually entail and then time permitting we'll talk a little bit about
production how could you put something like this into a cloud server and actually serve real users with it great
so how is audio represented this should be pretty straightforward I think unlike
a two dimensional image where we normally have a 2d grid of pixels audio is just a 1d signal and there are a
bunch of different formats for audio but typically this one-dimensional wave that that is actually me saying something
like hello world is something like 8,000 samples per second or 16,000 samples per
second and each wave is quantized into eight or 16 bits so when we represent this audio
signal that's going to go into our pipeline you could just think of that as a one dimensional vector so when I have
that box called X that represented my audio signal you can figure this was being broke down broken down into
samples X 1 X 2 and so forth and if I had a one-second audio clip this vector
would have a length of either say 8,000 or 16,000 samples and each element would
be say a floating-point number that I had extracted from this eight or 16-bit sample this is really simple now once I
have an audio clip we'll do a little bit of pre-processing so there are a couple of ways to start the first is to just do
some vanilla pre-processing like convert to a simple spectrogram so if you look
at a traditional speech pipeline you're going to see things like M FCC's which are mell frequency capital coefficients
you'll see a whole bunch of plays on spectrograms where you take differences
in different kinds of features and try to engineer complex representations but
for the stuff that we're going to do today a simple spectrogram is just fine and it turns out as you'll see in a
second we lose a little bit of information when we do this but it turns out not to not to be a huge difference
now I said a moment ago that I think probably this is going to go away in the long run and that's because today you
can actually find recent research and trying to do away with even this pre-processing part and having your
neural network process the audio wave directly and just train its own feature transformation so there's some
references at the end that you can look at for this so it's a quick straw poll
how many people have seen a spectrogram or computed a spectrogram before pretty
good maybe 50% ok so the idea behind a spectrogram is that it's sort of like a
frequency domain representation but instead of representing this entire signal in terms of frequencies I'm just
going to represent a small small window in terms of frequencies so to to process
this audio clip the first thing I'm going to do is cut out a little window that's typically about 20 milliseconds
long and when you get down to that scale it's usually very clear that these audio signals are made up of sort of a
combination of different frequencies of sine waves and then what we do is we
compute an FFT it basically converts this little signal into the frequency domain and then we just take the log of
the power at each frequency and so if you look at your what the result of this
is it basically tells us for every frequency of sine wave what is the
magnitude what's the amount of power represented by that sine wave that makes up this original signal so over here in
this example we have a very strong low frequency component in the signal and
then we have differing magnitudes at different differing frequencies so we
can just think of this as a vector so now instead of representing this little 20 millisecond slice as sort of a
sequence of audio samples instead I'm going to represent it as a vector here where each element represents sort of
the strengths of each frequency in this little window and the next step beyond
this is that if I just told you how to process one little window you can of course apply this to a whole bunch of
windows across the entire piece of audio and and that gives you what we call a
spectrogram and you can use either disjoint windows that are just sort of adjacent or you can apply them to
overlapping windows if you like so there's a little bit of parameter tuning there but this is an alternative
representation of this audio signal that happens to be easier to use for a lot of
purposes okay so our goal starting from
this representation is to build what I'm going to call an acoustic model but which is really to the extent we can
make it happen is really going to be an entire speech engine that is represented by a neural network
so what we would like to do is build a neural net that if we could train it
from a whole bunch of pairs X which is my original audio that I turn into a spectrogram and Y star that's the ground
truth transcription that some human is given me if I were to train this big neural network off of these pairs what
I'd like it to produce is some kind of output that I'm representing by the character C here so that I could later
extract the correct transcription which I'm going to denote by Y so if I said
hello the first thing I'm going to do is run pre-processing to get all these spectrogram frames and then I'm going to
have a recurrent neural network that consumes each frame and processes them into some new representation called C
and hopefully I can engineer my network in such a way but I can just read the
transcription off of these output neurons so that's kind of the the intuitive picture of what we want to
accomplish so as I mentioned back in the outline there's one obvious fundamental
problem here which is that the length of the input is not the same as the length
of the transcription so if I say hello very slowly then I can have a very long
audio signal even though I didn't change the length of the transcription or if I say hello very quickly then I kind of
very short transcript or a very short piece of audio and so that means that this output of my neural network is
changing length and I need to come up with some way to reprimand neural
network output to this fixed length transcription and also do it in a way that we can actually train this pipeline
so the traditional way to deal with this problem if you were building a speech
engine several years ago is to just try to bootstrap the whole system so I had actually train a neural network to
correctly predict the sounds at every frame using some kind of data set like
timet where someone has lovingly annotated all of the phonemes for me and then I try to figure out the
alignment between my saying hello in a phonetic transcription with the input audio and then once I've lined up all of
the sounds with the input audio now I don't care about length anymore because I can just make a one-to-one mapping
between the audio input and the phoneme outputs that I'm trying to target but
this alignment process is horribly error-prone you have to do a lot of extra work to make it work well and so
we really don't want to do this we really want to have some kind of solution that lets us solve this
straightaway so there are multiple ways to do it and as I mentioned there's some current
research on how to use things like attentional model sequence to sequence models that you'll hear about later in
order to solve this kind of problem but as I said we'll focus on something
called connexion connectionist temporal classification or ctc that is sort of
current state of the art for how to do this so here's the basic idea so our recurrent neural network has
these output neurons that I'm calling C and the job of these output neurons is
to encode a distribution over over the output symbols so as because of the
structure of the recurrent Network the length of this symbol sequence C is the same as the length of my audio input so
if my audio inputs a was two seconds long that might have a hundred audio
frames and that would mean that the length of C is also a hundred a hundred different values so if we were working
on a phoneme based model then C would be some kind of phoning representation I
mean we would also include a blank symbol which is special for CTC but if
as we'll do in the rest of this talk we're trying to just predict the
graphemes trying to predict the characters in this language directly from the audio then I would just let C
take on a value that's in my alphabet or take on a blank or a space if my
language has spaces in it and then the second thing I'm going to do sigh my RNN gives me a distribution over
these symbols see is what I'm going to try to define some kind of mapping that can convert this long transcription C
into the final transcription Y that's like hello that's the actual string that
I want and now recognizing that C is itself a probabilistic creature there's
a distribution over choices of C that correspond to the audio once I apply
this function that also means that there's a distribution over Y there's a distribution over the possible
transcriptions that I could get and what I'll want to do to train my network is to maximize the probability of the
correct transcription given the audio so those are the three steps that we have
to accomplish in order to make CTC work so let's start with the first one so we
have these output neurons C and they represent a distribution over the different symbols that I could be
hearing in the audio so I've got some audio signal down here you can see the spectrogram frames poking up and this is
being processed by this recurrent neural network and the output is a big bank of
softmax in herranz so for the first frame of audio I have a neuron that
corresponds to each of the symbols that C could represent and they and this set
of softmax neurons here the with the output summing to 1 represents the probability of say C 1 having the value
ABC and so on or this special blank character so for example if I pick one
of the neurons over here then the first row which it represents the character B
and the 17th column which is the 17th frame in time this represents the
probability that C 1 7 represents the character be given the audio so once I
have this that also means that I can just define a distribution not just over
the visual characters but if I just assume that all of the characters are independent which is kind of a naive
assumption but if I bake this into the system I can define a distribution over all possible sequences of characters in
this alphabet so if I gave you a specific instance a specific character
string using this alphabet for instance I represent the string hello as HHH e
blank e blank blank LL blank ello and then a bunch of blanks this is a string
in this alphabet for for C and I can just use this formula to compute the
probability of this specific sequence of characters so that's how we we compute
the probability for a sequence of characters when they have the same length as the audio input so the second
step and this is in some sense the kind of neat trick in CTC is to define a
mapping from this long encoding of the
audio into symbols that crunches it down to the actual transcription that we're
trying to predict and the rule is this operator takes this character sequence
and it picks up all the duplicates all of the adjacent characters that are repeated and discards the duplicates and
just keep some of them and then it drops all of the blanks so in this example you
see you have three H's together so I just keep one H and then I have a blank
I throw that away and I keep an e when I have two L's so I keep one of the LS over here and then another blank and an
elbow and the one key thing to note is that when I have two characters that are
different right next to each other I just end up keeping those two characters in my output but if I ever have a double
character like ll in hello then I'll need to have a blank character that that
gets put in between but if our neural network gave me this
transcription told me that this was the right answer we just have to apply this operator and we get back Vic string
hello so now that we have a way to
define a distribution over these sequences of symbols that are the same length as the audio and we now have a
mapping from those strings into transcriptions as I said this gives us a
probability distribution over the possible final transcriptions so if I
look at the probability distribution over all the different sequences of symbols right
I might have hello written out like on the last slide and maybe that has probability 0.1 and then I might have
hello but written a different way with a different by say replacing this H with a blank that has a smaller probability and
I have a whole bunch of different possible symbol sequences below that and what you'll notice is that if I go
through every possible combination of symbols here there are several combinations that all
map to the same transcription so here's one version of hello there's a second
version of hello there's a third version of hello and so if I now ask what's the probability of the transcription hello
the way that I compute that is I go through all of the possible character
sequences that correspond to the transcription hello and I add up all of
their probabilities so I have to sum over all possible choices of C that
could give me that transcription in the end so you can kind of think of this as
searching through all the possible alignments right I could shift these characters around a
little bit I can move them forward backward I could expand them by adding duplicates or squish them up depending
on how fast someone is talking and that corresponds to every possible alignment between the audio and the characters
that I want to transcribe it sort of solves the problem of the variable length and the way that I get the
probability of a specific transcription is to sum up to marginalize over all the different
alignments that could be feasible and then if we have a whole bunch of other
possibilities in here like the word yellow-eyed compute them in the same way and so this equation just says to sum
over all the character sequences see so that when I apply this little mapping operator I end up with the transcription
why is oh I'm missing a EE you're
talking about this one so when we apply this sort of squeezing operator here we
drop this double e to get a single Ian hello so we remove all the duplicates so
the same way we did for an H right so
whenever you see two characters together like this where they're adjacent duplicates you sort of squeeze all those
duplicates out and you just keep one of them but here we have a blank in between so if we drop all the duplicates first
then we still have two L's left and then we remove all the blanks so this gives
the algorithm a way to represent repeated characters in the transcription there's another one in the back
oh I see yeah this is maybe I put a
space in here really I'd have put a space character in here instead of a blank really this could be h-e-l-l-o H
yeah so the this space here is erroneous
okay very good okay so once I've defined this right I
just gave you a formula to compute the probability of a string given the audio
so as as with every good starting to a machine learning algorithm we go and we
try to apply maximum likelihood I now give you the correct transcription and your job is to tune the neural network
to maximize the probability of that transcription using this model that I just defined so in equations what I'm
going to do is I want to maximize the log probability of Y star for a given
example I want to maximize the probability of the correct transcription
given the audio X and then I'm just going to sum over all the examples and
then what I want to do is just replace this with the equation that I had on the
last page that says in order to compute the probability of a given transcription I have to sum over all of the possible
symbol sequences that could have given me that transcription sum over all the possible alignments that would map that
transcription to my audio so Alex grades and co-authors in 2006 actually show
that because of this independence assumption there is a clever way there
is a dynamic programming algorithm that can efficiently compute this summation for you and not only commute compute
this summation so that you can compute the objective function but actually compute its gradient with respect to to
the output neurons of your neural network so if you look at the paper the algorithm details are in there
what school right now in the history of speech and deep learning is that this is
at the level of a technology this is something that's now implemented in a bunch of places so that you can download
a software package that efficiently will calculate this ctc loss function for you
that can calculate this likelihood and can also just give you back the gradient so I won't go into the equations here
instead I'll tell you that there are a whole bunch of implementations on the web that you can now use as part of deep
learning packages so one of them from Baidu implements CTC on the GPU is
called warp CTC Stanford and group they're actually one of Andrews students
has a CTC implementation and there's also now CTC losses implemented in
packages like tensor flow so this is something that's sufficiently widely distributed that you can use use these
algorithms off the shelf so the way that these work the way that we go about
training is we start from our audio spectrogram we have our neural network structure where you get to choose how
it's put together and then it outputs this Bank of softmax neurons and then
there are pieces of off-the-shelf software that will compute for you the CTC cost function they'll compute this
log likelihood given a transcription and the output neurons from your recurrent
Network and then the software will also be able to tell you the gradient with
respect to the output neurons and once you've got that you're set you can feed them back into the rest of your code and
get the gradient with respect to all of these parameters so as I said this is all available now in sort of efficient
off-the-shelf software so you don't have to do this work yourself so that's pretty much all there is to the high
level algorithm with this it's actually enough to get a sort of a working
Drosophila of speech recognition going there are a few a few little tricks
though that you might need along the way on easy problems you might not need these but as you get to more
difficult datasets with a lot of noise they can become more and more important so the first one that we've been calling
sort of grad in the vein of all of the grad algorithms out there is basically a
trick to help with recurrent neural networks so it turns out that when you
try to train one of these big RNN models on some off-the-shelf speech data one of
the things that can really get you is seeing very long utterances early in the process because if you have a really
long audience then if your neural network is badly initialized you'll often end up with things like underflow
and overflow as you try to go and compute the probabilities and you end up with gradients exploding as you try to
do back propagation and it can make your optimization a real mess and it's coming
from the fact that these utterances are really long and really hard and the neural network just isn't ready to deal
with those transcriptions and so one of the fixes that you can use is during the early parts of training usually in the
first epic is you just sort all of your audio by length and now when you process
a mini batch you just take the short utterances first so that you're working with really short rnns that are quite
easy to train and don't blow up and don't have a lot of catastrophic numerical problems and then as time goes
by you start operating on longer and longer addresses that get more and more difficult so we call this sort of grad
it's basically a curriculum learning method and so you can see some work from yoshua bengio and his team on a whole
bunch of strategies for this but you can think of the short utterances as being the easy ones and if you start out with
the easy utterances and move to the longer ones your optimization algorithm can do better so here's what an example
from one of the models that we've trained where your CTC cost starts up here and you know after a while you
optimize and you sort of bottom out around you know what a log likelihood of maybe 30 and then if you add this sort
of grad strategy after the first epic you're actually doing better and you can reach a better optimum than you
without it and in addition another strategy that's extremely helpful for
recurrent networks and very deep neural networks is batch normalization so so
this becoming very popular and it's also available as sort of an off-the-shelf package inside of a lot of the different
frameworks that are available today so if you start having trouble you can consider putting batch normalization
into your network okay so our neural network now spits out this big bank of
softmax neurons we've got a training algorithm we're just doing gradient descent how do we actually get a
transcription this process as I said is meant to be as close to characters as
possible but we still sort of need to decode these outputs and you might think
that one simple solution which turns out to be approximate to get the correct transcription is just go through here
and pick the most likely sequence of symbols for C and then apply our little
squeeze operator to get back the transcription the way that we defined it so this turns out not to be the optimal
thing this actually doesn't give you the most likely transcription because it's not accounting for the fact that every
transcription might have multiple sequences of C's multiple alignments in
this representation but you can actually do this and this is called the max
decoding and so for this sort of contrived example here
I put little red dots on the most likely C and if you see there's a couple of
blanks a couple of C's is another blank a more blanks bees more blanks and if
you apply our little squeeze operator you just get the word cab if you do this
it is often terrible it'll often give you a very strange transcription that
doesn't look like English necessarily but the reason I mention it is that this
is a really handy diagnostic that if you're kind of wondering what's going on in the network glancing at a few of
these will often tell you if the network's starting to pick up any signal or if it's just outputting gobbled
cook so I'll give you a more detailed example in a second of how that happens
all right so these are all the concepts of our of our very simple pipeline and
the demo code that we're going to put up on the web will basically let you work on all of these pieces so once we try to
train these I want to give you an example of the sort of data that we're training on a tanker is a ship designed
to carry large volumes of oil okay so this is just a person sitting there
reading The Wall Street Journal to us so this is a sort of simple data set it's really popular in the speech research
community it's published by the linguistic data consortium there's also a free alternative called libera speech
that's very similar but instead of people reading The Wall Street Journal is people reading Creative Commons audiobooks so in the demo code that we
have a really simple network that works reasonably well it looks like this so there's a sort of family of models that
we've been working with where you start from your spectrogram you have maybe one layer or several of convolutional
filters at the bottom and then on top of that you have some kind of recurrent neural network it might just be a
vanilla RNN but but you can also use like LS TM or GRU cells any of your
favorite RNN creatures from the literature and then on top of that we have some fully connected layers that
produce these softmax outputs and those are the things that go into CTC for training so this is pretty
straightforward the implementation on the web uses the the work CTC code and then we would just train this big neural
network with stochastic gradient descent Nesterov momentum all the stuff that you've probably seen in a whole bunch of
other talks so far all right so if you actually run this what is going on
inside so I mentioned that looking at the max decoding is kind of a handy way
to see what's what's going on inside this creature so I wanted to show you an
example so this is a picture this is a visualization
those softmax neurons at the top of one of these big neural networks so this is
the representation of see from all the previous slides so on the horizontal
axis this is basically time this is the frame number or which chunk of the spectrogram we're seeing and then on the
vertical axis here you see these are all the characters in the English alphabet or a space or a blank so after three
hundred iterations of training which is not very much the system has learned something amazing which is that it
should just output blanks and spaces all the time because these are by far because of all the silence and things in
your data set these are the most common characters right I just want to fill up the whole space with blanks but you can
see it's kind of randomly poking out a few characters here and if you run your little Mac's decoding strategy to see
what is the system think the transcription is it thinks it transcription is at and so but after
three hundred iterations that's okay but this is a sign that the neural networks not going crazy your gradient isn't
busted it's at least learned what is the most likely characters then after maybe
1500 or so you start to get a little bit of structure and if you try to like
mouthed these words you might be able to sort of see that there's some English
like sounds in here like they are just in frightened something kind of odd but
it's actually looking much better than just h it's actually starting to output something go a little bit farther it's a
little bit more organized you could start to see that we have sort of
fragments of possibly words starting to form and then after you're getting close
to convergence it's still not a real sentence but does this make sense to people he guess like what the correct
transcription might be yeah so you might have a couple of candidates the the
correct one is actually there just in front and so you can see that sort of
it's sort of sounding it out with English characters like I have a young son and I kind of figure I'm eventually
going to see him producing max Dakota puts of English and you're just going to
like sound these things that we like if they're just in front there but but this is why this max decoding strategy is
really handy because you can kind of look at this output and say yeah it's starting to get some actual signal out of the data it's not just gobbledygook
so because this is like my favorite speech recognition party game I wanted
to show you a few more of these so here's the max decoded output the poor
little things cried Cynthia think of them having been turned to the wall all these years so you can hear like the
sound of the breath at the end turns into a little bit of a word Cynthia is sort of in this transcription
and you'll find that things like proper names and so on tend to get sounded out but if those names are not in your audio
data there's no way the network could have learned how to say the name Cynthia and we'll come back to how to solve that
later did you see the true label the poor little things cried Cynthia and
that the last word is actually all these years and there isn't a word hanging off at the end so here's another one that is
true bad grade how many people figured
out what this is this is the max decoded transcription sounds sounds good to you
it sounds good to me if you told me that this was the ground truth like oh that's weird I have to go
what lookup what this is here's the actual true label turns out this is a
French word that means something like rubbernecking I had no idea what this
word was so this is again the cool examples of what these neural networks are able to figure out with no knowledge
of the language itself okay so let's go
back to decoding we just talked about max decoding which is sort of an approximate way of going from these
probability vectors to a transcription Y and if you want to find the actual most
likely transcription Y there's actually no algorithm in general that can give
you the perfect solution efficiently so the reason for that remember is that for a
single transcription why I have an efficient algorithm to compute its probability but if I want to search over
every possible transcription I don't know how to do that because there combinatorially or exponentially many
possible transcriptions and I'd have to run this algorithm to compute the probability of all of them so we have to
resort to some kind of generic search strategy and so one proposed in the original paper briefly is a sort of
prefix decoding strategy so I don't want to spend a ton of time on this instead I
want to step to sort of the next piece of the picture so there were a bunch of
examples in there right like proper names like Cynthia and things like but Dow Derby where unless you had heard
this word before you have no hope of getting it right with your neural network and so there are lots of
examples like this in the literature of things that are sort of spelled out phonetically but aren't legitimate
English transcriptions and so what we'd like to do is come up with a way to fold
in just a little bit of that knowledge about the language that take a small
step backward from a perfect end-to-end system and make make these transcriptions better so as I said the
real problem here is that you don't have enough audio available to learn all these things if we had millions and
millions of hours of audio sitting around you could probably learn all these transcriptions because you just hear enough words that you know how to
spell them all maybe the way a human does but unfortunately we just don't
have enough audio for that so we have to find a way to get around that data problem there's also an example of
something that in the AI lab we've dubbed the Tchaikovsky problem which is that there are certain names in the
world right like proper names that if you've never heard of it before you have no idea how it's spelled and the only
way to know it is to have seen this word in text before and to see it in context so part of the purpose of these language
models is to get examples like this correct so there are a couple of solutions one
would be to just step back to a more traditional pipeline right use phonemes because then we can bake new words in
along with their phonetic pronunciation and the system will just get it right
but in in this case I want to focus on just fusing in a traditional language
model that gives us the probability a priori of any sequence of words so the
reason that this is helpful is that using a language model we can train these things from massive text corpora
we have way way more text in the world than we have transcribed audio and so
that makes it possible to train these giant language models with huge vocabulary and they can also pick up the
sort of contextual things that will tip you off to the fact that Tchaikovsky concerto is a reasonable thing for a
person to ask and that this particular transcription which we have seen in the
past trike offski concerto even though composed of legitimate English words is
is nonsense so there's actually not much to see on
the language modeling front for this except that the reasons for sticking with traditional and grand models are
kind of interesting if you're excited about speech applications so if you go use a package like Ken LM on the web to
go build yourself a giant and Grahm language model these are really simple and well supported and so that makes
them easy to get working and they'll let you train from lots of corpora but for
speech recognition in practice one of the nice things about Engram models as opposed to trying to say use like an RNN
model is that we can update these things very quickly if you have a big distributed cluster you can update that
Engram model very rapidly in parallel from new data to keep track of whatever the trending words are today that your
speech engine might need to deal with and we also have the need to query this
thing very rapidly inside our decoding loop that you'll see in just a second
and so being able to just look up the probabilities in a table the way an Engram model is structured is very
valuable so I hope someday all of this will go away and be replaced with an
amazing neural network but this is the really best practice today so in order
to fuse this into the system since to get the most likely transcription right
probably of Y given X to maximize that thing we need to use a generic search algorithm anyway this opens up a door
once we're using a generic search scheme to do our decoding and find the most likely transcription we can add some
extra cost terms so in a previous piece of work from Audi haneun and several
co-authors what you do is you take the probability of a given word sequence
from your audio so this is what you would get from your giant RNN and you
can just multiply it by some extra terms the probability of the word sequence according to your language model raised
to some power and then multiplied by the length we raised to another power you see that if you just take the log of
this objective function right then you get the log probability that was your original objective you get alpha times
the log probability of the language model and beta times the log of the
length and these alpha and beta parameters let you sort of trade-off the importance of getting a transcription
that makes sense to your language model versus getting a transcription that makes sense to your acoustic model and
actually sounds like the thing that you heard and the reason for this extra term over here is that as you're multiplying
in all of these terms you tend to penalize long transcriptions a bit too much and so having a little bonus or
penalty at the end to tweak to get the transcription length right is very helpful so the basic idea behind this is
just to use beam search so beam search really popular search algorithm a whole bunch of instances of it and the rough
strategy is this so starting from time zero starting from T equals one at the
very beginning of your audio input I start out with an empty list that I'm
going to pop you late with prefixes and these prefixes are just partial transcriptions that represent what I think I've heard so far
in the audio up to the current time and the way that this proceeds is I'm going
to take at the current time step each candidate prefix out of this list and then I'm going to try all of the
possible characters in my soft max neurons that can possibly follow it so
for example I can try adding a blank I say if the next element of C is actually
supposed to be a blank then what that would mean is that I don't change my prefix right because the blanks are just
going to get dropped later but I need to incorporate the probability of that blank character into the probability of
this prefix right it represents one of the ways that I could reach that prefix
and so I need to sum that probability into that candidate and likewise
whenever I add a space to the end of a prefix that signals that this prefix
represents the end of a word and so in addition to adding the probability of the space into my current estimate this
gives me the chance to go look up that word in my language model and fold that into my current score and then if I try
adding a new character onto this prefix it's just straightforward I just go and update the probabilities based on the
probability of that character and then at the end of this I'm going to have a huge list of possible prefixes that
could be generated and this is where you would normally get the exponential blow-up of trying all possible prefixes
to find the best one and what beam search does is it just says take the que
most probable prefixes after I remove all the duplicates in here and then go
and do this again and so if you have a really large que then your algorithm will be a bit more accurate in finding
the best possible solution to this maximization problem but it'll be slower
so here's what ends up happening if you run this decoding algorithm if you just
run it on the are n n outputs you'll see that you it's actually better than straight max
decoding you find slightly better solutions but you still make things like spelling errors like Boston with an AI
but once you add in a language model that can actually tell you that the word Boston with an O is much more probable
than Boston with an AI see this so one
place they can also drop in deep learning that I wanted to mention very rapidly is just if you're not happy with
your Engram model because it doesn't have enough context where you've seen a really amazing neural language modeling
paper that you'd like to fold in one really easy way to do this and Link it to your current pipeline is to do
rescore eeen so when this decoding strategy finishes it can give you the
most probable transcription but it also gives you this big list of the top K transcriptions in terms of probability
and what you can do is to take what you
can do is take your recurrent Network and just rescore all of these and
basically reorder them according to this new model so in the instance of a neural
language model let's say that this is my N best list right I have five candidates
that were output by my decoding strategy and the first one is I'm a connoisseur
looking for wine and pork chops sounds good to me I'm a connoisseur looking for
wine and pork shots so this is actually quite subtle and depending on what kind
of connoisseur you are sort of up to interpretation what you're looking for but perhaps a neural language model is
going to be a little bit better if figuring out that wine and port are closely related and if you're a connoisseur you might be looking for
wine import shots and so what you would hope to happen is that a neural language model trained on a bunch of text is
going to correctly reorder these things and figure out that the second beam
candid is actually the correct one even though your Engram model didn't help you
okay so that is really the scale model that is the set of concepts that you
need to get a working speech recognition engine based on deep learning and so the
thing that's left to go to state-of-the-art performance and start serving users is scale so I'm going to
kind of run through quickly a bunch of the different tactics that you can use to try to get there so the two pieces of
scale that I want to cover of course our data and computing power where do you get them so the first thing to know this
is just a number you can keep in the back of your head for all purposes which is that transcribing speech data is not
cheap but it's also not prohibitive it's about 50 cents to a dollar a minute depending on the quality you want and
who's transcribing it and the difficulty of the data so typical speech benchmarks
you'll see out there maybe hundreds to thousands of hours it's like the Liberty
speech data set is maybe hundreds of hours there's another data set called Vox Forge and you can kind of cobble
these together and get maybe hundreds to thousands of hours but the real challenge is that the application
matters a lot so all the utterances I was playing for you are examples of read
speech people are sitting in a nice quiet room they're reading something wonderful to me and so I'm going to end
up with a speech engine that's really awesome at listening to The Wall Street Journal but maybe not so good at
listening to someone in a crowded cafe so the application that you want to target really needs to match your data
set and so it's worth at the outset if you're thinking about going and buying a bunch of speech data to think of what is
the style of speech you're actually targeting are you worried about red speech like the ones we're hearing or do
you care about conversational speech it turns out that when people talk in a conversation it when they're spontaneous
they're just coming up with what to say on the fly versus if they have something that they're just dictating and they
already know what to say they behave differently and they can exhibit all of these effects like disfluency and
stuttering and then in addition to that we have all kinds of environmental factors that
might matter for an application like reverb and echo we start to care about the quality of microphones and whether
they have noise canceling there's something called Lombard effect that I'll mention again in a second and of
course things like speaker accents where you really have to think carefully about how you collect your data to make sure
that you you actually represent the kinds of cases you want to test on so
the reason that red speech is really popular is because we can get a lot of it and even if it doesn't perfectly
match your application it's cheap and getting a lot of it can still help you so I wanted to say a few things about
red speech because for less than ten bucks an hour's often a lot less you can get a whole bunch of data and it has the
disadvantage that you lose a lot of things like inflection and conversation allottee but but it can still be helpful
so one of the things that we've tried doing and I'm always interested to hear
more clever schemes for this is you can kind of engineer the way that people read to try to get the effects that you
want so so here's one which is that if you want a little bit more conversation
ality you want to get people out of that kind of humdrum dictation you can start giving them reading material that's a
little more exciting you can give them like movie scripts and books and people will actually start voice acting for you
creep in set the witch and see if it is properly heated so that we can put the
bread in so these are really wonderful workers right there like kind of really
getting into it to give you better data
the wolf is dead the wolf is dead and danced for joy around about the well with their mother
so yeah people reading poetry they get this sort of lyrical quality into it that you don't get from from just
reading The Wall Street Journal and finally there's something called the Lombard effect that happens when people
are in noisy environments so if you're in like a noisy party and you're trying to talk to you friend who's a couple of chairs away
you'll catch yourself involuntarily going hey over there what are you doing you raise your inflection and you kind
of you try to use different tactics to get your signal-to-noise ratio up you'll
sort of work around the the channel problem and so this this is very problematic when you're trying to do
transcription a noisy environment because people will talk to their phones using all these effects even though the
noise canceling and everything could actually help them so one strategy we've tried with varying levels of success
then they fell asleep and evening pass but no one came to the poor children is
to actually play loud noise in people's headphones to try to get them to elicit
this behavior again here this person is kind of raising their voice a little bit in a way that they wouldn't if they were
just reading and similarly as I mentioned there are a whole bunch of
different augmentation strategies so there are all these effects of environment like reverberation echo
background noise that we would like our speech engine to be robust to and one
way you could go about trying to solve this is to go collect a bunch of audio from those cases and then transcribe it
but but getting that raw audio is really expensive so instead an alternative is
to take the really cheap read speech that's very clean and use some like off
the shores off the source off the shelf open source audio toolkit to synthesize
all the things you want to be robust to so for example if we want to simulate
noise in a cafe here here's just me talking to my laptop in a quiet room
hello how are you so if I'm just asking how are you and then here's the sound of
a cafe so I can obviously collect these
independently very cheaply then I can synthesize this by just adding these signals together hello how are you which
actually sounds I don't know sounds to me like my talking to my laptop at a Starbucks or something
and so for our work on deep speech we actually take something like 10,000 hours of raw audio that sounds kind of
like this and then we pile on lots and lots of audio tracks from Creative
Commons videos it turns out there's a strange thing people upload like noise tracks to the web that last four hours
is like really soothing to listen to the highway or something and so you can
download all all these this free found data and you can just overlay it on this voice and you can synthesize perhaps
hundreds of thousands of hours of unique audio and so the idea here is that it's
just much easier to engineer your data pipeline to be robust than it is to
engineer the speech engine itself to be robust so whenever you encounter an environment that you've never seen
before and your speech engine is breaking down you should shift your instinct away from trying to engineer
the engine to fix it and toward this idea of how do I reproduce it really cheaply in my data so here's that Wall
Street Journal example again is it designed to carry large volumes of oil or other liquid cargo and so if I wanted
to for instance deal with a person reading Wall Street Journal on a tanker maybe taking a ship designed to carry
large volumes of oil or other liquid cargo there's lots of reverb in this room so you can't hear the reverb on the
audio but basically you know you can synthesize these things with one line of socks on the command line so from some
of our own work with building a large scale speech engine with these technologies this helps a ton and you
can actually see that when we run on clean and noisy test utterances as we
add more and more data all the way up to about 10,000 hours and using a lot of
these synthesis strategies we can just steadily improve the performance of the engine and in fact on
things like clean speech you can get down well below 10% word error rate which is a pretty pretty strong engine
okay let's talk about computation because the caveat on that last slide is
yes more data will help if you have a big enough model and big models usually
mean lots of computation so what I haven't talked about is how big are
these neural networks and how big is one experiment so if you actually want to train one of these things at scale what
are you in for so here's the the back of the envelope it's going to take at least
the number of connections in your neural network so take one slice of that are n
n the number of unique connections multiplied by the number of frames once you unroll the recurrent network once
you unfold it multiplied by the number of utterances you've got a process in your data set times the number of training epochs the
number of times you loop through the data set times three because you have to do forward propagation to flops for
every connection because there's a multiplying and add so if you multiply this out for some parameters from the
the deep speech engine if I do you get something like 1.2 times 10 to the 19
flops so about 10 XO flops and if you run this on a Titan X card this will
take about a month now if you already know what the model is that might be
tolerable if you're you're on your epic run to get your best performance so far then this is okay but if you don't know
what model is going to work you're targeting some new scenario then you want it done now so you can try lots and
lots of models quickly so the easy fix is just to try using a bunch more GPUs
with data parallelism and the good news is is that so far it looks like speech
recognition allows us to use mini batch sizes we can process enough utterances in parallel that this is actually
efficient so you'd like to keep you know maybe a bit more than 64 utterances on each GPU
and up to a total mini batch size of like a thousand or maybe two thousand it's still useful and so if you've got
if you're putting together your your infrastructure you can go out and you can buy a server that'll fit eight of
these Titan GP using them and that'll actually get you to less than a week training time which is pretty
respectable so there are a whole bunch of ways to use GPUs if I do we've been
using synchronous SGD it turns out that you've got to optimize things like all
reduce code once you leave one node you have to start worrying about your network and if you want to keep scaling
than thinking about things like network traffic and the right strategy for moving all of your data becomes
important but we've had success scaling really well all the way out to things
like 64 GPUs and just getting linear speed ups all over the way so if you've
got a big cluster available these things scale really well and there are a bunch of other solutions for instance
asynchronous SGD is now kind of a mainstay of distributed deep learning there's also been some work recently of
trying to go back to synchronous SGD that has a lot of nice properties but using things like backup workers so
that's sort of the easy thing just throw more GPUs at it and go faster one word
of warning as you're trying to build these systems is to watch for code that
isn't as optimized as you expected it to be and so this back of the envelope
calculation that we did of figuring out how many flops are involved in our network and then calculating how long it
would take to run if our GPU are running at full efficiency you should actually
do this for your network this we call this the speed of light this is the fastest your code could ever run on one
GPU and if you find that you're just drastically underperforming that number
what could be happening to you is that you've hit a little edge case in one of
the libraries that you're using and you're actually suffering a huge setback that you don't need to be feeling right
now so one of the things we found back in November is that in libraries like Kublai's you can actually use mini batch
sizes hit these weird catastrophic cases in the library where you could be suffering
like a factor of two or three performance reduction so that might take your wonderful one-week training time
and blow it up to say a three week training time so that's why I wanted to
go through this and ask you to keep in mind while you're training these things try to figure out how long it ought to
be taking and if it's going a lot slower be suspicious that there's some code you could be optimizing another good trick
that's particularly speech you can also use this for other recurrent networks is
to try to keep similar length utterances together so if you look at your data set
like a lot of things you have this sort of distribution over possible utterance lengths and so you see there's a whole
bunch that are you know maybe within about 50% of each other but there's also a large number of utterances that are
very short and so what happens is when we want to process a whole bunch of
these uh pterence --is in parallel if we just randomly select say a thousand
utterances to go into a mini batch there's a high probability that we're going to get a whole bunch of these
little short utterances along with some really long uh pterence --is and in order to make all the ctc libraries work
and all of our recurrent Network computations easy what we have to do is pad these audio signals with zero and
that lines up meaning that we're wasting huge amounts of computation maybe a factor of two or more and so one way to
get around it is just sort all of your utterances by length and then try to
keep the mini-batches to be similar lengths so that you just don't end up with quite as much waste in each MIDI
batch and and this kind of modifies your your algorithm a little bit but in the
end is worthwhile all right this is kind of all I want to say about computation
if you're if you've got a few GPUs keep an eye on your running time so that you
know what to optimize and pay attention to the easy wins like keeping your utterances together you can actually
scale really well and I think for a lot of the jobs we see you can have your
your GPU running at something like 50% of the peak and that's all in with
network time with all the bandwidth bound stuff you can actually run a two to three teraflops on a GPU that can
only do five teraflops in the perfect case so what can you actually do with
this I one of my favorite results from one of our largest models is actually in
Mandarin so we have a whole bunch of labeled Mandarin data if I do and so one of the things that we did was we scaled
up this model trained it on a huge amount of Mandarin data and then as we always do we sit down and we do error
analysis and what we would do is have a whole bunch of humans sitting around try
to debate the transcriptions and figure out the ground truth that tend to be very high quality and then we go and
we'd run now a sort of holdout test on some new people and on the speech engine itself and so if you benchmark a single
human being against this deep speech engine in Mandarin that's powered by all
the technologies we were just talking about it turns out that the speech engine can get an error rate that's down
below six percent character error rate so only about six percent of the characters are wrong and a single human
sitting there listening to these transcriptions actually does quite a bit worse it's almost ten percent if you
give people a bit of an advantage which is you going to you now assemble a committee of people and you get them a
fresh test set so that no one has seen it before and we run this test again it turns out that the two engines are that
the two cases are actually really similar and you can end up with a committee of native Mandarin speakers sitting around debating no no I think
this person said this or no they have an accent it's from the north I think they're actually saying that and then
when you show them the deep speech transcription they actually go ah that that's what it was and so you can
actually get this technology up to a point where it's highly competitive with human beings even human beings working
together and this is sort of where I think all the speech recognition systems are heading thanks to deep learning and
the technologies that we're talking about here any questions so far
yeah go ahead yep sorry yeah so the
question is if humans have such a hard time coming up with the correct transcription how do you know what the truth is and the real answer is you
don't really sometimes you might have a little bit of user feedback but in this instance we have very high quality
transcriptions that are coming from many labelers teamed up with a speech engine and so that could be wrong we do
occasionally find errors where we just think that's a label error but when you have a committee of humans around the
the really astonishing thing is that you can look at the output of the speech engines and the humans will suddenly
jump ship and say oh no no no no this each engine is actually correct because
it'll often come up with an obscure word or place that they weren't aware of yeah
so so this is a you know an inherently ambiguous result but let's say that a
community of human beings tend to disagree with another committee of human beings about the same amount as a as a
speech engine does yeah yeah so this is
a so this is using the CTC cost right that's really the core component of this
system it's how you deal with mapping one variable length sequence to another and the CTC cost is not perfect it has
this assumption of Independence baked into the probabilistic model and because
of that assumption we're introducing some bias into the system and for languages like English where the
characters are obviously not independent of each other this might be a limitation in practice the thing that we see is
that as you add a lot of data and your model gets much more powerful you can still find your way around it but it
might take more data and a bigger model than necessary and of course we hope that all the new
state-of-the-art methods coming out of the deep learning community are going to give us an even better solution okay
right
empirically determined yeah so the question is for a spectrogram with we
talked about these little spectrogram frames being computed from 20 milliseconds of audio and is that number special is there a reason for it so this
is really determined from years and years of experience this is captured from the traditional speech community we
know this works pretty well there's actually some fun things you can do you can take a spectrogram go back and find
the best audio that corresponds to that spectrogram to listen to it and see if
you lost anything and spectrograms of about this level of quantization you can kind of tell what
people are saying it's a little bit garbled but it's still actually pretty good so amongst all the hyper parameters
you could choose this one's kind of a good trade-off in keeping the information but also saving a little bit
of the phase by doing it frequently yeah
I think in a lot of the models the in the demo for example we don't use
overlapping windows they're just adjacent yeah
yeah so those results are from from in-house software it Baidu if you use
something like open MPI for example on a cluster of GPUs actually works pretty
well on a bunch of machines but I think
some of the algorithms like all reduce once you start moving huge amounts of data they're not optimal you'll suffer a
hit once you start going to that many GPUs within a single box if you use the
CUDA libraries to move data back and forth just on a local box that stuff is pretty well optimized and you can often
do it yourself okay so I want to take a few more questions at the end and maybe we can run into the
break a little bit I wanted to just dive right through a few comments about production here so of course the
ultimate goal of solving speech recognition is to improve people's lives and enable exciting products and so that
means even though so far we've trained a bunch of acoustic and language models we
also want to get these things in production and users tend to care about more than just accuracy accuracy of
course matters a lot but we also care about things like latency users want to see the engine send them some feedback
very quickly so that they know that it's responding and that it's understanding what they're saying and we also need
this to be economical so that we can serve lots of users without breaking the bank so in practice a lot of the neural
networks that we use in research papers because they're awesome for beating benchmark results turn out not to work
that well on a production engine so one in particular that I think is worth
keeping an eye on is that it's really common to use bi-directional recurrent neural networks and so throughout the
talk I've been drawing my RNN with connections that just go forward in time but you'll see a lot of research results
that also have a pass that goes backward in time and this works fine if you just
want to process data offline but the problem is that if I want to compute
this neurons output up at the top of my network I have to wait until I see the entire audio segment so that I can compute this
backward recurrence and get this response so this sort of anti causal
part of my neural network that gets to see the future means that I can't respond to a user on the fly because I
need to wait for the end of their signal so if you start out with these
bi-directional rnns that are actually much easier to get working and then you jump to using a recurrent network that
is forward only it'll turn out that you're going to lose some accuracy and you might kind of hope that CTC because
it doesn't care about the alignment would somehow magically learn to shift the output over to get better accuracy
and just artificially delay the response so that it could get more context on its own but it kind of turns out to only do
that a little bit in practice it's really tough to control it and so if you find that you're doing much worse
sometimes you have to sort of engage in model engineering so even though I've been talking about these recurrent
networks I want you to bear in mind that there's this dual optimization going on
you want to find a model structure that gives you really good accuracy but you also have to think carefully about how
you set up the structure so that this little neuron at the top can actually see enough context to get an accurate
answer and and not depend too much on the future so for example what we could
do is tweak this model so that this neuron at the top that's trying to output the character L and hello can see
some future frames but it doesn't have this backward recurrence so it only gets to see a little bit of context that lets
us kind of contain the amount of latency in the model you skip over this so in
terms of other online aspects of course we want this to be efficient right we
want to serve lots of users on a small number of machines if possible and one
of the things you think you might find if you have a really big deep neural network or recurrent neural network is that it's really hard to deploy them on
conventional CPUs CPUs are awesome for or serial jobs you just want to go as
fast as you can for this one string of instructions but as we've discovered with so much of deep learning GPUs are
really fantastic because when we work with neural networks we love processing lots and lots of arithmetic in parallel
but it's really only efficient if the batch that we're working on the hunks of audio that we're working on are are in a
big enough batch so if we just process one stream of audio so that my GPU is multiplying matrices times vectors then
my GPU is going to be really inefficient so for example unlike a K 1200 GPU this
is something you could put in a server in the cloud what you'll find is that you get really poor throughput
considering the the dollar value of this Hardware if you're only processing one
piece of audio at a time whereas if you could somehow batch up audio to have say 10 or 32 streams going at once then you
can actually squeeze out a lot more more performance from that piece of hardware so one of the things that we've been
working on that works really well is not too too bad to implement is to just
batch all of the packets as data comes in so if I have a whole bunch of users talking to my server and they're sending
me little hundred millisecond packets of audio what I can do is I can sit and I
can listen to all these users and when I catch a whole batch of utterances coming in or a whole bunch of audio packets
coming in from different people that start around the same time I plug those all into my GPU and I process those
matrix multiplications together so instead of multiplying a matrix times only one little audio piece I get to
multiply it by a batch of say four audio pieces and it's much more efficient and
if you actually do this on a live server and you plow a whole bunch of audio
streams through it you could support maybe 10 20 30 users in parallel and as
the load on that server goes up I have more and more users piling on what happens is that the GPU will naturally
start batching up more and more packets into single matrix multiplications so as
you get more users you actually get much more efficient as well and so in
practice when you have a whole bunch of users on one machine you usually don't see matrix multiplications happening
with fewer than maybe a batch sizes of four so the summary of all of this is
that deep learning is really making the the first steps to building a
state-of-the-art speech engine easier than they've ever been so if you want to build a new state-of-the-art speech engine for some new language all the
components that you need are things that we've covered so far and the performance
now is really significantly driven by data and models and I think as we were discussing earlier I think future models
from deep learning are going to make that influence of data and computing power even stronger and of course data
and compute is important so that we can try lots and lots of models and keep making progress and I think this
technology is now at a stage where it's not just a research system anymore we're
seeing that the end end deep learning technologies are now mature enough that we can get them into productions I think
you guys are going to be seeing deep learning play a bigger bigger role in the speech engines that are powering all
the devices that we use so thank you very much
I think we're right at the end of time sounds good
alright we had one in the back who's waiting patiently go ahead more than one
voice simultaneously so the question is how does the engine handle more than one voice simultaneously so right now
there's nothing in this formalism that allows you to account for multiple speakers and so usually when you listen
to an audio clip in practice it's clear that there's one dominant speaker and so
this beach engine of course learns whatever it was taught from the labels and it will try to filter out background
speakers and just transcribe the dominant one but if it's really ambiguous then then undefined results
you customize the transcription to the specific characteristics of a particular
speaker so we're not doing that in these
pipelines right now but of course a lot of different strategies have been
developed in the traditional speech literature there are things like I've Ector 'z that try to quantify someone's
voice and those make useful features for improving speech engines you could also imagine taking a lot of the concepts
like embeddings for example and tossing them in here so I think a lot of that is left open to future work I do a question
button I think we have to break for time but I'll step off stage here and you
guys can come to me with your questions thank you so much
so we'll reconvene at 2:45 for presentation by Alex

----------

-----

--09--

-----
Date: 2016.09.27
Link: [# Deep Reinforcement Learning (John Schulman, OpenAI)](https://www.youtube.com/watch?v=PtAIh9KSnjo)

Notes:
### Advantages

- General applicability to a variety of tasks and environments.
- Ability to handle continuous and complex state spaces using neural networks.
- Flexibility in learning policies for decision-making.
- Capability to approximate various components (policy, value functions) effectively.
- Potential for high performance in complex domains like robotics and game playing.

### Drawbacks

- Sample inefficiency, especially in environments where obtaining data is costly or slow.
- High variance in policy gradient methods, leading to instability in learning.
- Complexity in tuning and selecting appropriate algorithms for specific problems.
- Limited interpretability of learned policies and value functions.
- Difficulty in generalizing learned policies to slightly different tasks or environments.

### Tips and Advice

- Consider the trade-offs between policy gradient methods and Q-learning based approaches.
- Use experience replay and target networks to stabilize learning in Q-learning methods.
- For policy gradient methods, employing techniques like trust region policy optimization can enhance stability.
- Incorporate exploration strategies effectively to balance the exploitation-exploration trade-off.
- Regularly evaluate and adjust the reward function to align with desired outcomes.

### Lecture Content

- Introduction to deep reinforcement learning (RL) and its core techniques: policy gradient methods and Q-learning.
- Overview of reinforcement learning basics, including agents, environments, and the goal of maximizing cumulative rewards.
- Discussion on the applications of deep RL in various domains such as robotics, inventory management, and machine learning problems like attention mechanisms and structured prediction.
- Explanation of Markov Decision Processes (MDPs) as the foundational model for RL tasks.

### Main Challenges

- Sample inefficiency and the high computational cost of training models.
- Stability and convergence issues, particularly with policy gradient methods.
- Designing reward functions that accurately reflect the objectives without unintended consequences.
- Generalization across different tasks or slight variations in the environment.

### Importance and Usefulness

- Deep RL offers powerful tools for automated decision-making and optimization in complex environments.
- It has shown remarkable success in domains like game playing, robotics, and automated control systems.
- The ability to learn from interaction with the environment without explicit programming for each task makes deep RL a versatile approach for a wide range of applications.

### Accomplishments

- Achievements in playing Atari games and beating champion-level players at Go through deep RL methods.
- Advancements in robotic manipulation and locomotion tasks, demonstrating the practical utility of deep RL in real-world applications.

### Summary of Content

- The talk provided a comprehensive overview of deep reinforcement learning, covering its definition, applications, and core methods.
- It highlighted the advantages and drawbacks of policy gradient methods and Q-learning, providing insights into when and how to apply these techniques effectively.
- The importance of deep RL in solving complex decision-making problems was emphasized, along with notable achievements in various domains.

### Interesting Quotes/Insightful Sentences

- "Reinforcement learning is about taking the right actions to maximize a cumulative reward in an environment, making it a highly general problem-solving framework."
- "Deep reinforcement learning combines the generality of reinforcement learning with the power of neural networks, enabling us to tackle previously intractable problems."
- "Choosing between policy gradient methods and Q-learning involves balancing between sample efficiency and the ease of implementation and understanding."

Transcription:

so good morning everyone so I'm going to
talk about some of the core methods in deep reinforcement learning so the aim
of this talk is as follows first I'll do a brief introduction to what deep RL is
and whether it might make sense to apply it in your problem I'll talk about some
of the core techniques so there on the one hand we have the policy gradient
methods then on the other hand we have methods that learn a cue function
including cue learning and sarsa and I'll talk a little at the end about what
are the pros and cons of these different methods so first what is reinforcement
learning it's a branch of machine learning consider concerned with taking sequences of actions so often it's
described in terms of an age and inter interacting with the previously unknown environment and it's trying to maximize
some kind of cumulative reward some kind of reward function that we've defined accumulated over time and pretty much
any kind of task where you have some kind of goal that you want to achieve can be stated in these terms so this is
an extremely general formulation what's
deep reinforcement learning it's pretty simple it's just reinforcement learning where you're using neural networks as
function approximator x' so the interesting thing about reinforcement
learning in contrast to supervised learning is it's actually not totally obvious what you should use your neural
network to approximate in reinforcement learning and there are different kinds of algorithms that approximate different
things so one choice is to use the neural network to approximate your policy which is how the agent chooses
its actions another choice is to approximate the value functions which measure how good or bad different states
are or have or actions and last you can use the you can try to learn a model of
the system a dynamics model which will make predictions about neck States and rewards okay so I'll now give
a few examples of different different places where you might apply reinforcement learning and what the
observations and actions would be so one example is robotics so here you could
imagine a robot where the observations are the camera images and the joint angles of the robot the actions are the
joint torques you're applying and the reward is going to depend on what you
want the robot to do so so this is something we as the algorithm designer
get to define so the rewards could be to stay balanced to navigate to some target
location or something more abstract like serve and protect humans so
reinforcement learning has also been used in a lot of more practical applications
well applications that have been practical in the past I think robotics will be very practical in the future but
for example when a one area is inventory
management so this is just one example of how you could use reinforcement learning for a decision making problem
so you you have to decide how much to stock up on of every item and your
observations would be your current inventory levels actions would be how much of each item you're going to
purchase and reward is your profit so people in operations research this is
this is a subfield study this kind of problem a lot ok there are also a lot of
machine learning problems where people have started to apply reinforcement learning techniques so one example is
attention so the idea and attention is you don't want to look at the whole input at once you want to just focus on
part of it so one example of this is with a large image you might want to
just crop out part of it and use that and just do detection on that part of the image so
here your observation would be your current image window action is where to look or where to crop your image and
reward is your whether you make a classification error or not
so here the actions are trying to here
you have to try to choose the right area of the image to look at so you'll do the correct classification reinforcement
learning has also been used in structured prediction problems which
having which in the past often weren't considered to be reinforcement learning problems but it turns out that like to
actually properly solve them it actually is a reinforcement learning problem so machine translation for example you so
you get a sort a sentence in the source language and you have to admit a sentence in this target language and you
can hear your observations are the sentence in the source language you're omitting one word at a time in the
target language and you have some reward function that looks at the whole sentence and tells you how good your
translation was so since this is non differentiable and it's you yeah you
can't just like differentiate through the whole thing and do gradient descent so it turns out you have to do you can
use a policy gradient method to optimize your translation system so people have started to do that ok so that's just
those are just a few examples not exhaustive at all but I just want to
since I just want to say a little bit about how reinforcement learning fits into the fits into the picture of all
the other types of machine learning problems so previous I mean previous
courses in this series have talked about supervised learning and supervised learning so how does reinforcement
learning relate to them how is it different so let's just first compare it
to let's look at supervised learning so in supervised learning first the environment samples and input/output
pair from some distribution row the agent makes a prediction y hat using
its function f and it receives some loss which tells it if it made the right
prediction of the wrong prediction so the interpretation is environment ask
the agent a question and then tells her the right answer so contextual bandits
are make this problem a little harder in that they give the learning agent a
little bit less information so now the environment samples and input but notice
that there's not a correct output associated with it then the agent takes an action and the agent receives some
cost which is from some probability distribution so here C is the cost we're
sampling it from some probability distribution and the agent doesn't know what this probability distribution is so
that's what makes the problem hard so environment ask the agent a question the
agent answers and the environment gives you a noisy score on the answer so this
is applied this actually has a lot of applications so personalized recommendations is one big one along
with advertising so you have to decide like customers who like this
I mean you for you have a customer and you know what they liked in the past so you have to make a prediction about what
they're going to like in the future so you show them appropriate ads or links like what either
like what ad what book you want to try to advertise to them or what video you want to show them and so on so here you
can the big difference between this and the supervised learning setting is you don't have access to the function the
loss function you're trying to optimize so in particular you can't differentiate through it we don't know the process
that generates C so we can't compute the gradient of a loss function and use that to tune the agents parameters so that
makes it so that makes the problem a bit harder or you have to use a different kind of algorithm
lastly reinforcement learning is almost the same as the contextual
and it's setting except now the environment is stateful so now instead of sampling the initial state from
scratch every time step from the same distribution the state evolves over time
so you have some transition probability distribution called P here where the the
state X sub T is conditioned on the previous state and the previous action
and that makes the problem quite a bit harder because now well for a number of
reasons for one thing the inputs you're getting depends on what actions you're taking so now that makes it harder to
develop a stable reliable algorithm because now as the agent starts to learn it gets different inputs so that can
lead to all sorts of out-of-control behavior and it also means you have
delayed effects because since the system is stateful you might need to take a lot
of actions to get into the right State so you might need to you can't just act greedily every time
step you have to you have to think ahead effectively okay so just to summarize
these differences there are two differences the first one is you don't have full analytic access to the
function you're trying to optimize you have to query it through interaction second you're interacting with a
stateful world which means that the input you get is going to depend on your previous actions and if you just take
the first of those differences between supervised learning and reinforcement learning you get the contextual bandit
setting so that's sort of halfway in between okay so I realized that there
multiple this audience probably has people with different interests some people are doing research and want to
know about what's the latest in the research world and some people are want
to apply these machine learning techniques to practical applications so this slide is for the latter group of
people so if you're wondering if you have some problem where you think reinforcement learning might be relevant
and you're one if you should apply reinforcement learning so first I should say that the
answer might be no it might be overkill especially deep reinforcement learning so this is a set of fairly new
techniques where it's not going to work out of the box very well and it's these
techniques aren't that well established so they require a lot of they have a lot of knobs to be tuned so it might be
overkill and yeah these techniques aren't that well established at the moment so it might be worth
investigating some other methods first so one one so if your problem has a
small number of parameters you're trying to optimize over and you have a simulator that you can like just do lots
of experiments on then derivative free optimization methods are likely to be
better than reinforcement learning or they're likely to be easier to get working so these methods just look at
they just you give them a black box function where you put in a parameter vector and it'll give you a noisy
estimate of the score and these algorithms will just optimize over the
parameters of that black box I mean that are being put into that black box so
yeah there's a variety of different methods for derivative free optimization but these are easier to understand than
reinforcement learning and they do kind of work out of the box okay a lot of
problems are actually can be seen as content are contextual bandit problems
and the statefulness of the world isn't that relevant so for example in advertising this is where people people
look at advertising as a contextual bandit problem most of the time because you decide what ad to present the user
with and then they either click on it or they don't but it's really the user is
kind of stateful because if you show them a terrible ad they might just go and download adblock so there is like
your actions do have some repercussions but often you can just approximate it as
being a contextual bandit problem where there is no so there's a better theoretical
understanding of contextual bandit problems and methods that are that have some guarantees so in that case so if it
is a contextual bandit problem you might want to use those kind of algorithms instead and lastly the operations
research field has been using these methods for a while and real problems
and they have a set of methods which are just pretty much the basic algorithms
policy iteration and value iteration but they're sort of well they're well
developed ways of doing feature engineering for these problems that end up working pretty decently so these
techniques are also worth considering instead of trying to throw a big neural network at it okay so now well now that
I've talked about what why not to use deep reinforcement learning or what it's not good for I'll just talk about some
recent success stories and deep reinforcement learning which are achievements that probably wouldn't have
been possible using these other techniques so a few years ago there is a
pretty influential result by many at all from deep mind where they used a deep
queue learning algorithm to play Atari games using the screen images as input
and that's hard because you have these get these games or you're trying to do
different things in all these games and there some of them are kind of complicated so it's pretty remarkable that you can just use a simple but a
simple algorithm can solve them all this the same algorithm can solve them all so
since then people have also solved or solved this domain using policy
gradients and another algorithm called dagger so another big groundbreaking
result was beating a champion level player at go also by deep mind using a
combination of supervised learning from like from expert games plus policy gradients to
fine tune the supervised learning policy plus Monte Carlo tree search plus value
functions to make the search work better so a combination of techniques in reinforcement learning robotic so some
of my colleagues at Berkeley had some very nice results learning in real time how to do manipulation tasks using an
algorithm called guided policy search using the pr2 robot and some of my
colleagues and I have been working on robotic locomotion using policy gradient
methods and people have been working on
locomotion for a while and have been able to achieve pretty good results using very like highly engineered
domain-specific methods but previously there hadn't been much success using
general methods to solve it and last there have been some recent results
playing 3d games using policy gradients in fact there is even a contest I heard
about a couple days ago with this new viz Doom task which is pretty nice so
you might want to check out visit to meeeee ok so that's that's it for the
high level overview part of this now I'm going to start getting into the actual
formalism and the technical details ok
so the basic object in the field of
reinforcement learning is the Markov decision process so the Markov decision process is defined by the following
components you have a state space this is all the different states of the system the action space these are all
the actions the agent can take and you have this probability distribution which
which determines the probability of next state and reward so R is the reward s
prime is the next state SN a are the actions so it's a conditional probability distribution sometimes
people let this out into a separate reward function but that's basically an equivalent formulation okay and
sometimes there's some extra objects to find it we'll be interested in the will
consider in it an initial state distribution so this is on the world starts out in a certain state and the
typical optimization problem you want to solve given this MDP is to maximize expected cumulative reward though there
are various ways of defining that more precisely which I'll go into later ok so
there are various different settings of reinforcement learning where you define a slightly different optimization
problem the one will be most concerned with is called the episodic setting so here the agents experience is split up
into a series of episodes which have finite length so in each episode we
first sample the initial state of the world from some probability distribution view and then the agent keeps on acting
until the world ends up in some terminal state so just to give an example of what
terminal States might be like and how an episode episode ik reinforcement learning problem might look so one
example is when termination is good and you want to terminate the episode as fast as possible so if we imagine
setting up a task with some kind of taxi robot that should get to the destination as fast as possible then the episode
would be like one trip and its terminate it's trying to terminate the episode as
fast as possible another example is a
waiter robot where you have a fixed length shift but the waiter has to
accumulate it has to do as well as possible during that shift so there the episode has a fixed length the waiter
has to say maximize tips or a customer happiness and then you could imagine
another kind of task where termination is bad and you want the episode to last as long as
possible so you can view life as an example of that but also you could
imagine having a walking robot where you want it to walk as far as possible
before it falls over and in this setting
it's pretty easy to find to define what the goal is - we just want to maximize
the expectation of the total reward per episode okay and the last object we're
going to introduce here is a policy so the policy is just the function that the
agent uses to choose its actions so we have deterministic policies which are
just the policy is denoted by PI so we have the action is just some function of the state and we also have stochastic
policies where the policy is a conditional probability distribution so
here is just we're just going to make a little bit more precise on the setting of the episodic MVP so first we sample
the initial state from this distribution mu then we then we get we sample the
first action from the policy a zero from the policy then we sample next state and reward from the transition probability
distribution and so on until we reach a terminal state S sub T and then the
quantity we care about is the sum of all these rewards are 0 plus R 1 dot plus R
sub t minus 1 and we want to maximize yet so ADA is ADA of Pi is just defined
as the expected total reward of the policy PI here's the picture that
illustrates exactly the same thing so you can look at it as a graphical model
ok and lastly in the policy gradient section in particular we're going to be
interested in parameterised policies so here we have a parameter vector theta which specifies which specifies exactly
what the policy is so for sample the family of policies could be just a neural net we you have a certain
neural network architecture and theta specifies all the weights of this neural network so we could have a deterministic
policy of course or stochastic policy and if you're wondering like concretely
what where the policy look like I mean how do you use a neural network to represent your policy it's actually
exactly you do exactly the same thing you would do if this were a classification or a regression problem
so in so s here the state here is your input and the action is your output so
if you have a discrete action space a discrete set of actions then you would
use a network that outputs a vector of probabilities the probabilities of the different actions this is exactly like a
classifier and if you have a continuous action space your you would have your
neural network output the mean and the diagonal of a covariance matrix of a Gaussian distribution so this is just
like you're doing regression so you can use the same kind of architectures you'd use in supervised learning okay so
that's that's just the that's it for the formalism of MVPs so now I'm going to go
into policy gradient methods which are one broad and general class of
reinforcement learning methods which are quite effective so to give a brief
overview of this here's here's the intuition of what policy gradient
methods are going to do so here capital R means the sum of rewards of the whole
episode so our optimization problem is we want to maximize the expectation of
the total reward given our parameterised policy PI sub theta and the intuition of
how our algorithm is going to work is we we're going to collect a bunch of trajectories I mean this is just run a
bunch of episodes using our policy and then we want to make the good trajectories more probable so I mean
some of the edge trees were lucky and they were really good some of them the agent was unlucky and they are bad and the good the ones
that were good meaning there was high reward that means the agent probably took good actions there so we want to
increase the probability of the actions from those trajectories so so the most
basic version of policy gradient methods just try to make the good trajectories more probable without trying to figure
out which were the good actions in which were the bad actions slightly better methods or more elaborate methods try to
figure out which actions were good and which ones are bad and then they try to make the good actions more probable and
lastly there's another class of methods which which actually try to push the actions towards better actions so they
differentiate the loss function with respect to the actions and they try to push the actions to better actions so
we're mostly going to talk about 1 & 2 here oh there's a question oh yeah good
questions so well we're maximizing over the policy we're trying to find the best
policy but here the policy is assumed to be parametrized so there's some parameter vector theta that specifies
the policy and now we just want to maximize with respect to theta any other
questions ok so there's a very very
fundamental fundamental concept which is called the score function grading estimator which underlies policy grading
methods so actually to introduce this we're not going to talk about policies in RL at all we're just going to assume
we have some expectation we have expectation of f of X where X is sampled
from some parametrized probability distribution so we want to compute the
grading of this expectation with respect to theta so there's a general formula that will do this and the way you derive
it is you just write the expectation as an integral and then you just move some things
around you swap the integral with the derivative and you you turn it back into
an expectation and what you get at the end is this bottom line which says that you take the expectation of function
value times grad log probability so the this is an unbiased estimator of the
gradient meaning if we get enough samples it'll converge on the right thing so the way you can compute this
estimator meaning the way you can get a noisy estimate of the grading of the expectation is you just collect one you
just get one sample from this distribution and then you compute then you multiply f of X times grad log
probability so the only requirement for
being able to use this estimator is we need to be able to compute the probability density I mean we need to be
able to analytically compute it and we need to be able to differentiate it with respect to theta and often it needs to
be differentiable there's another way of
deriving it using important sampling so you write down the important sampling estimator for the expectation and then
you just swap the derivative with the expectation and you get the same thing okay so so now let me just give a little
bit of intuition about this estimator
okay so f of X is measuring how good the sample X is so that means that so G hat
here is our grading estimator meaning this is what we get if we take one sample X sub I and we compute our
estimator this is our estimate of the gradient so if we move in Direction G
hat that pushes up the log probability of our sample X sub I in proportion to
how good it is so if we have really good if we got a really good function value
then we're going to try to push up its log probability a lot and if it was a bad function value then we're not going
to try to push it up very much so pretty simple intuition the really nice thing
is this is valid even if f of X is discontinuous or if f of X is unknown
meaning you only you don't get to differentiate it you just get to see the function values or the sample space is a
discrete set so X doesn't even have to be continuous and this is quite this is
quite remarkable actually that you don't even need to have access to the full you
don't need to know exactly what the function is that you're optimizing you just have to be able to query it for the
function value and this means this is a way of being able to differentiate
functions through a system that has non differentiable pieces so for example in
in robotic locomotion one issue is that you have contacts between the robot's
foot and the ground and contact you make and break contact and that causes a
discontinuous change in the dynamics so that makes it really hard to do smooth optimization techniques to come up with
the right behavior so when you use this kind of grading estimator along with policy gradients which I'm going to talk
about very soon you can actually just differentiate you can optimize the
system even though it has differentiable pieces in it okay so okay so here's
another little picture of what's going on so we have our function f of X which
we're trying to maximize the expectation of and then we have our probability that's the DP of X so we just sample a
bunch of values from our probability density those are the blue dots on the x axis and then we so then we we look at
the function values and we're trying to push the probability distribution so
that the probability goes up at these samples in proportion to the function
value so over on the right side of the curve that means we're
trying to push that probability value up really hard and on the left side we're pushing it up softly so what's going to
happen is the probability density is going to slide to the right if you can
imagine a sort of physical analogy there okay so that's that's the score function
grading estimator this is a general technique it can be used in various
machine learning problems now we're going to apply it to the reinforcement
learning setting and we're going to take our random variable X to be a whole
trajectory so the trajectory consists of state action reward state action reward
and so on until the end of the episode and now to get our gradient estimator to
get the do to get the gradient of the expected reward all we've got to do is
compute the grad log probability times the total reward so so this probability
of the trajectory that sounds like a really unfriendly quantity because there's a long complicated process that
generates this trajectory with lots of lots of time steps but logged okay so we
can write out what this process is what this probability density is so we have it's just a product of probabilities
we've got our initials we've got our mu if s 0 which is just our initial state
distribution and then every time step we have we sample the action according to PI and we sample the next state and
reward according to our dynamics model so log turns that product into a sum and
here's the cool part everything that doesn't contain theta drops out so the
thing is we didn't know there are parts of this probability distribution P of
tau given theta that we don't have access to so if this is reinforcement learning we don't assume that we know
the dynamics model of the system we just find out about it by sampling by doing
sample doing episodes so nice so since this
product turns into a sum all the pieces like the log log P there and the log mu
which we don't know just drop out so it doesn't matter and what we get in the
end is we get a sum of log problem sum of log probabilities of actions so grad
log PI of action given State so our formula looks like our formula for the
grading of the expectation is just the expectation over trajectories of total
reward of the trajectory times grad grad of the sum of all the log probs so the
interpretation of this is we're taking our good trajectories and we're trying
to increase their probability and proportion to how good they are and you can think of this as being similar to
supervised learning where we treat the good trajectories with high rewards as positive examples in our supervised
learning problem so we're using those to train the policy on what which actions
are good we're basically treating those actions as positive examples okay now we
can improve this formula a little bit so that was just the most basic I mean this
is an unbiased estimator for the policy gradient so if we just take that expression inside the expectation on the
right hand side and we take one sample of that it has the right mean so if we
just get enough of them we're going to get the policy gradient okay so that's but we can also write down some other
formulas that have the same mean but have lower variance so we can come up with better estimators for the policy
gradient and that's actually quite important because the one from the previous slide is really bad when you
have along a large number of time steps meaning it has really high variance so
the first thing we can do is you we can use the temporal structure of the problem by the way
to derive these next bunch of formulas it just takes a bunch of really straightforward manipulation where you move around expectations and I'm not
going to go through all the math but I'll just say what the formulas are so
okay so we can repeat the same argument from the previous slide to just derive
the grading estimator for a single reward term so we end up with that reward term times the grad sum of log
probs and just summing over that we get a new formula where we're not
multiplying the sum of the the grad log probable thing times the sum of all
rewards now so let's look at that bottom formula now we have a sum over time of
grad log probability of the action at ty at that time times the sum of future
rewards um so so now I mean in the formula from the previous slide we would
have had all the rewards in that sum but now we just have the future rewards and
that kind of makes sense because an action can't affect the probability of the previous rewards so to figure out if
the action is good we should have only we should only be looking at the future rewards so this is a slightly better
formula than the one on the previous slide meaning it has the exact same mean except different the expression inside
the expectation there has lower variance and we can further reduce the variance by introducing a baseline so now we can
take any old function B which takes in a state and it outputs a real number and
we can subtract it from our sum of future rewards and we didn't affect the
mean of the estimator at all so we yeah we didn't change the expectation at all
by introducing this baseline so yeah for
any choice of B this gives us an unbiased estimator by the way if you're not that familiar with the terminology
of estimators what I'm saying is we have a expectation on the right hand side of
that for a formula and the quantity inside that expectation is what's called
the estimator and if we get a bunch of samples then we can get an estimate of of the thing on the left-hand side which
is what we care about so so when I say it's an unbiased estimator that just
means that well that just means that this equation is correct meaning that the thing on the right hand side equals
the thing on the left hand side so yeah this works for any choice of baseline
and a near optimal choice is to use the expected return so the expected sum of
future rewards and the interpretation of
that is if we took an action we only want to increase the probability of the
action if it was a good action so how do we tell if it was a good action well the sum of her words after that action
should have been better than expected so the B of S is the expected sum of
rewards and we're just taking the difference between the measured thing and the expect thing yeah okay so that's
okay that's the that was a pretty key thing for variance reduction and I'm
going to introduce one last variance reduction technique and actually all three of these are really important so
basically nothing's going to work except for maybe really small scale problems unless you do these things so the last
variance reduction technique is to use discounts so the discount factor ignores
delayed effects between actions and rewards so what we're going to do here looks kind of like a hack but there's an
explanation for it which is instead of taking the sum of rewards we're going to
take a discounted summer of rewards meaning that we we add this exponential
factor gamma so that when so when we're multiplying the grad log probability by
some future award we multiply it by some quantity that decays with time so people
typically use like equals point 99 or gamma equals point 95 so that means like if you use point 99
that means after a hundred time steps you're going to be you're going to be
reducing the reward by a factor of one over e so so you're exponentially you're
decaying the effect of the future rewards and the intuition is that an action the action shouldn't affect
rewards really far in the future like the system should this is this is like
the assumption is that the system doesn't have really long term memory and it's sort of reset sits or the there
aren't effect the effects aren't that far delayed so you can just ignore the interaction between action and a reward
way way in the future that's the intuition so now instead of
taking the baseline to be the expected sum of future rewards we want to do a discounted sum so now we're measuring if
the action was better than expected according to this like the according to the discounted sum and now there's a
more general class of formulas that looks like the one that I just wrote so this this one that's on the top of the
slide is pretty good and this is like almost as good as anything you're going to do to within a small constant factor
but there's there's a more general class of formulas that look like grad log
probability times some quantity a hat which we call the advantage estimate and
this is in general just going to be an estimate of this isn't it has a more
precise definition which is how much how like how much was this action better
than the average action taken by the policy but in but informally this just
means how much better was the action than expected so and and this formula
makes a lot of sense because we want to increase the probability of the good actions and decrease the probability of the bad ones
so we should we should increase it in proportion to the goodness of the action
okay so just to summarize so I just told you there's this gradient estimator meaning there's this expression you can
compute which gives you a noisy estimate of the policy gradient so how do you actually turn this into an algorithm so
this is silly algorithm seven so so
here's what the algorithm looks like it's pretty much what you'd expect you you take your policy you initialize your
policy parameter and your baseline function you for each each iteration you
execute the the current policy to get a bunch of whole episodes meaning whole
trajectories and each time step in the trajectory you should compute the return
meaning the sum of rewards following that time step there's some of discounted rewards and the advantage
estimate which is the sum of discounted rewards minus the baseline then you
refit the baseline by trying to make the baseline function equal of the returns and then you update the policy using a
policy grading estimator so you're just doing SGD while updating the baseline as you go along
yeah so that's that's the vanilla policy grading algorithm and this is I'll
briefly talk this has been used to obtain some pretty good results so it's not that bad of an algorithm but they're
there several different directions that it can be improved so one one issue that
you run into is with step sizes so in supervised learning step sizes aren't
that big of a deal because maybe you take too big of a step but that's okay
you'll fix it the next update and your current function your current classifier
for example doesn't affect what inputs you're getting so even if you just are doing really even if your network is
just kind of thrashing around for a while because you're taking two big steps that's not going to cause any problems but
yeah and reinforced so yeah so step sizes aren't that big of a deal you can just anneal them you can start off with
a large step size and anneal them down to zero and that works pretty well in
reinforcement learning if you take too big of a step you might wreck your policy and even if you don't actually
change the network that much so you don't lose all your nice features you
you might just change its behavior a little too much and now it's going to do something totally different and visit a
totally different part of state space so since in reinforcement learning the system is stateful and your state
distribution depends on your policy that makes that like brings a really a
different problem and now like after you took that step the next batch of data
you're going to get was collected by the bad policy and now you're never going to recover because you just forgot
everything yeah so one way that my
colleagues and I well one way to fix this is to try to to try to stop the
basically try to stop the policy from taking too big a step so you can look at
the KL divergence between the old policy and the new policy like before the
update and after the update and make sure that the distributions aren't that different so you're not taking too big
of a step it's kind of an obvious thing to do so my colleagues and I developed an algorithm called trust region policy
optimization which looks at the yeah it looks at the action distributions and tries to make sure the KL divergence
isn't too large and there's this is very closely related to previous method
natura policy gradient methods which which are based on which are doing is
something similar but usually it's not set up as a hard constraint on the KL divergence so another type of extension
of policy gradient methods is to do more to use value value functions to do more
variance reduction instead of just using them as a baseline you can also
you can use them more aggressively and introduce some bias so I won't go into
the details in this talk but sometimes these are called actor critic methods
there's also another type of approach which I briefly touched on in the earlier slide where instead of just
trying to increase the probability of the good actions you actually differentiate your loss with respect to the actions this is like the Reaper
amortization trick which is used for like for density modeling and unsupervised learning so here you're
trying to instead of just increasing the probability of the good actions you're trying to push the actions towards better actions and I'd say both of these
bullet points you're potentially decreasing your variance a lot but at
the cost of increasing bias so it's actually makes the algorithms a little harder to like to understand and to get
them working because with high variance if you just crank up the amount of data
you can always drive your variance down as much as you want but with bias even if no matter how much data you get
you're not going to get rid of the bias so if you're grading is pointing in the wrong direction then you're not going to
learn anything okay so now that that's
it for the policy gradient section of this on this talk so I wanted to show a
quick video of some work that my colleagues and I did on learning locomotion controllers with policy
grading methods which I think well I found pretty exciting when I saw it so
hopefully it's you find it interesting so here what we've got is a humanoid
simulated let's see okay yeah it's a simulated humanoid robot in a physics
simulator a realistic physics simulator called macoco and it has a neural
network policy which takes in the joint angles of the robot and maybe some and a
little bit of other kinematic formation like joint has got joint velocities and also positions of the
different links of the robot so that's what the input is it's pretty much the raw state of the robot like no clever
feature engineering there and the output is going to be the joint torques which are set a hundred times a second so
we're just mapping from joint angles to joint torques and we define a reward
function which is to move forward as fast as possible so it gets a reward for moving forward
and it gets a so the episode ends when
it its head goes below a certain height meaning it fell over so that's basically the setup there was a little bit of
tweaking for the reward function but not too extensive so oops
yeah so you can see first it just falls forward a lot of times and then slowly it starts to develop a half-decent
looking lock and eventually it gets it
down pretty well and at the very end of this it could just keep running indefinitely so I think it was actually
stable in a strong sense meaning I could just leave it for 15 minutes and it wouldn't fall over it would just keep
going so here's another robot model then we just created without too much thought
I mean we just decided to put a bunch of legs on this thing and so we don't even
know how this thing is supposed to walk and just give it to the same algorithm
and it just figures out some kind of crazy way to walk so that's the nice
thing about reinforcement learning you don't even need to know what you want it to do I think this is also the physics
are a little unrealistic here but here
we set up we use this a similar model to the one in the first demo but here we
just give it a reward for having its head at a certain height so there's the word telling it to get its head up as
high as possible and then it figures out how to get up off the ground oh let's see and I have a low battery
does anyone have a charger that I could
thanks a lot your lifesaver okay any
questions about policy gradients before I move on to the next part oh yeah so
the question was is the system time invariant yes the that's that's assumed is that it's stationary oh right and
also that it doesn't change from one episode to the next of course in some real-world problems that might not be
the case so that's I think that's also an interesting problem setting where you have a non stationary environment so the
question was for the baseline to learn a good baseline do you need to know the dynamics of the system so no you can
just learn it by doing regression you just estimate the empirical returns and
then you do regression to try to fit a function to that
yeah so the question is there's a discount factor which causes the which
should cause the policy to disregard any effects that are delayed by more than 100 time steps so how does it still work
that this guy learns how to stand up even though that might take more than 100 time steps is that correct yeah so
yeah you're right and in fact I would say that these methods aren't guaranteed to work well
if you have more than 100 time steps so sometimes they work anyway often they
work anyway but there's no guarantee so I think there's actually something pretty fundamental missing and how like
how to deal with really long timescales and people have recently been thinking about hierarchical reinforcement
learning where you have different levels of detail of the system where you might
have a like one level of description where you have a like a short time small
time step and then you have successively larger time steps and you can that allows you to plan over much longer
horizons so that's something that's currently an active area of research but yeah I would say that none of these
methods are going to do it are guaranteed to do anything reasonable if you have more than one over one minus
gamma time steps between action and reward oh yeah so in this kind of task
if you introduced terrain or something could I think if it didn't if you didn't
train it to deal with terrain then it then it might fail it probably would failed actually I don't think it would
fail because the funny thing about these policies are actually really robust because you train them with the
stochastic policy so there's a lot of noise being generated by the policy itself so in practice it's it's actually
so it's able to deal with huge noise introduced by the policy and as a result I found that if you change the dynamics
parameters a little it usually still work but yeah there's no guarantee that they'll do anything if you give it
something you didn't train it for I think that you probably could train it do the same kind of training with
terrain I didn't have any terrain so I didn't try it but I'd be nice to try ok
I'm going to move on to the next part of the talk feel free if you have more questions to find me afterwards
okay so now I'm going to talk about a different type of reinforcement learning algorithm so okay so these so the
previous kind of methods are distinguished by the fact that they learn they explicitly represent a policy
which is the function that uses your actions and they try to optimize it with respect to the parameters of the policy
so the nice thing about the policy grading methods we just talked about is that you're optimizing the thing you
care about so and you're optimizing it with gradient descent so that makes it
kind of easy to understand what's going on because if you take if you're getting the proper grading estimate and you take
small enough steps then you should be improving I mean of course you still could get stuck in a local minimum but
at least you get stuck in a bad local minimum but at least it's a local minimum and you can use the our
understanding of optimization to figure out what's going on so these next class
of methods are a little different because they're not optimizing the policy directly they're learning
something else called the cue function which measures how good state action
pairs are so it measures I'll say that more formally late later but it's just
measuring how good the actions are and these methods are actually the these are
able to exactly solve em DPS efficiently in the setting where you have a finite
number of states and actions so these are these are the preferred methods for exactly solving them in those settings
but you can also apply them with continuous States and actions and using
using expressive function approximator is like neural networks but it's a
little harder to understand what's going on in these methods like when they're going to work and when they're not going
to work so I'll define the relevant
quantities here so the Q function is defined as these expected sum of rewards
when we condition on the first state and the first action so
we're conditioning on s0 equals s a0 equals a and we're we're and the Q
function is the expected discounted sum of rewards when we're acting under the policy PI so by convention I'm starting
out with time step 0 I could have also said that we're taking RT plus RT plus 1
plus RT plus 2 and so on but since we're assuming the system is stationary it
should be exactly the same so I'm just by convention I'm going to say that the first I'm going to always use time 0 1 2
3 and so on just for ease of notation so the Q function is just telling you how
good in this state action para is under your current policy the value function
well the state value function usually called V is just conditioning on the
state it's telling you how good that
state is what's the expected reward at that state and lastly there's an it the advantage function is the difference
between the Q function in the state value function meaning how much better is that action than what the policy
would have done we're not going to talk about advantage functions in this section but it was actually this
corresponds to the notion of an advantage estimator we briefly mentioned in the previous section so here we're
going to consider methods that explicitly store and update the Q function instead of the policy and
updates them using what are called bellman equations so so the bellman
equation so a bellman equation in general is a consistency equation that should be satisfied by a value function
so here I'm writing down the bellman equation for Q PI and what it's saying
is that the expected sum of rewards should be the first reward plus this
expected sum of rewards after the first time step so it's saying something pretty simple that's so R 0 is the first reward
beep I of s1 is just adding up all the rewards at after at after time step 0 so
in the second equation we write out this relationship just involving the Q
function so we have a consistency equation that the Q function should satisfy we can slightly generalize this
to use K time steps instead of just one time step so we can expand out the
expectation the expected sum of rewards to write write out k rewards explicitly
and then cap it off with the value function at the very end which accounts for all the rewards after that okay so
here's the bellman equation from the previous slide so now I'm going to introduce a very important concept called a bellman back up so so we have
this equation that the value the value function Q PI should satisfy but we
don't know Q let's assume we don't know Q PI so let's say we have some some
other Q function so we define this bellman back up operator that that
operates on an arbitrary Q function so an absolute Q function to a new Q function and it's defined by just taking
the right-hand side of the bellman equation and plugging in our Q function
our new Q function Q instead of the Q PI
so Q PI is a fixed point of this operator meaning if we apply the backup
operator we get it the same thing back and and very nicely if we keep applying
this backup operator repeatedly to any old arbitrary initial Q function Q the
series will converge to Q Pi which is the fixed point of the operator so that's yeah so that's so that way you
can you can one way you can use an iterative
algorithm to estimate q pi by taking any old initial q function and repeatedly
applying this backup operator so now there's another kind of q function that
we're going to introduce call it Q star so the previous Q function q pi was this
is the telling you the value function under the current under some policy pi so it only makes sense with regard to
some particular fixed policy PI Q star is going to be it is going to involve
the optimal policy instead so so Q star is just defined as the q function of the
optimal policy so here we have PI star the optimal policy and Q star is just the q function of the optimal policy and
it also happens to be the point wise maximum over all policies of the q
function at each state action pair so so
the optimal policy is deterministic and it should satisfy this equation that it
takes the Arg max of the optimal q function so recall that the Q function tells you your expected return if you
take the given action so obviously the optimal policy should take the action
that has the best expected return so that's why that's why this last equation
is evident so so now now that we know
this property of the optimal policy we can rewrite the bellman equation so so
on the first equation is that's just the bellman equation from the previous slides for a given policy PI now we can
take that expectation over actions and replace it by what the optimal policy is going to do which is just going to take
it's going to take the Arg max of the optimal q function there's a typo on my
slide that should say Q star inside on the right-hand side so so now we have a
bellman equation that the optimal policy should satisfy now we can do the same thing with a
backup operator so we we take that bellman equation and we plug in an
arbitrary Q function on the right-hand side instead of the optimal Q function Q star so Q star is a fixed point of this
bellman operator that's just a restatement of the bellman equation and
again if we reply this bellman operator repeatedly to an arbitrary initial Q
function it converges to Q star which is the optimal Q function this is the
binocs fixed point theorem in both cases can be used to prove it okay so based on
these ideas there are two classic algorithms for exactly solving MDPs these are sometimes called dynamic
programming algorithms because they're actually quite related to the kind of dynamic programming algorithms that are
used to solve search problems so one is
called value iteration and you just initialize your Q function arbitrarily and you repeatedly do bellmen backups
until it converges now the second one is
called policy iteration you initialize your policy arbitrarily then each step
you first can compute either exactly or approximately the Q function of that
policy and then you update your policy
to be the greedy policy for the Q function you just compute it so that means that you your new policy just
takes the Arg max of the Q function so it takes the action that's best according to that Q function so I didn't
say anything about how you compute Q PI so one way to do it is to compute it you can compute it exactly because it
happens that the bellman equation for Q Pi is a linear system of equations so often you can just solve them exactly
more commonly well if you have a large-scale problem you might not be able to solve this system
so what people often do is they do a finite number of bellman backups which
gives you which doesn't exactly converge on Q pi but it gives you something that's approximately q pi okay so that's
I just told you algorithms that you can implement if you have full access to the MDP like you know the whole table of
probabilities but in reinforcement learning usually the assumption is that you don't know any of these probability
distributions you don't know they're a word function you don't know the transition probabilities so all these things have to be estimated from data or
they have to or you're only able to access the system through interaction so now it turns out that these algorithms
can be also implemented if you only access the system through interactions which is kind of remarkable I think um
so so the way it works is so let's recall our backup formulas for Q PI and
Q star so we can so we can in both cases
we have this a certain quantity inside and expectation in both both cases we
can compute an unbiased estimator of the right of that quantity inside the
expectation just using a single sample meaning if we have if we sampled some
data from our system using any old policy then we can get an unbiased
estimator of the quantity on the right hand side of those expectations I mean
the quantity on inside of the right hand expectations so basically we can do an
approximate version of this bell minim backup which is unbiased and even with
this noise so we're doing a noisy version of the bellmen backup even with this noise it can be proven that if you
do if you choose your step size appropriately with the right schedule you're still going to converge to q pi
or q star depending on which algorithm you're implementing
okay so now well I'll say at this point that this is pretty much the fundamental
idea and now you can you can come up with algorithms that can be applied in
the reinforcement learning setting where you're just accessing the system through sampling and you can also start
introducing function approximation here so in seventh said anything about what
the Q function is I've just told you it's a function of state and action but now we can start having neural network
cue functions for example so so we can
parametrize the Q function with the neural network let's call it Q theta and
now instead of doing the bellmen backup I mean it doesn't make sense to do the
bellmen backup exactly because we're not just setting the values of the neural network output the best we can do is try
to like encourage the neural network to have some output values so what we do is
instead of doing the the way we do this backup is we set up a least squares problem so we write down this quadratic
objective that says that the Q function should be approximately equal to the backed up value and then we just
minimize it with SVD so one version of
this algorithm which was introduced about ten years ago called neural fitted Q iteration well it works exactly the
way you'd expect use sample trajectories using your current policy which might be
determined by the Q function or it could be any old policy as it turns out and
then you you solve the least squares problem where you're trying to minimize
this quadratic you try to minimize this quadratic error which is based on the
bellmen backup the backup for Q star so
one thing I haven't mentioned so far is what do you actually use as your policy so I said sample trajectory using your
policy so if you have a Q function you can turn it into a policy by just taking
the action that has the highest Q value that's what you typically do so the Q function measures the goodness of
all your actions so you can easily turn that into a policy by taking your best action or by taking actions where the
log probability is proportional the goodness or something like that so you
you might take typically probability as is exponential of Q value over some kind
of temperature parameter that's called Boltzmann exploration whereas if you use
just the greedy if you just take the Arg max that's called the greedy policy so
it turns out that with these kind of Q learning algorithms you don't have to execute the greedy policy to for
learning to work there's you actually have some freedom in what policy you can execute which is actually one very nice
property of these algorithms that you can use an exploration technique which
where your policy is actively trying to reach new states or do something new and
still learn the correct still converge still move towards Q star or q pi as the
case may be okay so that's so that's a
very basic neural fitted Q iteration is sort of a basic way of doing this a more recent algorithm that's gotten a
lot of attention is the one that was from Mini at all from deep mind which is
basically an online version of this algorithm with with a couple of useful
tweaks in it so and but actually when you look at the two tricks they're
actually kind of very they make a lot of sense if you just think about what value
iteration is doing so one one technique is you use this replay pool where it's a
rolling history of your past data and that's just the data you're going to use to fit your Q function so that makes
sure you have like a representative sample of data to fit your Q function to
and the second the second idea is to use a target network so instead of using
your cute current Q function and just doing bellmen backups on that you have
some lags version of your q function so you have this target network which is a copy of your cue function at some
earlier time and you use that in the backups so that also if you think about value iteration you're trying to you
have your old cue function and you're trying to make the new one equal to the backed up version of the old one so using the target network just is sort of
the natural thing to do if you're trying to implement value iteration in an online way so and there have been many
extensions proposed since then I've got a bunch of citations at the bottom of the slide so this algorithm the dqn
algorithm is is using the backup B which
is the backup for Q star remember that I also introduced this other backup B PI
which is the backup for Q PI so so there's another algorithm like a very
classic algorithm called sarsa which is an online way of doing the B PI backup
essentially well it's sort of an online version of policy iteration but so it's
it's actually found to work as well or better than DQ a well better than using
the B back up in some settings not all settings so I think the jury's still out
on exactly how these things compare but it's I think it's worth considering both
policy iteration and value iteration and and all the different online versions these algorithms and taking them
seriously because it's not clear right now exactly which are how they all
compared each other in the function approximation setting okay so that's
that's the overview of all the technical parts and now I just have a couple
conclusion slides so so let me just summarize the current state of affairs I
introduced two kinds of algorithms policy grading algorithms which explicitly represent a policy and
optimize it and Q function learning algorithms which explicitly represent a
Q function which is the goodness of different actions and use that to implicitly represent a
policy so so policy grading methods there's a lot of so there have been some
successes with different kinds different variants of it so you have vanilla policy grading methods there is a recent
paper on this a three C method which is an async of implementation of it which
gets very good results there's also another kind of methods are the natural
policy gradient methods trust reagent methods so the video I showed you was obtained using trust region policy
optimization which is one of these in the second category so that makes it I think these trust region methods net and
natural policy grading methods are more sample efficient than the vanilla methods because you end up you're doing
more than one grading update with each little bit of data you collect so with the vanilla policy gradient you just
compute one little grading estimate and then you throw it away with natural policy gradient you're solving a little
optimization problem with it so you get more juice out of it so that's that's
what we have in the policy gradient world and in the Q function world we
have the DQ n algorithm and some of its relatives and these are sort of
descendants of value iteration where you're approximating the bellmen back up
using value iteration and then sarsa is also it's related to policy iteration
these are both different I mean these are estimating different they're dealing
with different bellman equations so it's kind of interesting that both kinds of methods work and they all they're both they have fairly similar behaviors as it
turns out so here's what I would say that here's how I would compare them and
this is like anecdotal evidence but I think this is the consensus right now
the q function methods are more sample efficient when they work but they don't
work as generally as policy grading methods and it's a little harder to figure out what's going on when they don't work and
that kind of makes sense because in the policy gradient methods you're optimizing exactly the thing you care about with gradient descent
whereas with cue function methods you're doing something indirect where your Optima you're trying to learn a cue
function and then you're hoping that it gives you a good policy and yeah so I
would also point out that there there are also some confounds so it's hard to make a good conclusion at this point
because people use different like time horizons in the policy gradient methods
versus the cue function methods so they do one step look Ahead's on the cue functions and multi-step look Ahead's on
the policy gradients so it's not clear if the extra if the differences come from like using different time horizons
or some differences in how the algorithms are working because you're
either doing regression for a cue function versus learning a policy using
policy grades so just to summarize it I would say here here are some of our core
model free reinforcement learning algorithms and they are whoops I'm
missing a word in the first column which I think should say wrote like
reliability and robustness so this just means like is it going to work on new
problems without like without parameter tuning or is it you're going to
mysteriously either work or not work so this this would be my slightly sloppy
summary of all these different algorithms I would say there's still
some room for improvement there might be some improvements in the basic methods because there are some nice properties
of the Q function methods that we don't have in the policy gradient methods like you can easily do off you can easily
explore with a different policy than the one that you're learning the Q function for and that's really important you
can't do that very easily with policy gradient methods whereas the policy
grading methods just seem like they're more you can just apply them in there like more likely to work and it's well
understood what's going on so I think yeah there's still I don't know if it's possible to get the
best of both worlds but that's that's the hope and that's it for my talk thank
you
any questions
oh yeah so in model-based reinforcement
learning what lines of research do I find most interesting i think the work
for my colleagues on guided policy search is very nice so i would say that's the kind of model-based reinforcement learning I also like there
are some methods that are using the model for faster learning like for variance reductions so there's a paper
called stochastic value gradients that I like a lot I think it's a pretty wide
open area so I don't think there have been a lot of really compelling results
where you're able to learn extremely fast like you're able to learn with much
better sample efficiency using a model so it seems like that should be possible but I don't think it's been demonstrated
yet so maybe in the next couple years we'll see that happen hello hi thanks
for the talk so I have a question is that is it true or not true that most of this problem requires some kind of
simulated well to to run experiments in the episodes right oh yeah so are you
asking does this work in the real world is that the question organs yeah I would
say it does work if you have a lot of patients and you're willing to execute
this thing for a while so the locomotion results I should add up to about two
weeks of real time so it's actually not that bad especially when you consider the babies toddlers take awhile to learn
how to walk properly even though evolution already puts in a lot of built-in information so I'd say
maybe yeah I'd say it's it can be run in the real world some of my colleagues in Berkeley are doing some experiments
where they are running just regular reinforcement learning algorithms in the real world very brave but I hope to see
some nice results from that soon thank you hi thanks for talking here on the other
side here I was wondering what was your intuition on the lost surface of those
deep reinforcement learning optimization problems and maybe especially how it
evolves in the from the as the policy learns and I should specify in the
policy gradient case so I think the situation is a little bit different in
reinforcement learning from a supervised learning so in reinforcement learning the loss you have you have one kind of
local minima in policy space so for
example let's say you want your so I'm going to keep going back to the locomotion example because I spent a lot
of time on it but let's say you want your robot to walk there's one local minimum where it just stands and it
doesn't bother to walk because there's too much penalty for falling over and there's another local minimum where it just dives forward because it gets a
little bit of reward for that before it falls to its doom so so even so I think
that that's actually the the hard part about the like the optimization problem
is actually define is because of the different space of behaviors and actually has nothing to do with the
neural network so I've also found that yeah it matters surprisingly little what
kind of architecture you use like what kind of neural network architecture you use because I think that most of the
hardness and the weirdness of the problem comes from like what the behavior space looks like rather than
what the actual numerical optimization landscape looks like OOP thank you so
there are many problems where the reward is only observed at the end of the task
so in the final in the terminal State in each episode and you don't see rewards
and intermediate states so how much harder to these problems become for deep reinforcement learning experience Thanks
yeah so you have if you don't get the reward until the end then then you can't
well then it's probably it might be harder to learn yeah I I don't have anything anything precise to say about
that I think it's going to be harder if you have less if your rewards are further away so for example for your in
your video for the last example of getting up and getting the head above a certain height yeah or example that
could be one where you only get a plus one if you're above and you don't get anything below all right doing something
that was kind of if you get your head higher then you still get something partial yeah so I think we came up with
a reward like distance from height squared which made the problem easier yeah the problem would have been a lot
harder if you get zero reward until you get your head above the height um and it's actually that would be a problem of
exploration which is that you have to explore all the different states people
to figure out where you're going to get good reward Thanks okay one last
question so I have a question about how do you choose to quantize the space-time
because in your locomotion example you clearly has the continuous system right
yeah so it's actually really important how you discretize time like what time step you use because if because the
algorithm has I mean the algorithm does care about what the time step is so it's
not like yeah because you you have discount factors and you're also
sampling a different action at every time step so yeah so if you choose too
small of a time step then you then the rewards will be delayed by more time
steps so that makes the like the credit assignment harder and also your
exploration will be more like a random walk because you're changing your minds really frequently so yeah the time step
is pretty important and I'd say that's that's a flaw in current methods okay
thank you
thank you so take a short break reconvene in 15 minutes

----------

-----

--08--

-----
Date: 2016.09.27
Link: [# Theano Tutorial (Pascal Lamblin, MILA)](https://www.youtube.com/watch?v=OU8I1oJ9HhI)

Notes:
#### Advantages:

- Tiano allows for easy manipulation of mathematical expressions using numpy syntax.
- Supports basic to complex mathematical operations and neural network architectures.
- Enables automatic and symbolic differentiation, optimizing for numerical stability and speed.
- Provides tools for debugging and code inspection.
- Has a global user and contributor base, driving research and industrial applications.
- Serves as a foundation for other software projects and machine learning libraries.

#### Drawbacks:

- The learning curve for understanding and using symbolic expressions and graph optimizations effectively.
- Dependency on Python for execution, complicating the distribution of models without requiring Python installation.

#### Tips and Advice:

- Utilize Tiano's GPU capabilities for improved performance, especially with float32 or float16 data types.
- Explore Tiano's debugging and diagnostic tools for better understanding and troubleshooting of models.
- Consider using docker containers for distributing models to alleviate the need for Python and compilers on the user's end.
- Engage with the Tiano community through mailing lists and Stack Overflow for support and feedback.

#### Lecture Content:

- Introduction to Tiano's concepts and capabilities.
- Practical examples demonstrating logistic regression on MNIST dataset, LinNet architecture, and LSTM for character-level text generation.
- Advanced topics like scan for loops in graphs, visualization, and debugging tools.
- Discussion on new features, optimizations, and future directions for Tiano.

#### Main Challenges:

- Mastering the symbolic expression system and graph optimizations for efficient model development.
- Navigating the complexities of GPU acceleration and memory management.
- Debugging and optimizing Tiano code for performance and accuracy.

#### The Importance and Usefulness of the Topic:

- Understanding Tiano is crucial for researchers and practitioners in machine learning and deep learning for developing, debugging, and optimizing models.
- The ability to use GPUs and perform symbolic differentiation significantly accelerates the experimentation and development process.

#### Accomplishments:

- Tiano has contributed to numerous research papers, prototypes, and industrial applications.
- The development of a vibrant ecosystem of libraries and tools building on Tiano.

#### Summary of the Content:

- The lecture provided a comprehensive overview of Tiano, covering its principles, advantages, practical applications, and troubleshooting methods. Through hands-on examples, the lecture demonstrated how to implement and optimize various neural network models using Tiano.

#### Interesting Quotes or Insightful Sentences:

- "Tiano is a mathematical symbolic expression compiler."
- "Automatic differentiation changes the game for neural network training."
- "Debugging in Tiano: Connecting execution errors back to their symbolic origins."
- "With Tiano, the power of GPUs is unleashed for deep learning."


Transcription:

okay so today I'm going to briefly introduce you tno how to use it and go
over the basic principles behind the libraries and if you paid attention during yesterday's presentation of
tensor flow some concepts will be familiar to you as well and if you paid
attention to you go lava Shell's introduction area talk you'll see some
some serie concept as well so there's going to be four main parts so the first
one is well this slide and introduction about what the concept of Tiano are
there is a companion ipython notebook that's on github so if you go on that
page or clone that github repository there is an eye Python notebook that
basically has all the code snippets from these slides so that you can run them at
the same time then we're going to have a more hands-on example basically applying
logistic regression on the Emnes digits data set and then if we have time we'll
go quickly over to more examples concepts so the basic Linette
architecture and an STM for character level generation of text so Tiano is we
can say mathematical symbolic expression compiler so what does that mean it means
that it makes it possible to define expressions that represent mathematical
expression using numpy syntax so it's
easy to use and it supports all the kind of basic mathematical operations like
main max addition subtraction all the kind of basic things not only larger
blocks like layers of neural nets whole networks or things like that it
makes it possible to manipulate those expressions during rough substitutions
cloning and replacement things like that and also making possible to go through
that graph and perform things like automatic differentiation a symbolic
differentiation actually all the our operator for forward differentiation
applying some optimizations for increased numerical stability and then
it's possible to use that optimized graph and the Endo's runtime to actually
compute some values some output values even inputs we also have a couple of
tools that help debug both pianos code
and the users code and try to inspect and understand better what's actually happening when you're using Tianna
so when I was currently more than 8 years old it started small with only a couple of
contributors from the ancestor of Mila and which was called Lisa at the time
and it grew a lot we now have contributors from all over the world users from all over the world
and it's been used to drive a lot of research papers prototypes for
industrial application in startups and in larger companies tno has also been
the base of other software projects that build on top of the nose so for instance
blocks Kara's Lezyne our machine learning deep learning libraries that
used ya know as a back-end and provides user interface that is a higher level so
that has concepts of layers of training algorithms of this kind of things
whereas ya know is modern backends SK don't ya know as well which is nice
because it has a converter to load cafe models from the cafe zoo and use them in
Tiano and does a lot of other things as well pi MC 3 actually uses t anode not to do
machine learning but for ballistic programming and we have two other
libraries platoon that Mira is developing and TN o MP I developed a 12
with our layers on top of T and O to help train on multiple machines multiple
GPUs and have some level of model parallelism and data parallelism so how
to use TN well first of all we are working with symbolic expression
symbolic variables so that will make up
a computation graph so let's see how how to do that so to define the symbolic
expression so we defined the expression first then we want to compile a function
and then execute that function on values so to define the expression we start by
defining inputs so the inputs are symbolic variables that have some type
so you have to define in advance whether like this variable is like a vector or
matrix what's its data type is floating-point integers and so on so things like the
number of dimensions have to be known in advance but the shape is not fixed the
memory layout is not fixed so you could have shapes that change between like 1
mini-batch and the next or different calls to do to the function in general so x and y are purely symbolic variables
here we will give them values later but
for now that's just that's just empty there's another kind of input variables
that is share variables and they they're symbolic but they also hold
a value and that value is persistent across function calls it's shared
between different IANA functions it's usually used for instance for storing
parameters of the model that you want to learn and yet these values can be updated as well so here we create two
other variables from social variables from from values this one has two
dimensions because its initial values after dimensions and this one has only
one so that's basically weight matrix and the bias we can name variables by assigning to the name attribute short
variable do not have a fixed side either there are usually kept fixed in most
models but it's not a requirement then from these inputs we can define
expressions that will build new variables intermediate variables which are the result of some computation and
so for instance here we can define well the product of X and W at the bias apply
sigmoid function on that and they say this is our output variable and from the
output Y ball and Y we can define just say the squared error cost so those new
variables are connected to the previous ones through the operations that we define and we can visualize the graph
structure like that by using for instance by dot print which is a helper function so variables are those square
boxes and we have other nodes here we call apply nodes that represent the
mathematical operation that connects them so input variables and shared
variables do not have any ancestors they don't have any road connecting from them
but then you see that intermediate result and and more of them
usually when we visualize we don't necessarily care about all the
intermediate variables unless they have a name or something and so this is a simplified version of exactly the same
the same graph where we hide the unnamed intermediate variables but you can still see all the operations and actually you
see that the type on the edges so once
you have defined some graph say your forward computation for your model we
want to be able to use back propagation to to get your idioms so this is just
the basic concept of the chain rule we have a scalar crossed we have
intermediate variables that here are vectors here's just the general starting
from the from the cost and so the whole
derivative of say that that function G is actually a whole Jacobian matrix
that's M by n if the intermediate variables are vectors of size N and M
and usually you don't need that and it's actually usually a bad idea to compute
it explicitly unless you need it for some other purposes what the only thing you need is an expression that given any
vector representing the gradient of the cost with respect to the output will
compute you the gradient of the cost with respect to the input so basically
the dot product between that vector and the whole Jacobian matrix so that's also
called the L operator sometimes and so almost all operations in Tiano implement
a function that returns that and it
actually returns not numbers not a numerical expression for that but it
returns a symbolic expression that represents that computation
again usually without having to explicitly represent or define that
whole Jacobian matrix so you can call
Tia no grant which will back propagate through the graph from the cost towards
the inputs that that you give and along the way it will call that grad method of
each operation back propagating means starting from one for the cost and back propagating through the whole graph
accumulating when you have the same variables that used more than once and
so on and again here so DCW and this is DB they are symbolic expression the same
way as if you had manually defined the gradient expression using T&O operations
like the dot product the sigmoid and so on that we that we've seen earlier so we
have non numerical values at that point and they are part of the computation
graph so the completion graph was extended to add these these variables
and we can continue extending the graph from these variables for instance to
compute update expressions corresponding to gradient descent something like that like we do here so for instance this is
what the extended graph for the gradient looks like so you see there's like a lot
of small operations that have been inserted and outputs you have actually
here the gradients with respect to the bias which is both an output and an
intermediate result that will help compute the gradient with respect to the
weights and here's the graph or the
update expressions so you have as intermediate as intermediate variables
the gradients that we had on the previous slide and then this uses the scaled version
with constant 0.1 that's somewhere so
once we have defined the whole graph the whole expression that we actually care
about from the input and initial weights to the weight updates for our training
algorithm we want to compile a function that we'll be able to actually compute
those numbers given inputs and perform the weight updates so to compute values
what we do is called Tiano dot function and you provide it with the input
variables that you want to feel and the output variables that you want to get and you don't have necessarily to
provide values for all the inputs that you might have declared especially if
you don't want to go all the way through the end of the graph you can have a
function that only computes sub set expression for a subset of the graph for
instance we can have a predict function here that goes only from X to out we don't need values from Y we don't need
and so the gradient and so on will not be computed it's just going to take a
small part of the graph and make a function out of it so so that's it you
can first compile it get value and call it so you have to provide values for all
the input variables that that you define you don't have to provide values for
shared variables W and B that we declared earlier there are implicit inputs to all the functions and their
value will automatically be be fetched when it's needed can declare other
functions like monitoring function that computes both the output and the cost so
you have two output you also need the second input Y you can compute the
function that does not start from the beginning like for instance I want an error function that only computes the
the mismatch between the prediction and the actual targets then I don't have to
start from the input I can just start from the prediction and compute the cost
then the next thing that you might we want to do is update your Bibles for
training it's necessary and again you can pass duty and functions updates a
list of updates and updates are pairs of shared variable and the symbolic
expression that will compute the new value for that shared Bible so you can
see a big W and up they'd be here as implicit outputs of the function like W
and B were implicit inputs update W update B are implicit outputs that will compute it that will be completed at the
same time as C and then after all the outputs are computed the updates are
actually effective and the values are updated so here if we print the value of
B before and after having calling after
having called the same function then we see the value has changed what happens
also during graph compilation is that the graph that we selected for that
particular function gets optimized and what we mean by that is that it's going
to be rewritten in parts there are some expressions that will be substituted and so on and there are different different
goals for that some are quite simple that for instance if we have the same computation being
defined twice we only want it to be executed once if you have expressions
that are not necessary you don't want to compute them at all for instance if you have X divided by X you don't know and
and X is not used anywhere else we just want to replace that by one there are
numerical stability optimizations for instance well log of one plus
can under fill' if X is really small and this would give 0 whereas which would be
close to X things like log of softmax get optimized into more stable locks of
Max operation it's also the time where in place and destructive operations are
inserted for instance if an operation is the last to be executed on some numbers it can instead of allocating output
memory I can just work in place on its input and so on also the transfer of the
graph expression to the GPU is due is done during the optimization phase so by
default Kanno tries to apply most of the optimizations so that you have the run
time that's almost as fast as possible except for a couple of checks and assertions but if you're iterating and
want fast feedback and don't care that much about Timothy about the runtime
speed then you have a couple of ways of enabling and disabling some set of
optimizations and you can do that either globally or function by function so to
have a look at for instance what happens during the the graph up to my different
phase here's the the original and optimized graph going from the inputs X
and W going to the output prediction it's the same one that we've seen before
and if we compare that with the function the compile function that goes from
these input variables to out which was called predicts this is what we have I
won't go into details about what's happening in there but here you have a gem G operation which basically calls an
optimized Blas routine that can also
do multiplication and accumulation at the same time we have a sigmoid
operation here can will work in place destructively on its input which is
denoted by the red arrow here if you have a look at for instance the
operation optimized graph completing the expression for the updated W and B this
was the original one and the optimized one is much smaller
it has also in place operations it has fused LM wise operations like for
instance if you have a whole tensor and then you do an element-wise a addition
with with a constant and then a sigma eight and then something else and so on you want to only loop once through the
array and apply all these carrier operations on each element and then go
to the next and so on and not iterate each time that you want to apply a new new person and those kind of things
happen often when you have automatically generated gradient expressions oh and
here you see the update for the shared eyeballs which are inputs so you see the
cost and the implicit outputs for the updated wnb here and here another
graphitization tool that exists is the back print which basically prints
text-based tree like structure of of the graph assigning arbitrary ids and
printing the variable names and so on so
here you can see more in detail like what the structure is and you see the
inputs of gmv and the scaling parameters and so on so when the function is
compiled then we can actually run it so T no function is call a ball python
objects that that we can that we can call and we've seen those examples
here for instance where we call train and so on but what happens to have say
optimized run time it's not only the degree of optimizations but we also
generate C++ or CUDA code for instance for the LMS loop fusion that I mentioned
we can't know in advance which elementwise operation will be will occur
in which order in any drive that the user might define so we have on-the-fly
code generations for that you generate Python module written in C++ or in CUDA
that gets compiled and imported back so that we can use it from Python the
runtime environment then calls in the
right order the different operations that have to be executed from the inputs to the outputs so that we so that we get
the desired results we have a couple of different ones and in particular there's
one which was written in C++ which avoids having to switch contacts between
the Python interpreter and the C++ execution engine something else that's
really crucial for speed and performance is GPU so how to use a GPU in TN oh we
wanted to make it as simple as possible in usual cases so now it supports a
couple of different data types not only float 32 but double precision
if you really need that integers as well and we have now easier interaction with
GPU arrays from Python itself so you can just use Python code to handle GP arrays
outside of a Tiano function if you'd like all of that he will be in
future 0.9 release that we hope to get out soon and to use it well you select
the device that you want to use the primary device that you want to use with
just the configuration flag for instance you could to get the first GPU that's
available or one specific one and if you specify that in the configuration then
all share variable will by default be created in GPU memory and the
optimizations that move the computation from CPU to GPU so that replace the CPU
operation by GPU operations are going to be applied usually you want to make sure
you use 432 or even float16 for storage which is experimental but because most
GPUs don't have a good performance for for for double precision so how you set
those configuration flags you have in order that you never see configuration
file that you can it's just basic configuration file from for for Python
you have an environment variable where you can define those and the environment variable overrides the config file and
you can also set things directly from Python but some flags have to be known
in advance before you know is is imported so for instance the device
itself you have to set it either in the configuration file or throw flags
so I'm going to quickly go over more advanced topics and if you want to learn
more about that there's other tutorials available online and there's a documentation on the planning up net so
to have loops in the graph we've seen that the expression graph is basically a
directed acyclic graph and we cannot have loops in there one way if you know
if you know in advance the number of iterations it's just to unroll the loop use a for loop in Python that builds all
the nodes for all the time steps it doesn't work if you want for instance
to have dynamic no dynamic size for the
loop for models that generate sequences for instance it can be an issue so what
we have for that in India know is called scan and basically it's one node that
encapsulate another whole T&O function and that the end of function or step
function is going to compute the is going to represent the computation that
has to be done at each time step so you have at the end of function that performs the competition for one time
step and you have the scan node that calls it in the loop taking care of the
bookkeeping of indices and sequences and feeding the right slice at the right point and feeding back the outputs where
needed and having that structure makes it also possible to define gradient for
that node which is basically another scan node another loop that goes backwards and applies back drops with
time and it can be transferred to GPU as well in which case the internal function is going to be transferred to G and
recompile on GPU and there's an example of scan in the
lsdm example later this is just a small
small example but it's we don't really have time for that we also have a
visualization debugging and diagnostic tools one of the reason it's important
is that in piano like in terms of flow the definition of a function is separate
from its execution and if something doesn't work during the execution if you
encounter errors and so on then it's not obvious how to connect
that from where the expression was actually defined so we try to have
infirmity of error messages and we have some completion modes that enable to for
instance check for not a number fall out values you can assign test values to the
symbolic variables so that each time you create a new symbolic intermediate
variable each time you define a new expression then it the test value gets
computed and so you can evaluate on one piece of data at the same time as you
build a graph which can be useful to detect shape mismatch errors or it's
like that it's possible to extend ya know a couple of ways you can create an
app just from Python by calling python
wrappers for existing efficient libraries you can extend ya know by
writing C or CUDA code and you can also add optimizations either for increased
numerical stability for instance or for more efficient computation or for
introducing your new ops instead of the nave versions that that a user might
have used we have a couple of new
features that have been recently added to to the analyst I mentioned the new GPU back-end
with support for many data types and we've had some performance improvements
especially for convolution 2d and 3d and especially on GPU we made some progress
on the time of the graph optimization phase and also have introduced new ways
of avoiding recompiling the same graph over and over again and we have new diagnostic tools that are quite useful
and interactive visualization an interactive graphical ization tool and pdb breakpoints that enables you to
monitor a couple of eyeballs and only break if some condition is met rather
than monitoring something every time the before for every every piece of data in
the future well we're still working on new operations on GPU we still want to
wrap more convenient operations for for better performance in particular the
basic errand ends should be completed in the following days hopefully someone has
been working on that a lot recently we want better support for 3d convolutions
still faster optimization and more work on data parallelism as well so what we
want to thank well most of my colleagues and main tno developers and people who
contributed one way or another to a lab and the software development efforts and
of course recognizing the organizers for volley school now yeah so the slides are
available online as I mentioned as a companion notebook and now we can start
to go and and more resources if you want to go to go further and now I think that
it's time to start the practical examples so for
those who have not clone the repository yet then this is the command line you
want to two nouns for those who had cloned it you might want to do a git
ball just to get the latest to make sure we have the latest versions and you can
launch Jupiter notebook on the on the
repository itself so we have three examples that we are going to go over
logistic regression comes net and the rest yeah so I've launched the Jupiter
notebook here and let's start with so intro TN o was the companion notebooks
there's nothing new in there just the code snippets I've showed your alrighty and okay let's go with a logic
regression is that big enough for do we need to increase the font size okay so
I'm going to skip over the text because you probably know already about the model we have some we've packaged the
amnesty database with the on on github
with the repository so let's load the data and here let's see how we define
the model so it's basically the same way that we did in in the styles we define
sizes that will be useful for the shell variables we define an input variable
here it's a matrix because we want to use mini-batches and we have survived
balls initialized from zeros then we
define the our model so here's our
predictor so the probability of the class given the input and we're
going to use well so here the fine model
and then the softmax on top of it and the prediction if you want to help
prediction it's going to be the class of maximum probability so hard max over
that axis because we still want one prediction for each element of of the
mini batch then we define the loss function so here is going to be the log
likelihood of the label given the input or the cross entropy and we define it
simply we don't have like we don't need to have one croissants for P or log
likelihood operation by itself you can just build it from the basic building
blocks so we take the log of the probability you take the index of the
actual target and then you take the mean of that to have them in prediction over
the mini batch then derived equations
derive the update rules so again we don't have like one gradient descent
objects or something like that we just build whatever rule we we want so yeah
we could use momentum by defining other shape variables that will hold the velocity and then you have that
expressions for both the velocity and the survival itself and then we compile
a training function going from X&Y outputting the laws and the dating W and
B so while the code is getting generated
and compiled and the graph is getting optimized let's see the next step well
we also want to monitor not only the log-likelihood but actually actually the misclassification
rate on validation and test set so it's
simply the different like how many elements are different between the
prediction which was the arc max and the actual target and the rate is the mean
or the mini-batch and we create another we compile another two and a function
outputting that and not doing any updates of course so to train the model
well first we need to process the data a little bit so we want to feed the model
one mini batch of data at a time so here we have simply a generator I mean not
really pay attention right over just a helper function that gives us the mini batch number I and it's going to be the
same fraction used both for the training and validation and test set
we define a couple of parameters for early stopping in that training loop
it's not necessary it's just like a way
of knowing when to stop and use only like the best model that was encountered
during the optimization so let's let's define that and this is the main
training loop it's a bit more complex that it might be but it's because we use
this early stopping and we want to only validate when we are confident that the
training error has gone down enough but basically the the most important part is
you loop over the epochs unless unless
you encounter the early stopping conditions and then during each epoch
you want to loop over the mini batches and call train model then every once in
a while you want to validate and print some result of the validation error so
here we call test model on the validation set for that and then keep
track of what the best model currently is and get the the test error as well
and save the best one so to save the best one to save the model we usually
just save the values of all parameters which is more robust than trying to pick
all the whole Python object and it also enables more easily transferred to other
frameworks to visualization frameworks and so on so let's try to execute that
so of course it's a simple model the data is not that big so it should it
should not take that long so you see
that at the beginning well almost at each iteration we are better on the training set and then
after a while the progress is slower and
okay so just wait a little bit more seems to stall more and more and okay
and here it's the end after 96 epochs so
now if we want to visualize what filters
were learned or what the final train model looks like we just using a helper
function call here to visualize the filters it's not really important but
here what we use is we call get value on the weights to access the internal value
of the shell variable and then we use that to to plot the different filters
and we can see it's kind of reasonable like this is the filter for class zero and see
kind of like zero one part did what's important for the two is to have like an
opening here and so on so yeah if we
have a look at the final error well we can see that the training error is well
to hit training you know not plotting it but the validation and the test error I are quite high and we know that the
human level performance is quite low and the performance of our model is quite low so it really means that the model is
too simple and we should use something more advanced so to use something more
advanced if you go back to the home of
the Jupiter notebook can have a look at the continent and run Lynnette so this
new example is basically it's the same data it's still amnesty because it has the other edge of training fast even on
an older laptop and but this time we're going to use a completion net we look up
all of conclusion layers and then fully connected layers and then the final
classifier so I'm going to make for that float X is float 32 here and let's see
how we could use Tiano to define helper classes that are layers that can make it
easier for a user to compose them if they want to you to replicate some
results or use some classical architectures this is done usually in
frameworks built on top of Tiano like carrots like blocks like lasagna some
people also develop their own mini framework with their own versions of layers and so
on that they find useful and intuitive so
this logistic regression layer basically holds well parameters weight and bias
and compute the well the conditional
probability of classes prediction holds the params and have expressions for the
negative log likelihood and errors so if you were to use only that class then
it's doing essentially the same as what we did by hand in the previous notebook
and in the same way we can define a
layer that has convolution and pooling so again in the init methods we pass it
well filter shape image shape data side of pooling and so on we initialize the
weights using the formula from grow and venture at 2010 and buyers from zeros
and then from the inputs while we compute to the convolution with the
filters we then computes max pooling and output wealth and H of the pooling plus
the bias and here the bias is only like one number for each channel so which
means that you don't have a different bias for each location in the image so
you could actually apply such a layer on images of various size without having to
initialize new parameters or return that and then the same way we define the
hidden layer which is just a fully connected layer again initializing
weight and by and expression going from so the symbolic expression going from the input
and the shared variables to the output after activation and again we want to
collect the parameters so that we know what we will want to Train and then
here's a function that has that the main the main loop in the main training loop
so we have a mini batch generator again it's synced as as before and here we are
building the whole graph so always the same the same process we define input
symbol symbolic input variables matrix and a vector of int here so L vector is
a vector of long because the targets here are in this's and not not one Hots
vectors or masks or something like that and we create the first layer which is a
Linette compo layer with size we want to have the next one with also so yeah here
the image size changes this is mostly for efficiency actually you don't really
have to to pass that for for those particular models but you still need
like the shape of filters I mean you have the filters anyway and then it's
useful to have those size still because even if the convolution layers can
handle arbitrary sized images then after that we want to flatten the whole the
whole feature Maps and feed that into a fully connected layer and then to the projection layer so this one has to be
fixed so we have to know what the last comes layer will will have four
dimensions and here we here we go a fully connected layer and the output
layer that's just logic regression class same as before we want the final cost to
be the log likelihood of that we have
again the errors which is the misclassification rate parameters or the
concatenation of the parameters of all layers and once we have that we can
build the gradient so just one call of grad of cost with respect to parameter
updates so again just regular SGD but we could
have a class or something that performs like momentum a degree that a delta
whatever you need compile the function and here we have again the early
stopping routine with the same main loop for all a parks until we are done then
loop over the mini-batches and validate every once in a while and stop when it's finished so let's just declare that
loading the data exactly the same as before and here we can actually run run
that so this was the result of a previous run it that took 5 minutes so I
will probably not have time to do that but here you can see basically what
happens and if you want to run it or try that during the lunch break or or later
you're welcome to to play with it and after that yeah you can visualize the
the the round filters as well here you you have them for the first layer and
for the and here you have the an example of the
activations of the first layer for one example so we have just a little bit
more time to cover the lsdm tutorial
I mean example so if you go back to the
home of the Jupiter notebook and go to ASTM
then so this model is an SEM network that tries to predict the next character
of our sentence given the previous ones so not going to go into details but here
you can see that the LSM layer is defined here with like shot variables
for all the the matrices that that you
need and the different biases for the different gates and so on so you have a
lot of parameters it would be possible and sometimes more efficient to actually
define say only one variable that contains the concatenation of a couple
of matrices and that way you can do more efficient bigger matrix matrix multiply
but this is just one one simple implementation and here's an example of
how to use scan for the loop so here we define the step function that takes well
a couple of different different inputs so you have like the different
activation and so on from the previous time steps you have the current sequence
input and so on and from them here's basically the DSM formula where you have
the dot product and Sigma 8 or 10 H of the different connection inside the cell
and in the end you have the hidden and
that it so once you have that that's
step function is going to be passed to Tiano dot scan where the sequences are
the masks and input so the mask is is useful because
we're using mini batches of sequences and not all the sequences in the same
batch have the same length also for efficiency we usually want to group them with two group example of similar length
together but they may not always be exactly the same length so in that case
we pad that to only the longest sequence in the mini batch not the longest sequence in the whole set just for the
mini batch but we still have to pad and remember like what's the length of the
different sequences is in order for us to correctly predict and back propagate
so let's define that here we define the
cost function that's the categorical cross-entropy of the sequence and here again you see that the mask is used so
that we don't consider the predictions after the end of the sequence logistic
regression the same as before does the final cost here for processing the data
we're using fuel which is another tool being developed by students at Mira and
it's nice because it can read from just
plain text data do some pre-processing on-the-fly including things that I mentioned earlier like grouping
sequences by similar length and then shuffling them and padding and doing all
of that and so it outputs like a
generator that you can then feed in your main loop through a channel function so
that whole processing happens outside of tno and then the processed values are
fed into into the channel function so
yeah here we build our final key on a graph we have symbolic inputs for well
the input and masks we create lsdm layered a lot
correct layer define our cost parameters are the concatenation of the parameters
of logistic regression and the current layer take the gradients of course with
right to all parameters so as I mentioned it's going to use back prop
through time to get the gradient through the scan operation the update rule again
simple SGD no momentum nothing it's something that you could add if you want to play with it and compile to function
to evaluate the model so here the main
loop is training and we also have
another function that generates one character at a time given the previous ones that's why we will declare like
input here and so does that speak function that get probability
predictions we normalize them because we are working in float32 and sometimes if you divide by the sum and RISM then it
doesn't add up to one so we want a higher precision for just that operation and then try to generate to generate a
sequence every once in a while so again this is the result of a previous run so
we see in the so for for monitoring we
seed that prediction with the meaning of
life is and then we let the network generate so if I try to run it now it's
going to be long but here's some examples that I generated yesterday in
the previous run so it starts with not that much and it has like a couple of
unusual characters I mean it's usually it's not usual to have like
one Chinese character in the middle of words you have like concentration in the
middle of word and so on but then as it as it progresses you see
that it's getting slowly better and
better and the meaning of life is is the
dets and so of course this is not what's
going to give you the the actual meaning of life but yeah a tons lot of ham why
not and and this is this so so yeah so I
interrupted the the training at some point but you can play with it a little
bit and here are some suggestions of things you might want to do like better
training algorithms different nonlinearities inside the lsdm sell
different initialization of weights try to generate something else that the
meaning of life is and yeah so I hope I
could give you a good introduction of what you know is what it can be used for
and what you can build on top of it and if you have if you have any questions
later then we have general users mailing
lists we are answering questions on Stack Overflow as well and we would be
happy to have your feedback
have time for a few quick quick questions that's right here could you go to the
mic can you just give a quick example of
what debugging might look like in Theon Oh could you just break something in there and show us what happens and how you figure out what it was
actually yeah I think I had one okay so let's let's go to say a simple simpler
example okay so I'm just going to go to the logistic regression 1 and say for
instance that when I initialize my thing
I don't have the right I don't have the
right shape so you can still build the the whole symbolic graph and at the time
where you want to actually execute it then you have an error message that
tells you shape mismatch X has of Cowen's and some rows but Y has only
that number of rows and the apply node that caused the error is that dot
product and gives the inputs again and in that case it tells you it's not
really able to tell where it was defined but if you remove the optimizations then
it might so we can we can do that and we
can go back to where the train operation was defined train Model T a new function
and then I'll just say optimizer equals none
sorry I have to do my Audi calls piano
note optimized or not that's correct yes
so it's recompiling the function let's
record everything
and then he updated our message says
back-trace when the node was created and it's somewhere in my kernel and it's on
the line py given X equals that so of course we have like lots of things in
there but you know that there's a dot product and it's probably a mismatch
between those so that's that's one example then there are other techniques
that we can use we can have the breakpoints as I said and so on I don't
have right now tutorial about that but have some one line and I could point you to that I have some models I'd like to
distribute and I don't want to require people to install Python and a bunch of compilers and so unfortunately at the
time we're pretty intermingled with Python a lot because all the memory
management during the execution is done by Python and we use an umpire and
arrays for our intermediate values on the CPU and the similar structure on the GPU even though that one might be easier
to convert but yeah all our C code deals with Python and does the ink ref and
Decker F and so on so that Python manages the memory so if you want to distribute that I would suggest like a
docker container something like that recently even for GPU and video docker
is quite efficient and we don't have any modest allowance that that we had seen
earlier so it's not ideal and if like
someone has some time and the wheel to to help us disentangle tno from the
Python runtime it would be awesome but that's a use project
okay let's thank Pascal again and we
reconvene in 55 minutes for the next talk have a good lunch

----------

-----

--07--  

-----
Date: 2016.09.27
Link: [# Torch Tutorial (Alex Wiltschko, Twitter)](https://www.youtube.com/watch?v=L1sHcj3qDNc)
Notes:
### Lecture Notes on "Machine Learning with Torch and AutoGrad"

#### Summary of Content:

- The lecture covers practical applications of Torch, focusing on its AutoGrad feature and the underlying concepts shared across all deep learning libraries.
- Emphasis is placed on the differences and commonalities among deep learning libraries and the rationale behind the multiplicity of these libraries.
- A significant portion of the talk is dedicated to explaining the Lua programming language, its advantages for deep learning applications, and how Torch leverages Lua for efficient computation, especially on GPUs.
- The lecture introduces the core data structure in Torch, the tensor, and provides a walkthrough of basic and advanced operations possible in Torch.
- An overview of Torch's ecosystem, including its community-driven nature, is provided along with a comparison with other deep learning libraries and languages.
- The second half of the lecture delves into AutoGrad, explaining its importance for deep learning, illustrating its usage in Torch, and comparing it with similar functionalities in other libraries.

#### Advantages of Using Torch:

- Torch offers an efficient array programming framework similar to NumPy but in Lua, making it highly suitable for deep learning applications.
- It supports interactive GPU computation seamlessly, allowing easy data manipulation and computational operations on the GPU without extensive CUDA programming.
- Lua's performance, coupled with its simplicity and small footprint, makes Torch an excellent choice for embedded deep learning applications.
- Torch has a vibrant, community-driven ecosystem, not owned by any single industry player, fostering innovation and rapid implementation of cutting-edge research.

#### Drawbacks:

- Lua is less popular than Python, resulting in a smaller community and fewer resources for beginners.
- Data visualization and certain other ecosystem features may not be as developed as in Python.

#### Tips and Advice:

- For those transitioning from Python to Lua, the learning curve is relatively gentle, and one can become productive in a short period.
- Utilize the extensive Torch documentation and GitHub repositories for learning materials and examples.

#### Main Challenges:

- Navigating the transition from other languages to Lua might initially be challenging for some users.
- Understanding and effectively leveraging Torch's GPU capabilities requires some learning.

#### Importance and Usefulness of the Topic:

- Understanding Torch and its AutoGrad feature is crucial for researchers and practitioners aiming for efficient deep learning model development and deployment, especially in scenarios where computational resources are limited or where GPU utilization is critical.
- The lecture highlights the significance of automatic differentiation in modern deep learning, underlining the foundational role of AutoGrad across various libraries.

#### Accomplishments:

- The development of Torch and its adoption in both academic and industrial settings demonstrate its effectiveness in addressing deep learning challenges.
- Successful deployment of Torch models in production environments, like at Twitter, showcases its reliability and performance at scale.

#### Interesting Quotes/Insightful Sentences:

- "Lua is unreasonably fast for how convenient it is to use."
- "Automatic differentiation mechanically calculates derivatives as functions expressed as computer programs."
- "In deep learning, AutoGrad has democratized the experimentation with complex models by abstracting the gradient computation."

#### Lecture Content:

- Introduction to Torch and Lua for deep learning.
- Deep dive into Torch's AutoGrad feature.
- Comparison of deep learning libraries and discussion on the reasons for their diversity.
- Practical demonstrations of Torch's capabilities, including GPU acceleration and tensor operations.
- Overview of the Torch ecosystem, its community-driven nature, and its position relative to other deep learning tools.


Transcription:

so I'm gonna tell you about machine learning with torch and with torture Auto grads so the the description of the
talk isn't entirely correct I'm gonna do practical stuff for the first half and then what I want to do is dive into
torch Auto grad and some of the concepts that are behind it and those concepts also happen to be shared amongst all
deep learning libraries so I really want to give you a perspective of the common thread that links all deep learning
software you could possibly use and then also talk a bit about what makes each of the libraries different and why there's
I will I will hypothesize why there's so many and the different choices so one
thing I want to try there's been a lot of questions and we've gone over time but if there's not questions that go
over time in the room there's a lot of people watching online and if there's extra time we'll of course prioritize
people here but if you ask a question with the DL school hashtag or if you tweet at me directly I will try to
answer those questions from online and I'll certainly answer them offline as well so ask if you're watching at home
maybe that will kind of increase you know meaningful participation for people watching through the stream that aren't
here today umm a lot of this material was developed with sumus chintala at
Facebook he's kind of the Czar of the torch ecosystem these days and Hugo la
rochelle who you heard from yesterday and also Ryan Adams who's at Twitter with us and all this some material is
available on this github repository that you got actually on a printed sheet for
installing torch so all the examples that I'll show you will be in in one notebook and then there's a separate
notebook which it actually won't reference in the talk that's a full end-to-end walkthrough of how to train a convolutional neural network on CFR 10
so that's kind of a self-paced tutorial notebook that you can work through on your own time but I'm going to focus on
the basics on the fundamentals and hopefully give you some of the concepts and vocabulary that you can use to
really dive into torch on your own time so let's let's get going so torch is an
array programming language for Lua right so it's like numpy it's like MATLAB but
it's in the Lua language so torch is - Lua as numpy is - pi right so what you can do in torch you
can do in you know any language this is the absolute minimum basics you can grab strings and print them you can put
things in associative data types in Python there's tuples and lists and sets
and dictionaries in lua there's just one data type called a table so you'll see that a lot but you can do all those
things that I mentioned before with with a table and you got four loops and if statements the core type of torch is the
tensor just like in in numpy when you have the ND array which is a way of shaping sets of numbers into matrices or
tensors we have the tensor and you can fill it up with random numbers you can
multiply them standard stuff but the tensor is the core data type of torch and we've got plotting functionality
going over at a very high level I'll show you some more specific code in a moment so you can do all the kind of
standard stuff that you'd do in any other array based language there's all
the tensor functions that you'd like to like to use including all the linear algebra and convolutions and and you
know blast functions and I'm leaving this link here when the slides get uploaded you can follow this and kind of
dive into the documentation and see exactly what what kind of tools you have at your disposal in in the notebook and
the eye torch notebook which is something that seumas put together you can prepend any torch function with a
question mark and that gives you the help for that function so it makes it really nice to discover functionality in
the torch library in the notebook so why
is it in Lua alright it's kind of a maybe a strange maybe esoteric language to write things in Lua is is
unreasonably fast for how convenient it is to use especially a flavor of Lua
called Lua jet for loops in Lua jet are basically the same speed as C so this
for loop here is actually in production code in master and torch it's not C code
but this is perfectly fast enough right so that's a really nice aspect of Lua is
you can depend on super high-performance c-code and then on top
of it you've got this very convenient glue layer but you don't pay much of a speed penalty to use that glue layer so
that's one of the reasons why we've used Lua another advantage that some people might see as a plus is the language
itself is quite small so there's 10,000 lines of C code that define the whole language of Lua so you can really sit
down with the manual in an afternoon and understand most of the language on your own that same day another aspect
which is pretty critical for deep learning but also for other fields is that it's really easy to interoperate
with C libraries it was designed originally to be embedded so Lua was a
language that was designed to run inside of another C program but have a little scripting layer inside of it so it's
very easy to call indicee it's very easy for c to call into Lua so this is another reason why it's kind of an
appropriate choice for deep learning libraries the FFI for like the FF I call
signature and the idea has been copied into many other languages so C FF I and Python is a Python version of the Lua FF
I julia has something similar as well and as I mentioned it was originally
designed to be embedded and it's in all kinds of crazy places that you maybe wouldn't expect Lua to be so in World of
Warcraft all the graphics are in C++ or whatever they wrote it in but like the boss battles or the quests so like when
you go give the gem to the blacksmith or whatever and they give you back the magic sword the scripting of those
events that happens in Lua and if you write scripts for world of warcraft to make your own quests that's Lua Adobe
Lightroom is a photo processing app all the image processing is done in C++ but
all the UI and everything was done in Lua so again it was used to bind together high-performance code with a
with kind of a scripting layer and Redis and nginx which are kind of workhorses in the field of web development are both
scriptable with Lua and in fact if you go to github pages like my page github I
oh if somebody's hosting a web page on github that's served in part by Lua the
apocryphal story of why I was originally chosen maybe you could correct me is klimova Oh BAE was trying to build an
embedded machine learning application some device he could whereas helmut and classify the world with the CNN when he was a young student and he
was trying to do this with Python and it's incredibly frustrating to get Python to run on embedded chips maybe
it's easier now with raspberry pi but that just wasn't the case and then he stumbled upon Lua and turns out people had been building
Lua into embedded applications for years before that and so that kind of was the snowballing effect so that's that's the
hearsay for how we arrived at Lua but maybe there's there's another story
another really nice feature of torch is we have first-class support for GPU
computation interactive GPU computation so it's very very easy to get some data
from the CPU to the GPU and then everything that you do with that data happens on the GPU without you having to
worry about writing CUDA kernels right so this has been a feature of Lua torch which is becoming maybe a little bit
less unique now but this was this was a pretty solid feature when it first came out so interactive GPU computing and
I'll go very quickly over some of the basic features and all of these examples again are in a notebook which you can do
kind of at your own pace if you'd like so there's all the basic arithmetic like
creating matrices and and doing arithmetic between them taking maxes of
numbers and arrays clamping building tensors out of ranges boolean operations
over entire arrays special functions this is supported through a wrapper
around the Cepheus library this is what numpy uses to support things like 10h and atan2 and other kinds of functions
that I guess are in the special class and then sumif again has wrapped the
Bocage a/s library which is originally just for python but it provides really nice and beautiful plots in the eye
torch notebook and so we can you know draw random numbers from our favorite distributions and make nice histograms
of these so you can do nice data exploration in the eye torch notebook along with deep learning so one feature
that is attractive to some folks but just an interesting feature of the torch ecosystem is that although there's a lot
of industries support it is not industry owned so at Twitter and at Facebook air research in
at Nvidia we all contribute a lot to the torch community but we don't own it we can't
really steer it to go one way or the other definitively and there's a ton of other people that participate
academically in this ecosystem and that's a really nice feature and along
with I guess because of the really nice habits of people in deep learning when a
paper comes out there's often a high quality code implementation that follows it not not always but but very often at
least compared with with other fields and torch is one of the environments in which you'll often see high quality
implementations of really cutting-edge stuff so if you just browsed through github and you kind of follow
researchers on github you can see really high quality implementations of image captioning of neural style transfer so
you can just clone this github repository and run this yourself seek to seek models kind of the what is
whatever is the state of the art there's usually a torch implementation of it some of the recent work in generating
very realistic synthetic images with generative adversarial networks also has great torch code implementing it so
given that there's this active community on github in deep learning for torch how
does that stack up against other communities just to give you some context so the Python data science community is is pretty enormous and its
focuses are also very very varied if you enter into the data science
community in torch and lua you'll likely find deep learning people but not a lot
of other people so it's strengthened deep learning compared to its size is actually quite enormous and for those
that are kind of thinking of switching between Python and Lua and giving torch a try the effort to switch from Python
to Lua you can probably do that in a day if you've tried some Python programming so I was a Python programmer for a while
and getting started on Lua took took me maybe a couple days and I was you know actually productive at work and maybe a
week or so but you can actually run your code and understand and write new things pretty quickly if you've worked in a
scripting language like MATLAB or or Python so if you were intimidated or waiting to try it you should just dive in so how does torch compared to other
deep learning libraries specifically as opposed to languages and the first thing I'll say is there's really no silver
bullet right now there are a lot of deep learning libraries out there I say
tensorflow is by far the largest and this is a plot that was made by a
colleague of SU myths and I wish it kind of had confidence intervals on it because it's not strictly that these are
like you know points in in deep learning space but maybe this is a good guess of
where things kind of fit it seems as if tensorflow was engineered to be very good in an industrial production setting
and it seems like it's really fulfilling that Theano seems to have always had a research goal
in mind and has been really awesome in the research community for some time Torche tends to be more towards research
than industry I think Twitter maybe has pulled it a little bit towards production we maybe are the only example
I'd love to learn of others but were maybe the only example of a large company that uses torch in production to
serve models so every piece of media that comes in to Twitter goes through a torch model at this point so we're
really dealing with an enormous amount of data in a live setting the
development of torch just to give you a sense of how we think about how it was built and how we're extending it is
there's some kind of tenets of our core philosophy and if really the first is things should be as not to this isn't
necessarily good or bad this but this is our choice whenever you hit enter on a particular line and your I torch
notebook or on the command line you should get an answer back and this is something that we've we've tried to
stick to pretty pretty tightly so no compilation time imperative programming right so just write your code and you
know each each line of code executes something and passes it to the next line and minimal abstraction what I mean by
minimal abstraction is if you want a reason about how your code is performing it shouldn't take you that many jumps to
go to the C code that's actually being run in fact it usually is one or two jumps from the file that defines the
function that you care about to the actual C code so if you want a reason about performance or really understand what's going on it's it's it's quite
easy to do so in torch I want to take a little bit of a detour
and tell you about how torch thinks about its objects how it thinks about the tensor because this can help you
also reason about performance a lot of the reason why people come to torch is to build high-performance models very
quickly and easily so I mentioned tensors before so attentional tensor a
tensor is an N dimensional array and a tensor is actually just a pointer it's a
view into your member into your data that's sitting in memory all right so it's just a it's a shape it's um it's a
view into into what's actually being stored in your RAM and it's stored in a row major way so that means if I go to the first
element of my tensor in memory and I move over one I'm moving over one in a row and not one in a column column major
memory storage does exist it's just less common today so you often see row major so this tensor is defined by its link to
some storage and it's size 4 by 6 and it's tried six by one and six by one means if I move one down in the column
direction I actually have to skip six elements in memory right whereas the one
here means if I move over one in the second axis the row axis I have to go over one in memory so if I take a slice
of this tensor using the Select command so I select along the first dimension
the third element what he gives me back is a new tensor it doesn't give me a new memory this is a thing that that happens
a lot in torch is you'll deal with views into memory you won't do memory copies right so usually working with kind of
the raw data in RAM and so this creates a new tensor with the size of six
because there's six elements astride of one because we've pulled out a row not a column and an offset of 13 that means I
have to go 13 elements from the beginning of the original storage to find that piece of memory so if I pull
out a column then something different happens which is they still have or I have a size of four here and my stride
is now six because in order to grab each element of the column I have to skip six and then the offset of three is because
I grab the third element there all right so that's kind of a view of the of the memory model and if we act
run something like this like we instantiate a double-a tensor of double of foot double values inside of the
tensor and fill it with you know uniform uniform distribution and print it we can
see the values here and then we grab a slice B and print it it's just this row
and then we can fill B with just some number and print it now it's filled with that number now if we go back and print
a we've actually overwritten the values there so this is something you see a lot in torches is working on one big piece
of shared memory and as I mentioned before working with CUDA is really
really easy so if you just require ku torch which is installed automatically if you have a CUDA GPU using the
instructions on the github repository you can instantiate a tensor on the GPU and do the same thing and it will just
work so now I want to talk a bit about the frameworks that you'll use to
actually train neural networks in torch so this is a schematic kind of cartoon
of how we of the pieces we typically need to train a neural network so we've got our data stored on you know hard
drive or on a big distributed file system and we have some system for loading that data off of that file
system which goes into a nice queue and then some training code which orchestrates a neural network so the
thing actually making the prediction a cost function which is a measure of how good our neural network is at any point
in our training and an optimizer which is going to take the gradient of the cost with respect to the parameters in
the neural network and try to make the neural network better so in the torch ecosystem we've got some packages that
tackle each one of these separately so I won't talk about threads here there's actually several different libraries that will do this there's actually
several different libraries that will do each one of these things but this one is maybe the most common or the easiest to
start with and and then here we'll cover both the specification of the neural
network and the cost function as well as the mechanisms to push data through the neural network in the cost function and
pull the gradients back from the cost to the parameters and then the optimizer which is we've heard mentioned several
times today is to cast a gradient descent or we're outta grad so let me talk about NN
first give you a flavor of kind of how it works and what the pieces are so NN
is a package for building feed-forward neural networks mostly feed-forward
neural networks but kind of clicking Lego blocks together right so you might start with your input
and then click together a fully connected layer and then another fully connected layer and then maybe some output right so here I've defined a
sequential container which is going to be a container for all my Lego blocks and then I might click in a spatial
convolution so I'm going to be working with images maybe a non-linearity some max pooling some other layers as well to
kind of complete the whole neural network and then I might add a log softmax at the end to to compute class
probabilities so this this kind of the structure that you'll build neural networks with in NN is define a
container and then one by one add pieces down a processing hierarchy and I
mentioned the sequential container which is starting from inputs and then proceeding linearly there's two other types of containers that you might use
but generally NN shines when your architecture is linear right not when
it's got some crazy branches or anything like that the there's not a lot of API
to the NN package so if you if you learn these couple functions which will be in the slides for later if you want to
refer to them back you will understand all the mechanisms that you need to know to push data through a neural network
and then to push it through a criterion or a loss function and then to pull those gradients back in order to make a
gradient update to your model so these are really the API is the levers that you need to know to kind of drive your
neural network and of course we have a CUDA back-end for n n so in the same way
that you'll just call CUDA on some data you can call CUDA on a container and that will move the whole model onto the
GPU and then anything that you do with that model will occur on the GPU so it's kind of a one-liner to start training
models on a graphics processor so for doing feed-forward neural networks n n
is pretty great but for starting too weirder architectures like richard social yesterday mentioned a pretty
complicated NLP model that starts with glove vectors which are kind of like shallow neural networks and then a
recursive neural network and then an attention mechanism and all these things were interacting in strange ways that's
actually pretty hard to specify in NN at Twitter we have a package called torch Auto grab which makes these kinds of
gluing different model pieces together really easy and in fact the pieces can be as small as addition division
multiplication and subtraction so you can glue together any size piece of computation and still get a correct
model out and I'll talk more about that in a moment the optin package is what you need in
order to train models with like stochastic gradient descent or a degrade or out of delta whatever your optimizer
is that you that's your favor the API is pretty straightforward but
maybe a little bit different for people kind of coming from the Python world it's got a bit of a functional approach where it will actually you'll you'll
pass a function to opt in that will evaluate your neural network and pass back the gradients so that's just
something to be aware of it's a little bit of a different style another gotcha with optin that you might run into and
you'll see in some of the notebooks that are online is your parameters should be
linear in memory so if you want to optimize to neural networks that are interacting in some way you actually
need to first bring their parameters together into one tensor and then pass that to opt in there's just something to
be aware of so I want to talk for the rest of the talk about torch Auto grad
but also about some of the ideas that are behind torch Auto grad and how those link all the deep learning libraries
that you possibly could choose so first I want to take a step back and say that
just appreciate the wonderful stable abstractions that we have in scientific computing right so Fortran you know back
in 57 I don't think anybody uses Fortran 57 but people might actually still use Fortran 90 the idea of an array was
didn't exist on a computer and it really took some pretty crazy thinking I think
to build a system that made arrays something we take for granted same with linear algebra over about a 20-year
period starting in the late 70s people decided oh maybe we should think about linear algebra in a systematic way and
now we don't really worry about this if you want to multiply two matrices that used to be you know a phd's worth of
work to do that at scale and now we just you know we don't even actually import
Blas there's so many wrappers of blasts that we don't even think about this anymore so this is another abstraction and also the idea that we should have
all of the routines that we would possibly want to call in one place available that we don't have to write that was kind of invented I would say by
MATLAB in the mid-80s and then really popularized in the open-source community by numpy and we should take them for
granted we should totally forget about them that because they make us faster they make us better for us to assume
these things will work so machine learning has other abstractions besides
these computational ones that we take for granted all gradient based optimization that includes neural nets
as a subset relies on automatic differentiation to calculate those gradients right and and I like this
definition from Barack Perlmutter automatic differentiation mechanically calculates derivatives as functions
expressed as computer programs right so it doesn't derive things are right on a piece of paper with a pencil it derives
computer programs app machine precision and with complexity guarantees those
last two clauses differentiate it from finite differences where you take the input to a program you perturb it
slightly and you measure the gradient that way that's a very bad way to measure gradients it's it's numerically
very unstable and it's not symbolic differentiation so it's not writing down the symbolic expression of a neural
network putting it in Mathematica or maple and then it asking for the the derivative because your expression might
go from this to this so you get expressions well when you do naive symbolic differentiation and you don't
get that with automatic differentiation so automatic differentiation I would say
is the abstraction for gradient based machine learning it's been rediscovered
several times there's a review by Woodrow and there I think the first implementation where
it actually operates on a computer program was by Bert's bill pending in 1980 although it has been described back
you know in 1964 by Wengert in in neural networks rumble heart is the one that I
suppose popularized it as back propagation although back propagation is a special case of auto-da-f this this I
think is important in nuclear science and computational fluid dynamics and in weather modeling these people have been
using auto-da-f for years decades and their tools in many ways are much more sophisticated than we have in machine
learning there's a lot of ideas that we have yet to import from people that model the weather that would really
benefit our ability to train larger and larger models and I would clarify that
our abstraction and machine learning is actually reverse mode automatic differentiation there's two different
types two extremes I should say forward mode in Reverse mode you never hear about forward mode and you never hear
about forward mode of machine learning because it's a very bad idea to try forward mode and machine learning and I'll show you why so here is a cat
picture from the internet and my job at my job is to decide that that is in fact
a cat picture this is actually something that we do do at Twitter what I am doing
is passing this cat through successive layers of transformations than eventually producing a probability over
classes I'm getting it wrong my classifier thinks it's a dog so I'd like to train my neural net to think it's a
cat so I have a loss a gradient of my loss and I have it with respect to my
parameters and this is my gradient that will let me update my parameters and it is composed of multiple pieces and using
the chain rule I know that I can fold this together to actually compute the loss I want which is the gradient of the law through the respect to the
parameters the issue is I can do it either left to right or right to left so going from left to right looks like this
whoops that was very fast okay I'll do
two big matrix matrix multiplies so this is bad this is not good because we had
these huge matrix matrix products that we're keeping around it's actually worse than this and I'll show you in another
view of forward node so see I have a computer program so no longer a symbolic representation of a neural net this is
just some computer program and let's say I'd like to optimize a write a is the single parameter of my neural net it's a
very silly trivial example but I think it will help illustrate the point so I can execute this program and look at all
of the arithmetic operations that occur and build what's called a trace so I'll define say a is 3 I'll define B
is to C is 1 and then I'll start executing the code I'm actually going to look if B is greater than C and choose a
branch to operate on but then ignore it in my trace so I've chosen one of those
traces that one of those branches which is the first because B is greater than C and I have some output value D and I'll
return the output value all right so this is a trace execution of my program given some inputs so to calculate in
forward mode the derivative of my output D with respect to a I'll define a is 3
and then initialize a gradient of a with respect to itself and the idea is I eventually want the derivative of D with
respect to a and I'll build it up sequentially da da and then I'll do D be da and then Dissidia in ddd a so I'm
moving from the left to the right building up my gradient I can't do much about the derivative of B with respect
to a right now so I'll define C and remove C with respect to a and then I
have my value D and then I can define my target value which is the gradient of D with respect to a so if I wanted the
gradient of D with respect to B so if I had a two parameter neural network and I wanted optimize both at once I would
have to execute this whole thing again and initialize this guy here as DB DB
has one right so if you have a million parameters in your neural network or tens of millions if you have to do a
million evaluations of forward mode or tens of millions of evaluations of fort mode so it is a very bad idea to try
forward mode automatic differentiation on neural network and that's why you probably never heard of it so now you
can forget about it but the alternative is reverse mode and that's starting from
the right to the left so now I've got this nice matrix that your products which are much smaller and
the complexity is much better and there's an interesting difference when I actually go to do this in computer code
and you'll see these words are closer together and that's because for reverse
mode I actually have to evaluate the whole program before I can start deriving because I'm starting with the
derivative of D with respect to D and then decrementing derivative of D with respect to C with respect to D with
respect to a so I'm going the other way but I have to have all the information first before I start that so now I can
initialize derivative of D with respect to D and I can walk backwards and return
both the value and get gradient what's really nice about this is you'll notice
here I actually have all the information I need to calculate the derivatives of D with respect to these other parameters
so that's why we really like reverse mode auto-da-f aka back propagation for
neural nets is if you have a million of these guys you really want to be ready to compute them all at once right and
doing these with matrices is very efficient thing to do on the computer so we've implemented this trace based
automatic differentiation in a package called Auto grad and this is the entirety of a neural network so this is
how you would specify and train a neural network and autocrat so I'll initialize
my parameters we'll just be some random numbers and then here is my neural network function I'm multiplying my you
know image that I'm passing in by my white matrix and adding a bias non-linearity doing it again and then
returning some probabilities and I have a loss which will take in an image and
return a prediction so just using this function and then I'll just take the mean squared error or it's the sum
squared error in order to get the gradients of this function the derivative of the loss with respect to
these parameters all I have to do is import this autograph package and then call grad on this function this returns
a new function that returns the gradients of my original function so
it's a what's called a higher-order function it's inputs and its outputs are a function so whenever you see that
Noblet that upside-down triangle grad triangle this is the coding equivalent of that and then to Train
we'll just call our D loss function on our parameters our image and our label which I'm just pretending like you
already have a system to get here when we have our gradients and then we're updating with stochastic gradient
descent here all right so it's a very thin it's it's really just this this is the interface with which you talk with
Auto grad so what's actually happening so here's my simple function as we
evaluate it we're actually keeping track of everything that you're doing in order to be able to reverse it so we're actually
building that trace list that I described before and keeping track of it internally so we'll start online I guess
that's five so we'll multiply some things we'll keep track of the fact you multiplied and the inputs will keep
track of the addition and the inputs and also the output of addition will keep track of inputs outputs in the function
every time and we'll kind of walk down this function and build your compute graph just in time so as you're running
your code we're learning what you've done and the way we track that and I won't go into details we actually
replace every function and torch with like a like a spy function so instead of just running torch dot some our spy
function says oh I hear you're running torch dot some let me remember the parameters you gave me let me run some
on those parameters remember the output and then return it like nothing happened but internally we're remembering all
those things and the way we do this to actually compute the gradients is we're
walking back this list like I described before and every time we get to a point where we need to calculate a partial
derivative we look it up so we've written all of the partial derivatives for Torche functions and it really every
neural network library is going to do this at some level of granularity so let me walk you through another couple
examples just to show you what it could do so this is kind of a pretty vanilla one we can you know add and multiply
scalars and get the correct gradient this is where things get a little bit more interesting if there's an if
statement all right so this control flow can be a little bit difficult or awkward and a lot of existing deep learning libraries because we just listen to what
era medic functions get run we ignore control flow so we just go right through this stuff all right so we can get the
correct gradient even with if statements we actually care about tensors when
we're doing optimization or machine learning so everything I've shown you that works with scalars also works with
tensors just as easily this is in the notebook that is on the github repository if you want to play with it
this is where things get a little bit interesting for loops also work just fine and not just for loops that have a
fixed length which is something that is perhaps easy to unroll but for loops whose duration can depend on data you
just computed right or while loops whose stopping condition can depend on a computation that occurs in the while
loop we don't really care we're building your graph dynamically and when it's done and when you return some value will
calculate the derivative derivatives of the graph that we have you can turn any for loop into a recursive function this
is kind of wacky I mean I don't know how you would actually use this in practice but you can cook up a lot of crazy
things you might try with autograph and they just work so here we have a function f if B is at some stopping
condition will return a otherwise we'll call F and we're gonna differentiate this right so we're gonna differentiate
a fully recursive function and it works just fine another aspect which is coming
up more and more as papers are coming out that basically disrespect the sanctity of the partial you know of the derivative of the gradient and people
are computing synthetic gradients they're you know adding they're clipping two gradients or people are messing with
kind of the the internals of back propagation or of auto-da-f it's actually pretty easy to start to engage
with in Auto grad so say I'm going to sum the floor of a to the third power so
the floor operation is piecewise constant so the derivative is zero almost everywhere except for where it's undefined why would I want to do this
for instance if you wanted to build a differentiable JPEG encoder or differentiable MPEG encoder in
compression algorithms like that there's often a quantization step that will floor around or truncate numbers and if
you wanted to differentiate through that to build like a neural Jake algorithm or something you need to pass gradients through something that
ordinarily does not and so if we look at what the gradient is at zero everywhere I won't go into the details but you can
ask Auto grad to use your own gradient for anything so if you have a new module that you want to define and either
you've written high-performance code for it and you want to use it or you want to redefine or overwrite you know the
gradients that we have there's a pretty easy mechanism for doing that and then when you call your special dot floor you
can propagate gradients through it right and here I was just saying basically ignore the gradient of floor so this is
a toy example but there are real places where you have a non differentiable
bottleneck inside of your computer off and you want to either hop over it or find some approximation and auto grad
has a mechanism for very easily plugging those types of things in so that's a bit
of what auto grad is and what it can do and I want to turn our attention to how
autograph relates to other deep learning libraries and maybe how they're common and how they're similar and how they're
different so one big difference that I
found between different deep learning libraries is the level of granularity at which you are allowed to specify your
neural network so there's a lot of libraries where you say you get a confident or you get a feed-forward
neural network and that's it right so the menu is two items long and that's
fine I think Andre I really hit it on the head where if you want to solve a problem don't be a hero use somebody else's network so maybe this is vgg that
you've downloaded from from the model Zoo or something like that right so this is the don't be a hero regime on the left in the middle there's a lot of
really convenient neural net specific libraries like torch and n and Karras and lasagna and you get to put together
big layers and you don't really get to see what's inside those layers but you get to click together linear layers or
convolutions and usually that's kind of what you want to do and on the far end of the spectrum the things you can click
together are the function the the numeric functions in your kind of host
scientific computing library right like add multiply subtract and these are
features of projects like Otto grad and Theano and tensor flow and the reason why these boundaries are
made is because the developers have chosen to give you partial derivatives at these interfaces all right so this is
how they've defined their api's and these are the interfaces with you know across which you as a user cannot pass
if you want to new one of these modules for the type on the left or the type in
the middle you have to go in and build a whole new model and actually implement the partial derivatives but with the
types of libraries on the right you can build your own models by modules by composing primitive operations all right
so that's one difference that you can find in practice how these things are
implemented under the hood usually means this is the totally shrink-wrap stuff
and maybe they implemented this whole thing by hand usually these guys in the middle are rappers they're rapping some
other library and the guys on the right are usually actually implementing automatic differentiation so Auto grad
in theano and tensorflow all implement auto death and the guys in the middle are taking advantage of that to make
more convenient wrappers so another aspect that's different is how these
graphs are built so I'll remind you in Auto grad we build these things just in time by listening to what you're doing
and recording it but that's not how all neural network libraries are built and
this is an axis along which I think that they are differentiated meaningfully so there's a lot of libraries that build
these graphs explicitly where you say I'm going to click this Lego block into this Lego block where I'm going to give you this yamo specification file the
graph is totally static and you really have no opportunity for compiler optimizations there and then there are
the just-in-time library so Auto grad and chain ER is another one where you
get any graph the graph can be anything it can change from sample to sample it can be you know to the length of the graph can be determined by the compute
that occurs in the graph you have very little opportunity for compiler optimizations there so speed can be an
issue sometimes and in the middle there's a head of time libraries like tensorflow and Theano where you construct your graph using a
domain-specific language you hand it off to their runtime and then they can do crazy stuff to make it faster the
problem with that is it can be awkward to work with I guess that got cut off it can be awkward to work with control flow and I think
there's a reason why it can be awkward to work with control flow and it's because of the types of graphs that
these libraries are actually manipulating so we say compute graph a lot we say data flow graph a lot data
flow graph has a pretty restricted meaning and it means that the nodes in
your graph do computation and the edges are data and there's no room for control flow in a graph that is a data flow
graph right so static data flow is the type of graph that N and n Cafe use because all the ops are the nodes and
the edges are just the data and the graph can't change get data flow just in
time compiled data flow like Auto grad and chain ER has the same characteristics but the graph can change from iteration to iteration because we
wait until you're done computing the forward pass to build the graph in the middle there's kind of a hybrid and I
don't know what to call that graph type the ops are nodes the edges are data but
then there's special information that the runtime gets in order to expand control flow or for loops so scan is in
Theano is an instance of this where the Theano runtime has special information that allows it to make scan work but
it's kind of it's it's it's conspiring with the graph data type to do that there's actually another graph type that
naturally expresses control flow and data flow together that I haven't seen implemented in a deep learning library
it's called see of nodes from cliff clicks thesis in the mid-90s it seems
like a really natural thing to try and man maybe that's something that comes up in the future but that's kind of a big question marks
maybe one of you will we'll try that out and see how well it works so in practice
this level of granularity can sometimes slow us down having to work with addition and multiplication can be nice
if you want to try crazy stuff but if you know you want to make a confident why don't you just rush all the way over
to the left if you want to take you know inception and add another layer where you want to use the type in the middle
an autograph allows you to do that so I'll just kind of walk through writing a
neural net three ways very quickly and then and then close questions shortly thereafter so using
the fully granular approach there's a lot of text on the screen but the top half is basically let's instantiate our
parameters the way that we want to and then here just like I've showed you in previous slides let's do a multiply and
let's do an addition and put it through non-linearity we're being very explicit right so we're breaking all the abstraction boundaries and we're just
using primitive operations we can use the layer based approach so in Auto grad we have a facility to turn all of the N
and modules of which there are a lot may be an exhaustive list for what you'd want to use for standard deep learning
applications you can turn them into functions and then just use them so linear one on the linear parameters and
your input and some activation you can go through your neural network this way so you can use a layer based approach if
you want and if you just want your network just a feed-forward neural network we've got a couple of these kind
of standard models just ready to go so you can just say give me a neural network give me log softmax and a loss
and let me blow these guys together so you can do it any of those three ways
Auto grad at Twitter has had a pretty cool impact we use NN for a lot of stuff
when we use Auto grat as well but being able to reach for autograph to try something totally crazy and just knowing
that you're going to get the right gradients has really accelerated the pace of high risk potentially high payoff attempts that we make so one
crazy thing you might want to try is experiment with loss functions so instead of I have a hundred image
classes and I want to have my convolutional neural network be good at classifying this hundred image classes
maybe you have a taxonomy of classes maybe you have a vehicle and then a bus
a car and a motorcycle and if you guess any one of those you kind of want partial credit for vehicle or if you guess motorcycle you want partial credit
for for car so building that kind of a tree loss is actually really straightforward an auto grad and you can
do that in in just one sitting but might be more complicated to do that in other libraries we have to crack open the
abstraction barrier write your own partial derivatives glue it back together and then use that module that you've built we've trained models that
are in production in auto grad so this is something that's a battle-tested to a sense and is running on
large amount of media Twitter in a sense Auto grad doesn't actually matter when you're running in production because you
just you have your function definition for your prediction of your neural network and then the gradient part just
goes away or so all the fancy stuff where we play Storch with our secret you know listener functions all that just
goes away and you just have some numerical code so there's actually no speed penalty a test time at all and we
have an optimized mode which does a little bit of compiler stuff still work in progress but for the average model
it's as fast sometimes faster than n N and for really complicated stuff if you
wrote that by hand you'd probably be faster but the time to first model fit using Auto grad is dramatically reduced
because you don't have to worry about correctness so this is a big wall of text but it's meant to put in your head
some ideas of things from automatic differentiation from that world that we
don't have yet that we really want right to be able to train models faster and better so the first is checkpointing
this does not check pointing where you save your model every 10 iterations this is check pointing where on your forward
pass you might you in normal reverse mode automatic differentiation you have
to remember every single piece of computation you do because you might need it to calculate the derivatives and checkpointing you just delete them you
let them go away because you think that some of those might actually be easier to recompute than to store alright so
for point wise nonlinearities for instance it might be easier once you've loaded your data just to recompute the
reloj as opposed to saving the result of reloj and loading that back in again mixing forward and reverse mode is
something that you can imagine being important for kind of complicated architectures although I don't really
know how much impact that would have so in the chain rule you can either go from left to right or you could start in the middle and go out you can do all kinds
of crazy stuff if you want and we really just do reverse mode for diamond shape
graphs where your computation explodes out and it comes back in that might be
useful to start with forward mode and then finish with the reverse mode or an hourglass you might want to start with reverse mode and end with forward mode
stencils are a generalization of convolutions that people use a lot in
computer graphics automatically calculate really efficient derivatives of image processing just general image processing
algorithms is under active investigation in the graphics world and in the computer vision world so these are two
references that are kind of neat papers source to source transformations is something that hasn't really made it it
basically has kind of been dormant for about ten or fifteen years so the gold standard used to be you take a piece of
code as text and you output another piece of code as text what we're doing now in deep learning is we're always
building runtimes we're always building some domain-specific layer that depends on you actually running code it used to
be that you just read that text and kind of like a compiler spit out the gradient this this was the gold standard it might
not be now but I think it's worth three investigating and then higher order gradients so Hessian vector products and
kind of Hessian based optimization maybe doesn't always have full payoff I actually don't recall hearing anything
about this at this school so far because it's very expensive and difficult to do
expensive computationally fashion is just if you take the grad of F it gives you the gradients if you want the second
derivative right so you take grad a grad of F so there's efficient ways to do this it's still kind of an open problem
but there are libraries out there the Python version of autograph dust as well diff sharp and hype both also do this as
well so to kind of close out you should just try it out it's really easy to get
it if you have anaconda if you use Python we've made it so that Lua is
fully installable with anaconda so if you're already using it it's very very easy to get all of the tools that I've
showed you today and that's kind of the single line to interface with it and if
you have any questions you can find me on Twitter or email or github but I'm happy to to answer any questions that
you have
oh yeah I have no idea
thanks thanks for the great talk oh yeah I was wondering what's the state
of the data visualization facilities in Lua compared to say Python if I'm Frank
it's it's not as good python has been at this for you know five ten years really actively building matplotlib and you
know Seabourn and all these other libraries and in Lua were importing other people's work so book ajs is
really the best that i've seen so far and that's something you can use in a notebook so you have the full suite of
that of that particular library yeah
hey thanks for the luck is it possible to convert a model train with torch in
into a C model that's deployable in you know production we just run torch in
production we use a little model but you want to run it and see so the whole
layer of torch that's actually doing the work is in C and calling torch from C I
don't have a specific website I can point you to but you can very easily call and execute a Lua script from C
it's like three or four lines of code in C thank you the follow-up the question
about see just now just like if I'm gonna compile I mean I want to have Tosh into my sequence passcode what kind of
overhead do I see I see just animations yourself like I have a 10,000 line - what just-in-time compiler
I need to put that in there right oh I can I avoid that because for example I
think about if I'm going to put the one in an embedded system they have a mouth resource of anything during inference
time so I'm sorry during yet during inference time there's there's no appreciable overhead if I'm
understanding your question right so you you are importing a Louis so in your C code you're going to basically say Lua
please run this Lua script and that's going to call out into other C code so all this overhead I talked about with
autograph that's training time that doesn't exist at test time at all so so
during test time but the thing is I still need to have Lua compile into my C code right yeah so this is something
people have been doing for like 15 20 years it's pretty mature so Lua is in like microwaves for instance people have
done very embedded applications of Lua yeah I think the binary for Lu is like I
don't want to it's like a round it's a kilobytes it's very very small there's 10,000 lines of code so when it compiles
down on small
so there's a question from the twitters says i'm using a combination of Karros
and tensor flow why should I use torture auto grad if you're happy then you know
that's great I guess so people tend to reach for torch when they would like to be able to
reason very easily about performance the
kind of the more of a compiler infrastructure that gets added to a deep learning environment the harder it can
be for the end user right away from the people that originally made the library can be harder for the end user to reason
why is this slow why is this not working you might eventually see some github issue later my network is slow in these
conditions and then it gets closed a year after you had to have shipped your project right I mean these things can happen it's not the fault of anybody
it's just that torch was designed to basically be very thin a thin layer over C code so if that's something that you
care about torch is a really good thing to work for if careless and tensorflow is working great for you then keep deep
learning you know that's awesome so I'm
trying to see
it's hard to filter where will the slides be posted it's not
a deep learning question but they will be posted that's the answer to that
question I have a question now how do I access through so normally all the web
services production generally are another you know fast based application in Python or you know Java based Web
Services right or maybe in you know in the cellphone through Android which is also Java right so how do you call these
models which were you know trained in torch how would you actually access those there's a couple different ways
you can do that if you're using a feed-forward neural network writing the
Java code to do the matrix multiplies can be pretty straightforward and we've actually done that before or it's just
simpler tor just write the deep learning code load in the weights we'll serialize it however you know it needs to be
loaded that's one approach is kind of you know hacking short term at Twitter we've engineered a system where we
actually have Lua virtual machines running inside of Java and we talked over the j'ni so we have like a more
permanent solution for that but if you're using standard model architectures you might try to serialize
your weights and then use the native deep learning library that exists to load up those weights and then run for
it and that with some debugging I think that's perfectly fair approach if you have this split between testing and kind
of deployment where you're constrained by language or environment that's generally the thing that you know I mean
you do basically just you know C realize your model and then try to read it what about the latency actually so related to
you know this so when you see realize that hackish way at least you can get you know that latency things sold out
but is there any plan basically to have you know interfaces available for other languages so that you know you don't
have to do this extra step of serializing and then you know loading into language if you if you don't like
in your case you were mentioning that in Twitter you have
- available inside your Java JVM our access to the JVM using j'ni so what
what what impact does it have on the latency and by latency you mean time to
ship the model not the latency of how long it takes many predictions oh that's
gonna be very engineering dependent so if you're calling torch from C code the latency is not appreciable over if
you're just running Lua code and that can be extremely fast if you're going through some wrapper like through the
J&I or something like that you will incur an overhead and you should just try to pick the interfaces that reduce
that as much even if you incur engineering overhead to do so I don't know if that answers your question I'm a
little bit distant from the server side so I can't give you I just don't know but generally I think what I can say
this that's fair is we're constrained by machine learning you know model complexity latency we are not
constrained by overhead of like figuring out how to actually get those predictions like to an HTTP request for
instance serving which you know which is
kind of sort of solving this problem yeah not that I'm aware of
again the torch community is not centralized and so people could be working on a totally awesome
you know complement to the the tensorflow server but I am not aware of
it thank you okay we're going to take a
short break of 15 minutes let's thanks Alex again

----------

-----

--06--

-----
Date: 2016.09.27
Link: [Sequence to Sequence Deep Learning (Quoc Le, Google)](https://www.youtube.com/watch?v=G5RY_SUJih4)
Transcription:

eating that were divided in two parts so number one and we work with you and
develop the sequence to sequence learning and then that's the second part I would I will place sequin to sequence
in a broader context or a lot of exciting work in this area now so let's
multiply this by a an example so a week
ago I came back from vacation and my in my inbox I have five hundred and eight
emails and reply emails and a lot of emails I basically just require just yes
and no answer so let's try to see whether we can do a system that can
automatically reply these emails to say yes and no and for example so some of
the email would be you know from my my friend on she said hi in the subject and
she said are you visiting Vietnam for the New Year walk that would be her content and then my probable reply would
be yes so you can gather another set like this and then you know you have
some inputs content so less for now let's ignore the the the on the author
of the email and the subject but let's focus on the content so let's suppose that you gather some email and some
input would be something like are you visited in Vietnam for the New Year Kwok and the answer will be yes and then the
another email would be are you hanging out with us tonight the answer is no because I'm quite busy
so the third email would be did you read the coolness paper on breast net the
answer is yes because I liked it now let's let's do a little bit of
processing we're basically in the in the previous slide we have gear and comma
and then kwok and then question mark and so on so let's let's do a little bit of
processing and then put the the comma a
space between gear and comma and then Kwok and question mark and so on so this
step a lot of people call tokenization and normalization so let's do that with our emails now so and then the second
step I would do would be to do feature representation so in this step what I'm going to do is the following I'm going
to construct a 2,000 dimensional vector 2,000 represent the size of English
vocabulary and then I'm going to go through email I'm going to count how many times a particular word occur in my
email for example for example the world
are occur one in my email so I increase the counter and then you occur one so I
increased another counter and s etc and then I will reserve at the end a token
to reserve to just count all the words that just our vocabulary okay and then
now you now use successful you if you do this project a process you're going to
convert all of you or your email from input to output pairs where the input would be fixed line representation of
20,000 dimensional vector and output would be either year or one okay any
questions so far okay good
okay so I will get so as you said somebody in the audience that the order
of the words don't matter matter and the answer is yes so I'm going to get back to that issue later now so
that's x and y and now your job my job now is to try to find some W search that
W time X can approximate Y Y is the output right and Y here is yes and no so
because of this problem is has two categories you can think of it as a
logistic regression problem now if anybody follow the gray cs2 10:29 class
by andrew probably can formulate this very quickly but in a very short you the
album comes as follow you kind of try to come up with a vector for every email
your w is a two column matrix okay
the first column will find the probability for the eat whether the email have to be answer as yes second
column will be answered as no and then you basically take the dot product
between w1 at the first column now Adam is called the stochastic gwendy set so
you run for iteration one to like a million you run for a long long time you sample a random email X and then some
reply and then if the reply is yes then you want to update your w1 and w2 such
that you increase the probability that the answer is yes so you increase the
first probability now if your reply is if the correct reply is no then you're
gonna update w1 and w2 so that you can increase the probability of the is email
to be answered as you know so the second probability okay so let's call those a p1 and p2 now so because to
update I said to update the increase what does that mean what that means is that you find the gradient of the
partial gradient of the objective function with respect to some parameter so now you have to pick some alpha which
is the learning rate and then you say W 1 is equal to W 1 plus some alpha the
partial derivative of block of P 1 with respect to D of W 1 ok
now I cheated a little bit here because I used the log function it turns out because the log function is a mono is a
monotonic increasing function so increasing P 1 is equivalent to increase in the log of P 1 ok and it usually with
this formulation stochastic gradient descent works better any question so far
and then you can also update you know W 2 if the email is to be reply is yes and
you can you can have different way to update and to if the reply is no so
what's a and then if you have a new email coming in then you take X and then
then you control into the vector then you compute the first probability ok W 1
time X divided by W exponential W 1 time X plus exponential or W 2 time X and if
that probability is larger than 0.5 then you say yes and if that probability is
less than 0.5 then you say no ok so that's how you do prediction with this
now now this there's a problem with this representation is that there's some
information loss so somebody in the audience just said that the order of the words don't matter and that's that's
true now let's let's fix this problem by using something called the recurrent
Network and I think a rigid soldier already talked about recurrent networks
and some part of it yesterday and Andrei as well now there the idea of a
recurrent Network is basically you have also have fixed representation for your
input but it actually preserves some sort of info ordering information and
the way that you compute the hidden units the following so the function hash of Euro is
basically hyperbolic hyperbolic tangent of some some matrix you time the work
vector for the world are okay so Richard also talk about what vectors yesterday
so you you can take what vectors coming out of what to back or you can just
actually randomly initialize them if you want to okay so let's suppose that that's H of zero now H of one would be a
function of H zero and the vector for
you which is a times H of zero plus u
times V of vector u and then you can keep going with that to see one of my
three three most complicated slides so you are you should ask questions no
questions so everybody familiar with recording that sir well
okay so to make predictions with this but you you tack on the label at the
last step and then you say try to predict why for me how do you do that now here I I basically you you went the
way you did before and basically you make update on the W matrix which is the
the classifier at the top like what I said earlier now but you also have to
update all the relevant matrices which is the matrix you the matrix a and some
work vectors right so this is basically you have to compute the partial
derivative of the last function with respect to those parameters now that's
going to be very complicated and usually I when I do that I do that myself I get
that wrong but there's a lot of tools out there that you can use which is you
can use auto auto differentiation in tensor flow or you can call torch or you
can call piano to actually compute the derivatives and once you have the derivatives you can just make the update
right yeah yes so you the matrix you are
share so I'm going to go back to one side so this matrix you I share all for
all vertical matrices right and the size
you have to determine ahead of time for example the number of column would be
the size of the work vectors but the number of rows must be like like a
thousand if you want or maybe 255 you want so this is model selection and it
depends on whether you under fit in over fitting to choose a bigger model or a smaller model and your compute power so
that you can train a larger model a smaller model
the matrix you yeah so the the work
vectors the world vectors the number of work vectors that you use are the size
of vocabulary right which is so you gonna tend to end up with 20,000 work
vectors right but the the size of so that means you have 20,000 rows in
matrix U but the number of column you can sorry the number of column is 20,000
but the number of row would be you have to determine up just yourself okay any
other questions now okay so what's a big
picture so the big picture is I started with bag-of-words representations and
then I talked about a and n as a new way to represent variable size input that
can capture some sort of ordering information then I'll talk about Auto differentiation so that you can compute
the partial derivatives and these you can find auto intensive flow or piano or
torch now then I talked about stochastic when descent as a way to train the
neural networks and the question so far
okay you have a question oh that's also
depends on how big your your training set and how big is your computer and so
on right but usually if you use an N and if you used like a hidden state of a hundred you should take like a couple
hours yeah but it depends largely largely depends on you know size of
training data because you want to iterate for all a lot of you sample a lot of emails right you and you want
your algorithm to see as many emails as possible right so okay so if you use
such algorithm to just say yes no and just know then you might end up losing a lot of friends
because because because we don't just
say yes no because we went to say when
for example my friend asked me are you visiting Vietnam for the new year walk then maybe the better answer would be yes see you soon right that's not better
nicer way to approach this and then if if my friends ask me are you hanging out
with us tonight so instances say no I would say no I'm too busy or did you read the coop ok
right so let's let's see how we're going to fix this so so before I'm gonna tell
you the solution I would say this is the this problem is drew it basically
requires you to map between variable size input and some variable to some
variable size output right and if you can do something like this then there's a lot of applications because you can do
auto reply which is what we've been working on so far but we can also work on user to do translation just like
between English French you can do image captioning so input would be an a fixed
like vector or representation coming from conflict and then output would be the cat sat on the mat right or you can
do summarization the input will be a document and output would be some summer summary of it or you can do two speech
transcription where you can have input would be speech frames and output would
be words or you can do conversation so basically the input would be the
conversation so far and the output could be might reply or you can do cue night etc etc so we can keep going on now so
how do we solve this problem so so this is this is hard so let's check out what
Android capacity has to say about recurrent networks okay so so Android
say that there's more than one way that you can configure your network to do things so we can do you could use your
network to map recurrent networks to map one two right so the at the bottom that's an
input the the green would be the hidden state and the output would be the what
you want to predict now 1 1 2 1 is not what we want right because we have many
too many so it's probably more like the last two to the right right but we
arrived as the solution that I said in the red box and the reason why it does
that's a better solution is because the the the size of the input and the size
of output can vary a lot sometimes you have smaller input but larger output but
sometimes you have larger input and smaller output so if you do the one in
the red circle you can be very flexible right if you do the one to the extreme
right then maybe the output has to be smaller or at least the same with the
with the input right which what we are that's what we don't want so let's construct a solution that look
like that so okay so here's the solution so the input would be something like hi
how are you right and then let's put a special token unless let's say the token
is end and then you're going to predict the first token which is M and then you
predict the second token fine and then you predict the throat Oken thanks and then you keep going on until you predict
the world end and then you stopped now I
want to mention that B in the previous set of slides I was just talking about
yes and no and ingest no you have only two choices okay now you have more than
two choices you have actually 20,000 choices and you can actually use the
algorithm that are the the logistic regression and you can expand it to cover that more than one more than two
choices you can have a lot of choices okay and then the algorithm uses just
follow the same way now so dizzy my first solution when I say walk - sick
- sick but it turns out it didn't work very well and the reason why I didn't work very well is the model never know
what it actually predicted in the in the last step so it keep a keep going and
you keep synthesizing output but it didn't know what it said it didn't know what decision it committed in the
previous step so a better simpler solution would look like this a better solution is you back basically you feed
what the model predicts in the previous step as input to the next step alright
so for example in this case I'm going to take am I'm going to feed it in to the next step so that I'm conduct completing
the dance in the second world which is fine and etc so a lot of people call
this concept auto regressive so you you take your you eat your own output and
make it as your input any questions so far or whenever it produced end then
just stop there's a special token end yeah now okay so the so relevant architecture
here would be the end code people also call the encoder as the what the
recurrent network in the input and the decoder would be the recurrent network in the output okay okay so how do you
train this so again so you basically you run for a million steps you see all your
emails and then you say you sample and for each iteration you sample an email X
and a reply why why would be you know I'm fine thanks right and then the
sample random work YT in Y and then you update the iron and encoder and decoder
parameters so that you can increase the probability that Y of T is correct given
all what you seen before which is your YT minus 1 YT minus 2 etc and also all
the axes right and then you have to compute the partial derivatives to make
it work so the computing part partial this is very difficult so again I recommend you to use something like Auto
differentiation intensive flow or torch or Tiano okay you have a question yeah
but the recurrent Network the number of parameters didn't change because you have U and V a UV and I are fixed right
okay so the question in the in the audience is that there's um if the iron
and are different in four different example and the answer is yes so the number of steps and are different I have
a question there okay yeah I'm gonna get
to that in the next slide yeah okay all right so the question is a
in practice how long would I go to for the RN I would say if you usually stop
at like 400 steps or something like that because outside of that it's going to be too long to make the update and compute
it's very expensive to compute but you
can go more if you want to yeah I have a question yeah
yeah yeah yeah so that's a problem so if I'm going to talk about the prediction
next so let me go to the prediction and then you can ask questions so okay so how do you do prediction so this the
first algorithm that can we can you can do is go greedy decoding okay in greedy
decoding is for any incoming email X I'm going to find I'm going to predict the
first word okay and then you find the most likely word and then you feed back in and then you find the next most
likely word and then then you feed back in and etc so if you keep going you keep
going until you see the world end and then stop all it is exceed a certain length you stop okay
now that's just do greedy okay so let's let's do a little bit less greedy so it
turns out that so given X you can predict more than one candidate so let's say you can predict a candidate's let's
say three okay so you take three candidates and then for each candidate
you're going to feed in the next step and then you arrive at three so the next step you're going to be have nine candidates right and then you're going
to end up going that way so here's a picture so given input X I'm going to
predict the first token there would be hi yes and please and given every first
token like this I'm going to feed back into the network and the network will produce another three and etc so you're
going to end up with a lot of a lot of candidates so how did you select the best candidate well you can traverse
each beam and then you compute the John probability at each step and then you
find the sequence I have the highest probability to be the sequence of choice
what is your reply any question to see
the most complicated slide in my talk oh yeah yes so the question is what do
you do with our vocabulary works now it turns out in this algorithm what you do is that for any word that is our
vocabulary you create a token call unknown and you map everything to
unknown or anything that our vocal every vocabulary to be unknown so it doesn't
seem very nicely but usually it works well there's a bunch of algorithms to
address these issues for example they break it into like characters and things like that and then it you could fix this problem
yeah yeah the cost function is that so I
go back one slide so the cost function one more slide so the cost function is
that you sample a random were YT here
let's suppose that here I this is my input sofa or an input and I'm sample YT
let's say T is equal to 2 so which means the work fine okay I'm at the work fine
I want to increase the probability of the model to predict whoa fine
so the every time the model will make a lot of predictions some a lot of them
will be incorrect right so you have a lot of probabilities you have probability for the water and
the probably a and etc and then probably for zzzzz right and you have a lot of
probabilities you want the probability probabilities for the worst for the work
fine to be as high as possible you increase the probability does that make
sense or you condition on IIM so you condition
so when I'm at fine my input would be hi how are you and and um okay that's
that's all I see and then I need to make a prediction and I have to make that prediction right right and you know if
I'm at the world thanks my input would be hi how are you and I'm fine and I
gotta get my thanks for probability right okay yeah I have a question here
oh I haven't thought about it yet so the
question is how do you personalize so well one way to do it is basically embed a user as a vector so let's suppose that
you have a lot of users and you embed a user as a vector that's one way to do it yeah I have a question here
yeah yeah so the question is that let's
suppose that my beam search is 10 then you go to from 10 like a hundred and
then a thousand and suddenly it grows very quickly right it go to rule a if
you if your sequence is long then you end up with K to the N or something like that well one way to do it is basically
you do truncate that beam search where any any sequence with very low probability you just pick it up you
don't use it anymore so you go so you can do this you can do 3 9 and then you
ten to seven and then you go back up to 9 right and then you keep going so that
way you don't end up with a huge beam and usually in practice using like a
beam size of three or ten would work just fine and whoops wait yeah yeah I
have a question okay so for because it's
a 9n we don't have to Pat the input now to be fast sometimes we have to Pat the
input because we want to make use make sure that batch processing what's very
well so you'd be bad but we paired with only like zero tokens
okay yeah so let's suppose that you have
a sequence of ten then you have a graph of ten when you have a sequence a batch of all twenty you haven't made a graph
for twenty and etc yeah that will make the GPU very happy I have a question
that oh so so you are you asking sort of so
my interpretation of your question is how do you insert the world embedding into the model is that correct our user
embed an old if you want to personalize the thing then at the beginning you have a vector and that's a vector for quoc
with a ID one two three four five and then if is Peter then the vector would
be five six seven eight yeah yeah that's
one way to do it yeah well there's more than one way you can do it at the end or you can do it at
the beginning or you can insert a tab at every prediction steps but my proposal
is just predict put it at the beginning the simpler okay I have a question there yeah you
yeah
that's a very good question the question is what if the model details right if we
make a prediction and then that's a bad prediction and your model never see and then it keeps detailing and it will
pretty produce garbage yeah that's a that's a good question so I'm going to get to that so well so this is sly so
there's an algorithm for scheduled sampling so in scheduled sampling what you do is you you instead of feeding the
truth during training you can fee feet what sample from the sub max so what
generated by the model and then feed in as input so that the model understands
that if it produce something bad it would suck actually can recover from it right so that's that's one way to
address this issue is that make sense yeah any question there's a question
here okay yeah yeah yeah so in this
algorithm yeah the question is how large is the the size of the Dakota well my
answer is that try to be as large as possible but it's going to be very slow and in this algorithm what happens is
that you you use the same you use like
fixed length embedding for like to represent the very very much the long
term dependency like a huge input right and that's going to be a problem so I'm
going to come back to that issue with the attention model in a second okay
any question okay here's a question
ah so does the model learn synonyms is
that a question or what's the question oh I see well yeah it turns out that if
you learn it turns out that it mapped good and if you visualize embedding the good and fine and so on I'm not very
closely to the to the embedding space but in the output there's we don't know
what else to do the other approach is basically to train the world embeddings using water vac and then try to ask the
model to regress to the world imbalance right so that's one way to address this issue we tried something like that did
not work very well so whatever we have in here was pretty good okay I have to
keep going but like any way the algorithm that you've seen so far turns
out actually answer some emails so if you use the smart reply feature in inbox
it's already used this system in production now for example in the indc
me email my colleague Ricardo got an email from his friend saying that hey we
wanted to invite you to join us from the early Thanksgiving on November 22nd
beginning around 2:00 p.m. please bring your favorite dish and reserve by next week and then it would propose three
answers for example the first answer would be telecine second answer would be
will be there and the third answer is sorry we won't be able to make it now
this where do these three answer come from those those are the beams now there's an algorithm to actually figure
out the diversity as well of the beams so that you don't end up with very similar answers so there's an algorithm
that like a heuristic that make these beams a little bit more diverse and then
they pick the best three to present to you
okay any question yeah I have a question here
yeah there's no guarantees so the question is how do I guarantee that the the beam would terminate an end now
there's no guarantee it can go on forever the indeed there are certain cases like that if you don't train the
model very well now but if you train the model well with with very good accuracy then the model usually terminates highly
see any cases that it don't terminate it doesn't terminate yeah but there are
some corner cases that it will do funny things but you you can stop the model
after like a thousand or hundred or something like that so that you make sure that the model doesn't do that
doesn't go on crazy right I have a question here
that's very interesting yeah it just comes out because there's a lot of emails and if you invite someone there's
more than one person and it might be it learns about Thanksgiving it just mean inviting the whole family things like
that yeah it just learned from statistics yeah or
maybe that something like that yeah okay okay oh in industry algorithm so the
question is do I do any post processing to correct the grammar of the beams in this algorithm we did not have to do it
yeah okay I have another question
so okay so the question how contextual so I would say we don't have any user embedding in this so it's pretty general
the input would be the previous emails and the output would be the prediction
the reply that's all we have so it sees a context which is the threat sofa okay
did I answer your question okay yeah we
you can catch me up after the talk yeah oh yeah it ran down too so yeah slow
question oh oh I see
so the question is there's some some emails are not relevant for a smart apply maybe they've too long or you
should not reply or something like that so in fact we have two algorithms so one hour with them this is to say yes or no
to reply right and then after it passes
the threshold there's an algorithm to run to produce the threshold so it's a combine of two our rhythms that are
actually I presented earlier yeah I have
to get going but you can get back to the question so there's a lot of a more interesting stuff coming along okay so
so what's a big picture so far so the big picture is that we have an i NN encoder that it's all the input and then
we have an iron and decoder the trying to predict one token at a time in the output now everything else force is the
same way so you can use stochastic when you sent to train the algorithm and then
you you do beam search decoding usually you do app in search of up 3 and then
you should be able to find good food good beam with the highest probability now someone in the audience brought up
the issue that we use fixed length representation so just before you you make a prediction
the Japan the hm and the white thing right before you go to the Dakota okay
that is the fixed-line representation and you can think of it as like it's a vector that capture all everything in
the in the input right it could be a thousand words or could be five words
and you use a fixed length representation for a variable length input which is kind of not so nice so we
want to to fix that issue so there's an algorithm coming along and it's actually
invented at a at University of Montreal you're sure he's here so the idea is to
use an attention so how does an attention work so in principle what you
want is something like this every time before you make a prediction let's say you predict the world am you kind of won
a loop again at all the hidden state so far you want to look at all what you see
in the input software okay now say when you do fine you also want to see all the
all the hidden state of the input sofa and and on now how do you do that in as
a program so well you can do this so you H of M you predict a vector C let's say
that vector is the same dimension with all the H okay
so if the your H of one each dimension of 100 then C also have a dimension of
100 okay and then you take C and then you do dot product dot product with all the H okay and then you have
coefficients a 0 a 1 blah blah blah to a
to the N okay and those are scalars okay
and then after you have those scalars you compute something called the beta which is basically I stop max of all the
Alpha right so 2q compute that you take the exponent bi is an exponential our AI
divided by the sum of Exponential's okay okay and then you take those bi and then
multiply by H by and then you take the weighted average and then you take the sum and then you
send it to add additional signal to predict the war and and then you keep
going with that right so in the next step you also predict another C and then you take that C to compute the dot
product you compute the B the a and then you can compute the B you can take the B you do the weighted average and then you
send it to the next time to send it to the prediction and then you use stochastic when you send to Train
everything okay and this autumn is implemented in
tensorflow okay so how how into table
what is going on here so let's suppose that you want to use this for translation so in translation you wanna
for example the input would be hi how are you and the output is Ola combos
paths or something like that okay and then when you put it the first word you
want Ola to correspond to the world hi okay because there's an one-to-one
mapping between the word high and Ola so if you use the attention model the
beta's that you learn will put a strong wait for the words Ola for the world
high and then it has a smaller wait for all the stuff and then if you keep going then when you say Como's then it will
focus on how and etc okay so it moves that coefficient it put a strong
emphasis on the relevant world and especially for translation it's extremely useful because you know the
one-to-one mapping between the input and output any question so far this is
definitely very complicated yeah I have a question
all right now the beta other day and be alone so I don't I don't
and so the question is how do I deal with languages where the order them like reverse for example English to Chinese
Japanese right so some of the verbs get moved and things like that well I didn't I did not have cold air be they are
learned so by virtue of learning they will figure out what beta to put right
to wait the input and those are computer basically computed migrated set right so
they just keep on learning okay I have a
questionnaire okay yeah so the question
is are they any work on putting attention in the output yeah I think I think you can do that I'm not too
familiar with any work in here but I think it's possible to do it I think some people explore something like that
yeah any question oh I have a question another question
yeah yeah yeah yeah yeah so so the question
is less about because right now the world hi is capitalized at the first
character it doesn't mean I'm using two n or n vocabulary size so in practice
you we should do some normalization if you have a small data set what you should do is you normalize the tax so
high will be like lowercase and etc now if you have a huge data set doesn't
matter we just learn okay yeah I have a question there right yeah so it
so the question is in a sense it's capture the the positional information in the import yeah I agree I have a
question there a pattern punctuation ah
so the question is what do I do with punctuation well they are in right now
I just present the algorithm as if it's a very simple implementation like the
very basic but one thing that you can do is you you before you train the
algorithm you put a space between the world and the punctuation so that you do
some that is that step is called tokenization or normalization in language processing so you can use any
like a stanford NLP package or something like that to normalize your text so that
is easy to train now if you have infinite data then if you just learn itself okay so I should get going
because there's a lot of other interesting stuff okay so it turns out that the the basic implementation but if
you want to get good results and if you have big data sets so one thing that you can do is to make the network deep and
one way to make deep is is in the following way so you stack your your recurrent network
on top of each other right so you know like in the first sequence of sequence paper we use a network of four but
people are gradually increasing to like six I and so on right now and they getting better and better result like in image
net if you make a network people you also get better results okay so i if you
wanna train sequin to sequins with attention then do a couple years ago
when we like many laps working on this problem were behind the state-of-the-art
but right now in translation many translation tasks basically this model
our audio already achieved state-of-the-art without in a lot of these the pomt datasets so to train this
model so number one is that as i said you might end up with a lot of
vocabulary our vocal vocabulary issues
so what Barack Obama will be this an unknown right Hillary Clinton and season
unknown now you you might use something like what segments right so you segment
the words out for example Barack Obama would be bar and drag and etc or you can
use all the smart algorithms for example word character split you can split words
that have unknown to be in two characters and then you treat the meta character there's some work at Stanford
and they prove that it works very well so that's one way to do it you know tip number two is that you you when you
train this algorithm because you when you do back propagation or forward propagation you multiply you essentially
multiply a matrix many many times so you have explosion of function value or or
the gradient or implosion as well now one thing that you can do is you click
the grade in a certain value right so you say that if the gradient magnitude
of the gradient is larger than 10 set it to ten okay then tip number three is to
use giu or in our work we use a long short term memory okay so I want to
revisit this long short-term memory business a little bit okay so what's the long short-term memory so in
use an iron cell basically you can catenate your input and your the the
hidden state and then you multiply by some theta and then you apply with some
activation function let's say that's a hyperbolic tangent okay now that's the
simple function for n n now in lsdm you
basically you multiply the input and hash by a huge big matrix let's call
that theta that theta is four times bigger than the theta I said in the iron
and cell and then you're going to take that Z okay that coming out you split it
into four blocks its block you can compute the gates and then you you use
the the value of a something called like the cell and then you keep adding the newly computed computed values to the
cell so there's this apart here that I say that the integral of C is that what
it does is basically it keep a hidden state where it keep adding information to it so it doesn't multiply information
but it's keep adding information you don't need to know a lot of this if you want to just apply a SDM because it's
already implemented intensive law any
questions so far okay so in terms of applications you can
use this thing to do summarization so I've seen I started seeing work in some radiation pretty
exciting you can do image captioning so and the input in that case would just be
a representation of an image coming out from vgg or coming out for google net
and etc and then you send it to the I end and and we do the decoding for you or you can use it for speech recognition
or transcription or you can use it for QA so to the next part of the project
the top and we'll talk a little bit about speech recognition okay so well in speech recognition the
input could be maybe waveforms right and then an output could be some words you know hi how's it well
one thing that you can do is you drop your input into Windows that's the green
box is there and then you crop a lot of them and then you send a lot of them to an iron and then you convert it into MFC
see before you send to Ana MFC see or spectrogram or something like that okay and then you use the algorithm that I
said earlier and then with attention and then you do the transcription you
predict one word at a time in the output now the problem with this algorithm is
that in turn when it comes to speech you end up with a lot of input right you can
end up with thousands and thousand steps so back propagating in time even with attention can be difficult now one thing
that you can do is basically you do some kind of a pyramid to map the input so
you if you do enough layers you can divide your input into a factor of eight
or sixteen if you do enough layers right and then you produce the output so we we
work in on an implementation where the output is actually characters like like
the in the by - squawk where they have the ctc now I have to say that the
strength of this algorithm is that you actually have an implicit language model in the output so when I say I when I
have the word how is actually conditioned on hi and stop before right and including the
input so there's an implicit language model already but the problem with this
is that actually you have to wait until the end of the input to do the coding so
the decoding has to be done offline okay so if you use this for voice search it
might not be too nice because people want to see the some some output right
away okay so in that case there's an algorithm that can use it do it in an online fashion
block-by-block now also I have to
mention that in translation this hour the sequence sequence a wit attention
works great it's a among the stay of the art but when it comes to speech it doesn't work
as well as the CDC at least in published results we're not as good as CDC which
is whatever what Adam talked earlier or some of the hmm DNN hybrid which is
which is the most Wylie speech system currently so I want to pause there and
then I can take questions any questions I have a question at the back yeah yeah
yeah yeah
Oh so how does the book in translation
well in translation what we do is basically we have pairs of sentences so
for example hi how are you and then hola como estas right and then we have pairs
of sentences like this and then we just feed it into the turns out into the sequence two sequences attention at
every step we again we're going to predict one word at a time but before we make a prediction the model has the
attention so it actually see the the input once more before it makes a
prediction that's how it works now what is can you repeat okay what is the issue with with a model again please
yeah
I see well I I can't quite follow the question but let's take it offline
is that okay yeah yeah and then we can do some paper okay together
I have a question yeah yeah yeah okay so
the model I did the inbox thing that I presented it was on in English but there's no limitation in the model in
terms of language so let's suppose that you in your inbox that you sometimes you
write in English and sometimes you you write in in Vietnamese or sometimes you write it in Spanish whatever and you
personalize by user embedding that I would say that it will just learn your behavior and then we will basically
predict the world that you want you make you but make sure that your your output bank vocabulary is large enough so that
it covers not only the English words but also the Spanish word and etc like
Vietnamese and so on so your vocabulary gonna be not going to be 20,000 it's going to be like a hundred thousand
because you have more choices and then you have to change your model on on those examples yeah it's a matter of the
training data that's all okay I have a questionnaire yeah
yeah yeah I saw the question is that in the case of voice search right now you have to wait at the end to make a
prediction is there any otherwise yeah yeah the answer yes you can make a prediction block by block so you can
actually figure out like an algorithm a simple algorithm to actually segment the speech and then make a prediction and
then take the prediction and feed it it as input at the next block so you can keep going like that so you in theory
you can actually do online decoding but but I'm saying that the work on you can
do online decoding but that work is currently work in progress how about
that okay I have a question there yeah
over here so we have some input email and then some output email where export
written emails reply and then you can just strain it that way yeah yeah okay I
have a couple questions
yeah yeah the question is that in speech
recognition the CDC seems to be a very nice framework because it match it laser like a monotonic increase Minh in the
output and the input but let CTC make this independent assumption it doesn't
have a language model in it maybe that's the the sequence of sequence I can address this oh yeah I
think that's a great idea maybe we should write a paper together okay I think I think I haven't seen it
but I think that's a very good idea question
I say okay great so so the question is that is there because right now we
predict one step at a time is there any way to actually look globally at the output and maybe use some kind of
reinforcement learning to adjust the output and the answer is yes so there's a recently a recent paper at Facebook
who I think sequence level training or something like that where they don't optimize for one step at time but they
predict they look at the globally and then they try to improve world at a rate or they try to improve blue score or
things like that for translation and it seems to be making some improvement in the metrics that they care about now if
you show it to humans though people still prefer the output from this model
so some of the metrics that we use in translation and so on might not be what
the metrics that we optimize and the next step prediction seem to be what people like a lot in translation yeah so
so the question is can we add the GaN loss like it again lost yeah I think that's a great idea yeah I have a
question here yeah yeah
change yeah yeah
so let's suppose that you type the first ha hola then you can actually start the
beam from there so the question is is there any way to incorporate user input
so I say yeah it let's suppose that you wanna you say hola sorry
hi how are you right and then as soon as the person type hola that actually restrict your beam so you
can actually condition your beam on the first world Ola and your beam will be better yeah I think that's a good idea
I have a question oh so how much data
did we use so in translation for example we use the several several WMT coppices
Cobra and the W empty copper I usually have tens of millions of seven pairs of
tendencies something like that and every every sentence have like 20 words on
average twenty thirty words on average I can't remember but that's something like that order of magnitude yeah yeah I have a question there I
can't really hear also how's it compared
to Google search auto-completion I honestly I don't know what to use
underneath a Google search auto-completion but if I were if they you if I think they should use something
like this because it's okay I have still
lots of interesting stuff coming along so okay okay so what's a big picture so
the big picture is so far I talked about sequin to sequence learning and
yesterday Andrew was talking about most of the big trends in deep learning and
it talking about the second trend was basically doing end-to-end deep learning so you can characterize sequence of
sequence learning as an 2n deep learning as well now so the framework is very
general so it should work from a lot of NLP related tasks because a lot of them
you would have input sequence and output sequence in our NLP it could be input
would be some text and output would be some you know passing trees that's also possible but it works great when you
have a lot of data now when you don't have enough data then maybe you want to consider dividing your problems into
smaller components and then creating your sequin to sequence in the sub components and then merge them okay now
if you don't have a lot of data but you have a lot of related tasks then it's
also possible to actually merge all these tasks by combining the data and then have an indicator bit to say this
is translation this is summarization this is email reply and then change only
and that should improve your your output to now this basically conclude the parts
about sequence sequence and then the next part I'm going to apply sequence to sequence in a big picture of the active
on ongoing work in neural nets for NLP
so if you have any questions you you can ask now I take maybe two questions because I think I running out of time so
I have a question yeah
also the question is does the modem handle emoji I don't know but it's emoji
is like a piece of text to write so you can just like feed it into as another extra token if you make them if you make
your vocabulary 200,000 then you should be able to cover emoji as well yeah I
have a question also if you have new
data coming in so should I return the model where you I think towards the end
we lower the learning rate so if you add new data it just it will not make a lot
of good updates so usually we make you you can add new data increase the learning rate and then continue to Train
yeah that should work okay so I already took two questions let's keep going so
so this is an active area that actually is a very exciting which is in the area
of automatic unite so you can think that maybe the set up would be can you read a
Wikipedia page and then answer a question or can you read a book and answer your question now you in theory
you can use sequin to sequence with attention and then to do this task so
it's going to look like this you're going to read the book right one token a time and with the book then treat a
question and then you're going to use the attention to look at all the pages
and then you make a prediction of the tokens right so so that cut up that's
kind of sometimes you do we do answer this question that way sometimes we don't have knowledge about the fact so
we actually read the book again to answer the fact but a lot of the time if you ask me is Barack Obama the president
of the United States I would say yes because it's already in my memory so
maybe it's better to actually akhmet the iron with some kind of memory okay so that it will not to do this look
back again right it's kind of annoying look back again so there's an active area of this research
I'm not a definite expert but I'm very aware so I can place you in the right
context here so work in this area would be memory networks by Western and folks
at Facebook there will be new rotating machines that deepmind dynamic memory networks would
be a richer soldier presented yesterday and then stuck augmented iron ends by
Facebook again and etc now well let's list so I want to show you a like
high-level what is this augmented memory means okay so let's think about the
attention so the attention looked like this so you and in the end coder you're going to look at at some input okay and then
you have a controller which is your H variable and then you keep updating very high variable but along the side you're
gonna write down into memory your h1 h2 h3 and etc right you store it into a
memory clear-rite and in the decoder what you're going to do is you gonna continue
continue producing some output right are you going to update your controller G but you're going to read from memory
your H okay right so that so so again so
in the import you write to memory in and then in the output you read from memory
now now let's let's try to be a little bit more general and the general would
be at any point in time you can read and write right you have a controller and
you can read and write read and write all the time now to do that you you have to follow in architectures you have some
memory bank big memory back ok and then you you can use the right you can decide
to write some information into it from by a combination of the memory bank in
the previous step and the hidden variable in the previous step and then you also read into the hidden state to
and then you could make an amount update and then you can keep going forever like that so this concept is called an N with
augmented memory okay is that is that
somewhat clear any question you have a
question the question is when you read
do you read the entire memory bank a lot of these algorithms are actually soft
attention so yes it will look the entire memory you can actually predict where to
look right and then read that only that block now with the problem with that is
you end up with very it's not differentiable anymore right because this the thing that you
don't read don't contribute to the gradient so it's going to be hard to train but you can use to reinforce and
so on to train it so there's a reason our paper reinforcement learning new row
Turing machines but actually so there's something like this right not exactly but it will deal with discrete actions
okay any question no question Wow okay
so the another extension that a lot of people talk about is using an N with
augmented operations so you want to augment the neural network with some
kind of operations like addition subtraction multiplication the sine
function etc lot of love functions so to motivate you you can think about Q and I
can fall into this for example histor context the building was constructed in the year 2000 and then it was in later
all people say oh it was then destroyed in the year 2010 and then the question
would be how long it the building survived and the answer would be ten years now how would you answer this
question where you say 2010 subtract two thousand ten years now neural nets if
you can train with a lot example it can do that too you can learn too subtract numbers and things like that it
requires a lot of data to do so all right so maybe is better to augment them
with functions like addition and subtraction right so the way you can do
it is that the neural network will read all the token so far and we'll push the
numbers into a stack and then you get the more the neural net is augmented by
a subtraction and a addition function and these two phone and then you assign
these a probability for these two functions so green the more duck does
mean the higher probability okay so you aside to probability and these two you compute the weighted average of the
values coming out of these two function and then you take that and then you pop it and you push it into the stack in the
next step and then in the next step you will call the addition and subtraction again and etc that's the principle of
something called neural programmers or new neural programmer interpreters so there are two papers last year from
Google brain and nygma was talking about this so so that's that's some of the
related work in the area of augmenting recurrent networks with with operations
with memory etc now what's a big picture ok so the big picture I want to revisit
and I say so what I've talked to today
is sequin to sequence learning and it's an end-to-end deep learning task so it's
one of the big trends happening in natural language it's very general so
you can use if you have a lot and a lot of supervised data it's a very supervised learning algorithm so if you
have a lot of data it should work well but if you don't have enough supervised data then you consider dividing your
problem and then training different in different components or you can train jointly in an multitask settings and
people also train it jointly with auto encoder namely to read the input sentence and then predict the output
sentence again and that's also and then you train jointly with all the tasks and
works as well if you if you go home and
then you want to make impact at your work tomorrow then so far that that's so far so good that that can make some
impact now if you want to do some research and I think like things with memory operation operation augmentation
are some of the exciting areas but but it seems like still work in progress but
I would expect a lot of advances in this area in the near future so so you if you
want to know more you can take a look at pre-solar block you talk about attention
and of my augmented recurrent networks I also wrote some tutorials pretty simple
this the sequin to sequence with attention for translation is implemented
intensive flow so you can download and you can use you can actually download tensor flow and train it what I said
today now this there's a lot of work going on
in this area not on many of these are not mine so I so as you can see you can
even read the world just means how many papers come along in this this area so I
can pause there and I have five minutes to answer questions I have a question
there yeah yeah
yeah
yeah I see okay can you speak to the
microphone because I can't hear very well add a microphone and then I think people can hear that as well when you're
treating a Q&A network so you're taking the example of training from a book to answer questions yeah so if let's say
Harry Potter who was Harry Potter's father now there could be many books that have a character Harry so he has a
context resolution issue which is which Harry should I answer the question for ya how do you solve the context context
problem in your training this kind of Q&A type Network I think that's a great question so I think one thing is that
you can always personalize for example you know that the guy when I talk about you can have a representation for the
user and then you know that when he say Harry his because he actually been reading a lot of books about Harry
Potter so it's more likely to be Harry Potter but I think with the hour time I
said I just want to make sure that it's as simple as possible so the father if you do the juicer has to ask the
question Harry Potter rather than Harry but I'm saying if you
represent user vectors and then you inject more additional knowledge about
the users about the context into as additional token in the input of the net
the net can figure it out by itself yes so that's one way to do it yeah okay I
have a question yeah you did some work on Doc to Vic yeah do you have an idea
what the state of the art in generalizing were two veggies to more than one word oh I see
I think skip thoughts are interested in
directions here so dr. that is one way but skip thought so that the idea of
skip thoughts was Ruslan salakhutdinov with author on this a his idea is basically using sequence
to sequence to predict the next sentence so the input would be the current
sentence the output we would be the the the the previous sentence all connect sentence and then you can train a model
like that if the model is called skip four and I have heard a lot of good things about skip thoughts where you can
take the embedding at the end and then you can do document classification and
things like that and it works very well so that's that's probably one place that you can you know can go my colleague at
Google is also working on something called auto encoder so he instead of predicting the next sentence he predict
the current sentence so trying to repeat the current sentence and and that's kind of work well too yeah yeah see what was
your thoughts on how to solve the common sense reasoning problem Oh common sense I'm deeply interested in common sense
but I gotta say I have no idea I think maybe you can do something like I think
common sense is about a lot of first of all there's a lot of knowledge about the world that is not captured in text right
for example gravity and things like that so maybe you really need to actually combine a lot of morality that's that's
one way to think about it all the way all the thing is do you make sure that unsupervised learning work that's
another approach but I think this digital research area I think I'm just
making guesses right now is there a good way to have sent all these rules and you
know using some soft yes yes so the question is how do you represent
Dru's so so if you think about this network the neural programmer network that it actually augmented by addition
and and subtraction then these are rules
right you can augment it with a table of proofs and then ask the network to
actually attend into the truth table people have looked
to this direction so that's one way to do it okay saying basically argument is to do some logical reasoning yeah yeah
yeah hey okay great talk yeah thank you um are is there like a practical rule of
thumb for how many sequence pairs you need to train such a model successfully yes a is there are there any tips to
reduce how many pairs you need if you don't I said okay so usually the bigger
data set the better but like the corpus that people train this on translation for example English to German it's only
about about 3 5 million pairs of sentences or something like that so that's kind of small 3 million right and
still people are able to make it to the state of the art so that's that's pretty encouraging now if you don't even don't
have a lot of data that I would say things like pre-trained your work vectors with language models or a word
to vac right that's that's one area that you have a lot of parameters you can pre
train your model with some kind of language model and then you reduce the sub max that's another area that you
have a lot of parameters or use drop out in the input embed in or drop out some random word in the input sentence so
those things can improve the regular radiation when you don't have a lot of data okay yeah thank you okay yeah
thank you all so we'll reconvene at 6 o'clock for yoshua bengio
closing keynote

----------

-----

--05-- 

-----
Date: 2016.09.27
Link: [TensorFlow Tutorial (Sherry Moore, Google Brain)](https://www.youtube.com/watch?v=Ejec3ID_h0w)

Summary:
In this lecture, Sherry Moore from the Google Brain team delivers a comprehensive tutorial on TensorFlow, a machine learning library developed by Google. The session begins with an introduction to TensorFlow, explaining its purpose, functionalities, and its extensive use at Google. Sherry emphasizes TensorFlow's flexibility and suitability for machine learning applications, attributing its design to close collaboration with researchers.

The tutorial progresses to hands-on coding, guiding attendees through building models to address classic machine learning problems, particularly focusing on linear regression and classification. Sherry meticulously explains the core concepts, such as tensors, dataflow graphs, nodes, and how TensorFlow's architecture enables asynchronous operations.

The practical part of the session includes the construction of neural networks, with detailed walkthroughs of creating variable objects, inference graphs, training graphs, and the use of placeholders. Sherry introduces TensorFlow's modular design, emphasizing its front-end libraries, execution runtime, and compatibility with various devices, including CPUs, GPUs, TPUs, and mobile devices.

Throughout the tutorial, Sherry engages with the audience, answering questions, and encouraging exploration of TensorFlow's capabilities. She introduces advanced features like saving checkpoints, loading models, running evaluations, and adjusting hyperparameters. The practical exercises are designed to solidify understanding, encouraging attendees to modify and experiment with the code.

Sherry underscores TensorFlow's capability to streamline the process from research to prototyping to production, encouraging contributions to the open-source project and highlighting the active support and continual development of TensorFlow by the team.

In the Q&A session, Sherry and her team address various inquiries about TensorFlow's functionalities, including C++ API usage, Windows support, model portability, data loading, and integration with other frameworks. The session concludes with a note on TensorFlow's commitment to advancing machine learning, fostering a collaborative environment, and the availability of resources and support for users.

Transcription:

Introduction
so I'm going to take a picture so I remember how many of you are here smile
like Sammy says my name is sherry Moore I work in a Google brain team so today
I'll be giving a tutorial on tensor flow first I'll talk up a little bit about what tends to flow is and how it works
how we use it at Google and then the important part is that I'm going to work
with you together to build a couple models to solve the most classic machine
learning problems so Cal get your feet beat for those of you from New Zealand anybody from New Zealand so hopefully at
the end you'll be going home with all the tools that you have to build all the
wonderful things that you have watched today like all the image recognition the
training of different colors arts making music so that's the goal so before I go
any further has everybody installed tensorflow yay
Thank you
brilliant thank you and I would like to acknowledge so I know the link here says Syrian but if you have Wolff g TF
tutorial is perfectly fine Wolff is actually the my colleague who spent all the time verifying installation and
every single platform so I would really like to thank him thanks wolf if you're watching and also I have my wonderful
product bus a product manager in the audience somewhere so if you guys have any requests for tensorflow
make sure that you go find him and tell him why that tencel must support this
feature or exact somewhere all right so there he is so with that we can move
forward to talk about tensor flow so what exactly is tends to flow tensor
flow is a machine learning library that we developed at Google and we open-source the last November and ever
What is TensorFlow
since then we have become the most most popular machine learning library and get
help how do we know because we have over
32,000 stars those of you who track github you know how hard it is to get
rid of those acknowledgement and we also have over 14,000 Forks and we have over
800 no 8,000 contributions from 400 developers 400 individual developers and
we designed this specifically for machine learning however is your see
later because of its really flexible dataflow infrastructure it makes it
really suitable for pretty much any application that can fit into that model basically if your model can be
asynchronous in fire on when data is ready you can probably use tinder for it
originally we worked alongside with all the researchers as a matter of fact I was really fortunate when I joined the
team I sit right next to Alex the person who invented alex net so that's how closely we work together as we develop
tends to flow they would tell us no this is how we use it yes when you do this it
makes our lives a lot easier and this is why we believe that we have developed an infrastructure that will work really
well for researchers and also being Google we also always have in mind that we would like to take from research to
prototyping to production in no time we don't want to want you to write all the code that's typically you're just
throwing away we want to write code that can learn to cut and paste and save in the file and privatize it immediately so
tensile is really designing with that in mind so we will have way into your deep
learning school so can anybody tell me if you want to build a neural net what
must you have what are the primitives what are the yeah primitive I think is
the word I'm looking for what must you have to build neural net
anybody what is in the neural net
This American history so in an Iran that you have neurons that's right
so in and you need so all these neurons what do they operate on what do all
these neurons do they process data data and they operate on data and they do
something such as convolution matrix multiplication max pooling average
pooling dropout whatever that is so intensive flow all the data is held in
something called a tensor tensor is nothing more than a multi-dimensional
array for those of you who are familiar with numpy arrays it's very similar to the ND array and the graph I think one
of the gentlemen earlier this morning described there's this concept that the graph which is a composition of all
these neurons that do different functions and all these neurons are
connected to each other through the inputs and outputs so as data become available they would fire by firing men
they do what they're designed to do such as doing matrix multiplication or convolution and then they will produce
output for the next competition know that's computer connected to the output so by doing this so I don't know how
many of you can actually see this animation yeah so this is to really
How TensorFlow works
visualize how tensor flow works all these nodes the Oval ones are
computation this rectangle ones are staple nodes so all these nodes they would generate
output or they take input and as soon as all the inputs for a particular node are
available it would do its thing produce output and then the tensor all the data
which are how in tensors will flow through your network therefore tensor
flow yeah so everybody's like wow this sounds like
magic how does it work so who said is it sir other car glasses any sufficiently was
the word any sufficiently advanced technology is indistinguishable from magic so that's what this is it's just
really awesome excuse me for a second I
know I want I want to get through this as quickly as possible so we can actually do the lab that you're all dying to do so as any good
infrastructure so this is I want to give you a little image of you know how we design this in tensorflow just like any
well-designed infrastructure has to be really modular because being module allows you to innovate to upgrade to
improve to modify to do what everyone with any piece as long as you keep the api's consistent everybody can work in
parallel it's really empowering I think that's one of the wonderful things that's done at Google pretty much any
infrastructure at Google is really modular they talked really about to to each other all you need to maintain is
the API was stability so in this case we
have a front end I think you guys must have seen some examples of how you construct a graph so we have the
Frontend libraries
front-end libraries written your favorite language and if C++ and Python is no your favorite language feel free
to contribute we always welcome contribution so you write you can show your graph in your favorite language and
this graph will be sent to we call the core a tensile execution system that's
your runtime and that's what you all will be running today on your laptop when you open your up your Python
notebook or should be the notebook so the execution runtime depending on where
you are going to run this application it will send the kernel to the
corresponding device so it could be a CPU could be GPO could be your phone could be CPU anybody knows what TPU is
brilliant very nice I was a stratum say anybody knows what TPU is ever there's
like mmm translation so this is good so just to highlight our portability today
Portability
you'll be running intensive roll in your laptop we run it in our data center you everybody can run on your iOS on your
iPhone your Android phone I would love to see people putting it on Raspberry Pi
because can you imagine you can just write on tensorflow application it could
be your security system because you know somebody just stole my bike and my security camera capture all this grainy
stuff that I cannot tell wouldn't it be nice if you do machine learning on this
thing and they just start taking high-resolution pictures you know when when things are moving rather than
constantly capturing all this grainy images which is totally useless so I think the application literally
applications are limitless you know your imagination is delivered so we talked
about what tensorflow is how it works how do we use it at Google we use it
How we use it
everywhere I think you have seen some of the examples we use it to recognize pictures this is actually done with
inception they can recognize after the box one of thousand images you have to
retrain it if you wanted to recognize they're all your relatives or you know your pets it's not difficult and we I
have links you know for you to actually if you want to train on your own images it's really easy they should totally try
it will they be funny we go to your relative your own 40 year reunion you just go hi you know you who you I know
who you are you know just show off a little it would be brilliant and we also use it to do Google Voice Search this is
the one that's super awesome so how many of you use smart reply have you ever
Smart Reply
used smart reply yeah yeah this is awesome especially for those of you who are doing what you're not supposed to do
you know texting while driving your new song email coming in and you can just say oh yes I'll be there you know so
based on the statistics that collected in February over 10% of the
other responses and on mobile is actually done by our smart reply that's or I believe if we have maybe exact can
collect some stats for me later maybe by now you'll be like 80% it's actually
really funny at the very beginning when we train it the first answer is always I love you like that's probably it right
not the right answer we also play games
of you I'm sure have follow this the all kinds of games that are being developed
Games
it's really fun to watch if you watch it literally come up with scenarios for you to play as well it not only learns to
play the game but learns how to make a game for you is fascinating and of
course art everything many of you have done the Steve dream if we have time in the end of the lab we can we can try
this so if you're super fast we can all try to mix a Mart and all those what I
Models
just talked about of course Google being this wonderful generous company and wants to share our knowledge so we have
actually published our models so if you go to that link you'll find all these
inception and captioning language model on the billion words the latest rest net
on c14 sequence the sequence which I think Kwok will be talking about tomorrow and in the world we have many
Highlevel libraries
other high-level libraries so today my lab the lab that we would do will be on
the core tensorflow api's but there are tons of new higher level API such as
some of the mentioned cares and we have slim we have pretty tensor we have TF
learn we have many libraries that's developed on top of the court in several api's then we encourage people to do so
if whatever is out there does not fit your needs perfectly go for it develop yo and we welcome the contribution we
published a lot of that here I might have blurred some of the boundaries but these are basically all the models and
libraries that we have produced and we really love contribution if you have
developable really cool model please do send to us and we would you know we would showcase
your your work so that's the
introduction of tensorflow how does everybody feel are you all ready to get started
alright so okay before you bring up your Python notebook I want to say what we
are going to do first okay so as I mentioned there are two classic machine learning problems everybody does one is
linear regression the other is classification so we are going to do two simple laps to cover those I do have a
lot of small exercises you can play with I encourage you to play with it to be a lot more comfortable so the first one is
Linear Regression
a linear regression so I'm sure it has been covered yeah in today's lectures
somebody must have covered linear regression can you can anybody give me a
one-line summary what is the linear regression problem anybody the
professors well if you don't know go
google it so I didn't know the the audience you know when when semi asked
Mystery Creation
me to do this so I wanted to I wrote this for one of the high schools so I
think it's doom kind of makes sense right because all of us have done have played this game at one point of our
lives like I you know if you if you tell me a five or tell you ten and you try to
guess you know about the equation is we must have all done this I think my friends are still doing on Facebook
saying oh you know only Genius can solve this kind of equation and then they would be like yeah you I solved it I was
like my god if anybody you know I one friend you guys if you click on another one of those sir
but basically this is what we are trying to do in the first lab so we will have a
mystery creation it's really simple it's just a linear you know literally a line and then we I will tell you that this is
you know the formula but I'm not going to give you a weight W and B you know
all of you have learned by now W stands for weight and bias B stands for bias so the idea is that if you are given
enough samples if you're given enough x and y values you should be able to make
a pretty good guess what W and B is so that's what we are going to do so now
Jupiter Notebook
you can bring up your Jupiter notebook if you don't have it up already
yeah everybody have it up yes can I see show them hands everybody
those are yeah brilliant alright so um for pretty much any models
these are going to come up over and over again and just to make sure that you're all paying attention I do have I asked
them if I was supposed to bring Shrek and he said no but I do have a lot of tensorflow stickers and I have all kinds
of little toys so later I'm going to ask this question whoever can answer will
get some mystery present so really pay attention okay so pretty much with it
whenever you build any model there are I would say four things that you need you need input you need data so you're going
to see in both labs we're going to be defining some data you're going to be built building an inference graph I
think in other lectures is also called a forward graph to the point that it produces logits or the logistic outputs
and then you're going to have a training training operations which is where you
would define a loss an optimizer and I
think that's pretty much it hanging and there's the fourth thing yeah and then
you will basically run the graph so the three important things okay you'll always have your data your inference
graph you always have to define your laws and your optimizer and the training is
basically to minimize your loss so I'm going to be asking that later all right so now we know what we're going to do so
you can go start go to that lab yeah everybody have it so shift returned
we'll run the first one you said I have no idea what happening here return again still
nothing however let's see what we are producing here so you can also do the
same on your laptop you can uncomment that plot you can say so you know what
kind of data you're generating so in this case when he returned why are we seeing this is your input data this is
when you try to make a guess when your friend tell me oh you don't give me your x and y so this is how you know when
your X is 0.2 you know your Y is 0.32 so this is basically your input data
yeah everybody following if at any point you're kind of lost raise your hand and
your buddy next you will be able to help you so now
it's all okay I want to say one more thing so today the laughs are all on really core tends to flow ap is the
reason I want to do that I know there are a lot of people who will use carrots use another thing that we have heavily
advertised which is contribute contribute you've learned so I feel like I'm giving you all the ingredients so
even though you could go to Whole Foods and buy the packaged meal you know maybe
one day you don't like the way they cook it so I'm giving you all your lobsters your Kobe beef okay so that you can
actually assemble whatever you want to build to yourself so this next one is
very key it's a very key concept here you see variables so variable intensive
flow is how is corresponding to the square any of you remember this slide
okay I'm going to switch quickly don't freak out so actually I wanted you all
to commit this little graph to to your memory because you'll be seeing this
over and over again and it makes a lot more sense when you have this visual representation so intensive flow the way
we hold all the data the ways and the biases associated associated with your
network is using something called variable it's a state fold operation I'm going to switch back
okay so this is what we are doing in Section 1.3 we are building those square
Variable Objects
nodes in your network to hold these ways and variables and they are the ones when you train that's where their gradients
will be applied to so that they will eventually resemble the target network
that you are trying to to train for so now you have built it wonderful okay so
you can shift return do you see anything nope so exactly we'll have rebuilt let's
uncomment and take a look so these are called the variable objects so at the
bottom of the slide for this lab I have a link which is our Google 3 dogs the
API Docs which is available in github I think you should always have that up so
whenever you want to do something you will know what kind of operations are possible with this object for example I
can say here what's the name of this oh it's called variable 6y so call
variable 6 oh it's because when I create this variable I didn't give it a name so
I can say Sherri's sure you wait I hope that's not
but so see now my variable is called Sheree wait same thing with my so this
would be a good practice because late later
cheery-bye is
well because I ran this so many times every single time you run if you don't restart that it's going to continue to
grow your current path so to avoid that confusion let me restart it restart
ja I had the word sorry
so now so we have done built our input build an inference graph now we can
Training Graph
actually build our training graph and as you have all learned we need to define a
loss function we need to define an optimizer I think it's also called
something else record regulator maybe some other terms and your ultimate goal
is to minimize your loss so I'm not going to do it here but you can do it at your your leisure you can you know
income and all the these things that you have created and see what they are and I
can tell you these are different operations so that's how you actually get to learn about the network that you
have built really well in the next line I'm also not going to uncomment but you should at one point this is how you
cannot you can see what you have built so actually why don't do that because this is really critical and as you debug
this will become so this is the network
that you have built they have names different names they have inputs and outputs they have attributes and this is
how we connect all these nodes together this is your neural net so what you have
what you're seeing right now is your neural net that you have just built yeah everybody following so now the next step
now you're done right you build your network you build all your training now you can let's do some training so in
tensor flow do you remember in the architecture that I showed you have the front-end C++ and Python front-end you
use that to build your graphs and then you send a graph to your runtime and this is exactly what we're doing here
whenever you this is how we talk to the runtime we create something called session you get a handle to the session
and then when you say run you're basically sending this session your graph so this is different from the
other machine learning libraries I forgot which one those are so comparative your happens as you type
tensor flow is different you have to construct your graph and then you'll create session to talk to your runtime
so that it knows how to run on your different devices that's a very important concept
because people constantly compare and it's just different okay so now you can
also commented to see what the initial values are but we're not going to do that we're just going to run it and now
we're going to train the data is not so
what do you think of the data did we succeed in guessing is everybody
following what we're trying to do yeah yes no so what was our job objective
before I started the lab what did I say my objective was yes to guess the
Results
mystery function so have we succeeded it's really hard to tell all right so
now all of you can go to the end and income in this part let's see how
successful we are
so the Greenline was what we have initialized our weight and buyers - yeah
the blue dots were the initial value of the target values and the red dots is
our trained value makes sense so how successful are we great big success yeah
I would say so so any questions any questions so far so
other things so everybody should play with this you're not going to break it this is a notebook Python notebook the
worst that happens is they're just they okay clear all like what I just did and change it so what can you play with
since today you learn all these concepts about different loss motions different optimizers all this crazy different
inputs different data so now you can play with that how about instead of let's pick one so instead of gradient
Other optimizers
descent what are the other optimizers how can you find out I guess that's a
better question if I want to know what other optimizers are available in tensor flow how can I find out very good yes
the github good Google 3 the G 3 doc link with the AP is I'm going to switch
one more tab bear with me so this is when you go there this is what you can
find you can find all that I mean make it bigger so you can find all the
different optimizers so you can play with that so maybe gradient descent is not the
best optimizer you can use so you go there and say why the other optimizers and then you can learn a come here in
search optimizer oh you can say wow you know I have added Delta a dag red Adam
I'm sure there are more a momentum so we also welcome contribution if you don't
like any of these please do you know go contribute write a new optimizer send a
pull request we would love to have it so I would like to say this over and over again we love contribution is an open-source project
so keep that in mind we would love to see your code or your models on github so back to this one how is everybody
feeling this is too simple yeah should we go wet just yes can I say that
oh is that right
Learn something new
he tapped to see all the other optimizers human Oh brilliant
see I didn't even know that learn something new every day let me go there tap here oh yay so this is even easier
thank you clearly I don't program in notebook as often as I should have
so this is where you can all the wonderful things that you can do thank you this is probably a little too low
level I think it has everything but that's a very good tip thank you so
anything else you would like to see what linear regression is too simple you guys all want to register commend some digits
alright so that sounds like a consensus to me so let's move if you just go to
the bottom you can say click on this one
What is M
so this is our M this model so before you start the lab so once again more
return to do so we have all these handwritten digits what does M this
stand for does anybody know what is M does stand for
very good see somebody can Google very good so it has a stencil I think makes
National Institute of Standards and Technology something like that so they
have this giant collection of digits so you know if you go to the post office
you already know that it's the trivial to solve problem but I don't know if they actually use machine learning but
our goal today is to build little never using tensorflow that can recognize these digits once
again we will not have all the answers right so all we know is that the network
the input will give us a 1 and it will say it's a 9 and then the IMP and then
we have the so-called ground truth and then they will look at and say no you're wrong and then we have to say ok fine
this is a difference we are going to train the network that way so that's our goal yeah everybody see the network on
the side so now we can go to the lab so
What is important when building a network
can anybody tell me why the three or four things that's really important whenever you build a network what's the
first one your data second one inference
graph third one your train graph and with this slab I'm going to teach you a
little bit more they are like the rock you know it like when you go to restaurant I not only give you your
lobstery or your Kobe beef I'm also going to give you a little rock so you can cook it ok so in this lab I've also
teach some absolute absolutely critical additional infrastructure pieces such as
how to save a checkpoint how to load from a checkpoint and how do you evaluate your network I think somebody
at one point asked how do you know the network is enough you evaluated to see if it's good enough so those are the
three new pieces of information that I'll be teaching you and also I'll teach
you a really really useful concept is called place holder that was requested
by all the researchers we didn't used to have it but they they all came to us and say when I trained I want to be able to feed my
network any day that we want so that's a really key concept that's really useful for any practical training whenever you
start writing real training code I think you that will come in handy so those are the I think four concepts now that I
will introduce in this lab that's slightly different from the previous on how to save checkpoint how to go from
checkpoint how to run evaluation and how to use placeholders I think the placeholder is actually going to be the
first one so once again we have our typical boilerplate stuff so that you hit return you import a bunch of
libraries the second one this is just for convenience I define a set of
constants some of them you can play with such as the maximum number of steps
where you're going to save all your data how big the batch sizes are but some
other things that you cannot change because of the data that I am providing you for example the amnesties any
questions so far so now we'll read some data it's
everybody there in 2.3 I'm a 2.3 right now so now I use if you don't have slash
temp it might be an issue but hopefully you do if you don't have slash lim
change the directory directory name so
the next one is where we build inference so can anybody is just glancing and tell me what we're building what kind of
network how many layers am i building
I have two hidden layers you have all learned hidden layers here today and I
also have a linear layer which will produce logits that's correct so that's
what all the inference graphs will always do they always construct and they
produce logistic outputs so once again here you can uncomment it and see it what kind of graph you have built once
you have done the whole tutorial by yourself you can actually run tensor
board and you can actually load this graph that you have saved and you can visualize it like what I have shown in
the slide I didn't draw the slide by hand it's actually produced by tensor
boards so you can see the connection of all your notes so I feel that that visual representation is really
important also it's very easy for you to validate that you have indeed build a graph that you thought sometimes people
call something repeatedly and they have generated this check and they grab they're like oh that wasn't what I meant
so being able to visualize it's really important any questions so far see here
I have good habits I actually gave all my variables names once again the hidden layer one hidden layer two they all have
weights and biases weights and biases etc so now we're going to build our
train graphs so here is actually here
Train graphs
there's no new concept once again you define a loss function we once again pick gradient descent as our optimizer
we added a global step variable that's what we will use later when we save our
checkpoints so you actually know at which point what checkpoint this
corresponds to otherwise if you'll always save it to the same name then later you know you said wow this this
result is so wonderful but how long did it take you have no idea so that's a
training concept that we introduced it's called global step basically how long you have trained and we usually save
that with a chair point so you know which chair point has the best information yeah everybody is good
at 2.5 so now the next one is the additional stuff that I just mentioned oh that piece of rock that I'm giving
Placeholders
you now to cook yourself so one is the place holder so we are going to define
two one two hold your image and the other two hold your labels there we
build it this way so that we only need to build a graph once and we will be able to use it for both training
inference and evaluation later it's very handy you don't have to do it this way
and one of the exercises I put in my slides to try to do it differently but this is a very handy way and get you
very far with minimum work so as I said in the slides I know I don't have any
highlighters beam's but you see there it says after you create your placeholders
I said add to collection and remember this up and later we'll see how we're
going to recall this up and how we're going to use it in the next one we're
going to call inference build our inference is everybody following this
part okay and once again we remember our logits and then we create our train up
and our los up just like with linear regression just like with the linear
regression we're going to initialize all variables and now at the bottom of this
Saver
cell that's the second new concept that I'm introducing which is the saver this
is what you will use to do checkpoints to save the states of your network so that later you can evaluate it or if
your training was interrupted you can know from a previous checkpoint and continue training from there rather than
always we initialize all your variables and start from scratch when your training really big networks such as
Inception is absolutely critical because when I I think when I first trained
inception it took probably six days and then later when we have 50 replicas it
took still like stay of the Rs do two and a half days you don't want to have to start from scratch every single time so yeah
everybody got that the placeholder and the saver so now it's 2.7 we're going to
Reduce Loss
go to 2.7 hmm lots of code can anybody
tell me what it's trying to do so this
is an yes so it's trying to minimize loss we can actually see this so I will
run it once okay where did I go okay
very fast it's done but what if I really
want to see what it's doing so python is wonderful so I would like to actually
see did somebody show like how you know your training is going wow they show the
loss going down going down low I think my training is going really well so we're going to do something similar
sorry so I'm going to create a variable audio car losses which is just an array
so here I'm actually going to remember
it
pinned so am i collecting
man plot map la lip anybody remember
this is a plot let's try this whoa look
at that now do you see you're lost going down so it's you train your loss
actually goes down so this is how when you do large-scale training this is what
we typically do we have a gazillion of this jobs running in the morning with just glanced at it and we know which one
is doing really really well so of course you know that that's just when you are up for the typing that's a really really
handy tool but I'm going to show you something even better oh that's part of the exercise me and I
don't have it so it's one of the exercise I also put the answers in the
backup slides that you guys are welcome to cut and paste into a cell they can
actually run all all the evaluation sets against your checkpoint so that you know
how well you're performing so you don't have to rely on your eyes you know cleansing or my loss is going down or
relying on validating a single image but see this is how easy it is this is how
easy the prototype and you can learn it very often our researchers would cut and
paste their collab code and put in a file and and that's basically their
algorithm and they will publish that with their paper they would send it to
our data scientists or production people we would actually privatize some of their research this this is how easy
literally from research to prototyping to production really streamlined and you can do it in no time so for those of you
LS
who have run the step can you do an LS in your data path wherever you'll save
to that wherever you declare your trainer to be what do you see in there
checkpoints that's right that's the that's the money that's after all this
work well the Oldham one during all this training on all these gazillion machines that's where all your ways your biases
are stored so that later you can you know load this network up and do your
inception to recognize images to reply to email to do art etc etc so that's
really critical but how do we use it have no fear all right let's move on to
Checkpoint
2.8 if you are not already there so can somebody tell me what we're trying to do
first that's right first we load the
checkpoint and you remember all the things that we told them told our program to remember the logits
and the image placeholder in the label placeholder how are we going to use it now we're going to feed in some images
from our evaluation and see what it thinks so now if you hit return
Return
what's the ground truth 5 what's our prediction 3 what's the
actual image could be 3 could be 5 you
know but so the machine is getting pretty close why I would I would say that's a 3 ok let's try a different
different one so you can hear return again in the same cell ohai need to
somehow move this so what's the ground truth this time yeah I got it right so you can keep
hitting you know you can keep hitting return and see you know how well it's doing but instead of validating you know
instead of hitting return a hundred times and count how many times I had gotten it wrong as I said in one of the
exercises and I also put the answer in the slide so you can cut and paste and
actually do a complete validation on the whole validation set but what do you
think I really so you can actually hand write a different digit but the trick is that
a lot of people actually try there and told me it doesn't seem to work so remember in on the slide I said this is
what the machine sees this is where I see s and this is more the machine sees so in the end instead of set all the
numbers are between 0 and 1 I believe I could be wrong but I believe it's between 0 & 1 so if you just use a
random tool like your phone you write the number and you upload it number one the picture might be too big and need to
scale it down number two it might have a different representation sometimes it's
from 0 to 255 and you need to scale it you know to the range that M this that
how you have trained your network if you train your network with those data and then it should be able to recognize the
same set of data just like when we teach a baby right if if you have never been exposed to something you're not going to
be able to recognize it just like with oriole one of our colleagues caption was at that program a
while ago anytime when I see something that it doesn't recognize have anybody play with that captioning software is
super fun so you can take a picture and say you know two people eating pizza or
you know dog surfing but any time it sees something that has never been
trained on it would say men talking on the cell phone so for a while we had a
lot of fun with it we would put a watermelon on the post and they would say men talking on the cell phone you put a bunch of furniture in the room you
know with nothing and it was they men talking on the cell phone so it was really fun but just like with your
numbers if you have never trained it with that style like if I write Chinese
characters here it's never going to recognize it but this is pretty fun so you can play with it you know you can
Exercises
see how well see every time see so far is 100 percent other than the first one which I cannot tell either so what are
some of the exercises that we can do here what do you want to do with this lab it's too easy huh because I made it
so easy because know that you guys are all experts but now otherwise I would have done it much harder now let me see what things we can
do so you can uncomment all the graphs
oh so here's one actually you already see it so try this can you guys try
saving the checkpoints say every 100 steps and you're going to have a
gazillion but they're tiny tiny checkpoints so it's okay and try to run evaluation with a different checkpoint
and see what you get do you know how to do that yeah everybody not to do that so the idea is
that when you run the evaluation is in the it's very similar so we typically
run training in the evaluation in parallel or validation so us it trains
Training Evaluations
every so often say every half an hour depending on the the depending on your
problem so with inception every ten minutes we're also the run evaluation to see how well our model is doing so if
our model gets to say 78.6% which I believe is the state of the art we'll be like ooh my mother's done training so
that's how that's why you want to save checkpoints often and then you know validate them often if you're done with
that already did you notice anything if you try to load from a really already checkpoint how is your how good is it
when it tries to identify the the digits just take a wild guess
yeah very bad maybe whenever you every other one is wrong but this M this is
such a small data set is very easy to train and we have such a you know deep network if you only have one layer or
maybe it won't get it right so another
exercise I think all these you can you can do after the lecture after this session is that really try to learn too
evaluation from scratch rather than actually another part but run evaluation
on the complete validation set that's a that's a really necessary skill to
develop as you build bigger models and need to run validation so I think this
is the end of my my lab I do have bonus laughs but I want to cover this first
the bottom line is that tensorflow is really it's for machine learning is really from research to prototyping to
TensorFlow is for Machine Learning
production it's really designed for that and I really hope everybody in the audience can give it a try and if there
are any features that you find it lacking that you would like to see implemented either send us pour requests
Questions
we always welcome contribution I'll talk to my wonderful product manager Zack sitting over there he is taking requests
for features so with that yeah things
that have fun
Thank You Shari did we have time for questions for those who actually tried
it see you so well done and everybody feel like their experts are all ready to
go make arts now right right goes deep dream cool if there are no questions I
hope there's one question I think someone who's trying desperately hi my
Peachy
name is peachy low and first of all thank you for the for introducing tensorflow and for designing it I have
two questions so the first questions is I know the tensorflow have C++ API right so let's
say if I use Kira's or any of the Python front-end I train a model can I have
that sense of loss support adjust I can pull out the C++ model of it and then
just use that yes you can so even if I use for example Kira's custom layer that
I caught using Python I still can get those things correct oh it's just a front-end that's different how you
construct the graph nice but we are not as complete on our C++ API designs for
example a lot of the training libraries are not complete yes so but for the
simple models yes you can do well let's say not the training but let's say if I just want the testing part because I
don't need to do I mean the training I was doing is we do have that already Oh actually if you go to our website
there's a label images does CC I think that's literally just loading from SharePoint and run the inference in C
then that's all written in C++ so that's a good example to follow Oh a second one so another thing that I
noticed that you support almost everything except windows everything acceptable I mean I always Android have
no fear actually we are actively doing that but when I first joined the team I
think there were 10 of us and we have to do everything like before open sourcing all of us were in the in the
in the conference from together we're all riding dog we're fixing everything so now we have more people that's like
the top of our list we would love to support it so so I'm just curious because I I mean when I look at the road
man I didn't see a clear timeline for Windows but the thing I know they just
like the reason why you cannot support Windows is because of basil basil doesn't support Windows so let's say
theoretically well I mean what you think just like I know basil they're just that you will get window Phi at some someone
November that is why they say so once Plato can run in Windows can I expect
wedges I could immediately do tens of law do you foresee some other problem maybe Zac would like to take that
question okay this so yeah let's talk
offline yeah sure thank you very much sorry I great presentation yeah my name
is Yuri fish I have a question about GPUs are they available right now for testing and playing for non Google
employees are we I don't think so at the
moment and do you know when it might be available in the Google well would you like to take that one
I'm so glad we have a Prada boss here so he can okay thank you nice to Tori I
Tori
have a question are there any like plans to integrate tensorflow
with the like open-source framework like my sauce and HDFS to make the distribute
tensorflow easy so there are definitely plans we are also always actively
working on new features but we cannot provide a solid timeline right now so
that we do have like like is you know we do have plans we do have projects in
progress but we cannot commit on a timeline so I cannot give you a time
saying yes by November you have so thank you but if you have this type
of question I think Zac is the best person to answer today oh hi I was
Load your own data
wondering um the sensor flow having examples to load your own data of what witch did um so the current example has
a m nest of data set are there examples out there to load your own data set yes yes definitely I think we have two ones
called the tencel flow poet I think that one that example shows you how you can
know your own data set I think is there
another one Zack are you aware of another one that might be a loading your own data set I know we have retraining
model you know if you go to tensorflow we have an example to do retraining those you can download from anywhere so
in our example we just downloaded a bunch of flowers so you can definitely download whatever pictures that you want
to return thank you hello thank you for
your presentation I have a question concerning the the training you can't you can train using throw tensorflow
in any virtually any system like Android and what about the model is do you
provide anything to move the model to Android is there because I'm generally
you programming Java there and yes so that's a beautiful thing you remember the architecture that I showed yes you
build a model and they just send it to the runtime it's the same model running on any of the different platforms it can
be a laptop and Roy you have your own specific format for the model or it's
Justin you build a model is just bunch of matrix and matrix and values yeah is
there any special special format for your model because I sometimes it will
it is big yes comparable I will not recommend training the inception on your phone because all the convolution is a
bad problem probably kill it ten times over so definitely
so there will be that type of limitation like I think you guys talked about the number of parameters if it blows the
memory footprint on your phone it's just not going to work and if the compute like it special for convolution it uses
of other computers yes that's for straining but what inference you can run it anywhere okay thank you it's the same
model you just restore actually are examples like label image that's our C++
version I think I also wrote one's called classify images in Python that's all so you can run it on your phone so
any of these you can write your own and though the sharepoint and run it on your phone as well so definitely I encourage
you to do that thank you hi I have a question related to tens of
Writing TensorFlow in Python
flow solving so I went through the the online documentation and currently I
think it requires some coding in C++ and then combined with Python is there only
going to be you know only Python solution that's going to be provided or is it always going to be you know I
think you need to do some first step you need to create a module and then you know it just imported into water I am
actually surprised to hear that because I'm pretty sure that you can write the model in just Python or just C++ you
don't have to write it in one way or the other they might have a special exporter tool at one point that was the case they
wrote their exporter in C++ I think that's well probably what you were talking about but you don't have to
build it in any specific way the model is just you can write in whatever
language you like as long as it produces that graph and that's all it needs so so
tender for serving the tutorial actually if you go on the side it had yeah a
little steps actually okay so I will look into that so maybe you can come find me later and now I'll see what the
situations I do know that at one point they were writing the exporter in C++ only but that should have changed by now
because we are doing another version of the serving tensorflow serving and is
there any plan to you know provide API is for other languages like you know
like MX night has something called MX like J's and you know you mean that the front-end front-end yeah yeah we have Co
I think we have Co we have some other languages maybe as I can speak more to it and once again if those languages on
our favorite please do contribute and if you would like us to do it talk to Zach
and maybe he can put out you know maybe I don't know because there's somebody else for the Android you need Java for
nicely so I think that's going to help out in integrating these models with us yeah that's great great feedback will
definitely take note thank you thank you I have a caution I'm having embedded GP
board TX 1 which is an ARM processor and I really wanted to work with the tensorflow but I got to know that it can
only run on x86 boards so when can we expect the tensorflow can support ARM
processors we will have to get back to you after I have consulted with my Prada
boss say when we can add that support thank you sorry one last question thanks
for the presentation sherry I have a question regarding D when you have the model you want to run variants is it
possible to make an executable out of it so we can drop it into a container or
run it separately from serving is that that's something that you guys are looking into just run the inference yeah
just have it as the time as a binary yeah yeah you can definitely do that right now you can yeah you can all use
you are always able to do that you mean just save you man just save the you
won't what I mean is that if you can package it into a single binary source
that you can just pass around yes yes we actually do that today that's how the label image works it's just it's only
individual binary okay they actually converted all the chair points into constants so it doesn't even
need to do this lo etc it just reads a bunch of constants and runs it so it's super fast
yep okay that's a thanks sherry again
we are going to take a short break of ten minutes let me remind you for those who haven't noticed yet but all the
slides of all the talks will be available on the website so do not worry they will be available at some point as
soon as we get them from the speakers oh I forgot to ask my bonus question but in any case I have a lot of tensorflow
stickers up here if you like one to put proudly display your laptop come get it

----------

-----

--04--

-----
Date: 2016.09.27
Link: [Foundations of Deep Learning (Hugo Larochelle, Twitter)](https://www.youtube.com/watch?v=zij_FTbJHsk)
Transcription:

The talks at the Deep Learning School on September 24/25, 2016 were amazing. I clipped out individual talks from the full live streams and provided links to each below in case that's useful for people who want to watch specific talks several times (like I do). Please check out the official website ([http://www.bayareadlschool.org](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbks4OTJuUFNUSkNVeFZEQWRhdkM0cmNNSkN0UXxBQ3Jtc0ttNEY4RVk3QlhKMmJfZk4tRzJ2M25CUlRiaDY2a3hxMHJkMXhtRV84T1U0eUlxN09jczdUZVdzd3k1ZHlkSjd2dGhscnJuazZNMXBYcGplNWJYNmRMOFNuNi1jWmt1VXpFbGhxZzM3eUk3VDZWUmhyRQ&q=http%3A%2F%2Fwww.bayareadlschool.org%2F&v=zij_FTbJHsk)) and full live streams below. Having read, watched, and presented deep learning material over the past few years, I have to say that this is one of the best collection of introductory deep learning talks I've yet encountered. Here are links to the individual talks and the full live streams for the two days:
Intro

that's good all right cool so yes I was asked to give this presentation on the

foundations of deep learning which is mostly going over basic feed-forward neural networks and motivating a little

bit deep learning and some of the more recent developments and and some of the topics that you'll see across the next

two days so I as Andrew mentioned I have

just an hour so I'm gonna go fairly quickly on a lot of these things which I think will mostly be fine if you're

familiar enough with some machine learning and a little bit about neural nets but if you'd like to go into some

of the more specific details you can go check out my online lectures on YouTube it's now taught by a much younger

version of myself and so just search for you go to a shell and I am NOT the guy

doing a bunch of skateboarding and the geek teaching about neural nets so go

check those out if you want more details but so well I'll cover today is I'll

start with just describing and laying out the notation on feverel neural

FOUNDATIONS OF DEEP LEARNING

networks that is models that take an input vector X that might be an image or some text and produces an output f of X

so I'll just describe for propagation and the different types of units and the type of functions we can represent with

those and then I'll talk about how we actually train neural nets describing things like loss functions back

propagation that allows us to get a gradient for training with stochastic gradient descent and mention a few

tricks of the trade so some of the things we do in practice to successfully Train neural nets and then I'll end by

talking about some developments that are specifically useful in the context of

deep learning that is neural networks with several hidden layers that came out you know at the very after the beginning

of deep learning say in 2006 that is things like drop out batch normalization and if I have some time unsupervised

pre-training so let's get started and just talk about assuming we have some

neural network how do they actually functions how do they make predictions so let me lay down the notation so a

multi-layer neural feed-forward neural network is a model that takes as input

some vector X which I'm representing here with a different note for each of the dimensions in my input vector so each

dimension is essentially a unit in that neural network and then it eventually produces at its output layer a an output

and we'll focus on classification mostly so you have multiple units here and each

unit would correspond to one of the potential classes in which we would want to classify our input so if we're

identifying digits in handwritten character images and so we're focusing

on digits you'd have ten digits or you would have sort of zero from zero to nine so you'd have ten output units and

to produce an output the neural net will go through a series of hidden layers and

those will be essentially the components that introduce non-linearity that allows us to capture and perform very

sophisticated types of classification functions so if we have L hidden layers

the way we compute all the layers in our neural net is as follows we first start

by computing what I'm going to call a pre activation I'm going to note that a and imma go I'm going to index the

layers by K so a K is just the pre activation at layer K and that is only

simply going to be a linear transformation of the previous layer so

I'm going to note HK as the activation and the layer and by default I'll assume

that layer zero is going to be the input and so using that notation the pre

activation at layer K is going to correspond to taking the activation at the previous layer K minus one

multiplying it by a matrix WK those are the parameters of the layer those

essentially corresponds to the connections between the units between adjacent layers and I'm going to add a

bias vector that's another parameter in my layer so that gives me the pre activation and then next I'm going to

get a hidden layer activation by applying an activation function this will introduce some non-linearity in the

model so I'm going to call that function G and we'll go over a few choices we

have four common choices for the activation function and so I do this from

layer 1 to layer L and when it comes to the output layer I'll also compute a pre

activation by performing a linear transformation but then I'll usually apply a different activation function

depending on the problem I'm trying to solve so having said that let's go to

some of the choices for the activation function so some of the activation functions you'll see one common one is

this sigmoid activation function it's this function here it's just 1 divided by 1 plus the exponential of minus the

pre activation the shape of this function you can focus on that is this here it takes the pre activation which

can vary from minus infinity to plus infinite and it squashes this between 0 & 1 so it's bounded by below and above

below by 0 and above by 1 okay so it's a it's a function that saturates if you

have very large or very large magnitude positive or negative pre activations

another common choice is the hyperbolic tangent or tange activation function on

this picture here so squash is everything but instead of being between 0 & 1 s between minus 1 and 1 and 1

that's become quite popular in neural nets is what's known as the rectified

linear activation function or in papers you will see the relative unit that

refers to the use of this activation function so this one is different from

the others in that it's not bounded above but it is bounded below and it's actually it will output zeros exactly if

the pre activation is negative so those are the choices of activation functions

for the hidden layers and for the output layer if we're performing classification as I said in the our output layer we

will have as many units as there are classes in which an input could belong and what we'd like is potentially and

what we often do is interpret each units activation as the probability according

to the neural network that the input belongs to the corresponding class that it's labeled Y is the corresponding

class C so C would be like the index of that you in the output layer so we need an

activation function that produces probabilities produces a multinomial distribution over all the different

classes and the activation function we use for that is known as the softmax activation function it is simply as

follows you take your pre activations and you exponentiate them so that's going to give us positive numbers and

then we divide each of the exponentiated pre activations by the sum of all the PD

exponentiated pre activations so because I'm normalizing this way it means that all my values in my output layer are

going to sum to 1 and they're positive because I took the exponential so I can interpret that as a multinomial

distribution over the choice of all the SI different classes ok so that's what I'll use as the activation function at

the output layer and and now beyond the math in terms of conceptually and also

in the way we're going to program neural networks often we will do is that all these different operations the linear

transformations the different types of activation functions will essentially implement all of them as an object and

object that take arguments and the arguments would essentially be what other things are being combined to

produce the next value so for instance we would have an object that might correspond to the computation of pre

activation which would take as argument what is the weight matrix and the bias vector for that layer and take some

layer to transform and that would this object we sort of compute its value by applying the linear activation the

linear transformation and then we might have objects that correspond the specific you know activation functions

or like a sigmoid object or a 10 shop jacked or raloo object and we just combine these objects together chain

them into what ends up being a graph which I refer to as a flow graph that represents the computation done when you

do a forward pass in your neural network up until you reach the output layer so I mentioned it now because that's you'll

see you know the different software's that we presented over a weekend will essentially sort of you know exploit

some of that representation of the computation and neural nets it also be handy for computing gradients which I'll

talk about in a few minutes and so that's how we

perform predictions in neural network so we get an input we eventually reach an

output layer that gives us a distribution over classes if we're performing classification if I want to actually classify I would just assign

the class corresponding to the unit that has the highest activation that would correspond to classifying into the class

that has the highest probability according to the neural net and but then

you might ask the question okay what kind of problems can we solve with neural networks or more technically what

kind of functions can we represent mapping from some input X into some arbitrary output and so if you look at

if you go look at my videos I try to give more intuition as to why we have this result here but essentially if we

have a single hidden layer a neural network it's been shown that with a linear output we can approximate any continuous function arbitrarily well as

CAPACITY OF NEURAL NETWORK

long as we have enough hidden units so that is there's a value for these biases and these weights such that any

continuous function I can actually represent it as well as I want I just need to add enough hidden units so this

result applies if you use activation functions nonlinear activation functions like sigmoid and tan H so as I said in

my videos if you want a bit more intuition as to why that would be you can go check that out but that's a

really nice result it means that by focusing on this family of machine learning models that our neural networks

I can pretty much potentially represent any kind of classification function however this result does not tell us how

do we actually find the weights and the bias values such that I can represent a given function it doesn't essentially

tell us how do we train a neural network and so that's what we'll discuss next

let's talk about that how do we actually from a data set train a neural network

to perform good classification on for that problem so what we'll typically do

is use a framework that's very generic in machine learning known as empirical risk minimization or structural risk

MACHINE LEARNING

minimization if you're using regularization so this framework essentially transformed

a problem of learning as a problem of optimizing so what we'll do is that will

first choose a loss function that I'm noting as L and the last function it

compares the output of my model so the output layer of my neural network with the actual target

so I'm indexing with it exponent here with T to essentially ask the index over

all my different examples in my training set and so my loss function will tell me

is this output good or bad given that the label is actually Y and well I'll do

I'll also define a regularizer so theta here is you can think of it as it's just

the concatenation of all my biases and all of my weights in my neural net so those are all the parameters of my

neural network and the regularizer will essentially penalize certain values of

these weights so as I'll talk more specifically later on for instance you might want to have your way to not be

too far from zero that's a frequent intuition that we implement with regularizer and so the optimization problem that

we'll try to solve when learning is to minimize the average loss of my neural

network over my training example so summing over all training examples I have capital T examples plus some weight

here that's known as the weight DK some hyper parameter lambda times my regular Iser so in other words I'm going to try

to have my loss on my training set as small as possible over all the training

example and also try to satisfy my regularizer as much as possible and so now we have this optimization

problem and we learning will just correspond to trying to solve this problem so performing this finding this

argument here for over my weights and my biases and if I want to do this I can

just invoke some optimization procedure from the optimization community and the

one algorithm that you'll see constantly in deep learning is stochastic gradient descent this is the optimization

algorithm that will often use for training neural networks so SGD

stochastic gradient descent functions as follows you first initialize all of your parameters that

is finding initial values for my weight matrices and all of my bio C's and then

for a certain number of epochs so an epoch will be a full pass over all my examples that's what I'll call an epoch

so for a certain number of full iterations over my training set

I'll draw each training example so I pair X input X target Y and then I'll

compute what is the gradient of my loss with respect to my parameters all of my

parameters all my weights and all my biases this is what this notation here so nabla for the gradient of the loss

function and here I'm indexing with respect to which parameter I want the gradient so I'm going to compute what is

the gradient of my last function with respect to my parameters and plus lambda

times the gradient of my regularizer as well and then I'm going to get a direction in which I should move my

parameters since the greyman tells me how to increase the loss I want to go in

the opposite direction and decrease it so my direction will be the opposite so that's why I have a minus here and so

this Delta is going to be the direction in which I'll move my parameters by taking a step and the step is just a

step size alpha which is often referred to as a learning rate times my direction

which I just add to my current values of my parameters my biases and my weights

and that's going to give me my new value for all of my parameters and I iterate like that over going over all pairs x

wise computing my gradient taking a steps out in the opposite direction and then doing that several times okay so

that's how stochastic gradient descent works and that's essentially the learning procedure it's represented by

this this procedure so in this algorithm there are few things we need to specify to be able to implement it and execute

it we need a loss function the choice for the loss function we need a procedure that's efficient for computing the

gradient of the loss with respect to my parameters we need to choose a regularizer if you want one and we need

a way of initializing my parameters so next what I'll do is go through each of these these four

different things we need to choose before actually being able to execute the classic gradient descent so first

LOSS FUNCTION

the last function so as I said we will interpret the output layer as assigning probabilities to each potential class in

which I can classify my input X well in this case something that would be

natural is to try to maximize the probability of the correct class the actual class in which my example XT

belongs to I'd like to increase the value of the probability assigned by computed by my neural network and so

because we set up the problem in which we have a loss that we minimize instead

of maximizing the probability what we'll actually do is minimize the negative and the actual log probability so the log

likelihood of assigning X to the correct class Y so this is represented here so

given my output layer and the true label Y my loss will be minus the log of the

probability of Y for minor according to my neural net and that would be well take my output layer and look at the

unit so index the unit corresponding to the correct class so that's why I'm indexing by Y here we take the log

because numerically it turns out to be more stable we get nicer looking gradients and sometimes in certain

software's you'll see instead of talking about the negative log likelihood or log probability you'll see it referred as

the cross entropy and that's because you can think of this as performing a sum

over all possible classes and then for each class checking well is this potential class the target class so I

have an indicator function that is one if Y is equal to C so if my iterator

Class C is actually equal to the real class I'm going to multiply that by the

log of the probability actually assigned to that class C and this this function

here so this expression here is like a cross entropy between the empirical distribution which assigns 0 probability

to all the other classes but a probability of 1 to the correct class and the actual distribution over

that my neural net is computing which is f of X okay that's just a technical

detail you can just think about this here I only mention it because in certain libraries it's actually mentioned as the cross-entropy a loss so

that's for the loss then we need also a procedure for computing what is the gradient of my loss with respect to all

of my parameters in my neural net so the biases and the weights you can go look

at my videos if on the actual derivation of all the details for all of these different expressions I don't have time

for that so all I'll do and presumably a lot of you I actually seen you know these derivations if you haven't just go

check out the videos in any case I'm going to go through what the algorithm is I'm going to highlight some of the

key points that will come up later in understanding out actually back propagation functions so the basic idea

is that we'll compute gradients by exploiting the chain rule and we'll go from the top layer all the way to the

bottom computing gradients for layers that are closer and closer to the input as we go

and exploiting the chain rule to exploit or reuse previous computations we've made at upper layers to compute the

gradients at the layers of below so we usually start by computing what is the

gradient at the output layer so what's the gradient of my loss with respect to

BACKPROPAGATION

my output layer it actually it's more convenient to compute the loss with respect to the pre activation it's

actually a very simple expression so that that's why I have the gradient of this vector a L plus 1 that's the pre

activation at the very last layer of the loss function which is minus the log f of XY and it turns out this gradient is

super simple it's minus II of Y so that's the one Hut vector for class Y so

what this means is a of Y is just a vector filled with a bunch of zeros and then the one at the correct class so if

Y was the fourth class then in this case it would be this vector we have a one at the fourth dimension so e of Y is just a

vector it's we call it the one Hut vector full of zeros and the single one at the position corresponding to the

correct class so this part of the grain is essentially saying is that I'm going to increase I

want to increase the probability of the correct class I want to increase the pre activation which will increase the

probability of the correct class and I'm going to subtract what is the current probabilities assigned by my neural net

to all of the classes so f of X that's my output layer and that's the current beliefs of the neural net as to in which

class what's the probably of signing the input to each class so what this is

doing is essentially trying to decrease the probability of everything and specifically decrease it as much as I

the neural net currently believes that the input belongs to it and so if you

think about the subtraction of these two things well for the class that's the correct class I'm going to have one

minus some number between zero and one because it's a probability so that's going to be positive so I'm going to

increase the probability of the correct class and for everything else it's going to be zero minus a positive number so

it's going to be negative I'm actually going to decrease the probability of everything else so in two Li and it

makes sense this gradient has the right behavior and I'm going to take that pre activation gradient I'm going to

propagate it from the top to the bottom and and essentially iterating from the

last layer which is the output layer l plus 1 all the way down to the first layer and as I'm going down I'm going to

compute the gradient with respect to my parameters and then compute what's the gradient for the pre activation that the

layer below and then iterate like that so at each iteration of that loop I take

what is the current gradient of the loss function with respect to the pre

activation at the current layer and I can compute the gradient of the loss function with respect to my weight

matrix so not doing the derivation here it it's actually simply this vector so

my in my notation I assume that all the vectors are column vectors so this pre activation gradient vector and I

multiply it by the transpose of the activations so the value of the layer right below the layer K minus one so

because I take the transpose that's a multiplication like this you can see if I do the outer product essentially between these two vectors

I'm going to get a matrix of the same size as my weight matrix so it all checks out that makes sense it turns out that the

gradient of the loss with respect to the bias is exactly the gradient of the loss with respect to the pre activation so

that's very simple so that gives me now my gradients for my parameters and now I need to compute okay what is going to be

the gradient of the pre activations at the layer below well first I'm going to get the gradient

of the last function with respect to the activation at the layer below well

that's just taking my pre activation gradient vector and multiplying it by for some reason does it show here but

and multiplied by the transpose of my weight matrix super simple operation just a linear transformation of my

gradients at layer cake linear and transform to get my gradients of the activation at the layer K minus one and

then to get the gradients of the pre activation so before the activation function

I mean to I'm gonna take this gradient here which is the gradient of the activation function at the layer K minus

one and then I applied the gradient corresponding to the partial derivative of my nonlinear activation function so

this here this refers to an element-wise product so I'm taking these two vectors this vector here in this vector here I'm

going to do an element-wise product between the two and this vector here is just a partial derivative of the

activation function for each unit individually that I've put together into a vector okay this is what this

corresponds to now the key things to notice is first that this path computing

all the gradients and doing all these iterations is actually fairly cheap its complexity is essentially the same as

the one is doing a forward pass so all I'm doing are linear transformations

multiplying by matrices in this case the transpose of my weight matrix and then I'm also doing this sort of nonlinear

operation where I'm multiplying by the gradient of the activation function that's the first thing to notice and the

second thing to notice is that here I'm doing this element-wise product so if any of these terms here for a unit is

very close to zero then the pre activation gradient is going to be zero for the next layer and I highlight this

point because essentially whenever that's something to think about a lot when you're training neural nets

whenever this gradient here these partial derivatives come close to zero that it means the grain will not

propagate well to the next layer which means that you're not going to get a good gradient to update your parameters

now when does that happen when will you see these terms here being close to zero

ACTIVATION FUNCTION

well that's going to be when the partial derivatives of these nonlinear activation functions are close to zero

or zero so we can look at the partial derivative say of the sigmoid function it turns out it's super easy to compute

it's just the Sigma itself times 1 minus the sigmoid itself so that means that

whenever the activation of the unit for sigmoid unit is close to 1 or close to 0 I essentially get a partial there that's

close to zero you can kind of see it here the slope here is essentially flat and the slope here is flat that's the

value of the partial derivative so in other words if my pre activations are

very negative or very positive so if my unit is very saturated then gradients will have a hard time propagating to the

next layer that's the key inside here same thing for the tension so the turns

out the partial derivative is also easy to compute you just take the tangible you square it and going to subtract it

to 1 and yeah indeed if it's close to minus 1 or close to 1 you can see that

the slope is flat so again if the unit is saturating gradients will propagate I

have a hard time propagating to the next layer and for the relu the rectified

linear activation function the gradient is even simpler it's you just check

whether the pre activation is greater than 0 if it is the partial derivative is 1 if it's not at 0 so actually either

we're going to multiply by 1 or 0 you essentially get a binary mask when you're performing the propagation

through their value and you can see it the the slope here is flat and otherwise you have a linear function so actually

here at the shrinking of the grade and toward 0 is even harder it's exactly multiplying by

zero if your have a unit that's saturating below and beyond all the math

in terms of actually using those in practice during the weekend you'll see three different libraries that

essentially allows you to compute these gradients for you you actually usually don't write down backdrop you just use

all of these modules that you've implemented and it turns out there's a way of automatic automatic ly differentiating your loss function and

getting gradients for free in terms of effort in terms of programming effort with respect to your parameters so

conceptually the way you do this and you see essentially three different libraries doing it in slightly different

ways what you do is you up meant your flow graph by adding at the very end the

FLOW GRAPH

computation of your loss function and then each of these boxes which are conceptually objects that are taking

arguments and computing a value you're going to augment them to also have a method that's a backdrop or B prop

method you'll often see actually this expression being used be prop and what this method should do is that it should

take as input what is the gradient of the loss with respect to myself and then it should propagate to its arguments so

the things that its parents in the flow graph the things that takes to compute its own value it's going to propagate them using the chain rule what is their

gradients with respect to the loss so what this means is that you would sort

of start the process by initializing well the gradient of the loss with respect to itself is 1 and then you pass

the B prop method here 1 and then it's going to propagate to its argument what

is by using the chain rule what is the gradient of the loss with respect to f of X and then you're going to call B

prop on this object here and it's going to compute well add the gradient of the loss with respect to myself f of X from

this I can compute what's the gradient of my argument which is the pre activation at layer 2 which is back to

the loss so I'm going to reuse the computation I just got and update it using my what is essentially the

Jacobian and then I'm going to take the pre activation here which now knows what is the gradient of the loss with respect

to itself activation it's going to propagate to the weights and the biases and the layer

below update them with informing them of what is the grain into the last with respect to themselves and you continue

like this essentially going through the flow graph but in the opposite direction so the library torch the basic library

torch essentially functions like this quite explicitly it you construct you chain these elements together and then

when you're performing back propagation you're going in the reverse order of these chained elements and then you have

libraries like torch other grand piano and tens of which you learn about which are doing things slightly more

sophisticated there and you'll learn about that later on okay so that's the

REGULARIZATION

discussion of how you actually compute gradients of the last with respect to the parameters so that's another

component we need in stochastic grain in this end we can choose a regular Weiser one that's often used is the l2

regularization so that's just the sum of the squared of the all the weights and the gradient of that is just twice times

the weight so it's a super simple gradient to compute we usually don't regularize the biases

there's no particularly important reason for that it's just it there much for

your by see so it seems less important and often this l2 regularization is

often referred to as weight DK so if you hear about weight decayed that often refers to l2 regularization and then

INITIALIZATION

finally and this is also a very important point you have to initialize

the parameters before you actually start doing back prop and there are a few tricky cases you need to make sure that

you don't fall into so the biases often we initialize them to 0 there are

certain exceptions but for the most part we initialize them to 0 but for the weights there are a few things we can't

do so we can't initialize the weights to 0 and especially if you have 10 H activations the reason and I won't

explain it here but it's not a bad exercise to try to figure out why is that essentially when you do your first

pass you're going to get gradients for all your parameters that are going to be 0 so I'm going to be stuck at this 0

initialization so we can do that we can't initialize all the weights to

exactly the same value if again you think about it a little bit what's going

to happen is essentially that all the weights coming into a unit within the layer are going to have exactly the same

gradients which means they're going to be updated exactly the same way which means they're going to stay constant the

same that comes them but they're going to stay the same the whole time so it's as if you have multiple copies of the

same unit so you essentially have to break that initial symmetry that you would create if you initialize

everything to the same value so we end up doing most of the time is initialized the weights to some randomly generated

value often we generate them there are few other recipes but one of them is to initialize them from some uniform

distribution between lower and upper bound this is a recipe here that is

often used that has some theoretical grounding that's was derived specifically for the 10h there's this

pepper paper here by exactly Goho and yoshua bengio you can check out for some intuition as to oh you know how you

should initialize the weights but essentially this should be initially random and this should be initially close to zero random to break symmetry

and close to zero so that initially the units are not already saturated because

if the units are saturated then there are no gradients that are going to pass through the units you essentially going to get gradients very close to zero at

the lower layers so that's the main intuitions they have weights that are small and close to zero small and random

okay so those are the pieces we need for running stochastic gradient descent so

that allows us to take a training set and run the certain number of epochs and app the neural nets learn from that

training set now there are other quantities in our neural network that we haven't specified out to choose them so

those are the hyper parameters so usually we can have a separate validation set most people here are

familiar with machine learning so that's a typical procedure and then we need to select things like ok how many layers do

I want how many units per layer do I want what's the step size the learning rate of my stochastic gradient descent

procedure that alpha number what is the weight decay that I'm going to use so a

standard thing in machine learning is to perform a grid search that is if I have to our

MODEL SELECTION

parameters I list out a bunch of values I want to try so for the number of hidden units maybe I want to try a hundred a thousand and two thousand say

and then for the learning rate maybe I want to try 0.01 and 0.001 so a grid

search would just try all combinations of these three values for their hidden units and these two values for the

learning rates so that means that the more I providers there are it's the

number of configurations you have to try out blows up and grows exponentially so

another procedure that is now more more common which is more practical is to

perform a form of random search in this case what you do is for each parameter you actually determine a distribution of

likely values you'd like to try so it could be so for the number of hidden units maybe I do a uniform distribution

over all integers from a hundred to a thousand say or maybe a log uniform distribution and for the learning rate

may be again the log uniform distribution but from 0.001 to 0.01 say

and then to get an experiment to get values for my hyper parameters to do an

experiment with and get a performance of my validation set I just independently sample from these distributions for each

hyper parameter to get a full configuration for my experiment and then because I have this way of getting one

experiment I do it independently for all of my jobs all of my experiment that I will do so in this case if I know I have

like enough compute power to do 50 experiments I just sample 15 dependent

samples from these distributions for parameters perform these 50 experiments

and I just take the best one what's nice about it is that there are no unlike grid search there are never any holes in

the grid that is you just specify how many experiments you do if one of your jobs died well you just have one less

but there's no hole in your experiment and also one reason why it's particularly useful this approach is

that if you have a specific value in grid search for one of the hyper parameters that just makes the

experiment not work at all so learning rates are a lot like this if you have a learning rate that's too high it's quite

possible that convergence of the optimization will not converge well if you're using a grid search it means that

for all the experiments that use that specific value of the learning rate they're all going to be garbage they're all not going to be useful and you don't

really get this sort of big waste of computation if you do random search because most likely all the values of

your hyperparameters are going to be unique because their sample say from a uniform distribution over some some

range so that actually works quite well and and and quite recommended and there

are more advanced methods like methods based on machine learning bayesian optimization and or sometimes known as

sequential model based optimization that I won't talk about but that works a bit

better than random search and and that's another alternative if you think you

have an issue finding good hyper parameters is to investigate some of these more advanced methods now you do

KNOWING WHEN TO STOP

this for most of your hyper parameters but for the number of epochs the number of times you go through all of your

examples in your training set what we usually do is not grid search or random

search but we use a thing known as early stopping the idea here is that if I've trained a neural net for 10 epochs while

training a neural net with all the other hyper parameters kept constant but one more epoch is easy I just do one more

epoch so I shouldn't try to I shouldn't start over and then do say eleven epochs from scratch and so what we would do is

we would just track what is the performance on the validation set as I do more and more epochs and what we will

typically see is the training error will go down but the validation set performance will go down and eventually

go up the intuition here is that the gap between the performance on the training

set and the performance on the validation set will tend to increase and since the training curve cannot go below

usually some bound then eventually the validation set performance has to go up

sometimes it won't sell go up oh is sort of stay stable so with early stopping what we do is that if we reach a point

where the validation set performance hasn't improved from some certain number of iterations which we refer to as the

look-ahead we just stop we go back to the neural net that had the best performance overall in the validation set and that's

my neural network so I have now a very cheap way of actually getting the number of iterations or the number of epochs

over my training set a few more tricks of the trade so it's always useful to

OTHER TRICKS OF THE TRADE

normalize your data it will often have the effect of speeding up training if you have real valued data for binary

data that usually keep it as it is so what I mean by that is just subtract for

each dimension what is the average in the training set of that dimension and then dividing by the standard deviation

of each dimension again in my input space so this can speed up training we

often use a decay on the learning rate there are a few methods for doing this

one that's very simple is to start with a large learning rate and then track the performance on the validation set and

once on the validation set it stops improving you decrease your learning rate by some ratio maybe you're divided

by two and then you continue training for some time hopefully the validation set performance starts improving and

then at some point it stops improving and then you stop or you divide again by two so that sort of gives you an

adaptive using the validation set an adaptive way of changing your learning rate and that can again work better than

having a very small learning rate than waiting for a long time so making very fast progress initially and then slower

progress towards TM also I've described

so far the approach for training neural nets that is based on a single example

at a time but in practice we actually use what's called mini batches that is we compute the last function on the

small subset of example say 64 128 and then we take the average of the loss of

all these examples in that mini batch and that's actually we compute the gradient of this average loss on that

mini batch the reason why we do this is that it turns out that you can very

efficiently implement the forward pass over all of these 64 128 examples in my

mini batch in one pass by instead of doing vector matrix multiplications when

we come the pre activations doing matrix matrix multiplications which are faster than

doing multiple matrix vector multiplications so in your code often there will be this other hyper parameter

which is mostly optimized for speed in terms of how quickly training will proceed of the number of examples in

your mini batch other things to improve optimization might be using a thing like

momentum that is instead of using as the descent direction the gradient of your

last function I'm actually going to track a descent direction which I'm going to compute as the current gradient

for my current example or mini-batch plus some fraction of the previous

update the previous direction of update and better now is a hyper parameter you

have to optimize so what this does is if all the update directions agree oh across multiple updates then it will

start picking up momentum and actually make bigger steps in those directions and there are multiple even more

advanced methods for adding adaptive types of learning rates I mentioned them

here very quickly because you might see them in papers there's a method known as a de grab where the learning rate is

actually scaled for each descent for each dimension so for each weight and each by seize it's going to be scaled by

what is the square root of the cumulative sum of the squared gradients

so what I track is I take my gradient vector at each step I do an element-wise square of all the dimensions on my

gradients my gradient vector and then I accumulate that in some variables that I'm noting as gamma here and then for my

descent direction I take the gradient and I do an element-wise division by the square root of this cumulative sum of

squared gradients there's also rmsprop which is essentially like a de grab but instead of doing a cumulative stuff a sum we're

going to do an exponential moving average so we take the previous value x sub factor plus one minus this factor

times the current squared gradient so that's rmsprop and then there's adam which is essentially a combination of

rmsprop with momentum which is more involved and i won't have time to describe it here but that's

method that's often you know actually implemented in these different softwares and that people seem to use with a lot

of success and finally in terms of

GRADIENT CHECKING

actually debugging your implementations so for instance if you're lucky you can

build your neural network without difficulty using the current tools that are available in torch or 10 to Flora

Theano but maybe sometimes you actually have to implement certain gradients for a new module and a new box in your flow graph

that isn't currently supported if you do this you should check that you've implemented your gradients correctly and

one way of doing that is to actually compare the gradients computed by your code with a finite difference of

estimate so what you do is for each parameter you add some very small epsilon value say 10 to the minus 6 and

you compute what is the output of your module and then you subtract the same thing but where you've subtracted the

small quantity and then the divide by 2 epsilon so if epsilon is converges to

zero then you actually get the partial derivative but if it's small it's going to be an approximate and usually this

finite difference estimate will be very close to a correct implementation of the real gradient so you should definitely

do that if you actually implemented some of the gradients in your code and in another useful thing to do is to

DEBUGGING ON SMALL DATASET

actually do a very small experiment on the small data set before you actually run your full experiment on your

complete data set so you say 50 examples so just taking a random subset of 50

examples from your your data set actually just make sure that your code can over fit to that data can

essentially classify it perfectly given you know enough capacity that you would

think it should get it so if it's not the case then there's a few things that

you might want to investigate maybe your initialization is such that the units are already saturated initially and so

there's no actual optimization happening because some of the gradients on some of the weights are exactly zero so you

might want to check your initialization maybe your gradients are just you know you're using a model you implemented

gradients for and maybe there are gradients are not properly implemented maybe you haven't normalized your input

which creates some instability making it harder for stochastic gradient descent to work successfully maybe your

learning rate is too large then you should consider trying smaller learning rates that's actually a pretty good way

of having a some idea of the magnitude of the learning rate you should be using and and then once you actually over fit

in your small trainings that you're ready to do a full experiment on on a larger data set that said this is not a

replacement for gradient checking so backdrop is and stochastic gradient descent it's a great algorithm that's

very bug resistant you will pretend potentially see some learning happening

even if some of your gradients are wrong or say exactly zero so you should that's great you know if you're an engineer and

you're implementing things spun would code is somewhat bug resistant but if you're actually doing science and try to

understand what's going on that's that can be a complication so do do both gradient checking and a small experiment

like that all right and so for the last few minutes I'll actually try to

motivate what you'll be learning quite a bit about in the next two days that is

the specific case for deep learning so I've already told you that if I have a

neural net win enough hidden units theoretically I can potentially represent pretty much any function any

classification function so why would I want multiple layers so there are a few motivations behind this the first one is

taken directly from our own brains so we know in the visual cortex that the light

that hits our retina eventually goes through several regions in the visual cortex eventually reaching narrow known

as v1 when you have units that are or neurons that are essentially tuned to small forms like edges and then it goes

on to v4 where it's likely more complex patterns that the units are tuned for and then you reach AIT where you

actually have neurons are specific to certain objects or certain units and so the idea here is that perhaps that's

also what we want another artificial say you know vision system we'd like it if

it's detecting faces to have a first layer that detects simple edges and then another layer that perhaps puts these

edges together detecting slightly more complex things nose or mouth or eyes and then eventually have a layer that combines

these slightly less abstract or more abstract units to get something even

more abstract like a complete phase there's also some theoretical justification for doing using multiple

layers so the early results were mostly based on studying boolean functions or a

function that takes as input can think of it as a vector of just zeros and ones and you could show that there are

certain functions that if you add the essentially a boolean neural network or

essentially a boolean circuit and you restricted the number of layers of that

circuit that there are certain functions that in this case to represent certain boolean functions exactly you would need

an exponential number of units in each of these layers whereas if you allowed yourself to have multiple layers then

you could represent these functions more compactly and so there's that's another motivation that perhaps with more layers

we can represent fairly complex functions in a more compact way and then

there's the reason that they just work so we've seen in the past few years great success in speech recognition

where it's essentially revolutionized the field where everyone's using deep learning for speech recognition and same

thing for visual object recognition where again deep learning is sort of the method of choice for identifying objects

and images so then why are we doing this only recently why didn't we do deep

learning way back when back prop was invented which is essentially in 1980s and even

before that so it turns out training deep neural networks is actually not that easy there are few hurdles that one

can be confronted with I've already mentioned one of the issue which is that some of the gradients might be fading as

you go from the top layer to the bottom layer because we keep multiplying by the derivative of the activation function so

that makes training hard it could be that the lower layers at very small gradients are barely moving and

exploring the space of correct you know features to learn for a given problem so

that sometimes that's the problem you find you have a hard time just fitting your data and you're essentially underfitting

or it could be that with you know deeper neural nets Oh bigger neural nets we

have more parameters so perhaps sometimes actually overfitting we're in a situation where all the functions that

we can represent with the same neural net represented by this gray area function actually includes yes the right

function but it's so large that for a finite training set the odds that I'm going to find the one that's close to

the true classifying function the real system that like to have is going to be very different so in this case I mean

I'm essentially overfitting and that might also be a situation we're in and

unfortunately there's never there are many situations where one problem is

observed over fitting or under fitting and so we essentially have you know in

the field develop tools for finding both situations and I'm going to rapidly touch a few of those which you will see

will come up later on in multiple talks so one of the first hypothesis which

might be that you're under fitting well you can essentially just fight this by waiting longer so training longer if you

have your grayness are too small and this is essentially why you're progressing very slowly when you're training well if you're using GPUs and

are able to do more iterations over the same training set within less time that

might just you know solve your problem of underfitting and I think we've seen some of that and this is partly why GPUs

have been so game-changing for deep learning or you can use just better optimization methods also and if you're

overfitting well we just need better regularization i've been involved early

on in my PhD on using unsupervised learning as a way to regularize neural

nets if I have time I'll talk a little bit about that then there's another method you might have learned heard

about known as dropout so I'll try to touch at least two methods that are

essentially trying to address some of these issues so the first one that I'll talk about this dropout it's actually

DROPOUT

very easy very simple so the idea of if our neural net is essentially

overfitting so it's too good at training on the training set well we're essentially going to training

when I make it harder to fit the training set and where we're going to do that and dropout is that we will

stochastically remove hidden units independently so for each hidden unit

before we do a forward pass we'll flip a coin and we'd probably have we will

multiply the activation by zero with probability 1/2 we'll multiply it by 1

so what this means is that if a unit is multiplied by 0 it's effectively not in the neural net anymore and we're doing

this independently for each hidden units so that means that in a layer a unit

cannot rely anymore on the presence on any other units to try to sort of

synchronize and adapt to perform a complex classification or learn a

complex feature and that was partly the motivation behind dropout is that this procedure might encourage types of

features that are not co-adapted and are less likely to overfit so we often use

0.5 as the probability of dropping out a unit it turns out it often surprisingly

is the best value but that's another hyper parameter you might want to tune and in terms of how it impacts an

implementation of back prop it's it's very simple so the forward pass before I do it I just sample my binary masks for

all my layers and and then when I'm performing back drop well my gradient on

the oh sorry so that's the Ford pass yeah I'm just multiplying by this binary mask here so super simple change and

then in terms of back prop well I'm also going to multiply by the mask when I get

my gradient on the pre activation and also you know don't forget that the activations are now different they

actually include the masks in in my notation it's a very simple change the forward and backward pass when you're

training and also another thing that I shouldn't emphasize is that the mask is being resampled for every example so

before you do a forward pass you resample the mask you don't keep it you know sample at once and then use it the

whole time and then that test time because we don't really like a model

that sort of randomly changes its output because it will if we stochastically change the masks what we do is we

replace the mask by the probability of dropping out a unit so actually of

keeping a unit so if we 0.5 that's just 0.5 we can actually show

that if you have a neural net with a single hidden layer doing this transformation at test time multiplying

by 0.5 is equivalent to doing a geometric average of all the possible neural networks with all the different

binary mask patterns so it's essentially one way of thinking about drop out in the single layer case is that it's kind

of an in sembly method we have a lot of models an exponential number of models which are all sharing the same weights

but have different masks that intuition though doesn't transfer for deep neural

nets in the sense that you cannot show this result it really only applies to a single neural networks in gold hidden

layer so in practice it's very effective but do expect some slowdown in training

so often we tend to see that training or network to completion will take twice as many epochs if you're using dropout with

0.5 and here you have the reference if you want to learn more about different variations of dropouts and so on and

I'll and I'll probably won't talk about the unsupervised retraining for lack of time but I'll talk about another thing

that you'll definitely probably hear about and that's implementing these different packages which is Bachelor

organization Bachelor ization is kind of interesting in the sense that it's been shown to better optimize that is certain

networks that would otherwise under fit would not under fit as much anymore fuse Bachelor ization

but also it's been shown that when you use batch normalization dropout is not as useful and drop out being a

regularization method that suggests that perhaps patch normalization is also regularizing in some way so these things

are not you know one or the other they're not mutually exclusive you can have a regularizer that also turns out

helps you better optimize so the intuition behind batch normalization is

BATCH NORMALIZATION

you know much like I've suggested that normalizing your inputs actually can help speeding up training well how about

we also normalize all the hidden layers when I'm doing my forward pass so now

the problem in doing this is that I can compute the mean and the standard deviations of my inputs once and for all

because they're constant but my hidden layers are constantly changing because I'm training these parameters

so the mean and the standard deviation of my units will change and so I it

would be very expensive if every time I did an update of my parameters I recomputed the means and the standard

deviations of all of my units so bachelors ation addresses some of these issues as follows so the way works is

first batch the normalization is going to be applied on actually the pre activation so not the activation of the

unit but before the non-linearity during training to address the issue that we

don't want to compute means over the full training set because that would be too slow I'm actually going to compute it on each mini batch so I have to do

mini batch training here I'm going to take my small mini batch of 64 128 examples and that's the set of examples

on which I'm going to compute my means and standard deviations and then when I do back prop I'm actually going to take

into account the normalization so now there's going to be a gradient going through the computation of the mean and

the standard deviation because they depend on the parameters of the neural network and then that test time we'll

just use the global mean and global standard deviation once I finished training I can actually do a full pass

over the whole training set and got all of my means and standard deviations so

that's the essentially the pseudocode for that taken out of the paper directly so if X is a pre activation for a unit

and have multiple pre activations for a single unit across my mini batch I would

compute what is the average for that unit pre activation across my examples in my mini batch compute my variance and

then subtract the mean and divide by the square root of the variance plus some epsilon for numerical stability in case

the variance is too close to zero and then another thing is that actually batch normalization doesn't just perform

this normalization and outputs the normalize pre activation it then actually performs a linear

transformation on it so it multiplies it by this parameter gamma which is going to be trained by gradient descent and

it's often called a gain parameter of batchelomez ation and it adds a bias

better and the reason is that if I'm subtracting by the mean then each of these you

have the biased parameter so if I subtracted then this essentially here

there's no bias anymore it was present here was present here and now it's been subtracted to have to add the bias but

after the bachelor ization essentially so these betters here are essentially the new bias parameters and those will

actually be trained so we do gradient descent also on those so bachelor ization adds a few parameters all right

and I as I said I'm just gonna skip over this and you know I'm not showing what the gradients are when your backdrop through the mean and so on it's

describing the paper for Necedah gradients but otherwise in the different packages you actually have access to

you'll get the gradients automatically it's usually been implemented skipping

UNSUPERVISED PRE-TRAINING

over that I'll just finish if you actually want to learn about unsupervised retraining and why it works

NEURAL NETWORK ONLINE COURSE

videos on that so you can check that out and I guess that's it thank you

thanks you go so we have a few minutes for questions which are intermingled with a break so feel free to I your go

for our break or ask questions to Google I believe there are microphones and I'll also stick around so if you want to ask

your questions offline that's also fine if you want ask questions you can go to the mic

go to the microphone hi I mentioned the

rail ooh adds varsity can you explain why yeah so um so the first thing is

that it's observed in practice and they add some sparse some sparsity in part because you have the non-linearity at

zero below so it means that units are going to be exactly potentially exactly sparse exactly essentially absent of the

hidden layer the real there are a few reasons to sort of explain why you get

sparsity it turns out that this process of doing a linear transformation followed by the value activation

function is very close to some of the steps you would do when you're optimizing for sparse codes in the

sparse coding model if you know about sparse coding so they're like essentially in optimization methods that

given some sparse coding model we'll find what is the sparse representation hidden representation for some input and

it's mostly a sequence of linear transformations followed by this sort of like relu like activation function and I

think this is partly the explanation otherwise I don't I don't know a like solid you know explanation for why that

is beyond you know it's observed in practice more questions if not let's

thank you again and we are we reconvene

in ten minutes

summary:
**Key Points**:

1. **Introduction to Deep Learning**:
    
    - Hugo Larochelle presented on the foundations of deep learning, focusing on feed-forward neural networks, deep learning motivation, and recent developments.
    - Covered topics include basic neural network structure, loss functions, backpropagation, stochastic gradient descent, and various practical tips for training neural networks effectively.
2. **Neural Network Functioning**:
    
    - Explained the computation of neural networks starting with input vectors, going through hidden layers introducing non-linearity, and producing outputs for classification tasks.
    - Discussed the importance of non-linear activation functions like sigmoid, hyperbolic tangent (tanh), and rectified linear activation (ReLU) for hidden layers.
    - For the output layer, the softmax activation function is used for classification tasks to produce probabilities for each class.
3. **Training Neural Networks**:
    
    - Discussed the use of empirical risk minimization framework which includes choosing a loss function, an optimizer like stochastic gradient descent, a regularizer, and parameter initialization methods.
    - Highlighted the importance of the loss function in training, specifically the negative log likelihood or cross-entropy for classification tasks.
    - Emphasized the backpropagation algorithm, which is used to efficiently compute gradients of the loss function with respect to network parameters, utilizing the chain rule.
    - Talked about the importance of proper weight initialization to avoid issues like vanishing or exploding gradients.
4. **Challenges and Solutions in Deep Learning**:
    
    - Addressed common challenges such as underfitting and overfitting, emphasizing the use of GPUs for faster training and methods like dropout and batch normalization for regularization.
    - Dropout randomly deactivates neurons during training to prevent co-adaptation and overfitting.
    - Batch normalization normalizes the output of each layer to stabilize learning and also acts as a regularizer.
5. **Hyperparameter Tuning and Model Selection**:
    
    - Discussed methods for selecting hyperparameters like the number of layers, units per layer, learning rate, and weight decay, including grid search, random search, and more advanced methods like Bayesian optimization.
    - Introduced the concept of early stopping to prevent overfitting and select the number of training epochs based on validation set performance.
6. **Debugging and Improving Models**:
    
    - Highlighted the importance of normalizing input data, using learning rate decay, mini-batch training, momentum, and other optimization methods like AdaGrad, RMSprop, and Adam.
    - Advised on using gradient checking to ensure correct implementation of gradients and starting with a small dataset to ensure the model can overfit to it, indicating a correctly functioning model.
7. **Closing Remarks**:
    
    - Hugo concluded by reiterating the potential and challenges of deep learning, referencing his online course for a deeper understanding of the topics discussed.
    - The session ended with a Q&A, addressing specific questions about the ReLU activation function and its ability to add sparsity to the model.

The presentation provided a comprehensive overview of deep learning foundations, practical training tips, and strategies for effectively training and tuning neural network models.


----------

-----

--03--

-----
Date: 2014.12.23
Link: [Jimmy Pedro: Judo | Take It Uneasy Podcast](https://www.youtube.com/watch?v=7bO8rKtvDoE)
Transcription:

Jimmy Pedro is an American judo competitor and coach, World champion, 3x World medalist, 2x Olympic medalist; we talk about his father (Big Jim Pedro Sr), his early career, the times he wanted to quit, overcoming a neck injury, coming back from retirement, the life of an athlete vs the life of a coach, a system for developing elite-level judoka, Japanese vs Russian judo, periodization, a weekly program for an elite-level judoka, toughest moment as a coach, watching Travis Stevens lose the semifinals at the Olympics, mental game, visualization, IJF, judo as a spectator sport, the future of judo in the United States and the rest of the world, and more.
Based on the comments, the interview with Jimmy Pedro, an acclaimed American judo competitor and coach, was highly appreciated and resonated deeply with the judo community and listeners. The interview appears to have covered a comprehensive range of topics, reflecting Jimmy Pedro's extensive experience in the judo world. Here are the main takeaways based on the audience's reactions:

1. **Inspiring and Sincere**: Viewers found Jimmy Pedro's narrative inspiring and appreciated his sincerity and straightforwardness. His experiences, especially those involving his father, his early career struggles, and his coaching journey, seemed to strike a chord with many.
    
2. **Insightful on Professional Judo**: The interview provided valuable insights into the professional life of a judoka, discussing the intricacies of training, competition, and coaching at an elite level. Comments indicated that listeners found the discussion on the development of elite-level judoka, periodization, and weekly training programs particularly enlightening.
    
3. **Emotional Connection**: Some comments reflected a personal connection with Jimmy Pedro, indicating that his guidance and career had a significant impact on their judo journey. The mention of watching Travis Stevens lose in the Olympics suggested that the interview didn't shy away from discussing the emotional aspects of the sport.
    
4. **Educational Content**: The interview seems to have offered an in-depth look into the mind of an Olympic-level athlete, with Jimmy Pedro sharing detailed insights into mental preparation, visualization techniques, and the overall mindset required for high-level competition.
    
5. **Addressed Contemporary Judo Topics**: The interview tackled current issues and developments in judo, such as the rule changes by the International Judo Federation (IJF) and the future of judo in the United States and globally. This aspect of the discussion appeared to resonate well with judo enthusiasts concerned about the sport's direction and representation in the Olympics.
    
6. **Judo as a Martial Art and Sport**: Comments indicated that the interview addressed the balance between judo as a martial art and as a competitive sport. Discussions about judo's self-defense applications and its evolution for television ratings and Olympic standards seemed to engage listeners who are passionate about the sport's integrity and future.
    
7. **Affirmation of Jimmy Pedro's Legacy**: Numerous comments recognized Jimmy Pedro as a legendary figure in American judo, applauding his contributions to the sport both as a competitor and a coach. His influence on American judo and his role in shaping the careers of other notable judokas like Kayla Harrison, Ronda Rousey, and Travis Stevens were highlighted and celebrated.
    

In summary, the interview with Jimmy Pedro was well-received, with listeners praising the depth, honesty, and comprehensiveness of the conversation. It seemed to offer a blend of personal storytelling, professional insights, and thoughtful discussions on the future of judo, making it a valuable and enjoyable experience for the audience.

The interview with Jimmy Pedro, an American judo competitor and coach, covers a broad range of topics related to his illustrious career in judo, his coaching experiences, and his insights into the sport. Key discussion points include:


1. **Early Life and Career**: Jimmy talks about his upbringing and the influence of his father, Big Jim Pedro Sr, on his judo career. He reflects on the initial phase of his career and moments when he contemplated quitting.
    
2. **Injury and Comeback**: The conversation delves into a significant neck injury Jimmy sustained and how he overcame this obstacle, including his remarkable comeback from retirement.
    
3. **Athlete vs. Coach**: Jimmy contrasts the life of an athlete with that of a coach, shedding light on the different challenges and rewards each role presents.
    
4. **Development System for Elite Judoka**: The discussion covers Jimmy's approach and system for nurturing and developing elite-level judoka, offering insights into his coaching philosophy.
    
5. **Judo Styles and Techniques**:
   
   

----------

-----

--02--

-----
Date: 2014.06.05
Link:  [Ryan Hall: Value of Competition | Take It Uneasy Podcast](https://www.youtube.com/watch?v=94MBVD_tZeU)
Transcription:

Ryan Hall is an American black belt and instructor in Brazilian jiu-jitsu, and a professional mixed martial artist currently competing in the featherweight division of the Ultimate Fighting Championship (UFC). He is known for a number of competitive achievements, ranging from Mundial and ADCC victories to dozens of Grapplers Quest championships. He is the winner of The Ultimate Fighter Season 22. He holds notable victories over former UFC Lightweight and Welterweight Champion BJ Penn, former UFC Lightweight Title challenger Gray Maynard and Artem Lobov.

  
you have been in both the supporter and

a critic of competition what do you

think is the value of competition for

martial artists I believe that the value

of competition is in that it teaches you

the true purpose of martial arts and in

ending and the true purpose of martial

arts is being able to defend yourself

and whatnot in all of these other things

in real life yes because you let's say

you win a DCCC gold medal but you know

you get slapped around by someone bigger

and stronger you at a bar that people

will talk about how sweet that gold

medal is but for the rest of your life

it'll feel pretty Hollow that wouldn't

you know that's not what we're looking

for but what I'm talking about I guess

is what I believe competition develops

if approached properly is proper focused

proper dedication because anytime you

have a very defined goal and strong

opposition it will force you to be

better period the better your opposition

is if you focus and you take your what

you're doing seriously the better you

become period people are better

wrestlers today than they once were

people are better basketball players

today than they once were military is

better now that it was in the past

because of all of the competition

that you know that is existed over the

course of time and you know if you go

out competition if you go out and just

kind of it if you're like oh

I'm going to go out and see how it

happens like that's a cowardly way to

approach competition and that gets you

nothing that doesn't teach you to really

do the right things and the same thing

not for pot not properly preparing even

if you win that was a cowardly way to

approach it because you intentionally

left yourself an out which was if I win

man I'm talented in blah-dee-blah and if

I lose it's well you know I mean I could

I didn't really train that hard well if

that's the case then you shouldn't have

been out there win lose or draw I don't

care if I've got a student that I think

is going to win gold at the World

Championships he or she does not train

properly ahead of time I will not allow

them to go and if they go I will send

them off the team you know and hey they

can do what they want they're a grown

adult I'm not the boss of them but I am

the boss of my team it wasn't my gym and

that's not how we conduct ourselves and

it has way less to do about the physical

you know the result and it does about

the proper preparation is proper

preparation and proper focus and

dedication over the long haul yields

positive results but most importantly

it's about are we conducting ourselves

in a in an honorable and respectable

manner so I believe that the competition

really teaches us that because in the

room you know there's always like oh it

was practice so I was kind of this -

that happened today the other thing when

you go to competition everyone is that's

on everyone is on that day because

everyone is trained

for that specific moment and we'll see

what happens so you get the most honesty

out of it out of a time like that and

the higher the level the better it gets

in you know provided that there's not a

lot of cheating but regardless you know

from an athletic performance perspective

it is the most honest thing because and

it's the it's the toughest as well

because it takes it takes courage and it

takes some heart to really properly

prepare and put it on the line because

you're risking horrible disappointment

I've prepared so hard before and tried

so hard and I've won and I've prepared

other times and I've tried so hard and

I've failed and it hurts it really hurts

it doesn't hurt nearly as much if you

kind of half-ass it because you didn't

put that much into it but again that's

how a coward approaches things if you

have if you're going to conduct yourself

the right way you prepare properly you

train hard and then win lose or draw you

deal with the results and that's what I

believe is the real benefit of

competition if approached properly do

you admire somebody who sacrifices you

know like 10 20 years of their life in

that singular pursuit of competition

towards a gold medal at the Olympics a

almost of the Olympians do goodness

absolutely I mean I admire anyone that's

willing to sacrifice and willing to work

hard in any area of life actually a book

that I'm reading again that I really

really like is dune it's a I'm kind of

like sci-fi nerd hangout on everybody

but basically a you know one of the

things that you know one of the one of

the you know things that the the author

was going to Frank Herbert and it's why

the regarded is one of the you know

greatest science fiction novels ever if

not the preeminent but anyway well the

things you said you know is if you if

you search for freedom you actually end

up becoming a slave to your own desires

ironically and if you search for

discipline you find liberty because

you're able to make yourself do what you

want in the long run whereas if I'm like

oh I'm going to do whatever I want all

the time and screw you dead I'm going to

do what I want that's kind of like a

teenager type attitude you end up

getting into a bunch of nonsense but

anyone that's able and again this

doesn't matter it doesn't mean that it's

athletic it could be in any area of

human endeavor any area of life it could

be parenting it could be military it

could be athletics to be business it

could be school to be anything but as

long as you're making you know an

incredibly large commitment I have an

immense amount of respect for the for

the level of dedication that and the

level of commitment and a level of

risk that it that you're taking

emotionally psychologically because hey

like you said you work twenty years you

get that gold medal but there's other

people that work twenty years and got

the silver most people know the medal

now most people most people that think

they work hard don't I'll be frank

you know like seriously I said that in

class the other day like again it's like

I don't want to be too negative but most

the people that most people to think

they work hard do not how do you know

that if you're working hard or not I

think you know but most people are not

very honest with themselves they were

most people would press a button in my

experience you know they would prefer to

be look like the thing and then be the

thing and you know that's that's fine

but it really I can't I think Sun Tzu

said it's a victory is reserved for

those willing to pay its price and there

is a price and now that doesn't

guarantee that if you pay the price that

you will have victory but you guarantee

but I mean from a physical perspective

but you will have the moral victory

regardless because you will have you

will have learned discipline you'll have

shown not only to others forget others

you do it not for others but for

yourself you you show that you are the

master of your own mind and of your own

body and of your own circumstance and

you can discipline yourself and focus

and you deny yourself certain things in

the pursuit of something something that

is valuable to you and that is

incredibly useful in any area of life

and that's not mean not shocking to me

why the same reason that you'll see guys

that were you know like high-level

military like kind of big dogs on SF

world get hired by let's say for

instance a fortune 500 company because

what would they know about business

nothing but also everything because that

level of focus and dedicate like you

don't get to that level of ability in

something by accident and that's what I

think you know like again the value of

competition and what they do competition

only it's as serious as it gets you know

because if you don't get the gold medal

it you may not you may not walk out of

it but basically I have an immense

amount of respect for anyone that is

willing and able to over the long haul

put that time in but I have to trying

hard doesn't mean just getting on an air

Don bike and walking off the mat or

having to be carried off the mat it

means thinking approaching reassessing

reevaluating saying how could I be

better and it takes honest on a

self-analysis and

and also it takes a lot of times because

let's say you know I think I'm doing

well but I got to say hey Lex you know I

mean no matter how how well I believe I

look at myself I'm still biased I'm

still looking at myself what should I be

doing better I'm gonna find other people

that I respect and people that I think

can tell me and I'm gonna ask them and

then I'm gonna have the courage to

listen to them and not just dismiss what

they're saying out of hand and if you're

doing those things then I believe that a

lot of times you're working hard but I

know plenty of people that come in it's

just like in jiu-jitsu this point if you

limit training for 15 years it frankly

suck and there's plenty of people that

have been training for four that are

pretty dang good you know for real and

again are they the best person that's

been training for four years is still

compared to like Kareena not that good

but they could be really really really

good because they understand how to be

directed and how to focus and I believe

this is something I've discussed you

know before with some other you know

friends of mine you know that some of

whom were at a very high level may

others that are the high level of

jiu-jitsu wrestling or whatever um look

at guys like Randy Couture guys a Rick

Hawn they're on their second career they

started MMA when they were like 32 and

yet they got to the top maybe they

weren't always champion but they were

fricking good why because they're 26

year old self would be scary but you

know like hey um they know what it is to

be dedicated and work hard and because

again they're Olympians like you said on

a level that a regular person has no

concept of so I think that that is

ultimately the skill it's not the oh man

this person's dangerous because he's got

good judo or good wrestling no this

person is dangerous because he or she

knows how to work their ass off and be

focused on a level that most people

can't comprehend and that's what

produces success in any area of life in

might hit you yeah be brutally honest

with yourself at all times and it stings

sometimes you know I think yeah it's

like the price of of looking inward you

know objectively is that you're not

going to like what you see a lot you

know and because even if you're like oh

man I'm 90% the way I want to be it's

like that if you are going to take that

next step in my opinion it's you're

going to focus on the 10% because it's

like oh man we're doing a lot of things

good yeah who gives a let's talk

about what we need to improve on you

know and that's a little bit less fun

but in the long run I think it's what's

it's what's going to drive you to a

higher level but at the same time I

think it's what makes a lot of people

that are like that a little bit neurotic

and nutty by compared by a normal

standard

right but again you show me someone

that's super well adjusted and I'll show

you someone that's probably not a high

achiever

you talked about moral victory can you

explain how your morality contributes to

the way see the value of winning sure

I think there's absolutely such a thing

as a moral victory and sometimes people

that are trying to manipulate you or

trying to get you to buy something will

tell you differently and you know there

is there it goes in two directions for

instance let's say you're a blue belt

and you compete against you know a real

black goal and there's plenty of people

running around plat belts that are not

particularly at this point but you know

let's say for instance your blue belt

and you compete against a real black

belt the likelihood of you winning is

almost zero however if you go out there

and you try hard and you do your best

and again whether you come off the mat

you know a winner which would be very

fortunate and unlikely but or you come

off the mat you know on the other side

of things if you went after and you

tried and you you know let's say you had

some nerves but you kept that in check

and you fought hard you didn't let it

get the better of you you know that

would be in my opinion a moral victory

and and there would be nothing wrong for

recognizing it as such now that's not

the same thing as an actual physical

victory but there's nothing wrong with

saying let's say you're your opponent's

260 pounds in your 120 and you tie they

get the decision hey you know I remember

that happened to me at the quarter-final

or not the quarter-final rather in the

the third round at the absolute in the

Worlds in 2008 you know and you know

it's against a heavyweight or a super

heavyweight and in the 0-0 and I wasn't

happy about losing by any stretch of the

imagination but looking back I'm like

okay well you know generally speaking if

you end up level considering that I have

all of the resources you don't that is

you know you definitely performed a

little bit better than I did now at the

end of the day you know wins and losses

do matter and you do want to try to make

sure that I'm not shooting for the moral

victory I'm shooting for the actual

victory but every now and then it's very

very important to keep in mind that you

know am I just actually myself am i

conducting myself in a way that I

respect that hopefully other people of

value or respect and also a way that I

believe is going to produce actual

victory and actual positive results in

the long run as well I think it's

important to recognize that because

sometimes you'll see people get very

frustrated let's say for instance if I

box against people that are much more

experienced than me all the time

you know I'm not going to win anyone

that tells you differently has either

not training with people that are very

good or B they have no idea what they're

talking about

but what I can say is hey did I do a

little bit better today and better

doesn't mean that I land more punches in

this it was I more under control was I

more able to kind of keep my keep my

focus and execute what I wanted to

execute and if the answer to that is yes

you know I'm moving in the right

direction so as far as I'm concerned

there's all sorts of different types of

moral victory but it would be the the

same thing as you know let's say for

instance you know fade or slaps your

mother you got to hit him

you have to he's going to kick the

out of you almost certainly but you have

to hit him it would not be it would be a

technical like well I didn't get hurt so

that's a win if you ran away but that

would be the opposite of the moral

victory in that case trying your best

and losing would still be I would say

the honorable thing to do so what you're

saying is sometimes you have to pay the

price for a moral victory

absolutely but the reality is is that

martial arts doesn't just teach us about

how to beat someone up or technique or

this is that that's really not the core

of the martial arts the core of the

martial arts is heart discipline

dedication focus and if you have those

things they'll always be people better

than you and they'll always be people

lesser than you but that's not the only

metric by which you can judge

performance or judge a person

yeah I competed in boxing judo wrestling

jujitsu MMA have a man yeah now what do

you think is the best martial art for

defending yourself against an untrained

opponent there's so many different

factors but let's say for instance okay

most fights I've seen are one-on-one

they're the fights we hear about are an

ass-whipping like seven people versus

three people and okay the more variables

you add in it gets very very difficult I

would say them the best fighting style

is using your brain and because less

voiding the fight well avoid in the

fight but I would say intelligent it's

just like investment now I know nothing

about investing which is why I don't do

it but um let's say for instance a

Warren Buffett is looking at a stock

page now I'm speculating because I don't

know mr. Buffett but I do have someone

who's standing up how the world works so

I would say that some days he looks at

the page it says haha there's a good

investment here and other days

regardless of his skill and investment

he says there's nothing to do the best

thing to do is wait but your Warren

Buffett tell me the best investment yeah

the best investment today still sucks

and I'm not going to make it talked to

me next week and I'll see what's out

there

so till next week we're gonna make we're

gonna we're going to invest some money

maybe and then you know he'll make the

read as he sees it so let's say for

instance you walk into a room and your

goal was to kill everyone in the room

and you are armed but you walk in to

50/50 Jitsu and you're going to shoot

everybody but for whatever reason it's

bringing your gun to the gym Tuesday and

everyone else is sitting there polishing

their fully-loaded again rounding the

chamber safety off weapons hot all that

good stuff and you see everyone else is

armed what do you do wait until

Wednesday

you come back in on Wednesday when no

one's armed then you shoot us all to be

great so it's that would being the best

tactical shooter you could be as ninja

as you want we're going to kill you

there's too many of us in too few of you

could you make it out of there and like

some sort of Boondock Saints awesome

luckiness and managed to get everybody

sure I wouldn't bet on it

that's like Floyd Mayweather against

three people I would bet on him sometime

seriously anybody that tells you

differently is never fought an untrained

person because regular people can't

fight for [ __ ] and the other thing is

they get scared so the one Floyd

literally cripples the first guy with a

right hand the other two guys unless

they're really seriously probably go

oh and then hesitate but they may not

but let's so if you think about it

though four people five people three

people ah let's say it's one on one but

Floyd's minding his own business against

snuck over the shoulder as he's sitting

there ordering a drink at a bar all

these different things factored in so I

would say that when it comes to the

physical expression of how to best

defend yourself the most important thing

is situational awareness and

understanding what's going on around you

because you could be the world's

greatest ninja warrior and still run

yourself into a lose-lose situation

could I knock out Floyd Mayweather yeah

sure I could if you let me hit him but I

don't think I would come within spitting

distance of him if he didn't want me to

I would have no teeth before I even

tried but if he sits there and lets you

hit him he is a man he is mortal so

basically under the right circumstances

anyone can win onto the wrong

circumstances anyone can lose so I would

say that understanding that is two step

one to being able to be an effective

effective strategist who can read a

situation you say should I fight this

one out should I not should I get out of

here should I fight for four seconds and

run for it and you know when you take

into account the physical expression of

everything and you want to let's say for

instance you know the most important

things for defending yourself in real

life I would say probably wrestling or

jiu-jitsu probably jiu-jitsu really in

my opinion but that's just perfect for

fighting I would say other things if I

want to be able to beat up more than one

person I know it implies that I can

wrestle or I can because regular people

can't wrestle for [ __ ] you will you

could pick him up and slam your head on

the ground or something horrible you

know but boxing would be nice as well

but let's say you're a great boxer and

someone tackles you could a regular

person tackle a good boxer yes alright

could a regular person sucker a good

jujitsu guide for sure but it also

really comes down to the to the mental

and everything like that but basically

if I had to pick one art and the

everyone knows nothing I would picture

issue but in my opinion jiu-jitsu at a

high level involves wrestling so just

like you said grappling is this bigger

thing that involves wrestling's you know

everything no doubt that's like you take

an Olympic level wrestler and you let

him lean on top of your inside control

it's not pleasant

in class you you emphasize that we're

working with basic laws of physics so I

just read Einstein's biography he was

obsessed with finding a single theory

that would unify all the fundamental

forces of nature Wow

do you think there exists the unified

theory of grappling we can boil

everything down to just a few principles

I will

well first off if Einstein wasn't able

to come up with a unified theory I would

sincerely question my ability to go that

way but do I believe that something like

that could potentially exist absolutely

and I think that even if it doesn't a

belief in the possibility of it and the

search for it would would leave you

better off than where you started

whereas if I was a no no that's [ __ ]

that would never and then I don't look

even if I'm right because I didn't look

there's certain things I won't learn so

I think you know a lot of times just

let's say alchemy the idea that you're

going to turn lead to gold all right

let's a little bit nuts but who knows

maybe that's that kind of nutty and you

know on highly unlikely is highly

unlikely probability of success pursuit

yielded scientific progress elsewhere in

the search for that even though again

someone would look back and say oh

that's stupid who would blush and who

would do that

well if you spent years trying to figure

it out I guarantee you're going to learn

some other things as well so I think

that there at the very least the

principle based approach to grappling is

incredibly important with your process

like for learning new details and

understanding the principles behind the

techniques I certainly don't believe

that I have like a singular or perfect

approach by any stretch of the

imagination but you know I guess what I

try to do is block out extraneous

nonsense like for instance both Pete a

lot of people want to talk about 55

details and reasons for something that's

going on and the reality is is that

you're clouding your thought process for

instance there was a recent not to get

too political but there was a recent

issue where I remember an inmate was

executed and they used a new drug and it

was painful and oh my god and he died it

was horrible again that's when whether

you believe that capital punishment is

valid or not I think there's plenty of

arguments against it in fact I think

most of the arguments that make sense

are against it but pain has fuck-all to

do with it you know I don't care that's

like done hey Lex I'm going to kill you

but don't worry it's not going to hurt

man I mean don't know okay then yeah

it's like that doesn't make it okay it's

like believe me I'm for free I'm is

again if you're gonna kill me I prefer

that you don't burn me at the stake but

if someone was like don't worry it's not

even a sting I'm still going to try to

fight you to the death I'm absolutely

not allowing this to happen if someone

wants to say okay hold on let's get

let's cut the [ __ ] like feely

feelings out of here and say look forget

the pain does this person deserve it

uh-uh

let's let's step that back again is

there a potential for human error is

there potential for someone having this

guy behind bars for political gain like

okay these are the real reasons that you

say hey no way on the death penalty it

has nothing to do with does it hurt or

which drug is it are blah-dee-blah

or is it inhumane it's like none of that

if hey we're focusing on the wrong thing

so it reminds me of jiu-jitsu in the

same sense and again we're fighting were

generally speaking in my opinion debates

that happen in the public you know arena

they always focus on the wrong dang

thing and always focus on the wrong

aspect again there's 25 good reasons or

bad reasons to do almost anything but

generally speaking people will focus on

the hundred other ones that are

extraneous and [ __ ] so what I want I

guess what I would say is it reminds me

of jiu-jitsu is striking like man Floyd

likes to hold his hand this way or that

way or the other way and and this guy

likes to jab like this and it's really

important this person says you land with

this knuckle in that person says you

land with net knuckle and in jiu-jitsu

it's very very important that you grip

three inches up on the lapel and two to

the right

but Roger Gracie doesn't like this but

cabrini says it like this clearly they

all work under the right circumstances

and don't work on to the wrong ones and

it has nothing to do or very little to

do with these other things like hey does

it matter again I'm going to kill you

would you prefer for it to be painless

or horrific ly painful okay if it's

already a foregone conclusion death

alright yeah now we'll start to talk

about the the extreme like whether or

not it's going to sting but until we get

to that point

hey let's focus on the do we is it even

right or do I have the ability or the

capacity to do this justifiably okay so

that's where you come down to the

principles in my opinion say all right

yeah it doesn't matter which knuckle you

land with yeah I'm sure it does but it's

a hell of a lot less important than 25

other little things that make all of the

difference and in my experience a lot of

coaches and a lot of people particularly

guys that are trying to [ __ ] you

will focus on 45 little details and oh

it's there's 15 details to this ten

okay that's true but what are the two

most important ones because hey don't

get me wrong I'm not saying that these

details don't matter but just like

anything else in life there's a

hierarchy because would you say that

would you say that happiness and

self-actualization is a valuable thing

in life yes I would as well and you know

what we have the luxury of saying that

type of thing because we're sitting at

fifty fifty Jitsu in Falls Church

Virginia and there's no one trying to

kill us rape us and we are also I know

where food is tonight yeah if you were

to walk down if you were to talk to

someone like 2,000 years ago I'll be

like how are you feeling they would

stare a chili what are you [ __ ]

[ __ ] I'm starving yeah I'm hungry

that's my issue

are you are you satisfied in your life

it's like I'll be satisfied when you get

out of my way so I can find some food so

that's even the deeper question is are

you eating something tonight right and

so it's Maslow's hierarchy of needs when

we take care of the base needs first

then we start to work our way up toward

self-actualization and this and that and

but until you got your food water

shelter don't tell me about where you're

placing your grip it's like you're all

leaning out and you're you know your

posture is poor and you're out of

balance and you want to tell me your

grip that's like I'm starving to death

the you know the barbarians are at the

gates but I'm sitting here giving you a

philosophy lesson it's like this is

[ __ ] it doesn't make any sense

build a better wall a sharper knife and

get some food and then we'll cover all

the other stuff so I think that in my

when it comes to how I approach martial

arts in terms of learning as well as

teaching I really try to boil it down to

what I feel to be the most important

component parts and then if I one day

reached the level of where these tiny

details matter that's fantastic because

again the difference between you know

the ability to pass that try to pass

successfully against a cabrini or how

file Mendez and against a regular

run-of-the-mill black belt or you know

does come down to little details but

it's also presupposing that you're in

proper position that even allow these

details become relevant and I think that

a lot of times we put the cart before

the horse and that's not that's

problematic there's still in your

opinion undiscovered position

submissions of techniques and you just I

would say that there have to be there

absolutely are um you know I think that

what we see is jiu-jitsu now don't get

me wrong the core never changes because

physics doesn't change physics is the

same thing that's why I get a kick out

of

love like self-defense arguments it's

like [ __ ] it has another no physical

difference in self-defense beyond the

fact yes you can I guess mean though

I've never seen anything like that in

real life that you're crossing a pretty

serious psychological line if you're

putting your knuckles to you know your

thumb to knuckles deep into somebody's I

forget the fact that did you know that

we're legal more all other things like

that it's like I've never done that

before I wouldn't do it lightly there

probably be some hesitation there what

is so anyway what makes it different is

the the psychological component and all

these other things going on but

physically there's no difference again

people like Aldridge it's is really

different in MMA no it's not not in my

opinion not in my experience it is

absolutely not what are you talking

about physics are different inside of a

cage than they are on a mat and they are

out in a field

it's exactly the same now if I try to

sport grapple you under a non sport

grappling rule set then I may run myself

into trouble but that had nothing to do

with jiu-jitsu jiu-jitsu is physics is

proper expression of physics the same

way boxing is and the same way of

wrestling is the same way all these

things are so I would say that as long

as something adheres to the principles

that that allow something to be

effective and you know and fundamentally

sound that you can do almost anything

and I think that people will continue

particularly in the ghee it's going to

get nuts you know just the level of the

amount of things that you can get away

with and do and different grips that you

can make but I'd say what we're looking

at right now is going to look only

somewhat like what jujitsu is going to

look like in 30 years the same way the

jujitsu we see now is so much

fundamentally better honestly and and

more evolved and adaptive than it was

twenty years ago and people will swear

up and down like all back in the day

it's back in the day people did not

fight very well even twenty years ago

the level of people understanding how to

deal with jujitsu was very reduced so

you could get away with all sorts of

pretty questionable stuff like sitting

in front of someone in close guard and

have them not completely kick the [ __ ]

out of you but um yeah I think you know

with particularly the advent of Baron

bull the 50/50 position all these

different things which have always

existed there's they've always existed

and one that's I always hesitate to say

invent I don't like the word invent like

that certain people use a lot

I'd say Discoverer because you could

show me I can come up with something

let's say Lex you know you've got a

really good straight foot lock I was

watching a train last night

and I could be doing that in a way that

no one ever taught me that doesn't mean

I invented it because you've been doing

it forever but basically it's let's say

no one showed me the details you were

using and I managed to stumble across

them I didn't invent those details eyes

go yeah I discovered them that was neat

but again none of us have invented a

dang thing people have had two arms and

two legs for certainly as long as I can

remember and probably longer than that

and today here yeah that's that's the

word so in the history books

you

summary:

Ryan Hall, a notable martial artist and UFC fighter, delves into the intricacies of competition, dedication, and the essence of martial arts in this insightful interview. Here are the key points from the discussion:

1. **Value of Competition in Martial Arts**:
    
    - Hall recognizes competition as a means to understand the real purpose of martial arts: self-defense and real-life applicability. He emphasizes that the worth of a competition medal pales in comparison to the ability to defend oneself in practical situations.
2. **Competition as a Catalyst for Improvement**:
    
    - He argues that defined goals and formidable opposition in competitive environments compel individuals to improve. This principle, he notes, is evident in the evolution of various fields, including sports and the military, over time.
3. **Approach to Competition**:
    
    - Hall criticizes a lackadaisical approach to competition, labeling it cowardly. He stresses the importance of proper preparation, focus, and dedication, regardless of the outcome, and condemns using lack of preparation as an excuse for failure.
4. **Integrity and Team Standards**:
    
    - He maintains strict standards for his team, insisting on proper training and preparation for competitions. Hall values the process and integrity over results, advocating for honorable and respectable conduct in and out of competition.
5. **Honesty in Competition**:
    
    - According to Hall, competitions are the most honest platform where everyone brings their best due to the focused preparation for the specific moment, making it the toughest and most revealing environment.
6. **The Pain of Failure and Moral Victory**:
    
    - He openly discusses the pain associated with trying hard and failing in competitions, highlighting the importance of full commitment and dealing with the results, whether positive or negative. Hall believes in the concept of moral victory, where the effort and character shown in competition are as important as the actual outcome.
7. **Respect for Dedication in Any Field**:
    
    - Hall expresses immense respect for individuals who show long-term dedication and sacrifice in any field, emphasizing the importance of discipline and focus, which are applicable and beneficial across various aspects of life.
8. **Unified Theory of Grappling**:
    
    - While skeptical about formulating a unified theory of grappling akin to Einstein's pursuit of a unified theory of physics, Hall believes in the value of searching for fundamental principles that govern effective grappling techniques.
9. **Principle-based Approach to Learning**:
    
    - He advocates for focusing on the most important components in martial arts, cautioning against getting lost in minor details. Hall emphasizes the importance of understanding the fundamental principles and building upon them.
10. **Evolution and Discovery in Martial Arts**:
    
    - Acknowledging the constant evolution of martial arts, Hall anticipates future discoveries and innovations, especially in disciplines like jiu-jitsu. He prefers the term "discovery" over "invention," recognizing that martial arts techniques are often rediscoveries of pre-existing principles given the unchanging nature of human physiology.

The interview with Ryan Hall offers a profound look into the mindset of a dedicated martial artist, highlighting the importance of commitment, integrity, and continuous learning in the pursuit of mastery in martial arts and beyond.

----------

-----
--01--

-----
Date: 2014.05.08
Link: [Ido Portal: Movement](https://www.youtube.com/watch?v=o8nZaDw_mOs)
Transcription:

Ido Portal has spent the past few decades honing a physical credo and method that's now practiced by thousands of people all over the world - from office workers, to former CrossFitters, to NBA players, to the ever-controversial UFC titan Conor McGregor. Known as The Ido Portal Method, or simply "movement," his approach purports to take the "most potent" parts from a range of physical disciplines by shedding the dogmas that often accompany them. As he puts it: "I want the contents, not the container."
Intro

we choose to go to the moon and district eight and do the other things not

because they are easy but because they are hard

[Music]

in any particular sport with well-defined rules mastery is achieved

through specialization taking a few skills and perfecting them so naturally

most experts and teachers of movement are specialists of skillsets like

gymnastics hand balancing Olympic lifting capoeira Jitsu wrestling judo

and other martial arts so it's rare to come across a generalist someone who

takes a holistic approach to movement my guest today is IDO portal he is a guru

of movement a teacher with a large and quickly growing following as he says

movement is big bigger than any specific discipline were all human first mover

second and only then specialists I actually just finished reading a

Journey to becoming a generalist

biography of Albert Einstein by Walter Isaacson and the two of you have

something in common I desire to arrive at a unified theory in his case there's

a unified theory of physics you know like forces of nature in your case it's a unified theory of movement can you

tell me the story of your journey to becoming a generalist first I'd just say

that I'm no guru you know it's something that people use but I have a really hard

time with the word master or guru and I didn't arrive with the gospel truth I'm

not sitting on any mountains and I'm I'm on my way so people who join me as students are

basically following you know in the same journey maybe in certain circumstances

I'm a bit further ahead or sometimes I'm a bit behind and but it's definitely

walk along and not walk behind kind of thing yeah my journey I started as a

mover first and then I became a specialist and then I went back to

movement basically so yeah it all started in a young age and some some

Chinese martial arts and developed into some physical sports and

games in school primary school and high school and then I met capoeira and I I

was completely amazed basically by this dis art form and then pursued that for a

good 15 years in the middle somewhere military service and other physical a

physical pursuit and throughout changing and developing and moving between

disciplines and exploring just kind of got the same realization again and again

that there is some thread through all these disciplines something that is

attracting me back and basically it was movement I realized and the next thing

was okay I want to learn movement I want to get better as a mover in a general

way I seeked out a movement teachers and went around the world and looked around

and read a lot of books and there were some people who mentioned the word movement and I went to them and you know

heard myself as a student and they kept on teaching me disciplines another

isolated approach another speciality and I was very disappointed so eventually I

decided okay I'm going to become that person I'm going to become the movement teacher and the next realization after

years and years of trying was it's impossible and with that I stayed

basically because I realized if that's impossible it's a good goal to have in life something that will keep on moving

myself and my students and anybody involved forward that's where I'm at

right now I'm in teaching and learning moving around and trying to try to gain

some more knowledge about this impossible task yes you're still yourself or forever a student I would

prefer I prefer to be a student any day of the week than a teacher and but being

a teacher is part of human culture we are all teachers we we teach all the

time but whether you want it or not somebody asks you for direction in to it you become the teacher you have a

child he looks at you you're a teacher practicing teaching and practicing to

the student the discipleship both of them are extremely important for your development as a movie what is the price

of specialization what do we lose when we specialize we lose humanity first and foremost as humans we we evolved to

become humans as generalist we are the most generalist of all animals we are

able to imitate the ape and imitate the tiger and and we can hold our breath

underwater and we can do everything just a tiny bit we can't really run very fast

we can't really fight very well we can't you know climb as good as other animals

but we can do the most complex and generalize tasks out of all animals and

no animal even come close so speciality the price is humanity the price is your happiness the price is

your fulfillment as a human being it's deep beats it's philosophical but that's

that's we are do you think there is some beauty and fulfillment and some value

Specialization

and specialization in becoming the best at a very specific movement at a particular sport giving your body to you

know dedicating it to that sport that's an interesting thing because as a human race we benefited tremendously from the

work of specialists but those specialists suffered that's a very very

important points like without specialists we would never be here we would never be skyping right now on

these computers and and you know wearing these t-shirts and all kinds of stuff

but those specialists those human beings suffered the result of their highly

specialized nature and we become more and more specialized the reason move

towards being generalist again in the last few years maybe the last decade

there is a bit more talk about that but definitely we are also still

pursuing highly specialized fields if I make a joke in my workshop and I say

nowadays you go to it if you break your you know you break your hand you go to a hand specialist orthopedic surgeon in

five or ten years you'll go to a left hand specialist and the most evident problem is also our

leaders who are experts but now they're required to be generalist leaders of a

lot of stuff and their shitty leaders we keep on having the same problems again

and again because they are ex specialists whether it's a lawyer or a

military person or an economist these are not specialities that allow you the

full grasp of running a country yeah I think I think you put it beautifully that in my specialization might lead to

innovation but you lose the humanity mmm what do you find is the most

underdeveloped range of motion in athletes like what movement is most

restricted in pi level athletes in your experience is there any one that stands

out shoulders any particular other joints well it's it all depends on their

speciality of course in habits the shoulder the glenohumeral joint is the

most hyper mobile joint in the body even when it's restricted it still offers tremendous range of motion compared to

other areas but when it's restricted even if it's just a little bit it can

cause huge problems because we are dependent on that range of motion and

mobility around the glenohumeral joint and the simple reason is because with

the hands humans manipulate that's what we're meant to do with our hands and we

need that complexity around the scapula it's been a few years since I've said it

first that the scapula craves complexity but this complexity around the scapula

and range of motion is so important across the board I've read of your concept of isolate in

are graded and improvised okay describe the role of improvisation in movement

Improvisation

any profession and speciality should arrive at improvisation in the top and

tier the top level and whether you play the violin or you know you box you're

going to reach improvisation improvisation above all is the human condition it's the human the human

ability the highest form of living is improvisation you improvise basically life is improvisation you're born you

die and in between you improvise a shitload of improvisation movement is no

different the thing is people started to isolate concept and some people went the

next level and integrated them they present themselves as improvisation but

actually they're cheating people it's just a bit more integration yet it's

just another integration improvisation open improvisation real improvisation as

we call it that's very rare and that's the most enjoyable state it's also

called the zone it's also called the tunnel you just experience this

beautiful thing to be empty just to let things happen through you as Bruce Lee said I don't hit it hit it just happens

you know and that is improv that's what you need to do with movement if you

aspire for the highest things I like how a guy on reddit described you as IDO

portal may not be the nicest guy in the world but he's a great coach so that nicest

guy part I come from the wrestling world where the goal of a good coach and a

good program is to basically make you quit to break you there's zero patience for people who don't want to put in the

work to work hard do you find that tough love is the best approach to coaching

Tough Love

people whatever their level of ability no not necessarily I don't like the term

tough love because it kind of assumes the it's importance is itself it's not

enough for me it's like that's how that's the best way why that's the best way but on the other hand I don't think

people are made of sugar and I really believe that we've lost a bit side of

you know how resilient we are and another is people don't like the truth

you know it's dishonesty is above all so when people describe me is tough love

it's not because I believe in Tuffle 100% of the people who has been who have

had issues with me on a personal level or through coaching are people who

couldn't accept criticism what I offered you know took it personally weren't able

to deal with it etc I can't even you know in my head find one example of a person I've been

working with who received the criticism worked with it and still complained but

it's always these complainers and who cares complain first and do nothing yes yeah yeah you know it's like

when you go mainstream as we've went to a certain level you have to deal with it

because I'm not operating my elite unit my Special Op unit anymore now it's an

army and I need to accept the fact that I'm gonna meet a lot of slackers a lot of Poindexter's and all kinds of

you know they don't want to work they want to talk about it they want to do this they want to do that they don't

want to hear the truth they don't want to accept criticism or hear how much they suck and I just don't do that so

you know I'll have to accept the fact that from now from now and again you know what I'll have this issue and I'm

sure it will continue you've travelled all over the world you think there's a difference in this aspect in attitudes

in the United States and Israel and other countries big time horse big time

yeah there are many countries where I don't have this issue or very rarely we

have a word in in Hebrew actually it comes from Jay and I think or Yiddish it says touchless

it's like down to it you know the heart of it tough less people are people who

are like no you know directly tell me as it is and this stuff less it

it exists in certain cultures in other cultures it's a lot of chitchat and walk

around and you know I didn't know how to chitchat a few days ago one of my

students told me you know I can't cheat you it's exactly how I felt you know when I first came out of Israel started

to teach around it was Russia thank God and that was so similar to him where I

come from in many ways so no problem but then when I went to the US or Canada I

had a lot of issues with the chitchat with politically correct and walking

around the bush and don't give it to me too harshly you know cover it with a lot of sponges around it soft in the heat

and yeah it's definitely different between various countries and I need

nowadays I need filters which are my my stood my top students were helping me

teach and some of them are great filters in and in certain countries they'll do

much better than me what is perfect practice look like for you so do you believe in the value maybe this applies

more to specialized sports but like I I come from Russia actually and from the

wrestling world where repetition you know putting in ten fifty thousand hundred thousand repetitions on a

specific movement is is how you achieve success do you believe in the value of

that repetition even for generalist framework the repetition is the mother of skill yes there have been those that

corrected and said perfect repetition is the mother of skill well those who

usually say it are those who don't achieve Heights usually usually so I'll

be very frank again I'll be very extremely honest a lot of people talk about perfect perfect perfect but but

life is not perfect itself our surroundings are not perfect and when I

practice and when I move it's never on the perfect conditions it's never with the right optimal blood

sugar level and under the specific you know height of I don't know what and and

riding the wave of super compensation in the perfect way and usually when people try to adhere to that concept in a

perfect way they end up falling off the wagon on the other side don't be don't

be stupid don't just drill yourself into the wall and lose sight of everything it's not black and white the truth is

somewhere in between and it varies between people for me after seventeen

years teaching 18 years now teaching moving seeing people the hardest workers

are usually the elite performers of course some of them are carrying a

certain talent or this or that but it's always with very dedicated practice they

have built up that work capacity through that dedicated practice and they can then move that ability to other

disciplines true in a grappling world I'm not sure how much you're aware of it but Marcel Garcia is one of the Great's

and he believes boldly against the status quo I think that you should only

train jiu-jitsu his sport jiu-jitsu and not do anything else so to achieve

success trained only that but the majority of other athletes in the sport believe that you should do strength and

conditioning programs and all around that so they at least move slightly towards the more generalist framework

what do you think do you think that's value for the generalist mindset for like an elite athlete or should they

Generalist Mindset

just focus on their sport to some level to some level speciality can can reach a

plateau because of lack of general base of the pyramid in some cases but it's

not a very high level of you know Janet generalism nowadays you're you're

practicing against specialists and they devote more and more time to this specialty

when you're doing other stuff so it's a complex rhythm you know and and to each case his own but I'll tell you something

else when you reach the top of your field like marcelo garcia did in BJJ you

stopped being inspired by your own scene you can't gain inspiration knowledge and

motivation from your own scene because you are the leader you're on top of the mountain you have nowhere else to climb

so what you do you look to other scenes and that's where it's really really

valuable to become a bit more generalized yes you mentioned an

interview related to that a very interesting point the many people in the US in particular focus on learning more

than doing so focus too much on acquiring knowledge versus using that knowledge do you struggle this yourself

like how do you approach learning new things versus putting more time into old

things that you've already met Sinead it's not only a u.s. thing or North

American thing it's generally all across the globe all those are more practical people and less practical people you

know in each country has its own orientation habits you know characteristics but it's a good question

you need to be it's kind of being super intelligent and oriented towards the

information but then have this dumbed down practical mind it's like okay now I need

to work you know and having a balance across that and that's probably it that

probably means that you know a certain IQ for example will start to work

against you in certain fields and vice versa so you when you become too much

you know as the Chinese say the man who lives inside his head you start to have

this issue you know you you have a thirst for information great thirst but

information is toxic it's exactly like water water is toxic as well almost all compounds

toxic and then we drink we drink with it we kill ourselves we kill the process

and the knowledge it turns against us and that's a serious problem and that's

the problem of the age of misinformation that we live in it's not only that the

knowledge is toxic even when it's good knowledge now we also have bad knowledge

mostly bad knowledge mostly shitty advice the combination is listen just

people become paralyzed or just you know move from link to link to link with a

you know glazy eyes and just never actually do anything yes I know you

advocate building a huge word capacity so how many hours a day do you think this is also a debate for specialists

how many hours a day do you think is the most a person can train movement

intelligently before becomes not sustainable before their mind becomes

uninspired maybe as you said 24 hours 24 hours 24 hours a day there is a

choreographer in Israel very known choreographer called ohad Naveen he says when you wake up in the morning in bed

between the sheets you can you can practice movement and and is not there

talking about with your partner yeah so even there you can practice you know breathing moving it's it's all the time

around you but serious practice you know practice oriented that you know

repetition and success and building skill and moving from isolation to integration to improvisation in most

disciplines it's around six to eight hours a day some people go more and reach even the

ten hour mark and I've done that for periods of time in the military you go

even further than that other disciplines require less and it's also a highly

individual thing so let's say even within the sports of gymnastics you have a woman like in Nast who trains eight

hours a day and neck to her the same team also winning gold medals at the same level more or less

you have Shawn Johnson training three hours a day and she reached the top of

her field or liquid gold medal in the Olympics so how highly individual this

is very rare to see this three hour gold medal thing but definitely it

exists the difference there might be mental so the question I have is out of

the various elements like mind breathing developing muscular strength or joints

which is the biggest challenge to master as a student a movement highly

individual it depends on the person depends on is orientation some people

never require any form of mental training for example or psychological training especially in fields and like

sports and then team sports yeah so so that aspect is covered they're winners

they're oriented they're focused you know and then other people require help

in that regard some people have great difficulty developing mobility and just the nervous

system is panicked it holds on it protects them too much other people are hyper mobile and have a

difficulty creating tonus and in strength and that is a great challenge

for them and other people are you know great complex learners they can name

they can coordinate complex actions and learn movement very quickly while others are highly limited so it's very

individual for that process of learning that journey is individual to everyone

Listen to your body

so how does one take that journey just listen to your own body now now you can

listen to your body until tomorrow Hypatia you're not hearing anything you

know you're not hearing anything you need to learn you need to create a relationship with your body and you need

the help of of teachers right that's there is only you know a lot of

people say no I'll do it myself then you deny collective knowledge the most

powerful knowledge that mankind holds you know because we're the only animal that have collective knowledge we've

been able to move knowledge across generations and that's how we have reached space build the Internet

you know do all these crazy surgeries and and you know solve you know genetic

issues and etc it's you're not gonna do it by yourself you're just one small

person and we have collected knowledge generations upon generations so listen

to your body that's nice to say most people don't hear it's completely

silent and then you need to start to decipher the signals that the body gives

you and that goes through a practice and learning discipleship and exploring a

lot of different different stuff and it's a highly individual thing nowadays we don't have so much any anymore this

mentor student or teacher disciple relationship but I I really believe in

that I wouldn't be here without my mentors and my teachers the shoulders of

giants that lifted me up I still believe in it in in a way there is no other way

yeah on that do you think that training and learning movement for the majority

of the time is a fundamentally solitary activity or do we gain from like the

presence of others so when you when you think of movement when you're training is most of your training like the

repetitions done alone or with others both and I've trained years you know

alone and with my students and I spend large periods of time alone just

training alone but I also spend a lot of time being in a community in movement is

the best reason for gathering around in a community and you know people for

example nowadays they go do CrossFit or they do yoga what ever and then they have their yoga

friends and they have the real friends that that's you know your yoga friends can be your real friends because

we've been gathering around movement since the age of time creating communities around movement around

hunting gathering dance around the fire we've been moving together nowadays I can recommend move with your loved ones

moved with the people around you you know you join a BJJ Club it's a community you know you go there you meet

you move around you go to a capoeira Club it's a tribe you go to a CrossFit

gym it's a community you go to yoga it's a community you can move with your

children you can move with your dog in the park and it's important to move

together but it can also be done alone and some things are better done alone and some things are better than together

how do you think movement changes from solo movement you know that that whole

Movement

pattern of movement where you're moving alone versus the pattern of movement where there's two people either working

together against each other so together is like dancing partner dancing and against each other is like wrestling or

jiu-jitsu do you think the principles of movement are different for when it's two

people versus one person this is a whole nother world first they spend more time

moving with others against others in martial art because I spend most of my

life in martial arts and less time exploring stuff alone but definitely

there are some concepts that still exist like the quality of movement how you

organize your body in space not in relation to the partner only but first a

BJJ practitioner or or a stand up fighter he needs to organize his body in

relation to space first and then in relation to the partners well so some of

the concepts exist in both while others are very different and you can train

alone or all your life when somebody else is in the equation it's going to change the game completely a major

reason why we are under the fight laboratory we've departed in reality from a lot of

traditional martial arts and the delusions of training alone and doing forms and and repetitive you know

movements alone and then it's a shitstorm and you can't apply anything

and you you don't have any live practice and now we see that and definitely in

the fight game and the practices that stayed very real stayed very dirty in a

way but very real they are the ones who are providing tools for the chaotic

environment of a fight terms of injury how do you treat recover and work around

Injury

injury hmm injuries are a certainty

they're not a learnt the probability injuries and diseases they are also

required as nothing talib one of my biggest inspirations this day is a great

philosopher and in order to to anti fragile eyes to become anti fragile to

become robust to become more than resilient you must be able to enjoy

volatility you must be able to grow from this stuff and so first I oh I said I

said it before and I'll say it again I injure my students this happens and I

can do anything beneficial without it and basically we all get injured

constantly on a micro level a macro level it's part of our lives of course

we don't want to push into meaningless injury and we want to be able to grow

from it an in and basically develop from it how do you train around it how do you

train around it it's a hard question it involves a lot of stuff first and a big

believer in movement as a therapeutic tool movement itself if it offers you

adaptation and it does it's the way out not lack of movement rest

I don't believe in rest I believe in moving which means when I'm resting

I might help on the short term with certain aspects of the injury but at the

same time I'm creating a new problem because there that the adaptive process is taking me somewhere else

I'm not recovering towards movement I'm recovering towards no movement so we

have a problem here now in some cases you must rest and then deal with the

consequences later but in most cases there is a better approach than just

resting in that that requires a lot more taking responsibility which doctors

don't believe in your ability to take responsibility for yourself to be

intelligent and to know the amounts and the levels and that requires some some

form of a knowledge and experience and most people can't be trusted with it so we offer them you know this advice of

like just rest resting stop yeah but but definitely after years and years of working with

people and and taking them through crazy injuries near my right hand or Delia she

she went through a car accident she lost the kidney she broke her back she went through three knee surgeries which my

sister performed by the way nowadays she can move like few people I know on this

planet and and just the answer was always movement movement going back into

movement beautiful continue move continue to move yeah don't move

stupidly don't don't hurt yourself but that's an obvious no I guess not because when I say these things people write a

comment the but the bad people you know yeah but you're gonna injure yourself yeah genius

haha don't go into the injury and and

again deteriorate the escalate the situation of course not you must move

around it and you must be smart in the way that you allow adaptation to take you out like a wave you need to ride the

wave of adaptation out of the out the problem and that's tricky we know

and that's something that we need to educate people on and we need to believe in people's intelligence and ability to

take this responsibility in China where they still have in some areas and they

used to have bone setters they didn't put you in a cast you broke your hand or they they did it through bone setting

and yes they didn't have x-ray so that's a lot more complex to do and not as

successful as nowadays but having said that they did achieve amazing rates of

recovery because these reps that they use and the process allows some form of

movement and that creates an adaptation now take an arm a healthy arm your right

arm put it in a cast for six months take it down the cast what do you see the

observer the arm is basically moving towards death yes it's gotten good at not moving the arm

is great you have weird hairs growing out of it it stinks it really smells and

looks like death because movement is life no movement death we know that we like to just kill your

arm a tiny bit so that so the bones can reform together and then we'll bring it

back to life that's one approach but in other cases you can maintain the life

and the demands on the tissue safely enough at the same time allow the

recovery to happen yes and diet what are

Diet

some diet principles you follow well diet is very individual thing I've

been personally following a Paleolithic a caveman diet for a long time long

before it was called the Paleo diet and since 1997 or even 96 I've been doing

this for a long time I feel great on it still you know growing older and older

and functioning only better and being able to sustain maintain improve but that is

very very individual there is a lot of exploration to be done there what you can withstand how resilient is your

system yes nowadays there is a new movement towards not improving the fuel

sources not improving the quality and the quantities of the food but actually

making the system more resilient so it's able to basically withstand any almost

any quality and source and that's where we have been lacking and we will be

neglecting this area and that's something that and I believe is the latest innovation although it's still

highly misunderstood and a lot of a lot of folks are abusing this concept and

giving really poor advice just to be different just to say I'm not the paleo

guy you know so that's that's a small addition that will get bigger and bigger

I think so it's almost like how you recommend a movement to go outside of

quote-unquote proper alignment this is kind of the diet version of that is going outside of some kind of proper

framework of diet to some level but that's so that was an obvious thing you

know always right the problem is it's not enough because in fact it's more

what I'm suggesting with movement to go outside of the proper alignment it creates an adaptation but what if

that adaptation cannot happen for example a celiac disease person you'll

expose him to gluten and you'll have terrible consequences now maybe if you

can minimize enough the amounts and the dosages you can actually train him out

of tell yak to some level but death adaptational last very very far down the

road he's going to get some gains and then is going to plateau but what if you could take that celiac disease person

put them in the garage fix his mechanisms change his tires change his

engine you know oil him up everything and then put him back on the track as a

new animal and that that is where you know a lot of stuff is happening

nowadays so the genetic part of it and the gut biome and our digestive tract

that is is so so complex and and we discover that you know we live in

symbiosis with all these microorganisms that just are all over skin inside of us

and we live in combination with them and that's how you see some dudes in Brazil

and in Russia walking around with you know 5% body fat eating one cracker for

breakfast one cracker for dinner and you know training BJJ all day long

high-performance fueling with coca-cola and then at the same time you see people

doing everything almost perfectly and still having poor poor performance and

inability and they gain weight with any you know extra calorie or macronutrient

that they brought in because their system is different and it's not only

about genetics it's more about epigenetics and it's more about what

kind of system besides your own DNA what about these organisms that are supposed

to help us and live in symbiosis with you what kind of a system do you have there and that's just two areas and I

think it's going to get it's going to expand more and more in work and we're going to realize that there is a lot to

learn there do you think technology and science is ultimately a positive force for you know you look at movement as as

an element of our humanity do you think technology is taking humanity away or

it's adding to it I'm not smart enough to answer you that man yeah it's a big

question you know it's a I have no idea it's just it's a huge question and I think we're going to

struggle with that question for many generations to come still it definitely

created a lot of positive stuff but also brought tremendous suffering and

problems and perhaps will be the you know the end of us so technology might

have been you know the most terrible thing that ever happened to us who knows yes that beautiful no how can

people join the IDO portal movement if you have a website a hero portal calm

yeah IDO portal calm in phase B we are

on facebook you can find us on Facebook they do aim it or portal method a little

portal I do P ort al you can join the

movement culture on our website and that will lead to some updates coming up soon

you have a beautiful website by the way amazing thanks very much hey thank you very much I've been a very

fortunate to have a great team around me that helped me with that so I saw that you posted a couple of Tom

Waits songs and I even a Bukowski reference on your Facebook page and I

immediately understood something that I think only another fan or maybe I should

say student of weights and Bukowski can understand you don't shy away from the

strange and the profound wherever you can find it maybe that's how one way to put it is there a Tom Waits song that you find

yourself returning to often in your life mmm so many man so many is just that Tom

way you know I I can barely listen to anything else frankly it's been a real issue and Tom Tom Waits is his

discography it's like it it's a lifetime of discoveries I I it's been

accompanying me for years now it's not only it's not you know it's not something you go through very quickly

and I've spent a lot of time on Alice for example and

yeah just so much stuff so much that's always discover also you know I I find

this this weirdness this is eccentric part of things it's so important it's

the only thing really that can be you in many ways because as a culture

we're so blend we're becoming this one thing you know like you walk around in London it looks exactly like Hong Kong

it looks exactly like Tokyo it looks exactly like you know Sydney and we have

this like huge human thing going on which is great and we communicate very

easily but then we lost a lot on our our own stuff there is only one Tom Waits

you know because of his eccentric part because of his you know this this weird

genius and that's why it's so beautiful to me and then I try not to not to shy

away from my own eccentric side and when I was younger I I definitely hit that part more and you know you know guarded

and try to try to be to fit in but that's definitely an important thing I think so you recommend we cultivate the

weird yeah yeah cultivating the words cultivating yourself because that that's

truly you you know everybody's weird everybody there is no Homer Simpson you

know and everybody has this bar everything he'll everybody has this interesting stuff that's what also

interests me when I teach I want to see that weird you know I want to see that weird in your movement I want to see

that weird in you and that then I really met you but most people they hide it and

they don't a lie and they put this perfect picture but it well I'm not

stupid I know it's not perfect you know so just allow the weirdness to come out I think

it's a great lesson yeah from-from weights and and Bukovsky as well or so if you don't mind I'm gonna torture you

or something I would like to close by reading a Bukowski poem roll the dice

I want to force you to listen to it go ahead if you're going to try go all the way

otherwise don't even start this could mean losing girlfriends wives relative's

jobs maybe your mind it could be not eating for three or four days it could mean freezing on a park

bench it could mean jail it could mean division it could mean mockery isolation

isolation is the gift all the others are a test of your endurance of how much you

really want to do it and you'll do it despite rejection and the worst odds and it will be better than anything else you

can imagine if you're going to try go all the way there's no other feeling

like that you will be alone with the gods and the nights will flame with fire you will ride life straight to perfect

laughter it's the only good fight there is amen thanks al thanks for talking to

Daymond thank you so much man for

Summary:
The interview with Ido Portal, a prominent figure in the movement culture, offers a wealth of insights into his philosophy and approach to physical movement, teaching, and the human experience. Here are the key points from the interview:

1. **Holistic Approach to Movement**:
    
    - Ido Portal emphasizes a holistic, generalist approach to movement, transcending the confines of specialized disciplines like gymnastics, martial arts, or Olympic lifting. He believes movement is fundamental to human experience, preceding specialization.
2. **Journey to Generalism**:
    
    - Portal's journey began with early involvement in martial arts, evolving through various physical disciplines. His realization that movement is the common thread across all these led him to seek a generalist path, ultimately deciding to become a movement teacher himself.
3. **Challenges of Generalism**:
    
    - The quest to become a generalist in movement is an ongoing, perhaps impossible goal, which Portal embraces. It's a journey shared with his students, not a fixed destination or a set of absolute truths.
4. **Humanity and Specialization**:
    
    - Portal warns against the loss of humanity and fulfillment when one over-specializes. While specialization has driven technological and societal advancements, it often comes at the individual's expense, leading to a narrow, less fulfilled human experience.
5. **Movement and Community**:
    
    - Movement is not just a solitary activity but also a communal one. Portal values the communal aspect of movement seen in practices like yoga or martial arts, where people come together, forming communities and bonds.
6. **The Role of Improvisation**:
    
    - Improvisation is seen as the pinnacle of any discipline, including movement. It represents the ability to adapt, create, and live in the moment  an expression of the highest form of human ability.
7. **Approach to Coaching and Criticism**:
    
    - Portal believes in honesty and directness in coaching, distancing himself from the notion of 'tough love'. He emphasizes the importance of criticism and feedback for growth, acknowledging that not everyone is receptive to this approach.
8. **Movement and Adaptation**:
    
    - The human body is adaptable and should be exposed to a variety of movements and conditions. Over-specialization can lead to a plateau in performance. Exposure to a range of movements contributes to a broader base of skills and physical capabilities.
9. **Dealing with Injuries**:
    
    - Portal views injuries as opportunities for growth and adaptation. Rest is not always the best response to injury; instead, intelligent, adaptive movement can lead to better recovery and resilience.
10. **Diet and Individuality**:
    
    - Diet is highly individual. Portal follows a Paleolithic diet but acknowledges the need for resilience and adaptability in dietary habits. The relationship with food should be flexible and responsive to individual needs and environmental conditions.
11. **Technology and Humanity**:
    
    - Portal is cautious about the role of technology in human life, recognizing its immense benefits but also acknowledging the potential for it to lead to disconnect and even existential threats.
12. **Cultivating Individuality and Eccentricity**:
    
    - Embracing one's unique, 'weird' qualities is essential for true self-expression and fulfillment. Portal encourages embracing individual eccentricities, believing that they make us authentically human and contribute to genuine connections and learning.

The interview provides a rich tapestry of ideas on movement, teaching, human nature, and the importance of embracing a broad, adaptable, and deeply human approach to life and learning.

----------
